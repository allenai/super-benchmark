{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e88f876",
   "metadata": {},
   "source": [
    "### Query\n",
    "Apply the decomposed prompt tuning method to fine-tune the t5-small model on our sentence pair classification dataset, available at https://drive.google.com/file/d/1raGLX-2vdX1uizYx2DdqZ_JzAkVeyeDF/view?usp=drive_link. Report the resulting metrics using the `main_ddp.py` script. Report accuracy on the development set as a json structured as follows: {\"accuracy\": 0.0} (replace 0.0 with the actual value).\n",
    "\n",
    "Additional instructions:\n",
    "1. Run for only 1 epoch.\n",
    "2. Use the following hyperparameters: --enc_prompt_tokens 100 -ts 16 -e 1 --bottle_neck 10\n",
    "\n",
    "Git repository: https://github.com/xyaoooo/dpt\n",
    "\n",
    "I have already executed some of the required steps. Now, your goal is to complete the remaining steps and submit the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7b3030",
   "metadata": {},
   "source": [
    "### Prompt\n",
    "<details>\n",
    "[{'role': 'system', 'content': 'Interact in a jupyter notebook to solve the request.\\nOutput a thought and an action.\\nThought can reason about the current situation, and action allows you to interact with the Jupyter notebook. There are three types of actions:\\n(1) execute, which executes a notebook cell with the command under the \"content\" field. This could be a bash command (beginning with !), a python command, or magic command (beginning with %).\\n(2) edit, which allows you to replace existing lines in a file with new lines. To replace lines, you will need to provide the lines as they appear before the replacement, and the lines to replace them with.\\nIn the \"content\" field, format should be ```{filename}\\n<<<BEFORE_EDIT>>>\\nlines before edit\\nlines before edit\\n<<<AFTER_EDIT>>>\\nlines after edit\\nlines after edit```\\n  - Include the entire lines WITH indentation. After editing, make sure indentation is correct.\\n(3) submit, which submits the final answer (or null if there\\'s no answer to submit), formatted as JSON (make sure to use double quotes for keys and values).\\nAlways output exactly one \"thought\" and one \"action\", and encapsulate the content with <content>...</content> tags.\\nI will provide the cell output (observation) for you.\\n\\nTips:\\n- When a github repository is given, you should start by cloning it. You can read the readme file and browse the repo for installation and usage instructions.\\n- Avoid using conda, instead use pip for package management.\\n- Each execution has a 5-minute timeout. After this period, you can decide whether to interrupt the kernel or continue waiting.\\n  You should interrupt if: (1) you are prompted with an interactive message (e.g., \"Do you want to continue? [Y/n]\"). In that case, you can bypass it by executing echo \"y\" | command, or (2) the execution is taking longer than expected because the wrong task is being executed, such as a different model or incorrect number of epochs.\\n  You should continue to wait if the current execution is correct and proceeding as expected.\\n\\nHere are some examples.\\nGit repository: EleutherAI/lm-evaluation-harness\\nQuery: Evaluate EleutherAI/gpt-j-6B on hellaswag. What\\'s the score?\\nThought: First, I need to clone the GitHub repository \\'EleutherAI/lm-evaluation-harness\\'.\\nAction: execute\\n<content>\\n!git clone https://github.com/EleutherAI/lm-evaluation-harness.git\\n</content>\\nObservation: ```\\nCloning into \\'lm-evaluation-harness\\'...\\n```\\nThought: The repository has been cloned. Next, I should check the README file for installation and usage instructions to proceed with evaluating EleutherAI/gpt-j-6B on hellaswag.\\nAction: execute\\n<content>\\n!cat lm-evaluation-harness/README.md\\n</content>\\nObservation: ```# Language Model Evaluation Harness ... truncated ...\\n```\\nThought: The README provides detailed instructions for installing the `lm-eval` package and evaluating models like GPT-J-6B on tasks like hellaswag. I need to install the package and its dependencies first.\\nAction: execute\\n<content>\\n!pip install -e lm-evaluation-harness/.\\n</content>\\nObservation: ```\\nObtaining file:///home/nora/lm-evaluation-harness\\n  Installing build dependencies ... truncated ...\\n```\\nThought: The `lm-eval` package and its dependencies have been installed successfully. Now, I can proceed with evaluating the EleutherAI/gpt-j-6B model on the hellaswag task as per the instructions in the README.\\nAction: execute\\n<content>\\n!lm_eval --model hf --model_args pretrained=EleutherAI/gpt-j-6B --tasks hellaswag --device cuda:0 --batch_size 8\\n</content>\\nObservation: ```\\n{\\n  \"hellaswag\": {\\n    \"acc\": 0.24603658536585363,\\n    \"acc_norm\": 0.34436059706446426,\\n  },  ... truncated ...\\n```\\nThought: The `lm-eval` was executed successfully, providing the metrics I needed. The `acc` metric is 0.2460. The `acc_norm` metric is 0.3444\\nAction: submit\\n<content>\\n{\"acc\": 0.2460, \"acc_norm\": 0.3444}\\n</content>\\n\\nQuery: use pandas to read the file \\'data.csv\\' and display the first 2 rows.\\nThought: First, I need to read the file \\'data.csv\\' using the pandas library.\\nAction: execute\\n<content>\\nimport pandas as pd\\npd.read_csv(\\'data.csv\\').head()\\n</content>\\nObservation: ```\\n    name\\n  0  Alice\\n   1    Bob\\n  2  Charlie\\n```\\nThought: The file \\'data.csv\\' has been read successfully and the first 2 rows have been displayed.\\nAction: submit\\n<content>\\n[\"Alice\", \"Bob\"]\\n</content>\\n\\nQuery: Change the model name in the \\'config.json\\' file from \\'roberta-base\\' to \\'roberta-large\\'.\\nThought: First, I need to read the \\'config.json\\' file to find the location where I can replace the model name.\\nAction: execute\\n<content>\\n!cat config.json\\n</content>\\nObservation: ```\\n{\\n    \"model\": \"roberta-base\",\\n    \"batch_size\": 8,\\n    \"epochs\": 10\\n}\\n```\\nThought: The file has been read successfully. Now, I can replace the model name in the \\'config.json\\' file.\\nAction: edit\\n<content>\\nconfig.json\\n<<<BEFORE_EDIT>>>\\n{\\n    \"model\": \"roberta-base\",\\n    \"batch_size\": 8,\\n<<<AFTER_EDIT>>>\\n{\\n    \"model\": \"roberta-large\",\\n    \"batch_size\": 8,\\n</content>\\nObservation: \"editing file config.json\"\\nThought: The model name has been replaced in the \\'config.json\\' file.\\nAction: submit\\n<content>\\nnull\\n</content>\\n'}, {'role': 'user', 'content': 'Query: Apply the decomposed prompt tuning method to fine-tune the t5-small model on our sentence pair classification dataset, available at https://drive.google.com/file/d/1raGLX-2vdX1uizYx2DdqZ_JzAkVeyeDF/view?usp=drive_link. Report the resulting metrics using the `main_ddp.py` script. Report accuracy on the development set as a json structured as follows: {\"accuracy\": 0.0} (replace 0.0 with the actual value).\\n\\nAdditional instructions:\\n1. Run for only 1 epoch.\\n2. Use the following hyperparameters: --enc_prompt_tokens 100 -ts 16 -e 1 --bottle_neck 10\\n\\nGit repository: https://github.com/xyaoooo/dpt\\n\\nI have already executed some of the required steps. Now, your goal is to complete the remaining steps and submit the answer.'}]\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7f8848",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f4ffe3",
   "metadata": {},
   "source": [
    "Execution Start time: 00:46:45, End time: 00:46:47 (duration: 2 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b42ece",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f036d6fb",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2639e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cloning into 'dpt'...\nremote: Enumerating objects: 45, done.\u001b[K\nremote: Counting objects: 100% (45/45)\u001b[K\nremote: Counting objects: 100% (45/45), done.\u001b[K\nremote: Compressing objects: 100% (41/41)\u001b[K\nremote: Compressing objects: 100% (41/41), done.\u001b[K\nremote: Total 45 (delta 18), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (45/45)\nReceiving objects: 100% (45/45), 39.40 KiB | 9.85 MiB/s, done.\nResolving deltas: 100% (18/18)\nResolving deltas: 100% (18/18), done.\n/content/dpt\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!git clone https://github.com/xyaoooo/dpt\n",
    "%cd dpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1620261",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cbb40f",
   "metadata": {},
   "source": [
    "Execution Start time: 00:46:47, End time: 00:46:48 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d4fd2b",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24a8132",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9593e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n    name = \"mrpc\"\n    labels_list = [\"0\", \"1\"]\n    metric = [metrics.accuracy, metrics.f1_score_with_invalid]\n    metric_names = [\"accuracy\", \"f1\"]\n    split_to_data_split = {\"train\": \"train\",\n                           \"validation\": \"validation\",\n                           \"test\": \"validation\"}\n\n    def load_dataset(self, split):\n        # return datasets.load_dataset('glue', 'mrpc', split=split, script_version=\"master\")\n        #return datasets.load_dataset('glue', 'mrpc', split=split)\n        files = {\n        'train': 'mrpc_csv/train.csv',\n        'validation': 'mrpc_csv/val.csv',\n        'test': 'mrpc_csv/test.csv'\n        }\n\n        # Load the dataset\n        return datasets.load_dataset('csv', data_files=files, split=split)\n\n    def preprocessor(self, example, add_prefix=True, add_vb=False):\n        src_texts = [\"sentence1:\", example['sentence1'],\n                     \"sentence2:\", example[\"sentence2\"]]\n        tgt_texts = [str(example['label'])]\n        if add_vb:\n            verbalizer = \"{ 0 : not equivalent, 1 : equivalent }\"\n        else:"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/dpt/data_utils.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "\n",
    "    def load_dataset(self, split):\n",
    "        # return datasets.load_dataset('glue', 'mrpc', split=split, script_version=\"master\")\n",
    "        return datasets.load_dataset('glue', 'mrpc', split=split)\n",
    "\n",
    "    def preprocessor(self, example, add_prefix=True, add_vb=False):\n",
    "<<<AFTER_EDIT>>>\n",
    "\n",
    "    def load_dataset(self, split):\n",
    "        # return datasets.load_dataset('glue', 'mrpc', split=split, script_version=\"master\")\n",
    "        #return datasets.load_dataset('glue', 'mrpc', split=split)\n",
    "        files = {\n",
    "        'train': 'mrpc_csv/train.csv',\n",
    "        'validation': 'mrpc_csv/val.csv',\n",
    "        'test': 'mrpc_csv/test.csv'\n",
    "        }\n",
    "\n",
    "        # Load the dataset\n",
    "        return datasets.load_dataset('csv', data_files=files, split=split)\n",
    "\n",
    "    def preprocessor(self, example, add_prefix=True, add_vb=False):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a815ef06",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c8973c",
   "metadata": {},
   "source": [
    "Execution Start time: 00:46:48, End time: 00:49:34 (duration: 165 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a305c85b",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeecca7",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6b5a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip3 install -q huggingface-hub numpy scikit-learn scipy sentencepiece spacy tabulate tokenizers torch torchvision tqdm transformers datasets===2.19.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521ab0f5",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e4316e",
   "metadata": {},
   "source": [
    "Execution Start time: 00:49:34, End time: 00:49:39 (duration: 4 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb13219",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ee8e00",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7338d4e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Downloading...\nFrom: https://drive.google.com/uc?id=1raGLX-2vdX1uizYx2DdqZ_JzAkVeyeDF\nTo: /content/dpt/mrpc_csv.zip\n\n  0%|          | 0.00/3.96k [00:00<?, ?B/s]\n\n   creating: mrpc_csv/\n  inflating: mrpc_csv/val.csv        \n  inflating: mrpc_csv/test.csv       \n  inflating: mrpc_csv/train.csv      \n\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# URL of the Google Drive file\n",
    "file_url = \"https://drive.google.com/uc?id=1raGLX-2vdX1uizYx2DdqZ_JzAkVeyeDF\"\n",
    "output_file = \"mrpc_csv.zip\"\n",
    "\n",
    "# Download the file from Google Driveraise ValueError(f'mutable default {type(f.default)} for field ' ValueError: mutable default <class 'datasets.utils.version.Version'> for field version is not allowed: use default_factory\n",
    "gdown.download(file_url, output_file, quiet=False)\n",
    "!unzip mrpc_csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f279782",
   "metadata": {},
   "source": [
    "### Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fca1331",
   "metadata": {},
   "source": [
    "Execution Start time: 00:49:39, End time: 00:49:40 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e08763",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a6583d",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d018eed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\ndef run(local_rank, args):\n\n    is_master = local_rank == 0\n    world_size = args.world_size\n    is_mp = world_size > 1\n    # set the device\n    device = 'cpu' if not torch.cuda.is_available() else 'cuda'\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed_all(args.seed)\n    g = torch.Generator()\n    g.manual_seed(args.seed)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/dpt/main_ddp.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "    world_size = args.world_size\n",
    "    is_mp = world_size > 1\n",
    "    # set the device\n",
    "    device = local_rank\n",
    "\n",
    "    random.seed(args.seed)\n",
    "<<<AFTER_EDIT>>>\n",
    "    world_size = args.world_size\n",
    "    is_mp = world_size > 1\n",
    "    # set the device\n",
    "    device = 'cpu' if not torch.cuda.is_available() else 'cuda'\n",
    "\n",
    "    random.seed(args.seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c9c0ab",
   "metadata": {},
   "source": [
    "### Step 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e84f02a",
   "metadata": {},
   "source": [
    "Execution Start time: 00:49:40, End time: 00:49:40 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2252d5",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a85ede",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21de4e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n\n                for step, batch in enumerate(TrainDataloader):\n                    global_step += 1\n\n                    if global_step <= resume_steps:\n\n                        if step % update_stride == 0:\n                            pbar.update(update_stride)                        \n                        continue\n                    if len(batch['source_ids'].shape) == 3:\n                        source_ids = batch['source_ids'].squeeze(0).to(device)\n                        source_mask = batch['source_mask'].squeeze(0).to(device)\n                        labels = batch['target_ids'].squeeze(0).to(device)\n                        task_ids = torch.tensor([x[0] for x in batch['task_ids']]).to(device)\n                    else:\n                        source_ids = batch['source_ids'].to(device)\n                        source_mask = batch['source_mask'].to(device)\n                        labels = batch['target_ids'].to(device)\n                        task_ids = batch['task_ids'].to(device)\n\n                    outputs = model(input_ids=source_ids, attention_mask=source_mask, labels=labels, task_ids=task_ids)\n                    loss = outputs['loss']\n\n                    loss = loss / args.accumulate_steps\n                    loss.backward()\n                    step_count += 1\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/dpt/main_ddp.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "                            pbar.update(update_stride)                        \n",
    "                        continue\n",
    "                    if len(batch['source_ids'].shape) == 3:\n",
    "                        source_ids = batch['source_ids'].squeeze(0).to(local_rank)\n",
    "                        source_mask = batch['source_mask'].squeeze(0).to(local_rank)\n",
    "                        labels = batch['target_ids'].squeeze(0).to(local_rank)\n",
    "                        task_ids = torch.tensor([x[0] for x in batch['task_ids']]).to(local_rank)\n",
    "                    else:\n",
    "                        source_ids = batch['source_ids'].to(local_rank)\n",
    "                        source_mask = batch['source_mask'].to(local_rank)\n",
    "                        labels = batch['target_ids'].to(local_rank)\n",
    "                        task_ids = batch['task_ids'].to(local_rank)\n",
    "\n",
    "                    outputs = model(input_ids=source_ids, attention_mask=source_mask, labels=labels, task_ids=task_ids)\n",
    "<<<AFTER_EDIT>>>\n",
    "                            pbar.update(update_stride)                        \n",
    "                        continue\n",
    "                    if len(batch['source_ids'].shape) == 3:\n",
    "                        source_ids = batch['source_ids'].squeeze(0).to(device)\n",
    "                        source_mask = batch['source_mask'].squeeze(0).to(device)\n",
    "                        labels = batch['target_ids'].squeeze(0).to(device)\n",
    "                        task_ids = torch.tensor([x[0] for x in batch['task_ids']]).to(device)\n",
    "                    else:\n",
    "                        source_ids = batch['source_ids'].to(device)\n",
    "                        source_mask = batch['source_mask'].to(device)\n",
    "                        labels = batch['target_ids'].to(device)\n",
    "                        task_ids = batch['task_ids'].to(device)\n",
    "\n",
    "                    outputs = model(input_ids=source_ids, attention_mask=source_mask, labels=labels, task_ids=task_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd890cd",
   "metadata": {},
   "source": [
    "### Step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8a18ca",
   "metadata": {},
   "source": [
    "Execution Start time: 00:49:43, End time: 00:49:56 (duration: 13 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98c1e28",
   "metadata": {},
   "source": [
    "**Thought**: The repository has been cloned, and the necessary files have been edited to load the dataset and set the device correctly. The required packages have also been installed. Now, I need to run the `main_ddp.py` script with the specified hyperparameters to fine-tune the `t5-small` model using the decomposed prompt tuning method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b154e0fb",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d50b465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stating time:  08/21/2024 00:49:54\nusage: main_ddp.py [-h] [--seed SEED] [-ckpt FROM_CHECKPOINT]\n                   [--auto_resume AUTO_RESUME] [--save_ckpt_dir SAVE_CKPT_DIR]\n                   [--load_source_path LOAD_SOURCE_PATH]\n                   [--saving_steps SAVING_STEPS]\n                   [--saving_each_epoch SAVING_EACH_EPOCH]\n                   [--latest_ckpt_name LATEST_CKPT_NAME] [-e N_EPOCHS]\n                   [-ts TRAIN_BATCH_SIZE] [-es EVAL_BATCH_SIZE] [--lr LR]\n                   [--max_grad_norm MAX_GRAD_NORM]\n                   [--weight_decay WEIGHT_DECAY]\n                   [--accumulate_steps ACCUMULATE_STEPS]\n                   [--lr_scheduler_type LR_SCHEDULER_TYPE]\n                   [--num_warmup_steps NUM_WARMUP_STEPS]\n                   [--max_train_steps MAX_TRAIN_STEPS]\n                   [--min_training_epoch MIN_TRAINING_EPOCH]\n                   [--early_stopping_patience EARLY_STOPPING_PATIENCE]\n                   [--mode {train,eval,pred}] [-c CLOSE_TQDM]\n                   [-me MAX_EPOCHS_BEFORE_STOP] [--model_name MODEL_NAME]\n                   [--datasets DATASETS]\n                   [--max_source_length MAX_SOURCE_LENGTH]\n                   [--max_target_length MAX_TARGET_LENGTH] [-pt PROMPT_TYPE]\n                   [--is_dynamic_share IS_DYNAMIC_SHARE]\n                   [--eval_in_train EVAL_IN_TRAIN] [-el ENC_PROMPT_TOKENS]\n                   [--dec_prompt_tokens DEC_PROMPT_TOKENS] [-sr SHARING_RATIO]\n                   [-ss SHARE_SEED] [--add_task_prefix ADD_TASK_PREFIX]\n                   [-vb ADD_VERBALIZER] [-p PORT] [-gp GUPIDS [GUPIDS ...]]\n                   [-sm {uniform,stratified,t5,unifiedqa}] [-sl SIZE_LIMIT]\n                   [-st STOCH_TASK] [-bn BOTTLE_NECK]\n                   [--model_output_path MODEL_OUTPUT_PATH]\n                   [-lp LOCAL_FILE_PREFIX]\nmain_ddp.py: error: unrecognized arguments: --model_name_or_path t5-small --task_name mrpc --do_train --do_eval --output_dir output --overwrite_output_dir\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python3 main_ddp.py --enc_prompt_tokens 100 -ts 16 -e 1 --bottle_neck 10 --model_name_or_path t5-small --task_name mrpc --do_train --do_eval --output_dir output --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf1f5af",
   "metadata": {},
   "source": [
    "### Step 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd946e3",
   "metadata": {},
   "source": [
    "Execution Start time: 00:49:57, End time: 00:50:09 (duration: 11 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8eda390",
   "metadata": {},
   "source": [
    "**Thought**: \n",
    "Thought: It seems that the `main_ddp.py` script does not recognize some of the arguments I provided. I need to check the script or any documentation to understand the correct arguments to use for running the script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb87b69",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84439bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stating time:  08/21/2024 00:50:07\nusage: main_ddp.py [-h] [--seed SEED] [-ckpt FROM_CHECKPOINT]\n                   [--auto_resume AUTO_RESUME] [--save_ckpt_dir SAVE_CKPT_DIR]\n                   [--load_source_path LOAD_SOURCE_PATH]\n                   [--saving_steps SAVING_STEPS]\n                   [--saving_each_epoch SAVING_EACH_EPOCH]\n                   [--latest_ckpt_name LATEST_CKPT_NAME] [-e N_EPOCHS]\n                   [-ts TRAIN_BATCH_SIZE] [-es EVAL_BATCH_SIZE] [--lr LR]\n                   [--max_grad_norm MAX_GRAD_NORM]\n                   [--weight_decay WEIGHT_DECAY]\n                   [--accumulate_steps ACCUMULATE_STEPS]\n                   [--lr_scheduler_type LR_SCHEDULER_TYPE]\n                   [--num_warmup_steps NUM_WARMUP_STEPS]\n                   [--max_train_steps MAX_TRAIN_STEPS]\n                   [--min_training_epoch MIN_TRAINING_EPOCH]\n                   [--early_stopping_patience EARLY_STOPPING_PATIENCE]\n                   [--mode {train,eval,pred}] [-c CLOSE_TQDM]\n                   [-me MAX_EPOCHS_BEFORE_STOP] [--model_name MODEL_NAME]\n                   [--datasets DATASETS]\n                   [--max_source_length MAX_SOURCE_LENGTH]\n                   [--max_target_length MAX_TARGET_LENGTH] [-pt PROMPT_TYPE]\n                   [--is_dynamic_share IS_DYNAMIC_SHARE]\n                   [--eval_in_train EVAL_IN_TRAIN] [-el ENC_PROMPT_TOKENS]\n                   [--dec_prompt_tokens DEC_PROMPT_TOKENS] [-sr SHARING_RATIO]\n                   [-ss SHARE_SEED] [--add_task_prefix ADD_TASK_PREFIX]\n                   [-vb ADD_VERBALIZER] [-p PORT] [-gp GUPIDS [GUPIDS ...]]\n                   [-sm {uniform,stratified,t5,unifiedqa}] [-sl SIZE_LIMIT]\n                   [-st STOCH_TASK] [-bn BOTTLE_NECK]\n                   [--model_output_path MODEL_OUTPUT_PATH]\n                   [-lp LOCAL_FILE_PREFIX]\n\nprocess user given parameters\n\noptions:\n  -h, --help            show this help message and exit\n  --seed SEED           random seed\n  -ckpt FROM_CHECKPOINT, --from_checkpoint FROM_CHECKPOINT\n                        load the model from a checkpoint to resume the\n                        training\n  --auto_resume AUTO_RESUME\n  --save_ckpt_dir SAVE_CKPT_DIR\n                        load the latest checkpoint in this dir for resuming\n  --load_source_path LOAD_SOURCE_PATH\n                        source prompt file for target-FT\n  --saving_steps SAVING_STEPS\n  --saving_each_epoch SAVING_EACH_EPOCH\n  --latest_ckpt_name LATEST_CKPT_NAME\n                        the name of the latest checkpoint in this dir for\n                        resuming\n  -e N_EPOCHS, --n_epochs N_EPOCHS\n                        total number of training epochs to perform.\n  -ts TRAIN_BATCH_SIZE, --train_batch_size TRAIN_BATCH_SIZE\n  -es EVAL_BATCH_SIZE, --eval_batch_size EVAL_BATCH_SIZE\n  --lr LR               learning rate\n  --max_grad_norm MAX_GRAD_NORM\n                        max grad norm (0 to disable)\n  --weight_decay WEIGHT_DECAY\n                        l2 weight decay strength\n  --accumulate_steps ACCUMULATE_STEPS\n  --lr_scheduler_type LR_SCHEDULER_TYPE\n  --num_warmup_steps NUM_WARMUP_STEPS\n  --max_train_steps MAX_TRAIN_STEPS\n  --min_training_epoch MIN_TRAINING_EPOCH\n  --early_stopping_patience EARLY_STOPPING_PATIENCE\n  --mode {train,eval,pred}\n                        run training or evaluation\n  -c CLOSE_TQDM, --close_tqdm CLOSE_TQDM\n  -me MAX_EPOCHS_BEFORE_STOP, --max_epochs_before_stop MAX_EPOCHS_BEFORE_STOP\n                        stop training if dev does not increase for N epochs\n  --model_name MODEL_NAME\n                        model name\n  --datasets DATASETS   A list of datasets, seperated by the semicolon\n  --max_source_length MAX_SOURCE_LENGTH\n  --max_target_length MAX_TARGET_LENGTH\n                        SQuAD needs long decoding length\n  -pt PROMPT_TYPE, --prompt_type PROMPT_TYPE\n                        two implementations of prompting, embed or param\n  --is_dynamic_share IS_DYNAMIC_SHARE\n  --eval_in_train EVAL_IN_TRAIN\n                        do evaluation during the training\n  -el ENC_PROMPT_TOKENS, --enc_prompt_tokens ENC_PROMPT_TOKENS\n  --dec_prompt_tokens DEC_PROMPT_TOKENS\n  -sr SHARING_RATIO, --sharing_ratio SHARING_RATIO\n                        Sharing prompts across tasks\n  -ss SHARE_SEED, --share_seed SHARE_SEED\n  --add_task_prefix ADD_TASK_PREFIX\n                        add task perfix by default\n  -vb ADD_VERBALIZER, --add_verbalizer ADD_VERBALIZER\n                        add verbalizer to the context\n  -p PORT, --port PORT  port\n  -gp GUPIDS [GUPIDS ...], --gupids GUPIDS [GUPIDS ...]\n                        default is None\n  -sm {uniform,stratified,t5,unifiedqa}, --sampling_method {uniform,stratified,t5,unifiedqa}\n  -sl SIZE_LIMIT, --size_limit SIZE_LIMIT\n                        size limit for t5 sampling, default is 2**18\n  -st STOCH_TASK, --stoch_task STOCH_TASK\n  -bn BOTTLE_NECK, --bottle_neck BOTTLE_NECK\n  --model_output_path MODEL_OUTPUT_PATH\n                        path to the model outputs\n  -lp LOCAL_FILE_PREFIX, --local_file_prefix LOCAL_FILE_PREFIX\n                        local path prefix for external datasets, exp., MRQA\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python3 main_ddp.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f99e137",
   "metadata": {},
   "source": [
    "### Step 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b89b46b",
   "metadata": {},
   "source": [
    "Execution Start time: 00:50:12, End time: 00:50:12 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cb4b51",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddb8f02",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f2a96c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab7ca9e0",
   "metadata": {},
   "source": [
    "### Step 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c32cb5b",
   "metadata": {},
   "source": [
    "Execution Start time: 00:50:13, End time: 00:50:14 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5914ec42",
   "metadata": {},
   "source": [
    "**Thought**: The error message indicates that some of the arguments I used are not recognized by the `main_ddp.py` script. I need to review the help message to understand the correct arguments to use for running the script. The help message shows the available arguments, but it seems to be truncated. I need to check the script directly to understand the correct arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f200e9",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860337cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import os\nimport sys\nimport random\nimport argparse\nimport numpy as np\nfrom tqdm import tqdm\nfrom datetime import datetime\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import optim\n\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler\n\nimport transformers\nfrom transformers import (\n    AutoTokenizer,\n    T5TokenizerFast,\n    T5ForConditionalGeneration,\n    AutoConfig,\n    AdamW,\n    get_scheduler,\n    set_seed,\n)\ntransformers.logging.set_verbosity_error()\n\nfrom modeling_t5 import T5PromptForConditionalGeneration_param\nfrom data_utils import AutoTask\nfrom eval_utils import AutoPostProcessor\nfrom metrics import *\nfrom options import *\nfrom utils import *\nfrom loader import *\n\nimport datasets\nfrom datasets import concatenate_datasets\nfrom datasets.utils.logging import set_verbosity_error\nfrom eval_utils import *\nset_verbosity_error()\n\nimport logging\nlogging.disable(logging.WARNING)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!head -n 50 main_ddp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8d23e9",
   "metadata": {},
   "source": [
    "### Step 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c8af0b",
   "metadata": {},
   "source": [
    "Execution Start time: 00:50:16, End time: 00:50:17 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a86fd8",
   "metadata": {},
   "source": [
    "**Thought**: The `main_ddp.py` script imports several modules and functions, but the argument parsing section is not visible in the first 50 lines. I need to look further into the script to find the argument parsing section, which will help me understand the correct arguments to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b0aef0",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ab89a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!sed -n '/def parse_args/,/return parser/p' main_ddp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305d610b",
   "metadata": {},
   "source": [
    "### Step 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6b844e",
   "metadata": {},
   "source": [
    "Execution Start time: 00:50:18, End time: 00:50:18 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3181d16d",
   "metadata": {},
   "source": [
    "**Thought**: The `sed` command did not return any output, which suggests that the argument parsing section might be structured differently or is not present in the script. I should manually inspect the script to identify the correct arguments for running the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b361a3",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bcd040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!cat main_ddp.py | grep \"parser.add_argument\" -A 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2623954",
   "metadata": {},
   "source": [
    "### Step 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b493e85f",
   "metadata": {},
   "source": [
    "Execution Start time: 00:50:20, End time: 00:50:20 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201ee0c3",
   "metadata": {},
   "source": [
    "**Thought**: The `grep` command did not return any output, which suggests that the argument parsing might be handled in a different way or is located in a different file. I should check the `options.py` file, as it might contain the argument parsing logic for the script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc52c769",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f52daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    parser.add_argument('--seed', default=42, type=int, help='random seed')\n    parser.add_argument('-ckpt', '--from_checkpoint', default='', type=str, help='load the model from a checkpoint to resume the training')\n    parser.add_argument('--auto_resume', type='bool', default=False)\n    parser.add_argument('--save_ckpt_dir', type=str, default='./saved_models', help=\"load the latest checkpoint in this dir for resuming\")\n\n    # parser.add_argument('--save_source_dir', default='', help='model output directory')\n    parser.add_argument('--load_source_path', default='', help='source prompt file for target-FT')\n    parser.add_argument('--saving_steps', type=int, default=1000)\n    parser.add_argument('--saving_each_epoch', type='bool', default=True)\n    parser.add_argument('--latest_ckpt_name', type=str, default='latest_checkpoint.pt', help=\"the name of the latest checkpoint in this dir for resuming\")\n\n    # optimization\n    parser.add_argument('-e', '--n_epochs', default=20, type=int, help='total number of training epochs to perform.')\n    parser.add_argument('-ts', '--train_batch_size', default=32, type=int)  \n    parser.add_argument('-es', \"--eval_batch_size\", default=128, type=int)\n    parser.add_argument('--lr', default=0.3, type=float, help='learning rate')\n    parser.add_argument('--max_grad_norm', default=1.0, type=float, help='max grad norm (0 to disable)')\n    parser.add_argument('--weight_decay', default=1e-5, type=float, help='l2 weight decay strength')\n    parser.add_argument('--accumulate_steps', type=int, default=1)\n    \n    parser.add_argument('--lr_scheduler_type', type=str, default='linear')\n    parser.add_argument('--num_warmup_steps', type=float, default=0.1)\n    parser.add_argument('--max_train_steps', type=int, default=0)\n\n    parser.add_argument('--min_training_epoch', type=int, default=20)\n    parser.add_argument('--early_stopping_patience', type=int, default=10)\n\n    # useful arguments\n    parser.add_argument('--mode', default='train', choices=['train', 'eval', 'pred'], help='run training or evaluation')\n    \n    # training arguments\n    parser.add_argument('-c', '--close_tqdm', type='bool', default=True)\n    parser.add_argument('-me', '--max_epochs_before_stop', default=2, type=int, help='stop training if dev does not increase for N epochs')\n\n    parser.add_argument('--model_name', default='t5-base', help='model name')\n    parser.add_argument('--datasets', type=str, default='rte', help='A list of datasets, seperated by the semicolon')\n\n    parser.add_argument('--max_source_length', type=int, default=256)\n    parser.add_argument('--max_target_length', type=int, default=128, help='SQuAD needs long decoding length')\n\n    parser.add_argument('-pt', '--prompt_type', type=str, default='param', help='two implementations of prompting, embed or param')\n    parser.add_argument('--is_dynamic_share', type='bool', default=False)\n    parser.add_argument('--eval_in_train', type='bool', default=True, help='do evaluation during the training')\n\n    # PT parameters\n    parser.add_argument('-el', '--enc_prompt_tokens', type=int, default=100)\n    parser.add_argument('--dec_prompt_tokens', type=int, default=0)\n    parser.add_argument('-sr', '--sharing_ratio', type=float, default=1.0, help='Sharing prompts across tasks')\n\n    # Data preprocessing\n    parser.add_argument('-ss', '--share_seed', type=int, default=42)\n    parser.add_argument('--add_task_prefix', type='bool', default=True, help='add task perfix by default')\n    parser.add_argument('-vb', '--add_verbalizer', type='bool', default=False, help='add verbalizer to the context')\n\n    # DDP\n    # parser.add_argument('--local_rank', type=int, default=-1, metavar='N', help='Local process rank.') \n    parser.add_argument(\"-p\", \"--port\", type=int, default=12355, help=\"port\")\n    parser.add_argument('-gp', \"--gupids\", nargs='+', type=int, help=\"default is None\")\n\n    parser.add_argument('-sm', \"--sampling_method\", type=str, default='uniform', choices=['uniform', 'stratified', 't5', 'unifiedqa'])\n    parser.add_argument('-sl', \"--size_limit\", type=int, default=262144, help=\"size limit for t5 sampling, default is 2**18\")\n    parser.add_argument('-st', '--stoch_task', type='bool', default=False)\n\n\n    # decomposion\n    parser.add_argument('-bn', \"--bottle_neck\", type=int, default=10)\n\n    parser.add_argument('--model_output_path', type=str, default='./saved_outputs', help='path to the model outputs')\n    parser.add_argument('-lp', '--local_file_prefix', type=str, default='/gpfs/u/home/DPTV/DPTVhnwz/scratch/mrqa_datasets/datasets', \n                        help='local path prefix for external datasets, exp., MRQA')\n\n    args = parser.parse_args()\n\n    \n    args.datasets = args.datasets.strip().split(',')\n\n    args.datasets_names = [TASK_NAME_MAPPING[x] for x in args.datasets if x != '']\n\n    if args.save_ckpt_dir and not os.path.exists(args.save_ckpt_dir):\n            os.makedirs(args.save_ckpt_dir)\n\n    if args.model_output_path and not os.path.exists(args.model_output_path):\n            os.makedirs(args.model_output_path)\n\n    if args.model_name == '':\n        args.model_name = 't5-base'  # default model name\n\n    print('Raw Arguments: ', args)\n    print('Process ID: ', os.getpid())\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!cat options.py | grep \"parser.add_argument\" -A 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b78ad6",
   "metadata": {},
   "source": [
    "### Step 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24016fe",
   "metadata": {},
   "source": [
    "Execution Start time: 00:50:23, End time: 00:52:03 (duration: 100 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb70aa2b",
   "metadata": {},
   "source": [
    "**Thought**: The `options.py` file contains the argument parsing logic. The correct arguments for running the training process include `--model_name`, `--datasets`, `--mode`, and other hyperparameters. I will use these arguments to run the `main_ddp.py` script with the specified hyperparameters for fine-tuning the `t5-small` model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010450cf",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba76acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stating time:  08/21/2024 00:50:30\nRaw Arguments:  Namespace(seed=42, from_checkpoint='', auto_resume=False, save_ckpt_dir='output', load_source_path='', saving_steps=1000, saving_each_epoch=True, latest_ckpt_name='latest_checkpoint.pt', n_epochs=1, train_batch_size=16, eval_batch_size=128, lr=0.3, max_grad_norm=1.0, weight_decay=1e-05, accumulate_steps=1, lr_scheduler_type='linear', num_warmup_steps=0.1, max_train_steps=0, min_training_epoch=20, early_stopping_patience=10, mode='train', close_tqdm=False, max_epochs_before_stop=2, model_name='t5-small', datasets=['mrpc'], max_source_length=256, max_target_length=128, prompt_type='param', is_dynamic_share=False, eval_in_train=True, enc_prompt_tokens=100, dec_prompt_tokens=0, sharing_ratio=1.0, share_seed=42, add_task_prefix=True, add_verbalizer=False, port=12355, gupids=None, sampling_method='uniform', size_limit=262144, stoch_task=False, bottle_neck=10, model_output_path='output', local_file_prefix='/gpfs/u/home/DPTV/DPTVhnwz/scratch/mrqa_datasets/datasets', datasets_names=['mrpc'])\nProcess ID:  75\nUse single gpu!\n\ntokenizer_config.json:   0%|                        | 0.00/2.32k [00:00<?, ?B/s]\n\n\nspiece.model:   0%|                                  | 0.00/792k [00:00<?, ?B/s]\n\n\n\ntokenizer.json:   0%|                               | 0.00/1.39M [00:00<?, ?B/s]\n\n\n0 mrpc Train\t {'source': 'sentence1: The companies uniformly declined to give specific numbers on customer turnover , saying they will release those figures only when they report overall company performance at year-end . sentence2: The companies , however , declined to give specifics on customer turnover , saying they would release figures only when they report their overall company performance .', 'target': '1', 'task': 'mrpc'}\n0 mrpc Val\t {'source': 'sentence1: The additional contribution brings total U.S. food aid to North Korea this year to 100,000 tonnes . sentence2: The donation of 60,000 tons brings the total of U.S. contributions for the year to 100,000 .', 'target': '1', 'task': 'mrpc'}\n0 10 10\n# all training samples: 10\n{'source': 'sentence1: The companies uniformly declined to give specific numbers on customer turnover , saying they will release those figures only when they report overall company performance at year-end . sentence2: The companies , however , declined to give specifics on customer turnover , saying they would release figures only when they report their overall company performance .', 'target': '1', 'task': 'mrpc'}\n\nconfig.json:   0%|                                  | 0.00/1.21k [00:00<?, ?B/s]\n\n\nmodel.safetensors:   0%|                             | 0.00/242M [00:00<?, ?B/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngeneration_config.json:   0%|                         | 0.00/147 [00:00<?, ?B/s]\ngeneration_config.json: 100%|███████████████████| 147/147 [00:00<00:00, 882kB/s]\nParameters to optimize:  ['encoder.prefix_first', 'encoder.prefix_second']\nPrefix Parameters:  ['encoder.prefix_first', 'encoder.prefix_second']\nTrainable Parameters:  ['encoder.prefix_first', 'encoder.prefix_second']\n#Trainable Parameters:  6120\nAll trainable parameters: 6120; per task: 6120.0\n\n***** running training *****\n| batch_size: 16 | num_epochs: 1 | num_train: 1 |\n\nEpoch 0/1:   0%|                                           | 0/1 [00:00<?, ?b/s]\nEpoch 0/1:   0%|                          | 0/1 [00:10<?, ?b/s, loss=4.58, lr=0]\nEpoch 0/1: 100%|██████████████████| 1/1 [00:10<00:00, 10.79s/b, loss=4.58, lr=0]\nEpoch 0/1: 100%|██████████████████| 1/1 [00:10<00:00, 10.83s/b, loss=4.58, lr=0]\nSaved soft prompts at: output/mrpc.soft_prompts.epoch.0.pt\nEpoch 0 - Validation:  {'mrpc': {'accuracy': 0.0, 'f1': 0.0}}\nEpoch Best Val: 0.0 at Epoch 0.\n***** training ends *****\n\nbest dev acc: 0.00000 (at epoch 0)\nbest dev acc: -inf (at step 0)\n\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python3 main_ddp.py --model_name t5-small --datasets mrpc --mode train --enc_prompt_tokens 100 -ts 16 -e 1 --bottle_neck 10 --eval_in_train True --model_output_path output --save_ckpt_dir output --saving_each_epoch True --close_tqdm False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056fcf77",
   "metadata": {},
   "source": [
    "### Step 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c1dce4",
   "metadata": {},
   "source": [
    "Execution Start time: 00:52:05, End time: 00:52:05 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87112d51",
   "metadata": {},
   "source": [
    "**Thought**: The training process has completed successfully for 1 epoch using the specified hyperparameters. The accuracy on the development set is reported as 0.0. I will now submit this result in the required JSON format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff914643",
   "metadata": {},
   "source": [
    "**Action (submit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2629d78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'accuracy': 0.0}"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
