{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc25a41d",
   "metadata": {},
   "source": [
    "### System Prompt\n",
    "<details>\n",
    "SETTING: You are an autonomous programmer, and you're working directly in the command line with a special Jupyter notebook interface.\n",
    "\n",
    "The special Jupyter notebook interface consists of a file editor that shows you 100 lines of a file at a time.\n",
    "You can execute commands in the notebook using:\n",
    "\n",
    "1. Bash commands: Commands starting with !.\n",
    "2. Python commands: Standard Python code.\n",
    "3. Magic commands: Commands starting with %, e.g., %cd <path>.\n",
    "\n",
    "Additionally, you can also use the following commands to help you navigate and edit files.\n",
    "\n",
    "COMMANDS:\n",
    "\n",
    "open:\n",
    "  docstring: opens the file at the given path in the editor. If line_number is provided, the window will be move to include that line\n",
    "  signature: open <path> [<line_number>]\n",
    "  arguments:\n",
    "    - path (string) [required]: the path to the file to open\n",
    "    - line_number (integer) [optional]: the line number to move the window to (if not provided, the window will start at the top of the file)\n",
    "\n",
    "create:\n",
    "  docstring: creates and opens a new file with the given name\n",
    "  signature: create <filename>\n",
    "  arguments:\n",
    "    - filename (string) [required]: the name of the file to create\n",
    "\n",
    "goto:\n",
    "  docstring: moves the window to show <line_number>\n",
    "  signature: goto <line_number>\n",
    "  arguments:\n",
    "    - line_number (integer) [required]: the line number to move the window to\n",
    "\n",
    "scroll_down:\n",
    "  docstring: moves the window down {WINDOW} lines\n",
    "  signature: scroll_down\n",
    "\n",
    "scroll_up:\n",
    "  docstring: moves the window down {WINDOW} lines\n",
    "  signature: scroll_up\n",
    "\n",
    "submit:\n",
    "  docstring: submits your final answer\n",
    "  signature: submit <answer>\n",
    "  arguments:\n",
    "    - answer (string) [required]: the answer to be submitted\n",
    "\n",
    "search_dir:\n",
    "  docstring: searches for search_term in all files in dir_path. If dir_path is not provided, searches in the current directory\n",
    "  signature: search_dir <search_term> [<dir_path>]\n",
    "  arguments:\n",
    "    - search_term (string) [required]: the term to search for\n",
    "    - dir_path (string) [optional]: the directory to search in (if not provided, searches in the current directory)\n",
    "\n",
    "search_file:\n",
    "  docstring: searches for search_term in file. If file is not provided, searches in the current open file\n",
    "  signature: search_file <search_term> [<file>]\n",
    "  arguments:\n",
    "    - search_term (string) [required]: the term to search for\n",
    "    - file (string) [optional]: the file to search in (if not provided, searches in the current open file)\n",
    "\n",
    "find_file:\n",
    "  docstring: finds all files with the given name in dir_path. If dir_path is not provided, searches in the current directory\n",
    "  signature: find_file <file_name> [<dir_path>]\n",
    "  arguments:\n",
    "    - file_name (string) [required]: the name of the file to search for\n",
    "    - dir_path (string) [optional]: the directory to search in (if not provided, searches in the current directory)\n",
    "\n",
    "edit:\n",
    "  docstring: replaces lines <start_line> through <end_line> (inclusive) with the given text in the open file. The replacement text is terminated by a line with only end_of_edit on it. All of the <replacement text> will be entered, so make sure your indentation is formatted properly. Python files will be checked for syntax errors after the edit. If the system detects a syntax error, the edit will not be executed. Simply try to edit the file again, but make sure to read the error message and modify the edit command you issue accordingly. Issuing the same command a second time will just lead to the same error message again.\n",
    "  signature: edit <start_line>:<end_line>\n",
    "<replacement_text>\n",
    "end_of_edit\n",
    "  arguments:\n",
    "    - start_line (integer) [required]: the line number to start the edit at\n",
    "    - end_line (integer) [required]: the line number to end the edit at (inclusive)\n",
    "    - replacement_text (string) [required]: the text to replace the current selection with\n",
    "\n",
    "\n",
    "\n",
    "Please note that THE EDIT COMMAND REQUIRES PROPER INDENTATION. \n",
    "If you'd like to add the line '        print(x)' you must fully write that out, with all those spaces before the code! Indentation is important and code that is not indented correctly will fail and require fixing before it can be run.\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "Your shell prompt is formatted as follows:\n",
    "(Open file: <path>)\n",
    "(Current directory: <cwd>)\n",
    "In [ ]\n",
    "\n",
    "You need to format your output using two fields: discussion and command.\n",
    "Your output should always include _one_ discussion and _one_ command field EXACTLY as in the following example:\n",
    "DISCUSSION\n",
    "First I'll start by using ls to see what files are in the current directory. Then maybe we can look at some relevant files to see what they look like.\n",
    "```\n",
    "!ls -a\n",
    "```\n",
    "\n",
    "You should only include a *SINGLE* command in the command section and then wait for a response from the shell before continuing with more discussion and commands. Everything you include in the DISCUSSION section will be saved for future reference.\n",
    "If you'd like to issue two commands at once, PLEASE DO NOT DO THAT! Please instead first submit just the first command, and then after receiving a response you'll be able to issue the second command. \n",
    "You're free to use any other bash commands you want (e.g. find, grep, cat, ls, cd) in addition to the special commands listed above.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12588ed",
   "metadata": {},
   "source": [
    "### Query\n",
    "Train and evaluate bart-base on the xsum dataset using the quick start pipeline. Report the rouge-1, rouge-2, rouge-L and rouge-w-1.2 metrics as a json structured as follows: {\"rouge-1\": 0.0, \"rouge-2\": 0.0, \"rouge-L\": 0.0, \"rouge-w-1.2\": 0.0} (replace 0.0 with the actual values).\n",
    "\n",
    "Additional instructions:\n",
    "1. Load only the first 10 rows of each set in the dataset.\n",
    "2. Train only one epoch.\n",
    "3. Codebase expects one line per sample. Replace \\n with ` [SEP] `.\n",
    "\n",
    "Git repository: https://github.com/RUCAIBox/TextBox\n",
    "\n",
    "I have already executed some of the required steps. Now, you should install all required dependencies. Once dependencies are installed, you can re-run any of the pre-executed steps, and finally should successfully run\n",
    "```\n",
    "!python run_textbox.py --model=BART --dataset=xsum --model_path=facebook/bart-base --epochs=1\n",
    "```\n",
    "to submit the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a336bec",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dc3689",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820e68ba",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26cf7da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Clone model repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd42b051",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68977864",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d68320a",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bba2cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cloning into 'TextBox'...\r\nremote: Enumerating objects: 11190, done.\u001b[K\r\nremote: Counting objects:   0% (1/2598)\u001b[K\rremote: Counting objects:   1% (26/2598)\u001b[K\rremote: Counting objects:   2% (52/2598)\u001b[K\rremote: Counting objects:   3% (78/2598)\u001b[K\rremote: Counting objects:   4% (104/2598)\u001b[K\rremote: Counting objects:   5% (130/2598)\u001b[K\rremote: Counting objects:   6% (156/2598)\u001b[K\rremote: Counting objects:   7% (182/2598)\u001b[K\rremote: Counting objects:   8% (208/2598)\u001b[K\rremote: Counting objects:   9% (234/2598)\u001b[K\rremote: Counting objects:  10% (260/2598)\u001b[K\rremote: Counting objects:  11% (286/2598)\u001b[K\rremote: Counting objects:  12% (312/2598)\u001b[K\rremote: Counting objects:  13% (338/2598)\u001b[K\rremote: Counting objects:  14% (364/2598)\u001b[K\rremote: Counting objects:  15% (390/2598)\u001b[K\rremote: Counting objects:  16% (416/2598)\u001b[K\rremote: Counting objects:  17% (442/2598)\u001b[K\rremote: Counting objects:  18% (468/2598)\u001b[K\rremote: Counting objects:  19% (494/2598)\u001b[K\rremote: Counting objects:  20% (520/2598)\u001b[K\rremote: Counting objects:  21% (546/2598)\u001b[K\rremote: Counting objects:  22% (572/2598)\u001b[K\rremote: Counting objects:  23% (598/2598)\u001b[K\rremote: Counting objects:  24% (624/2598)\u001b[K\rremote: Counting objects:  25% (650/2598)\u001b[K\rremote: Counting objects:  26% (676/2598)\u001b[K\rremote: Counting objects:  27% (702/2598)\u001b[K\rremote: Counting objects:  28% (728/2598)\u001b[K\rremote: Counting objects:  29% (754/2598)\u001b[K\rremote: Counting objects:  30% (780/2598)\u001b[K\rremote: Counting objects:  31% (806/2598)\u001b[K\rremote: Counting objects:  32% (832/2598)\u001b[K\rremote: Counting objects:  33% (858/2598)\u001b[K\rremote: Counting objects:  34% (884/2598)\u001b[K\rremote: Counting objects:  35% (910/2598)\u001b[K\rremote: Counting objects:  36% (936/2598)\u001b[K\rremote: Counting objects:  37% (962/2598)\u001b[K\rremote: Counting objects:  38% (988/2598)\u001b[K\rremote: Counting objects:  39% (1014/2598)\u001b[K\rremote: Counting objects:  40% (1040/2598)\u001b[K\rremote: Counting objects:  41% (1066/2598)\u001b[K\rremote: Counting objects:  42% (1092/2598)\u001b[K\rremote: Counting objects:  43% (1118/2598)\u001b[K\rremote: Counting objects:  44% (1144/2598)\u001b[K\rremote: Counting objects:  45% (1170/2598)\u001b[K\rremote: Counting objects:  46% (1196/2598)\u001b[K\rremote: Counting objects:  47% (1222/2598)\u001b[K\rremote: Counting objects:  48% (1248/2598)\u001b[K\rremote: Counting objects:  49% (1274/2598)\u001b[K\rremote: Counting objects:  50% (1299/2598)\u001b[K\rremote: Counting objects:  51% (1325/2598)\u001b[K\rremote: Counting objects:  52% (1351/2598)\u001b[K\rremote: Counting objects:  53% (1377/2598)\u001b[K\rremote: Counting objects:  54% (1403/2598)\u001b[K\rremote: Counting objects:  55% (1429/2598)\u001b[K\rremote: Counting objects:  56% (1455/2598)\u001b[K\rremote: Counting objects:  57% (1481/2598)\u001b[K\rremote: Counting objects:  58% (1507/2598)\u001b[K\rremote: Counting objects:  59% (1533/2598)\u001b[K\rremote: Counting objects:  60% (1559/2598)\u001b[K\rremote: Counting objects:  61% (1585/2598)\u001b[K\rremote: Counting objects:  62% (1611/2598)\u001b[K\rremote: Counting objects:  63% (1637/2598)\u001b[K\rremote: Counting objects:  64% (1663/2598)\u001b[K\rremote: Counting objects:  65% (1689/2598)\u001b[K\rremote: Counting objects:  66% (1715/2598)\u001b[K\rremote: Counting objects:  67% (1741/2598)\u001b[K\rremote: Counting objects:  68% (1767/2598)\u001b[K\rremote: Counting objects:  69% (1793/2598)\u001b[K\rremote: Counting objects:  70% (1819/2598)\u001b[K\rremote: Counting objects:  71% (1845/2598)\u001b[K\rremote: Counting objects:  72% (1871/2598)\u001b[K\rremote: Counting objects:  73% (1897/2598)\u001b[K\rremote: Counting objects:  74% (1923/2598)\u001b[K\rremote: Counting objects:  75% (1949/2598)\u001b[K\rremote: Counting objects:  76% (1975/2598)\u001b[K\rremote: Counting objects:  77% (2001/2598)\u001b[K\rremote: Counting objects:  78% (2027/2598)\u001b[K\rremote: Counting objects:  79% (2053/2598)\u001b[K\rremote: Counting objects:  80% (2079/2598)\u001b[K\rremote: Counting objects:  81% (2105/2598)\u001b[K\rremote: Counting objects:  82% (2131/2598)\u001b[K\rremote: Counting objects:  83% (2157/2598)\u001b[K\rremote: Counting objects:  84% (2183/2598)\u001b[K\rremote: Counting objects:  85% (2209/2598)\u001b[K\rremote: Counting objects:  86% (2235/2598)\u001b[K\rremote: Counting objects:  87% (2261/2598)\u001b[K\rremote: Counting objects:  88% (2287/2598)\u001b[K\rremote: Counting objects:  89% (2313/2598)\u001b[K\rremote: Counting objects:  90% (2339/2598)\u001b[K\rremote: Counting objects:  91% (2365/2598)\u001b[K\rremote: Counting objects:  92% (2391/2598)\u001b[K\rremote: Counting objects:  93% (2417/2598)\u001b[K\rremote: Counting objects:  94% (2443/2598)\u001b[K\rremote: Counting objects:  95% (2469/2598)\u001b[K\rremote: Counting objects:  96% (2495/2598)\u001b[K\rremote: Counting objects:  97% (2521/2598)\u001b[K\rremote: Counting objects:  98% (2547/2598)\u001b[K\rremote: Counting objects:  99% (2573/2598)\u001b[K\rremote: Counting objects: 100% (2598/2598)\u001b[K\rremote: Counting objects: 100% (2598/2598), done.\u001b[K\r\nremote: Compressing objects:   0% (1/1020)\u001b[K\rremote: Compressing objects:   1% (11/1020)\u001b[K\rremote: Compressing objects:   2% (21/1020)\u001b[K\rremote: Compressing objects:   3% (31/1020)\u001b[K\rremote: Compressing objects:   4% (41/1020)\u001b[K\rremote: Compressing objects:   5% (51/1020)\u001b[K\rremote: Compressing objects:   6% (62/1020)\u001b[K\rremote: Compressing objects:   7% (72/1020)\u001b[K\rremote: Compressing objects:   8% (82/1020)\u001b[K\rremote: Compressing objects:   9% (92/1020)\u001b[K\rremote: Compressing objects:  10% (102/1020)\u001b[K\rremote: Compressing objects:  11% (113/1020)\u001b[K\rremote: Compressing objects:  12% (123/1020)\u001b[K\rremote: Compressing objects:  13% (133/1020)\u001b[K\rremote: Compressing objects:  14% (143/1020)\u001b[K\rremote: Compressing objects:  15% (153/1020)\u001b[K\rremote: Compressing objects:  16% (164/1020)\u001b[K\rremote: Compressing objects:  17% (174/1020)\u001b[K\rremote: Compressing objects:  18% (184/1020)\u001b[K\rremote: Compressing objects:  19% (194/1020)\u001b[K\rremote: Compressing objects:  20% (204/1020)\u001b[K\rremote: Compressing objects:  21% (215/1020)\u001b[K\rremote: Compressing objects:  22% (225/1020)\u001b[K\rremote: Compressing objects:  23% (235/1020)\u001b[K\rremote: Compressing objects:  24% (245/1020)\u001b[K\rremote: Compressing objects:  25% (255/1020)\u001b[K\rremote: Compressing objects:  26% (266/1020)\u001b[K\rremote: Compressing objects:  27% (276/1020)\u001b[K\rremote: Compressing objects:  28% (286/1020)\u001b[K\rremote: Compressing objects:  29% (296/1020)\u001b[K\rremote: Compressing objects:  30% (306/1020)\u001b[K\rremote: Compressing objects:  31% (317/1020)\u001b[K\rremote: Compressing objects:  32% (327/1020)\u001b[K\rremote: Compressing objects:  33% (337/1020)\u001b[K\rremote: Compressing objects:  34% (347/1020)\u001b[K\rremote: Compressing objects:  35% (357/1020)\u001b[K\rremote: Compressing objects:  36% (368/1020)\u001b[K\rremote: Compressing objects:  37% (378/1020)\u001b[K\rremote: Compressing objects:  38% (388/1020)\u001b[K\rremote: Compressing objects:  39% (398/1020)\u001b[K\rremote: Compressing objects:  40% (408/1020)\u001b[K\rremote: Compressing objects:  41% (419/1020)\u001b[K\rremote: Compressing objects:  42% (429/1020)\u001b[K\rremote: Compressing objects:  43% (439/1020)\u001b[K\rremote: Compressing objects:  44% (449/1020)\u001b[K\rremote: Compressing objects:  45% (459/1020)\u001b[K\rremote: Compressing objects:  46% (470/1020)\u001b[K\rremote: Compressing objects:  47% (480/1020)\u001b[K\rremote: Compressing objects:  48% (490/1020)\u001b[K\rremote: Compressing objects:  49% (500/1020)\u001b[K\rremote: Compressing objects:  50% (510/1020)\u001b[K\rremote: Compressing objects:  51% (521/1020)\u001b[K\rremote: Compressing objects:  52% (531/1020)\u001b[K\rremote: Compressing objects:  53% (541/1020)\u001b[K\rremote: Compressing objects:  54% (551/1020)\u001b[K\rremote: Compressing objects:  55% (561/1020)\u001b[K\rremote: Compressing objects:  56% (572/1020)\u001b[K\rremote: Compressing objects:  57% (582/1020)\u001b[K\rremote: Compressing objects:  58% (592/1020)\u001b[K\rremote: Compressing objects:  59% (602/1020)\u001b[K\rremote: Compressing objects:  60% (612/1020)\u001b[K\rremote: Compressing objects:  61% (623/1020)\u001b[K\rremote: Compressing objects:  62% (633/1020)\u001b[K\rremote: Compressing objects:  63% (643/1020)\u001b[K\rremote: Compressing objects:  64% (653/1020)\u001b[K\rremote: Compressing objects:  65% (663/1020)\u001b[K\rremote: Compressing objects:  66% (674/1020)\u001b[K\rremote: Compressing objects:  67% (684/1020)\u001b[K\rremote: Compressing objects:  68% (694/1020)\u001b[K\rremote: Compressing objects:  69% (704/1020)\u001b[K\rremote: Compressing objects:  70% (714/1020)\u001b[K\rremote: Compressing objects:  71% (725/1020)\u001b[K\rremote: Compressing objects:  72% (735/1020)\u001b[K\rremote: Compressing objects:  73% (745/1020)\u001b[K\rremote: Compressing objects:  74% (755/1020)\u001b[K\rremote: Compressing objects:  75% (765/1020)\u001b[K\rremote: Compressing objects:  76% (776/1020)\u001b[K\rremote: Compressing objects:  77% (786/1020)\u001b[K\rremote: Compressing objects:  78% (796/1020)\u001b[K\rremote: Compressing objects:  79% (806/1020)\u001b[K\rremote: Compressing objects:  80% (816/1020)\u001b[K\rremote: Compressing objects:  81% (827/1020)\u001b[K\rremote: Compressing objects:  82% (837/1020)\u001b[K\rremote: Compressing objects:  83% (847/1020)\u001b[K\rremote: Compressing objects:  84% (857/1020)\u001b[K\rremote: Compressing objects:  85% (867/1020)\u001b[K\rremote: Compressing objects:  86% (878/1020)\u001b[K\rremote: Compressing objects:  87% (888/1020)\u001b[K\rremote: Compressing objects:  88% (898/1020)\u001b[K\rremote: Compressing objects:  89% (908/1020)\u001b[K\rremote: Compressing objects:  90% (918/1020)\u001b[K\rremote: Compressing objects:  91% (929/1020)\u001b[K\rremote: Compressing objects:  92% (939/1020)\u001b[K\rremote: Compressing objects:  93% (949/1020)\u001b[K\rremote: Compressing objects:  94% (959/1020)\u001b[K\rremote: Compressing objects:  95% (969/1020)\u001b[K\rremote: Compressing objects:  96% (980/1020)\u001b[K\rremote: Compressing objects:  97% (990/1020)\u001b[K\rremote: Compressing objects:  98% (1000/1020)\u001b[K\rremote: Compressing objects:  99% (1010/1020)\u001b[K\rremote: Compressing objects: 100% (1020/1020)\u001b[K\rremote: Compressing objects: 100% (1020/1020), done.\u001b[K\r\nReceiving objects:   0% (1/11190)\rReceiving objects:   1% (112/11190)\rReceiving objects:   2% (224/11190)\rReceiving objects:   3% (336/11190)\rReceiving objects:   4% (448/11190)\rReceiving objects:   5% (560/11190)\rReceiving objects:   6% (672/11190)\rReceiving objects:   7% (784/11190)\rReceiving objects:   8% (896/11190)\rReceiving objects:   9% (1008/11190)\rReceiving objects:  10% (1119/11190)\rReceiving objects:  11% (1231/11190)\rReceiving objects:  12% (1343/11190)\rReceiving objects:  13% (1455/11190)\rReceiving objects:  14% (1567/11190)\rReceiving objects:  15% (1679/11190)\rReceiving objects:  16% (1791/11190)\rReceiving objects:  17% (1903/11190)\rReceiving objects:  18% (2015/11190)\rReceiving objects:  19% (2127/11190)\rReceiving objects:  20% (2238/11190)\rReceiving objects:  21% (2350/11190)\rReceiving objects:  22% (2462/11190)\rReceiving objects:  23% (2574/11190)\rReceiving objects:  24% (2686/11190)\rReceiving objects:  25% (2798/11190)\rReceiving objects:  26% (2910/11190)\rReceiving objects:  27% (3022/11190)\rReceiving objects:  28% (3134/11190)\rReceiving objects:  29% (3246/11190)\rReceiving objects:  30% (3357/11190)\rReceiving objects:  31% (3469/11190)\rReceiving objects:  32% (3581/11190)\rReceiving objects:  33% (3693/11190)\rReceiving objects:  34% (3805/11190)\rReceiving objects:  35% (3917/11190)\rReceiving objects:  36% (4029/11190)\rReceiving objects:  37% (4141/11190)\rReceiving objects:  38% (4253/11190)\rReceiving objects:  39% (4365/11190)\rReceiving objects:  40% (4476/11190)\rReceiving objects:  41% (4588/11190)\rReceiving objects:  42% (4700/11190)\rReceiving objects:  43% (4812/11190)\rReceiving objects:  44% (4924/11190)\rReceiving objects:  45% (5036/11190)\rReceiving objects:  46% (5148/11190)\rReceiving objects:  47% (5260/11190)\rReceiving objects:  48% (5372/11190)\rReceiving objects:  49% (5484/11190)\rReceiving objects:  50% (5595/11190)\rReceiving objects:  51% (5707/11190)\rReceiving objects:  52% (5819/11190)\rReceiving objects:  53% (5931/11190)\rReceiving objects:  54% (6043/11190)\rReceiving objects:  55% (6155/11190)\rReceiving objects:  56% (6267/11190)\rReceiving objects:  57% (6379/11190)\rReceiving objects:  58% (6491/11190)\rReceiving objects:  59% (6603/11190)\rReceiving objects:  60% (6714/11190)\rReceiving objects:  61% (6826/11190)\rReceiving objects:  62% (6938/11190)\rReceiving objects:  63% (7050/11190)\rReceiving objects:  64% (7162/11190)\rReceiving objects:  65% (7274/11190)\rReceiving objects:  66% (7386/11190)\rReceiving objects:  67% (7498/11190)\rReceiving objects:  68% (7610/11190)\rReceiving objects:  69% (7722/11190)\rReceiving objects:  70% (7833/11190)\rReceiving objects:  71% (7945/11190)\rReceiving objects:  72% (8057/11190)\rReceiving objects:  73% (8169/11190)\rReceiving objects:  74% (8281/11190)\rReceiving objects:  75% (8393/11190)\rReceiving objects:  76% (8505/11190)\rReceiving objects:  77% (8617/11190)\rReceiving objects:  78% (8729/11190)\rReceiving objects:  79% (8841/11190)\rReceiving objects:  80% (8952/11190)\rReceiving objects:  81% (9064/11190)\rReceiving objects:  82% (9176/11190)\rReceiving objects:  83% (9288/11190)\rReceiving objects:  84% (9400/11190)\rReceiving objects:  85% (9512/11190)\rReceiving objects:  86% (9624/11190)\rReceiving objects:  87% (9736/11190)\rReceiving objects:  88% (9848/11190)\rReceiving objects:  89% (9960/11190)\rReceiving objects:  89% (9982/11190), 44.86 MiB | 44.89 MiB/s\rReceiving objects:  89% (9983/11190), 100.81 MiB | 50.43 MiB/s\rReceiving objects:  90% (10071/11190), 100.81 MiB | 50.43 MiB/s\rReceiving objects:  91% (10183/11190), 100.81 MiB | 50.43 MiB/s\rReceiving objects:  92% (10295/11190), 100.81 MiB | 50.43 MiB/s\rReceiving objects:  93% (10407/11190), 100.81 MiB | 50.43 MiB/s\rReceiving objects:  94% (10519/11190), 100.81 MiB | 50.43 MiB/s\rReceiving objects:  95% (10631/11190), 100.81 MiB | 50.43 MiB/s\rReceiving objects:  96% (10743/11190), 100.81 MiB | 50.43 MiB/s\rReceiving objects:  97% (10855/11190), 128.04 MiB | 51.23 MiB/s\rReceiving objects:  98% (10967/11190), 128.04 MiB | 51.23 MiB/s\rReceiving objects:  99% (11079/11190), 128.04 MiB | 51.23 MiB/s\rremote: Total 11190 (delta 1523), reused 2489 (delta 1502), pack-reused 8592 (from 1)\u001b[K\r\nReceiving objects: 100% (11190/11190), 128.04 MiB | 51.23 MiB/s\rReceiving objects: 100% (11190/11190), 130.08 MiB | 50.53 MiB/s, done.\r\nResolving deltas:   0% (0/6922)\rResolving deltas:   1% (70/6922)\rResolving deltas:   2% (139/6922)\rResolving deltas:   3% (208/6922)\rResolving deltas:   4% (277/6922)\rResolving deltas:   5% (347/6922)\rResolving deltas:   6% (416/6922)\rResolving deltas:   7% (485/6922)\rResolving deltas:   8% (554/6922)\rResolving deltas:   9% (623/6922)\rResolving deltas:  10% (693/6922)\rResolving deltas:  11% (762/6922)\rResolving deltas:  12% (831/6922)\rResolving deltas:  13% (900/6922)\rResolving deltas:  14% (970/6922)\rResolving deltas:  15% (1039/6922)\rResolving deltas:  16% (1108/6922)\rResolving deltas:  17% (1177/6922)\rResolving deltas:  18% (1246/6922)\rResolving deltas:  19% (1316/6922)\rResolving deltas:  20% (1385/6922)\rResolving deltas:  21% (1454/6922)\rResolving deltas:  22% (1523/6922)\rResolving deltas:  23% (1593/6922)\rResolving deltas:  24% (1662/6922)\rResolving deltas:  25% (1731/6922)\rResolving deltas:  26% (1800/6922)\rResolving deltas:  27% (1869/6922)\rResolving deltas:  28% (1939/6922)\rResolving deltas:  29% (2008/6922)\rResolving deltas:  30% (2077/6922)\rResolving deltas:  31% (2146/6922)\rResolving deltas:  32% (2216/6922)\rResolving deltas:  33% (2285/6922)\rResolving deltas:  34% (2354/6922)\rResolving deltas:  35% (2423/6922)\rResolving deltas:  36% (2492/6922)\rResolving deltas:  37% (2562/6922)\rResolving deltas:  38% (2631/6922)\rResolving deltas:  39% (2700/6922)\rResolving deltas:  40% (2769/6922)\rResolving deltas:  41% (2839/6922)\rResolving deltas:  42% (2908/6922)\rResolving deltas:  43% (2977/6922)\rResolving deltas:  44% (3046/6922)\rResolving deltas:  45% (3115/6922)\rResolving deltas:  46% (3185/6922)\rResolving deltas:  47% (3254/6922)\rResolving deltas:  48% (3323/6922)\rResolving deltas:  49% (3392/6922)\rResolving deltas:  50% (3461/6922)\rResolving deltas:  51% (3531/6922)\rResolving deltas:  52% (3600/6922)\rResolving deltas:  53% (3669/6922)\rResolving deltas:  54% (3738/6922)\rResolving deltas:  55% (3808/6922)\rResolving deltas:  56% (3877/6922)\rResolving deltas:  57% (3946/6922)\rResolving deltas:  58% (4015/6922)\rResolving deltas:  59% (4084/6922)\rResolving deltas:  60% (4154/6922)\rResolving deltas:  61% (4223/6922)\rResolving deltas:  62% (4292/6922)\rResolving deltas:  63% (4361/6922)\rResolving deltas:  64% (4431/6922)\rResolving deltas:  65% (4500/6922)\rResolving deltas:  66% (4569/6922)\rResolving deltas:  67% (4638/6922)\rResolving deltas:  68% (4707/6922)\rResolving deltas:  69% (4777/6922)\rResolving deltas:  70% (4846/6922)\rResolving deltas:  71% (4915/6922)\rResolving deltas:  72% (4984/6922)\rResolving deltas:  73% (5054/6922)\rResolving deltas:  74% (5123/6922)\rResolving deltas:  75% (5192/6922)\rResolving deltas:  76% (5261/6922)\rResolving deltas:  77% (5330/6922)\rResolving deltas:  78% (5400/6922)\rResolving deltas:  79% (5469/6922)\rResolving deltas:  80% (5538/6922)\rResolving deltas:  81% (5607/6922)\rResolving deltas:  82% (5677/6922)\rResolving deltas:  83% (5746/6922)\rResolving deltas:  84% (5815/6922)\rResolving deltas:  85% (5884/6922)\rResolving deltas:  86% (5953/6922)\rResolving deltas:  87% (6023/6922)\rResolving deltas:  88% (6092/6922)\rResolving deltas:  89% (6161/6922)\rResolving deltas:  90% (6230/6922)\rResolving deltas:  91% (6300/6922)\rResolving deltas:  92% (6369/6922)\rResolving deltas:  93% (6438/6922)\rResolving deltas:  94% (6507/6922)\rResolving deltas:  95% (6576/6922)\rResolving deltas:  96% (6646/6922)\rResolving deltas:  97% (6715/6922)\rResolving deltas:  98% (6784/6922)\rResolving deltas:  99% (6853/6922)\rResolving deltas: 100% (6922/6922)\rResolving deltas: 100% (6922/6922), done.\r\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!git clone https://github.com/RUCAIBox/TextBox.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6217c8b8",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359d4eea",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427c049d",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9463dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Install model repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfec161",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6040f61c",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca36647",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b350bc40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/TextBox\n/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd ./TextBox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7868aaaf",
   "metadata": {},
   "source": [
    "### Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d47757",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5150407",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c576e153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LICENSE      TODOS.md\t instructions\t   run_hyper.py       textbox\r\nLeaderboard  asset\t packages\t   run_multi_seed.py\r\nMANIFEST.in  dataset\t requirements.txt  run_textbox.py\r\nREADME.md    install.sh  run_analysis.py   style.cfg\r\n\r\n████████╗███████╗██╗  ██╗████████╗██████╗  ██████╗ ██╗  ██╗\r\n╚══██╔══╝██╔════╝╚██╗██╔╝╚══██╔══╝██╔══██╗██╔═══██╗╚██╗██╔╝\r\n   ██║   █████╗   ╚███╔╝    ██║   ██████╔╝██║   ██║ ╚███╔╝\r\n   ██║   ██╔══╝   ██╔██╗    ██║   ██╔══██╗██║   ██║ ██╔██╗\r\n   ██║   ███████╗██╔╝ ██╗   ██║   ██████╔╝╚██████╔╝██╔╝ ██╗\r\n   ╚═╝   ╚══════╝╚═╝  ╚═╝   ╚═╝   ╚═════╝  ╚═════╝ ╚═╝  ╚═╝\r\n\r\nInstallation may take a few minutes.\r\n\u001b[0;32mInstalling requirements ...\u001b[0m\r\nCollecting git+https://github.com/microsoft/fastseq.git (from -r requirements.txt (line 19))\r\n  Cloning https://github.com/microsoft/fastseq.git to /tmp/pip-req-build-jrt7y_df\r\n  Running command git clone --filter=blob:none --quiet https://github.com/microsoft/fastseq.git /tmp/pip-req-build-jrt7y_df\r\n  Resolved https://github.com/microsoft/fastseq.git to commit 7b19f8fe5f2e66edee1aef286bb5e719eca15a7c\r\n  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \berror\r\n  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\r\n  \r\n  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\r\n  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\r\n  \u001b[31m╰─>\u001b[0m \u001b[31m[17 lines of output]\u001b[0m\r\n  \u001b[31m   \u001b[0m Traceback (most recent call last):\r\n  \u001b[31m   \u001b[0m   File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\r\n  \u001b[31m   \u001b[0m     main()\r\n  \u001b[31m   \u001b[0m   File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\r\n  \u001b[31m   \u001b[0m     json_out['return_val'] = hook(**hook_input['kwargs'])\r\n  \u001b[31m   \u001b[0m   File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 118, in get_requires_for_build_wheel\r\n  \u001b[31m   \u001b[0m     return hook(config_settings)\r\n  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-no8s8a5u/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 332, in get_requires_for_build_wheel\r\n  \u001b[31m   \u001b[0m     return self._get_build_requires(config_settings, requirements=[])\r\n  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-no8s8a5u/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 302, in _get_build_requires\r\n  \u001b[31m   \u001b[0m     self.run_setup()\r\n  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-no8s8a5u/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 502, in run_setup\r\n  \u001b[31m   \u001b[0m     super().run_setup(setup_script=setup_script)\r\n  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-no8s8a5u/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 318, in run_setup\r\n  \u001b[31m   \u001b[0m     exec(code, locals())\r\n  \u001b[31m   \u001b[0m   File \"<string>\", line 5, in <module>\r\n  \u001b[31m   \u001b[0m ModuleNotFoundError: No module named 'torch'\r\n  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\r\n  \r\n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\r\n\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\r\n\r\n\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\r\n\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\r\n\u001b[31m╰─>\u001b[0m See above for output.\r\n\r\n\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\r\n\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n\u001b[?25h\u001b[0;32mInstalling requirements (rouge) ...\u001b[0m\r\nCollecting git+https://github.com/pltrdy/pyrouge.git\r\n  Cloning https://github.com/pltrdy/pyrouge.git to /tmp/pip-req-build-_ta8rfjf\r\n  Running command git clone --filter=blob:none --quiet https://github.com/pltrdy/pyrouge.git /tmp/pip-req-build-_ta8rfjf\r\n  Resolved https://github.com/pltrdy/pyrouge.git to commit dfdff03cff1b69bfb18d42dadfb79c79f1caaa18\r\n  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\r\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n\u001b[?25hBuilding wheels for collected packages: pyrouge\r\n  Building wheel for pyrouge (pyproject.toml) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\r\n\u001b[?25h  Created wheel for pyrouge: filename=pyrouge-0.1.3-py3-none-any.whl size=191911 sha256=ac95a553e5e52fee843366f07167c5160302cb0faaa88c5a98a139242fe82df7\r\n  Stored in directory: /tmp/pip-ephem-wheel-cache-crrg5bo8/wheels/16/e8/c7/28be7897c7074d902b680d0a8ebf69e7746dbfca543df33021\r\nSuccessfully built pyrouge\r\nInstalling collected packages: pyrouge\r\nSuccessfully installed pyrouge-0.1.3\r\n\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\nCloning into 'files2rouge'...\r\nremote: Enumerating objects: 278, done.\u001b[K\r\nremote: Counting objects:   5% (1/20)\u001b[K\rremote: Counting objects:  10% (2/20)\u001b[K\rremote: Counting objects:  15% (3/20)\u001b[K\rremote: Counting objects:  20% (4/20)\u001b[K\rremote: Counting objects:  25% (5/20)\u001b[K\rremote: Counting objects:  30% (6/20)\u001b[K\rremote: Counting objects:  35% (7/20)\u001b[K\rremote: Counting objects:  40% (8/20)\u001b[K\rremote: Counting objects:  45% (9/20)\u001b[K\rremote: Counting objects:  50% (10/20)\u001b[K\rremote: Counting objects:  55% (11/20)\u001b[K\rremote: Counting objects:  60% (12/20)\u001b[K\rremote: Counting objects:  65% (13/20)\u001b[K\rremote: Counting objects:  70% (14/20)\u001b[K\rremote: Counting objects:  75% (15/20)\u001b[K\rremote: Counting objects:  80% (16/20)\u001b[K\rremote: Counting objects:  85% (17/20)\u001b[K\rremote: Counting objects:  90% (18/20)\u001b[K\rremote: Counting objects:  95% (19/20)\u001b[K\rremote: Counting objects: 100% (20/20)\u001b[K\rremote: Counting objects: 100% (20/20), done.\u001b[K\r\nremote: Compressing objects:   7% (1/14)\u001b[K\rremote: Compressing objects:  14% (2/14)\u001b[K\rremote: Compressing objects:  21% (3/14)\u001b[K\rremote: Compressing objects:  28% (4/14)\u001b[K\rremote: Compressing objects:  35% (5/14)\u001b[K\rremote: Compressing objects:  42% (6/14)\u001b[K\rremote: Compressing objects:  50% (7/14)\u001b[K\rremote: Compressing objects:  57% (8/14)\u001b[K\rremote: Compressing objects:  64% (9/14)\u001b[K\rremote: Compressing objects:  71% (10/14)\u001b[K\rremote: Compressing objects:  78% (11/14)\u001b[K\rremote: Compressing objects:  85% (12/14)\u001b[K\rremote: Compressing objects:  92% (13/14)\u001b[K\rremote: Compressing objects: 100% (14/14)\u001b[K\rremote: Compressing objects: 100% (14/14), done.\u001b[K\r\nReceiving objects:   0% (1/278)\rReceiving objects:   1% (3/278)\rReceiving objects:   2% (6/278)\rReceiving objects:   3% (9/278)\rReceiving objects:   4% (12/278)\rReceiving objects:   5% (14/278)\rReceiving objects:   6% (17/278)\rReceiving objects:   7% (20/278)\rReceiving objects:   8% (23/278)\rReceiving objects:   9% (26/278)\rReceiving objects:  10% (28/278)\rReceiving objects:  11% (31/278)\rReceiving objects:  12% (34/278)\rReceiving objects:  13% (37/278)\rReceiving objects:  14% (39/278)\rReceiving objects:  15% (42/278)\rReceiving objects:  16% (45/278)\rReceiving objects:  17% (48/278)\rReceiving objects:  18% (51/278)\rReceiving objects:  19% (53/278)\rReceiving objects:  20% (56/278)\rReceiving objects:  21% (59/278)\rReceiving objects:  22% (62/278)\rReceiving objects:  23% (64/278)\rReceiving objects:  24% (67/278)\rReceiving objects:  25% (70/278)\rReceiving objects:  26% (73/278)\rReceiving objects:  27% (76/278)\rReceiving objects:  28% (78/278)\rReceiving objects:  29% (81/278)\rReceiving objects:  30% (84/278)\rReceiving objects:  31% (87/278)\rReceiving objects:  32% (89/278)\rReceiving objects:  33% (92/278)\rReceiving objects:  34% (95/278)\rReceiving objects:  35% (98/278)\rReceiving objects:  36% (101/278)\rReceiving objects:  37% (103/278)\rReceiving objects:  38% (106/278)\rReceiving objects:  39% (109/278)\rremote: Total 278 (delta 9), reused 11 (delta 6), pack-reused 258 (from 1)\u001b[K\r\nReceiving objects:  40% (112/278)\rReceiving objects:  41% (114/278)\rReceiving objects:  42% (117/278)\rReceiving objects:  43% (120/278)\rReceiving objects:  44% (123/278)\rReceiving objects:  45% (126/278)\rReceiving objects:  46% (128/278)\rReceiving objects:  47% (131/278)\rReceiving objects:  48% (134/278)\rReceiving objects:  49% (137/278)\rReceiving objects:  50% (139/278)\rReceiving objects:  51% (142/278)\rReceiving objects:  52% (145/278)\rReceiving objects:  53% (148/278)\rReceiving objects:  54% (151/278)\rReceiving objects:  55% (153/278)\rReceiving objects:  56% (156/278)\rReceiving objects:  57% (159/278)\rReceiving objects:  58% (162/278)\rReceiving objects:  59% (165/278)\rReceiving objects:  60% (167/278)\rReceiving objects:  61% (170/278)\rReceiving objects:  62% (173/278)\rReceiving objects:  63% (176/278)\rReceiving objects:  64% (178/278)\rReceiving objects:  65% (181/278)\rReceiving objects:  66% (184/278)\rReceiving objects:  67% (187/278)\rReceiving objects:  68% (190/278)\rReceiving objects:  69% (192/278)\rReceiving objects:  70% (195/278)\rReceiving objects:  71% (198/278)\rReceiving objects:  72% (201/278)\rReceiving objects:  73% (203/278)\rReceiving objects:  74% (206/278)\rReceiving objects:  75% (209/278)\rReceiving objects:  76% (212/278)\rReceiving objects:  77% (215/278)\rReceiving objects:  78% (217/278)\rReceiving objects:  79% (220/278)\rReceiving objects:  80% (223/278)\rReceiving objects:  81% (226/278)\rReceiving objects:  82% (228/278)\rReceiving objects:  83% (231/278)\rReceiving objects:  84% (234/278)\rReceiving objects:  85% (237/278)\rReceiving objects:  86% (240/278)\rReceiving objects:  87% (242/278)\rReceiving objects:  88% (245/278)\rReceiving objects:  89% (248/278)\rReceiving objects:  90% (251/278)\rReceiving objects:  91% (253/278)\rReceiving objects:  92% (256/278)\rReceiving objects:  93% (259/278)\rReceiving objects:  94% (262/278)\rReceiving objects:  95% (265/278)\rReceiving objects:  96% (267/278)\rReceiving objects:  97% (270/278)\rReceiving objects:  98% (273/278)\rReceiving objects:  99% (276/278)\rReceiving objects: 100% (278/278)\rReceiving objects: 100% (278/278), 212.16 KiB | 19.29 MiB/s, done.\r\nResolving deltas:   0% (0/133)\rResolving deltas:   1% (2/133)\rResolving deltas:   2% (3/133)\rResolving deltas:   3% (4/133)\rResolving deltas:   4% (6/133)\rResolving deltas:   5% (7/133)\rResolving deltas:   6% (8/133)\rResolving deltas:   7% (10/133)\rResolving deltas:   8% (11/133)\rResolving deltas:   9% (12/133)\rResolving deltas:  10% (14/133)\rResolving deltas:  11% (15/133)\rResolving deltas:  12% (16/133)\rResolving deltas:  13% (18/133)\rResolving deltas:  14% (19/133)\rResolving deltas:  15% (20/133)\rResolving deltas:  16% (22/133)\rResolving deltas:  17% (23/133)\rResolving deltas:  18% (24/133)\rResolving deltas:  19% (26/133)\rResolving deltas:  20% (27/133)\rResolving deltas:  21% (28/133)\rResolving deltas:  22% (30/133)\rResolving deltas:  23% (31/133)\rResolving deltas:  24% (32/133)\rResolving deltas:  25% (34/133)\rResolving deltas:  26% (35/133)\rResolving deltas:  27% (36/133)\rResolving deltas:  28% (38/133)\rResolving deltas:  29% (39/133)\rResolving deltas:  30% (40/133)\rResolving deltas:  31% (42/133)\rResolving deltas:  33% (45/133)\rResolving deltas:  34% (46/133)\rResolving deltas:  35% (47/133)\rResolving deltas:  36% (48/133)\rResolving deltas:  37% (50/133)\rResolving deltas:  38% (51/133)\rResolving deltas:  39% (52/133)\rResolving deltas:  40% (54/133)\rResolving deltas:  41% (55/133)\rResolving deltas:  42% (56/133)\rResolving deltas:  43% (58/133)\rResolving deltas:  44% (59/133)\rResolving deltas:  45% (60/133)\rResolving deltas:  46% (62/133)\rResolving deltas:  47% (63/133)\rResolving deltas:  48% (64/133)\rResolving deltas:  49% (66/133)\rResolving deltas:  50% (67/133)\rResolving deltas:  51% (68/133)\rResolving deltas:  52% (70/133)\rResolving deltas:  53% (71/133)\rResolving deltas:  54% (72/133)\rResolving deltas:  55% (74/133)\rResolving deltas:  56% (75/133)\rResolving deltas:  57% (76/133)\rResolving deltas:  58% (78/133)\rResolving deltas:  59% (79/133)\rResolving deltas:  60% (80/133)\rResolving deltas:  61% (82/133)\rResolving deltas:  62% (83/133)\rResolving deltas:  63% (84/133)\rResolving deltas:  64% (86/133)\rResolving deltas:  65% (87/133)\rResolving deltas:  66% (88/133)\rResolving deltas:  67% (90/133)\rResolving deltas:  68% (91/133)\rResolving deltas:  69% (92/133)\rResolving deltas:  70% (94/133)\rResolving deltas:  71% (95/133)\rResolving deltas:  72% (96/133)\rResolving deltas:  73% (98/133)\rResolving deltas:  74% (99/133)\rResolving deltas:  75% (100/133)\rResolving deltas:  76% (102/133)\rResolving deltas:  77% (103/133)\rResolving deltas:  78% (104/133)\rResolving deltas:  79% (106/133)\rResolving deltas:  80% (107/133)\rResolving deltas:  81% (108/133)\rResolving deltas:  82% (110/133)\rResolving deltas:  83% (111/133)\rResolving deltas:  84% (112/133)\rResolving deltas:  85% (114/133)\rResolving deltas:  86% (115/133)\rResolving deltas:  87% (116/133)\rResolving deltas:  88% (118/133)\rResolving deltas:  89% (119/133)\rResolving deltas:  90% (120/133)\rResolving deltas:  91% (122/133)\rResolving deltas:  92% (123/133)\rResolving deltas:  93% (124/133)\rResolving deltas:  94% (126/133)\rResolving deltas:  95% (127/133)\rResolving deltas:  96% (128/133)\rResolving deltas:  97% (130/133)\rResolving deltas:  98% (131/133)\rResolving deltas:  99% (132/133)\rResolving deltas: 100% (133/133)\rResolving deltas: 100% (133/133), done.\r\nfiles2rouge uses scripts and tools that will not be stored with the python package\r\nwhere do you want to save it? [default: /root/.files2rouge/]Copying './files2rouge/RELEASE-1.5.5/' to '/root/.files2rouge/'\r\nrunning install\r\n/usr/local/lib/python3.10/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\r\n!!\r\n\r\n        ********************************************************************************\r\n        Please avoid running ``setup.py`` directly.\r\n        Instead, use pypa/build, pypa/installer or other\r\n        standards-based tools.\r\n\r\n        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\r\n        ********************************************************************************\r\n\r\n!!\r\n  self.initialize_options()\r\n/usr/local/lib/python3.10/site-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\r\n!!\r\n\r\n        ********************************************************************************\r\n        Please avoid running ``setup.py`` and ``easy_install``.\r\n        Instead, use pypa/build, pypa/installer or other\r\n        standards-based tools.\r\n\r\n        See https://github.com/pypa/setuptools/issues/917 for details.\r\n        ********************************************************************************\r\n\r\n!!\r\n  self.initialize_options()\r\nrunning bdist_egg\r\nrunning egg_info\r\ncreating files2rouge.egg-info\r\nwriting files2rouge.egg-info/PKG-INFO\r\nwriting dependency_links to files2rouge.egg-info/dependency_links.txt\r\nwriting entry points to files2rouge.egg-info/entry_points.txt\r\nwriting top-level names to files2rouge.egg-info/top_level.txt\r\nwriting manifest file 'files2rouge.egg-info/SOURCES.txt'\r\nreading manifest file 'files2rouge.egg-info/SOURCES.txt'\r\nreading manifest template 'MANIFEST.in'\r\nadding license file 'LICENSE'\r\nwriting manifest file 'files2rouge.egg-info/SOURCES.txt'\r\ninstalling library code to build/bdist.linux-x86_64/egg\r\nrunning install_lib\r\nrunning build_py\r\ncreating build\r\ncreating build/lib\r\ncreating build/lib/files2rouge\r\ncopying files2rouge/__init__.py -> build/lib/files2rouge\r\ncopying files2rouge/files2rouge.py -> build/lib/files2rouge\r\ncopying files2rouge/settings.py -> build/lib/files2rouge\r\ncopying files2rouge/utils.py -> build/lib/files2rouge\r\ncopying files2rouge/settings.json -> build/lib/files2rouge\r\ncreating build/bdist.linux-x86_64\r\ncreating build/bdist.linux-x86_64/egg\r\ncreating build/bdist.linux-x86_64/egg/files2rouge\r\ncopying build/lib/files2rouge/__init__.py -> build/bdist.linux-x86_64/egg/files2rouge\r\ncopying build/lib/files2rouge/files2rouge.py -> build/bdist.linux-x86_64/egg/files2rouge\r\ncopying build/lib/files2rouge/settings.py -> build/bdist.linux-x86_64/egg/files2rouge\r\ncopying build/lib/files2rouge/utils.py -> build/bdist.linux-x86_64/egg/files2rouge\r\ncopying build/lib/files2rouge/settings.json -> build/bdist.linux-x86_64/egg/files2rouge\r\nbyte-compiling build/bdist.linux-x86_64/egg/files2rouge/__init__.py to __init__.cpython-310.pyc\r\nbyte-compiling build/bdist.linux-x86_64/egg/files2rouge/files2rouge.py to files2rouge.cpython-310.pyc\r\nbyte-compiling build/bdist.linux-x86_64/egg/files2rouge/settings.py to settings.cpython-310.pyc\r\nbyte-compiling build/bdist.linux-x86_64/egg/files2rouge/utils.py to utils.cpython-310.pyc\r\ncreating build/bdist.linux-x86_64/egg/EGG-INFO\r\ncopying files2rouge.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\r\ncopying files2rouge.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\r\ncopying files2rouge.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\r\ncopying files2rouge.egg-info/entry_points.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\r\ncopying files2rouge.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\r\nzip_safe flag not set; analyzing archive contents...\r\nfiles2rouge.__pycache__.settings.cpython-310: module references __file__\r\ncreating dist\r\ncreating 'dist/files2rouge-2.1.0-py3.10.egg' and adding 'build/bdist.linux-x86_64/egg' to it\r\nremoving 'build/bdist.linux-x86_64/egg' (and everything under it)\r\nProcessing files2rouge-2.1.0-py3.10.egg\r\ncreating /usr/local/lib/python3.10/site-packages/files2rouge-2.1.0-py3.10.egg\r\nExtracting files2rouge-2.1.0-py3.10.egg to /usr/local/lib/python3.10/site-packages\r\nAdding files2rouge 2.1.0 to easy-install.pth file\r\nInstalling files2rouge script to /usr/local/bin\r\n\r\nInstalled /usr/local/lib/python3.10/site-packages/files2rouge-2.1.0-py3.10.egg\r\nProcessing dependencies for files2rouge==2.1.0\r\nFinished processing dependencies for files2rouge==2.1.0\r\n\u001b[33mWARNING: Skipping py-rouge as it is not installed.\u001b[0m\u001b[33m\r\n\u001b[0mCollecting rouge\r\n  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\r\nRequirement already satisfied: six in /usr/local/lib/python3.10/site-packages (from rouge) (1.16.0)\r\nDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\r\nInstalling collected packages: rouge\r\nSuccessfully installed rouge-1.0.1\r\n\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ls\n",
    "!echo \"\" | bash install.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463943f0",
   "metadata": {},
   "source": [
    "### Step 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ca97ae",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70bbbcc",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f692013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Install additional packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6b1445",
   "metadata": {},
   "source": [
    "### Step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f98f089",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f6e903",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d504c733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Inspect expected data format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8a0577",
   "metadata": {},
   "source": [
    "### Step 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0ed12d",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55689e68",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96be5778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Amanda: I baked cookies. Do you want some? [SEP] Jerry: Sure! [SEP] Amanda: I'll bring you tomorrow :-)\"\r\n'Olivia: Who are you voting for in this election? [SEP] Oliver: Liberals as always. [SEP] Olivia: Me too!! [SEP] Oliver: Great'\r\n'Amanda baked cookies and will bring Jerry some tomorrow.'\r\n'Olivia and Olivier are voting for liberals in this election.'\r\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!head -2 dataset/samsum/train.src\n",
    "!head -2 dataset/samsum/train.tgt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3869f10a",
   "metadata": {},
   "source": [
    "### Step 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782ad9e4",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b6b059",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f17e1a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Download and format dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06502e82",
   "metadata": {},
   "source": [
    "### Step 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732d78f8",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73ddf94",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4da4d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Error: ModuleNotFoundError: No module named 'datasets'\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)Cell \u001b[0;32mIn[34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m      6\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset/xsum\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "data_dir = Path(\"dataset/xsum\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "all_datasets = {\n",
    "    \"train\": load_dataset(\"xsum\", split=\"train\"),\n",
    "    \"valid\": load_dataset(\"xsum\", split=\"validation\"),\n",
    "    \"test\": load_dataset(\"xsum\", split=\"test\"),\n",
    "}\n",
    "\n",
    "for split, dataset in all_datasets.items():\n",
    "    pd.Series(dataset[\"document\"][:10]).str.replace(\"\\n\", \" [SEP] \").to_csv(f\"dataset/xsum/{split}.src\", index=False, header=False)\n",
    "    pd.Series(dataset[\"summary\"][:10]).to_csv(f\"dataset/xsum/{split}.tgt\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4290d9d",
   "metadata": {},
   "source": [
    "### Step 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193f568f",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2b91cd",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc91d1ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "head: cannot open 'dataset/xsum/test.src' for reading: No such file or directory\r\nhead: cannot open 'dataset/xsum/test.tgt' for reading: No such file or directory\r\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!head -1 dataset/xsum/test.src\n",
    "!head -1 dataset/xsum/test.tgt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199b2354",
   "metadata": {},
   "source": [
    "### Step 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec6ed3f",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46278fbc",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cbe4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\nTrue"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d3dd03",
   "metadata": {},
   "source": [
    "### Step 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a0fd02",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd3a59d",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57698004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Use the same config as in samsum dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a89ba98",
   "metadata": {},
   "source": [
    "### Step 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c888465",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5727f1f1",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cf7e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!cp textbox/properties/dataset/samsum.yaml textbox/properties/dataset/xsum.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c910d8",
   "metadata": {},
   "source": [
    "### Step 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30849105",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398ccc68",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0922abe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Run model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a63b7e",
   "metadata": {},
   "source": [
    "### Step 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7960a64b",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90556918",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7839515c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf4f302",
   "metadata": {},
   "source": [
    "### Step 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1107d6fa",
   "metadata": {},
   "source": [
    "Execution Start time: 04:38:19, End time: 04:38:20 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb81a47",
   "metadata": {},
   "source": [
    "**Thought**: Now that the environment is set up and the dataset is prepared, let's proceed to run the training and evaluation command for the BART model on the xsum dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6478a12",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab523058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Traceback (most recent call last):\n  File \"/content/TextBox/run_textbox.py\", line 2, in <module>\n    from textbox import run_textbox\n  File \"/content/TextBox/textbox/__init__.py\", line 4, in <module>\n    from textbox.utils.enum_type import PLM_MODELS, CLM_MODELS, SEQ2SEQ_MODELS, SpecialTokens, RNN_MODELS\n  File \"/content/TextBox/textbox/utils/__init__.py\", line 1, in <module>\n    from textbox.utils.logger import init_logger\n  File \"/content/TextBox/textbox/utils/logger.py\", line 12, in <module>\n    from accelerate.logging import get_logger\nModuleNotFoundError: No module named 'accelerate'\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python run_textbox.py --model=BART --dataset=xsum --model_path=facebook/bart-base --epochs=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed018df",
   "metadata": {},
   "source": [
    "### Step 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a406861",
   "metadata": {},
   "source": [
    "Execution Start time: 04:38:21, End time: 04:38:24 (duration: 3 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a181b67c",
   "metadata": {},
   "source": [
    "**Thought**: It seems like the `accelerate` package is missing. Let's install it using pip.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220c91f2",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e6fc89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting accelerate\n  Downloading accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/site-packages (from accelerate) (1.25.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from accelerate) (24.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/site-packages (from accelerate) (5.9.8)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/site-packages (from accelerate) (2.2.1+cpu)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/site-packages (from accelerate) (0.23.3)\nRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/site-packages (from accelerate) (0.4.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2023.6.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.7)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.6.2)\nRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading accelerate-0.33.0-py3-none-any.whl (315 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/315.1 kB ? eta -:--:--\n\n\u001b[?25hInstalling collected packages: accelerate\nSuccessfully installed accelerate-0.33.0\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78b03d3",
   "metadata": {},
   "source": [
    "### Step 19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767dcd02",
   "metadata": {},
   "source": [
    "Execution Start time: 04:38:25, End time: 04:38:29 (duration: 3 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eb6c10",
   "metadata": {},
   "source": [
    "**Thought**: The `accelerate` package has been successfully installed. Let's try running the model training and evaluation command again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdf93dc",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5dd744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Traceback (most recent call last):\n  File \"/content/TextBox/run_textbox.py\", line 2, in <module>\n    from textbox import run_textbox\n  File \"/content/TextBox/textbox/__init__.py\", line 4, in <module>\n    from textbox.utils.enum_type import PLM_MODELS, CLM_MODELS, SEQ2SEQ_MODELS, SpecialTokens, RNN_MODELS\n  File \"/content/TextBox/textbox/utils/__init__.py\", line 1, in <module>\n    from textbox.utils.logger import init_logger\n  File \"/content/TextBox/textbox/utils/logger.py\", line 15, in <module>\n    from colorama import init, Fore, Style\nModuleNotFoundError: No module named 'colorama'\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python run_textbox.py --model=BART --dataset=xsum --model_path=facebook/bart-base --epochs=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8af5205",
   "metadata": {},
   "source": [
    "### Step 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c953b2f9",
   "metadata": {},
   "source": [
    "Execution Start time: 04:38:29, End time: 04:38:33 (duration: 3 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b45e4a",
   "metadata": {},
   "source": [
    "**Thought**: The `colorama` package is missing. Let's install it using pip.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8350a411",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4f7ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting colorama\n  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\nDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\nInstalling collected packages: colorama\nSuccessfully installed colorama-0.4.6\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install colorama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb20bcbb",
   "metadata": {},
   "source": [
    "### Step 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17536db",
   "metadata": {},
   "source": [
    "Execution Start time: 04:38:34, End time: 04:38:39 (duration: 4 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd067cd8",
   "metadata": {},
   "source": [
    "**Thought**: The `colorama` package has been successfully installed. Let's attempt to run the model training and evaluation command once more.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20ec4dd",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c6cd6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Traceback (most recent call last):\n  File \"/content/TextBox/run_textbox.py\", line 2, in <module>\n    from textbox import run_textbox\n  File \"/content/TextBox/textbox/__init__.py\", line 8, in <module>\n    from textbox.quick_start.hyper_tuning import run_hyper\n  File \"/content/TextBox/textbox/quick_start/hyper_tuning.py\", line 14, in <module>\n    from .experiment import Experiment\n  File \"/content/TextBox/textbox/quick_start/experiment.py\", line 13, in <module>\n    from ..trainer.trainer import Trainer\n  File \"/content/TextBox/textbox/trainer/__init__.py\", line 1, in <module>\n    from textbox.trainer.trainer import Trainer\n  File \"/content/TextBox/textbox/trainer/trainer.py\", line 16, in <module>\n    from textbox.utils.dashboard import get_dashboard, Timestamp, EpochTracker\n  File \"/content/TextBox/textbox/utils/dashboard.py\", line 13, in <module>\n    import wandb\nModuleNotFoundError: No module named 'wandb'\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python run_textbox.py --model=BART --dataset=xsum --model_path=facebook/bart-base --epochs=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d71fa75",
   "metadata": {},
   "source": [
    "### Step 22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1af3cb",
   "metadata": {},
   "source": [
    "Execution Start time: 04:38:40, End time: 04:38:46 (duration: 5 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7925b7ee",
   "metadata": {},
   "source": [
    "**Thought**: The `wandb` package is missing. Let's install it using pip.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7bf2fd",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6982cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting wandb\n  Downloading wandb-0.17.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/site-packages (from wandb) (8.1.7)\nCollecting docker-pycreds>=0.4.0 (from wandb)\n  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\nCollecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/site-packages (from wandb) (4.2.2)\nRequirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/site-packages (from wandb) (5.27.1)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/site-packages (from wandb) (5.9.8)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/site-packages (from wandb) (2.31.0)\nCollecting sentry-sdk>=1.0.0 (from wandb)\n  Downloading sentry_sdk-2.13.0-py2.py3-none-any.whl.metadata (9.7 kB)\nCollecting setproctitle (from wandb)\n  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from wandb) (68.1.2)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nCollecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.6.2)\nCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\nDownloading wandb-0.17.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.3 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/9.3 MB ? eta -:--:--\n\n\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\nDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/207.3 kB ? eta -:--:--\n\n\u001b[?25hDownloading sentry_sdk-2.13.0-py2.py3-none-any.whl (309 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/309.1 kB ? eta -:--:--\n\n\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\nDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/62.7 kB ? eta -:--:--\n\n\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\nInstalling collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\nSuccessfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.13.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.7\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7b1037",
   "metadata": {},
   "source": [
    "### Step 23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27172fc5",
   "metadata": {},
   "source": [
    "Execution Start time: 04:38:47, End time: 04:38:53 (duration: 6 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d784638",
   "metadata": {},
   "source": [
    "**Thought**: The `wandb` package has been successfully installed. Let's try running the model training and evaluation command again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5943735",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c4907d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "W&B online. Running your script from this directory will now sync to the cloud.\nDetected kernel version 4.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n22 Aug 04:38    INFO 66 parameters found.\n================================================================================\n\n# General Hyper Parameters: \n\ngpu_id: 0\nuse_gpu: True\ndevice: cpu\nseed: 2020\nreproducibility: True\ncmd: run_textbox.py --model=BART --dataset=xsum --model_path=facebook/bart-base --epochs=1\nfilename: BART-xsum-2024-Aug-22_04-38-51\nsaved_dir: saved/\nstate: INFO\nwandb: online\n\n\n# Training Hyper Parameters: \n\ndo_train: True\ndo_valid: True\noptimizer: adamw\nadafactor_kwargs: {'lr': 0.001, 'scale_parameter': False, 'relative_step': False, 'warmup_init': False}\noptimizer_kwargs: {}\nvalid_steps: 1\nvalid_strategy: epoch\nstopping_steps: 2\nepochs: 1\nlearning_rate: 3e-05\ntrain_batch_size: 4\ngrad_clip: 0.1\naccumulation_steps: 48\ndisable_tqdm: False\nresume_training: True\n\n\n# Evaluation Hyper Parameters: \n\ndo_test: True\nlower_evaluation: True\nmultiref_strategy: max\nbleu_max_ngrams: 4\nbleu_type: nltk\nsmoothing_function: 0\ncorpus_bleu: False\nrouge_max_ngrams: 2\nrouge_type: files2rouge\nmeteor_type: pycocoevalcap\nchrf_type: m-popovic\ndistinct_max_ngrams: 4\ninter_distinct: True\nunique_max_ngrams: 4\nself_bleu_max_ngrams: 4\ntgt_lang: en\nmetrics: ['rouge']\neval_batch_size: 16\ncorpus_meteor: True\n\n\n# Model Hyper Parameters: \n\nmodel: BART\nmodel_name: bart\nmodel_path: facebook/bart-base\nconfig_kwargs: {}\ntokenizer_kwargs: {'use_fast': True}\ngeneration_kwargs: {'num_beams': 5, 'no_repeat_ngram_size': 3, 'early_stopping': True}\nefficient_kwargs: {}\nefficient_methods: []\nefficient_unfreeze_model: False\nlabel_smoothing: 0.1\n\n\n# Dataset Hyper Parameters: \n\ndataset: xsum\ndata_path: dataset/xsum\ntgt_lang: en\nsrc_len: 1024\ntgt_len: 128\ntruncate: tail\nprefix_prompt: Summarize: \nmetrics_for_best_model: ['rouge-1', 'rouge-2', 'rouge-l']\n\n\n# Unrecognized Hyper Parameters: \n\nfind_unused_parameters: False\ntokenizer_add_tokens: []\nload_type: from_pretrained\n\n================================================================================\n/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\nconfig.json:   0%|                                  | 0.00/1.72k [00:00<?, ?B/s]\n\n\nvocab.json:   0%|                                    | 0.00/899k [00:00<?, ?B/s]\n\n\nmerges.txt:   0%|                                    | 0.00/456k [00:00<?, ?B/s]\n\n\ntokenizer.json:   0%|                               | 0.00/1.36M [00:00<?, ?B/s]\n\n22 Aug 04:38    INFO Pretrain type: pretrain disabled\nTraceback (most recent call last):\n  File \"/content/TextBox/run_textbox.py\", line 12, in <module>\n    run_textbox(model=args.model, dataset=args.dataset, config_file_list=args.config_files, config_dict={})\n  File \"/content/TextBox/textbox/quick_start/quick_start.py\", line 20, in run_textbox\n    experiment = Experiment(model, dataset, config_file_list, config_dict)\n  File \"/content/TextBox/textbox/quick_start/experiment.py\", line 56, in __init__\n    self._init_data(self.get_config(), self.accelerator)\n  File \"/content/TextBox/textbox/quick_start/experiment.py\", line 82, in _init_data\n    train_data, valid_data, test_data = data_preparation(config, tokenizer)\n  File \"/content/TextBox/textbox/data/utils.py\", line 23, in data_preparation\n    train_dataset = AbstractDataset(config, 'train')\n  File \"/content/TextBox/textbox/data/abstract_dataset.py\", line 25, in __init__\n    self.source_text = load_data(source_filename, max_length=self.quick_test)\n  File \"/content/TextBox/textbox/data/misc.py\", line 19, in load_data\n    raise ValueError('File {} not exist'.format(os.path.abspath(dataset_path)))\nValueError: File /content/TextBox/dataset/xsum/train.src not exist\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python run_textbox.py --model=BART --dataset=xsum --model_path=facebook/bart-base --epochs=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc91938",
   "metadata": {},
   "source": [
    "### Step 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39c0653",
   "metadata": {},
   "source": [
    "Execution Start time: 04:38:55, End time: 04:39:02 (duration: 6 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e495ea",
   "metadata": {},
   "source": [
    "**Thought**: It seems that the dataset files for the xsum dataset are missing. We need to ensure that the dataset is correctly downloaded and formatted. Let's try to address this by installing the `datasets` library and running the dataset preparation code again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad15106",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef8da97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting datasets\n  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from datasets) (1.25.2)\nCollecting pyarrow>=15.0.0 (from datasets)\n  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\nCollecting dill<0.3.9,>=0.3.0 (from datasets)\n  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from datasets) (2.0.3)\nCollecting requests>=2.32.2 (from datasets)\n  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/site-packages (from datasets) (4.66.4)\nCollecting xxhash (from datasets)\n  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nCollecting multiprocess (from datasets)\n  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2023.6.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/site-packages (from datasets) (3.8.3)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/site-packages (from datasets) (0.23.3)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from datasets) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (2.1.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.0.7)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.6.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDownloading datasets-2.21.0-py3-none-any.whl (527 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/527.3 kB ? eta -:--:--\n\n\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/116.3 kB ? eta -:--:--\n\n\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/39.9 MB ? eta -:--:--\n\n\n\n\n\n\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/64.9 kB ? eta -:--:--\n\n\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/134.8 kB ? eta -:--:--\n\n\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/194.1 kB ? eta -:--:--\n\n\u001b[?25hInstalling collected packages: xxhash, requests, pyarrow, dill, multiprocess, datasets\n  Attempting uninstall: requests\n    Found existing installation: requests 2.31.0\n    Uninstalling requests-2.31.0:\n      Successfully uninstalled requests-2.31.0\nSuccessfully installed datasets-2.21.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 requests-2.32.3 xxhash-3.5.0\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086ee87a",
   "metadata": {},
   "source": [
    "### Step 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5063291",
   "metadata": {},
   "source": [
    "Execution Start time: 04:39:05, End time: 04:39:07 (duration: 2 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d296a9b6",
   "metadata": {},
   "source": [
    "**Thought**: The `datasets` library has been successfully installed. Let's re-run the dataset preparation code to ensure the xsum dataset is correctly formatted.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbc56e6",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515d49b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Error: ValueError: The repository for xsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/xsum.\nPlease pass the argument `trust_remote_code=True` to allow custom code to be run.---------------------------------------------------------------------------ValueError                                Traceback (most recent call last)Cell In[64], line 9\n      5 data_dir = Path(\"dataset/xsum\")\n      6 data_dir.mkdir(exist_ok=True)\n      8 all_datasets = {\n----> 9     \"train\": load_dataset(\"xsum\", split=\"train\"),\n     10     \"valid\": load_dataset(\"xsum\", split=\"validation\"),\n     11     \"test\": load_dataset(\"xsum\", split=\"test\"),\n     12 }\n     14 for split, dataset in all_datasets.items():\n     15     pd.Series(dataset[\"document\"][:10]).str.replace(\"\\n\", \" [SEP] \").to_csv(f\"dataset/xsum/{split}.src\", index=False, header=False)\nFile /usr/local/lib/python3.10/site-packages/datasets/load.py:2606, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\n   2601 verification_mode = VerificationMode(\n   2602     (verification_mode or VerificationMode.BASIC_CHECKS) if not save_infos else VerificationMode.ALL_CHECKS\n   2603 )\n   2605 # Create a dataset builder\n-> 2606 builder_instance = load_dataset_builder(\n   2607     path=path,\n   2608     name=name,\n   2609     data_dir=data_dir,\n   2610     data_files=data_files,\n   2611     cache_dir=cache_dir,\n   2612     features=features,\n   2613     download_config=download_config,\n   2614     download_mode=download_mode,\n   2615     revision=revision,\n   2616     token=token,\n   2617     storage_options=storage_options,\n   2618     trust_remote_code=trust_remote_code,\n   2619     _require_default_config_name=name is None,\n   2620     **config_kwargs,\n   2621 )\n   2623 # Return iterable dataset in case of streaming\n   2624 if streaming:\nFile /usr/local/lib/python3.10/site-packages/datasets/load.py:2277, in load_dataset_builder(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\n   2275     download_config = download_config.copy() if download_config else DownloadConfig()\n   2276     download_config.storage_options.update(storage_options)\n-> 2277 dataset_module = dataset_module_factory(\n   2278     path,\n   2279     revision=revision,\n   2280     download_config=download_config,\n   2281     download_mode=download_mode,\n   2282     data_dir=data_dir,\n   2283     data_files=data_files,\n   2284     cache_dir=cache_dir,\n   2285     trust_remote_code=trust_remote_code,\n   2286     _require_default_config_name=_require_default_config_name,\n   2287     _require_custom_configs=bool(config_kwargs),\n   2288 )\n   2289 # Get dataset builder class from the processing script\n   2290 builder_kwargs = dataset_module.builder_kwargs\nFile /usr/local/lib/python3.10/site-packages/datasets/load.py:1923, in dataset_module_factory(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\n   1918             if isinstance(e1, FileNotFoundError):\n   1919                 raise FileNotFoundError(\n   1920                     f\"Couldn't find a dataset script at {relative_to_absolute_path(combined_path)} or any data file in the same directory. \"\n   1921                     f\"Couldn't find '{path}' on the Hugging Face Hub either: {type(e1).__name__}: {e1}\"\n   1922                 ) from None\n-> 1923             raise e1 from None\n   1924 else:\n   1925     raise FileNotFoundError(\n   1926         f\"Couldn't find a dataset script at {relative_to_absolute_path(combined_path)} or any data file in the same directory.\"\n   1927     )\nFile /usr/local/lib/python3.10/site-packages/datasets/load.py:1896, in dataset_module_factory(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\n   1887             pass\n   1888     # Otherwise we must use the dataset script if the user trusts it\n   1889     return HubDatasetModuleFactoryWithScript(\n   1890         path,\n   1891         revision=revision,\n   1892         download_config=download_config,\n   1893         download_mode=download_mode,\n   1894         dynamic_modules_path=dynamic_modules_path,\n   1895         trust_remote_code=trust_remote_code,\n-> 1896     ).get_module()\n   1897 else:\n   1898     return HubDatasetModuleFactoryWithoutScript(\n   1899         path,\n   1900         revision=revision,\n   (...)\n   1904         download_mode=download_mode,\n   1905     ).get_module()\nFile /usr/local/lib/python3.10/site-packages/datasets/load.py:1532, in HubDatasetModuleFactoryWithScript.get_module(self)\n   1525 importable_file_path = _get_importable_file_path(\n   1526     dynamic_modules_path=dynamic_modules_path,\n   1527     module_namespace=\"datasets\",\n   1528     subdirectory_name=hash,\n   1529     name=self.name,\n   1530 )\n   1531 if not os.path.exists(importable_file_path):\n-> 1532     trust_remote_code = resolve_trust_remote_code(self.trust_remote_code, self.name)\n   1533     if trust_remote_code:\n   1534         _create_importable_file(\n   1535             local_path=local_path,\n   1536             local_imports=local_imports,\n   (...)\n   1542             download_mode=self.download_mode,\n   1543         )\nFile /usr/local/lib/python3.10/site-packages/datasets/load.py:134, in resolve_trust_remote_code(trust_remote_code, repo_id)\n    131         signal.alarm(0)\n    132     except Exception:\n    133         # OS which does not support signal.SIGALRM\n--> 134         raise ValueError(\n    135             f\"The repository for {repo_id} contains custom code which must be executed to correctly \"\n    136             f\"load the dataset. You can inspect the repository content at https://hf.co/datasets/{repo_id}.\\n\"\n    137             f\"Please pass the argument `trust_remote_code=True` to allow custom code to be run.\"\n    138         )\n    139 else:\n    140     # For the CI which might put the timeout at 0\n    141     _raise_timeout_error(None, None)\nValueError: The repository for xsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/xsum.\nPlease pass the argument `trust_remote_code=True` to allow custom code to be run."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"dataset/xsum\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "all_datasets = {\n",
    "    \"train\": load_dataset(\"xsum\", split=\"train\"),\n",
    "    \"valid\": load_dataset(\"xsum\", split=\"validation\"),\n",
    "    \"test\": load_dataset(\"xsum\", split=\"test\"),\n",
    "}\n",
    "\n",
    "for split, dataset in all_datasets.items():\n",
    "    pd.Series(dataset[\"document\"][:10]).str.replace(\"\\n\", \" [SEP] \").to_csv(f\"dataset/xsum/{split}.src\", index=False, header=False)\n",
    "    pd.Series(dataset[\"summary\"][:10]).to_csv(f\"dataset/xsum/{split}.tgt\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96418154",
   "metadata": {},
   "source": [
    "### Step 26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a467f944",
   "metadata": {},
   "source": [
    "Execution Start time: 04:39:10, End time: 04:39:22 (duration: 11 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46a36bb",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the `xsum` dataset requires executing custom code to load it. We need to pass the `trust_remote_code=True` argument to the `load_dataset` function. Let's modify the dataset preparation code accordingly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b88b723",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffb9df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Error: DatasetGenerationError: An error occurred while generating the dataset---------------------------------------------------------------------------ValueError                                Traceback (most recent call last)File /usr/local/lib/python3.10/site-packages/datasets/builder.py:1767, in GeneratorBasedBuilder._prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\n   1758     writer = writer_class(\n   1759         features=writer._features,\n   1760         path=fpath.replace(\"SSSSS\", f\"{shard_id:05d}\").replace(\"JJJJJ\", f\"{job_id:05d}\"),\n   (...)\n   1765         embed_local_files=embed_local_files,\n   1766     )\n-> 1767 example = self.info.features.encode_example(record) if self.info.features is not None else record\n   1768 writer.write(example, key)\nFile /usr/local/lib/python3.10/site-packages/datasets/features/features.py:1987, in Features.encode_example(self, example)\n   1986 example = cast_to_python_objects(example)\n-> 1987 return encode_nested_example(self, example)\nFile /usr/local/lib/python3.10/site-packages/datasets/features/features.py:1277, in encode_nested_example(schema, obj, level)\n   1275         raise ValueError(\"Got None but expected a dictionary instead\")\n   1276     return (\n-> 1277         {k: encode_nested_example(schema[k], obj.get(k), level=level + 1) for k in schema}\n   1278         if obj is not None\n   1279         else None\n   1280     )\n   1282 elif isinstance(schema, (list, tuple)):\nFile /usr/local/lib/python3.10/site-packages/datasets/features/features.py:1277, in <dictcomp>(.0)\n   1275         raise ValueError(\"Got None but expected a dictionary instead\")\n   1276     return (\n-> 1277         {k: encode_nested_example(schema[k], obj.get(k), level=level + 1) for k in schema}\n   1278         if obj is not None\n   1279         else None\n   1280     )\n   1282 elif isinstance(schema, (list, tuple)):\nFile /usr/local/lib/python3.10/site-packages/datasets/features/features.py:1347, in encode_nested_example(schema, obj, level)\n   1346 elif isinstance(schema, (Audio, Image, ClassLabel, TranslationVariableLanguages, Value, _ArrayXD)):\n-> 1347     return schema.encode_example(obj) if obj is not None else None\n   1348 # Other object should be directly convertible to a native Arrow type (like Translation and Translation)\nFile /usr/local/lib/python3.10/site-packages/datasets/features/features.py:523, in Value.encode_example(self, value)\n    522     return int(value)\n--> 523 elif pa.types.is_floating(self.pa_type):\n    524     return float(value)\nFile /usr/local/lib/python3.10/site-packages/pyarrow/types.py:120, in is_floating(t)\n    117     return t.id == lib.Type_UINT64\n--> 120 @doc(is_null, datatype=\"floating point numeric\")\n    121 def is_floating(t):\n    122     return t.id in _FLOATING_TYPES\nFile /usr/local/lib/python3.10/site-packages/datasets/load.py:103, in _raise_timeout_error(signum, frame)\n    102 def _raise_timeout_error(signum, frame):\n--> 103     raise ValueError(\n    104         \"Loading this dataset requires you to execute custom code contained in the dataset repository on your local \"\n    105         \"machine. Please set the option `trust_remote_code=True` to permit loading of this dataset.\"\n    106     )\nValueError: Loading this dataset requires you to execute custom code contained in the dataset repository on your local machine. Please set the option `trust_remote_code=True` to permit loading of this dataset.\nThe above exception was the direct cause of the following exception:\nDatasetGenerationError                    Traceback (most recent call last)Cell In[66], line 9\n      5 data_dir = Path(\"dataset/xsum\")\n      6 data_dir.mkdir(exist_ok=True)\n      8 all_datasets = {\n----> 9     \"train\": load_dataset(\"xsum\", split=\"train\", trust_remote_code=True),\n     10     \"valid\": load_dataset(\"xsum\", split=\"validation\", trust_remote_code=True),\n     11     \"test\": load_dataset(\"xsum\", split=\"test\", trust_remote_code=True),\n     12 }\n     14 for split, dataset in all_datasets.items():\n     15     pd.Series(dataset[\"document\"][:10]).str.replace(\"\\n\", \" [SEP] \").to_csv(f\"dataset/xsum/{split}.src\", index=False, header=False)\nFile /usr/local/lib/python3.10/site-packages/datasets/load.py:2628, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\n   2625     return builder_instance.as_streaming_dataset(split=split)\n   2627 # Download and prepare data\n-> 2628 builder_instance.download_and_prepare(\n   2629     download_config=download_config,\n   2630     download_mode=download_mode,\n   2631     verification_mode=verification_mode,\n   2632     num_proc=num_proc,\n   2633     storage_options=storage_options,\n   2634 )\n   2636 # Build dataset for splits\n   2637 keep_in_memory = (\n   2638     keep_in_memory if keep_in_memory is not None else is_small_dataset(builder_instance.info.dataset_size)\n   2639 )\nFile /usr/local/lib/python3.10/site-packages/datasets/builder.py:1029, in DatasetBuilder.download_and_prepare(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\n   1027     if num_proc is not None:\n   1028         prepare_split_kwargs[\"num_proc\"] = num_proc\n-> 1029     self._download_and_prepare(\n   1030         dl_manager=dl_manager,\n   1031         verification_mode=verification_mode,\n   1032         **prepare_split_kwargs,\n   1033         **download_and_prepare_kwargs,\n   1034     )\n   1035 # Sync info\n   1036 self.info.dataset_size = sum(split.num_bytes for split in self.info.splits.values())\nFile /usr/local/lib/python3.10/site-packages/datasets/builder.py:1791, in GeneratorBasedBuilder._download_and_prepare(self, dl_manager, verification_mode, **prepare_splits_kwargs)\n   1790 def _download_and_prepare(self, dl_manager, verification_mode, **prepare_splits_kwargs):\n-> 1791     super()._download_and_prepare(\n   1792         dl_manager,\n   1793         verification_mode,\n   1794         check_duplicate_keys=verification_mode == VerificationMode.BASIC_CHECKS\n   1795         or verification_mode == VerificationMode.ALL_CHECKS,\n   1796         **prepare_splits_kwargs,\n   1797     )\nFile /usr/local/lib/python3.10/site-packages/datasets/builder.py:1124, in DatasetBuilder._download_and_prepare(self, dl_manager, verification_mode, **prepare_split_kwargs)\n   1120 split_dict.add(split_generator.split_info)\n   1122 try:\n   1123     # Prepare split will record examples associated to the split\n-> 1124     self._prepare_split(split_generator, **prepare_split_kwargs)\n   1125 except OSError as e:\n   1126     raise OSError(\n   1127         \"Cannot find data file. \"\n   1128         + (self.manual_download_instructions or \"\")\n   1129         + \"\\nOriginal error:\\n\"\n   1130         + str(e)\n   1131     ) from None\nFile /usr/local/lib/python3.10/site-packages/datasets/builder.py:1629, in GeneratorBasedBuilder._prepare_split(self, split_generator, check_duplicate_keys, file_format, num_proc, max_shard_size)\n   1627 job_id = 0\n   1628 with pbar:\n-> 1629     for job_id, done, content in self._prepare_split_single(\n   1630         gen_kwargs=gen_kwargs, job_id=job_id, **_prepare_split_args\n   1631     ):\n   1632         if done:\n   1633             result = content\nFile /usr/local/lib/python3.10/site-packages/datasets/builder.py:1786, in GeneratorBasedBuilder._prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\n   1784     if isinstance(e, SchemaInferenceError) and e.__context__ is not None:\n   1785         e = e.__context__\n-> 1786     raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\n   1788 yield job_id, True, (total_num_examples, total_num_bytes, writer._features, num_shards, shard_lengths)\nDatasetGenerationError: An error occurred while generating the dataset"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"dataset/xsum\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "all_datasets = {\n",
    "    \"train\": load_dataset(\"xsum\", split=\"train\", trust_remote_code=True),\n",
    "    \"valid\": load_dataset(\"xsum\", split=\"validation\", trust_remote_code=True),\n",
    "    \"test\": load_dataset(\"xsum\", split=\"test\", trust_remote_code=True),\n",
    "}\n",
    "\n",
    "for split, dataset in all_datasets.items():\n",
    "    pd.Series(dataset[\"document\"][:10]).str.replace(\"\\n\", \" [SEP] \").to_csv(f\"dataset/xsum/{split}.src\", index=False, header=False)\n",
    "    pd.Series(dataset[\"summary\"][:10]).to_csv(f\"dataset/xsum/{split}.tgt\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea7687a",
   "metadata": {},
   "source": [
    "### Step 27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5eca8c",
   "metadata": {},
   "source": [
    "Execution Start time: 04:39:25, End time: 04:40:43 (duration: 77 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a99b7a",
   "metadata": {},
   "source": [
    "**Thought**: It seems there is an issue with generating the dataset. Let's try to resolve this by ensuring that the `trust_remote_code=True` argument is correctly applied and re-run the dataset preparation code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0ea1ba",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ce4cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"dataset/xsum\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "all_datasets = {\n",
    "    \"train\": load_dataset(\"xsum\", split=\"train\", trust_remote_code=True),\n",
    "    \"valid\": load_dataset(\"xsum\", split=\"validation\", trust_remote_code=True),\n",
    "    \"test\": load_dataset(\"xsum\", split=\"test\", trust_remote_code=True),\n",
    "}\n",
    "\n",
    "for split, dataset in all_datasets.items():\n",
    "    pd.Series(dataset[\"document\"][:10]).str.replace(\"\\n\", \" [SEP] \").to_csv(f\"dataset/xsum/{split}.src\", index=False, header=False)\n",
    "    pd.Series(dataset[\"summary\"][:10]).to_csv(f\"dataset/xsum/{split}.tgt\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df7ae6b",
   "metadata": {},
   "source": [
    "### Step 28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a358a324",
   "metadata": {},
   "source": [
    "Execution Start time: 04:40:44, End time: 04:41:24 (duration: 39 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48035d7f",
   "metadata": {},
   "source": [
    "**Thought**: The dataset preparation code executed successfully this time. Now, let's proceed to run the model training and evaluation command again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553233a0",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195a4515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "W&B online. Running your script from this directory will now sync to the cloud.\nDetected kernel version 4.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n22 Aug 04:40    INFO 66 parameters found.\n================================================================================\n\n# General Hyper Parameters: \n\ngpu_id: 0\nuse_gpu: True\ndevice: cpu\nseed: 2020\nreproducibility: True\ncmd: run_textbox.py --model=BART --dataset=xsum --model_path=facebook/bart-base --epochs=1\nfilename: BART-xsum-2024-Aug-22_04-40-50\nsaved_dir: saved/\nstate: INFO\nwandb: online\n\n\n# Training Hyper Parameters: \n\ndo_train: True\ndo_valid: True\noptimizer: adamw\nadafactor_kwargs: {'lr': 0.001, 'scale_parameter': False, 'relative_step': False, 'warmup_init': False}\noptimizer_kwargs: {}\nvalid_steps: 1\nvalid_strategy: epoch\nstopping_steps: 2\nepochs: 1\nlearning_rate: 3e-05\ntrain_batch_size: 4\ngrad_clip: 0.1\naccumulation_steps: 48\ndisable_tqdm: False\nresume_training: True\n\n\n# Evaluation Hyper Parameters: \n\ndo_test: True\nlower_evaluation: True\nmultiref_strategy: max\nbleu_max_ngrams: 4\nbleu_type: nltk\nsmoothing_function: 0\ncorpus_bleu: False\nrouge_max_ngrams: 2\nrouge_type: files2rouge\nmeteor_type: pycocoevalcap\nchrf_type: m-popovic\ndistinct_max_ngrams: 4\ninter_distinct: True\nunique_max_ngrams: 4\nself_bleu_max_ngrams: 4\ntgt_lang: en\nmetrics: ['rouge']\neval_batch_size: 16\ncorpus_meteor: True\n\n\n# Model Hyper Parameters: \n\nmodel: BART\nmodel_name: bart\nmodel_path: facebook/bart-base\nconfig_kwargs: {}\ntokenizer_kwargs: {'use_fast': True}\ngeneration_kwargs: {'num_beams': 5, 'no_repeat_ngram_size': 3, 'early_stopping': True}\nefficient_kwargs: {}\nefficient_methods: []\nefficient_unfreeze_model: False\nlabel_smoothing: 0.1\n\n\n# Dataset Hyper Parameters: \n\ndataset: xsum\ndata_path: dataset/xsum\ntgt_lang: en\nsrc_len: 1024\ntgt_len: 128\ntruncate: tail\nprefix_prompt: Summarize: \nmetrics_for_best_model: ['rouge-1', 'rouge-2', 'rouge-l']\n\n\n# Unrecognized Hyper Parameters: \n\nfind_unused_parameters: False\ntokenizer_add_tokens: []\nload_type: from_pretrained\n\n================================================================================\n/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n22 Aug 04:40    INFO Pretrain type: pretrain disabled\nwandb: Tracking run with wandb version 0.17.7\nwandb: W&B syncing is set to `offline` in this directory.  \nwandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.\n/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\nmodel.safetensors:   0%|                             | 0.00/558M [00:00<?, ?B/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n22 Aug 04:40    INFO Pretrained_Models(\n  (model): BartForConditionalGeneration(\n    (model): BartModel(\n      (shared): Embedding(50265, 768, padding_idx=1)\n      (encoder): BartEncoder(\n        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n        (layers): ModuleList(\n          (0-5): 6 x BartEncoderLayer(\n            (self_attn): BartSdpaAttention(\n            )\n            (activation_fn): GELUActivation()\n          )\n        )\n      )\n      (decoder): BartDecoder(\n        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n        (layers): ModuleList(\n          (0-5): 6 x BartDecoderLayer(\n            (self_attn): BartSdpaAttention(\n            )\n            (activation_fn): GELUActivation()\n            (encoder_attn): BartSdpaAttention(\n            )\n          )\n        )\n      )\n    )\n  )\n)\nTrainable parameters: 139420416\n22 Aug 04:40    INFO ====== Start training ======\n\ntrain    1:   0%|                            | 0/1 [00:00<?, ?step/s, loss=None]\ntrain    1: 100%|████████████████████| 1/1 [00:25<00:00, 25.67s/step, loss=None]\ntrain    1: 100%|███████████████████████| 1/1 [00:25<00:00, 25.67s/step, loss=5]22 Aug 04:41    ERROR Traceback (most recent call last):\n  File \"/content/TextBox/textbox/utils/dashboard.py\", line 311, in new_experiment\n    yield True\n  File \"/content/TextBox/textbox/quick_start/experiment.py\", line 140, in run\n    self._do_train_and_valid()\n  File \"/content/TextBox/textbox/quick_start/experiment.py\", line 115, in _do_train_and_valid\n    self.valid_result = self.trainer.fit(train_data, valid_data)\n  File \"/content/TextBox/textbox/trainer/trainer.py\", line 452, in fit\n    loss = self._train_epoch(train_data, epoch_idx, valid_data)['loss']\n  File \"/content/TextBox/textbox/trainer/trainer.py\", line 236, in _train_epoch\n    self.accelerator.gradient_state._set_end_of_dataloader(False)\nAttributeError: 'GradientState' object has no attribute '_set_end_of_dataloader'\n\n\ntrain    1: 100%|███████████████████████| 1/1 [00:25<00:00, 25.68s/step, loss=5]\nwandb: \nwandb: Run history:\nwandb: loss/train ▁\nwandb: \nwandb: Run summary:\nwandb: loss/train 4.58426\nwandb: \nwandb: You can sync this run to the cloud by running:\nwandb: wandb sync saved/BART-xsum-2024-Aug-22_04-40-50/wandb/offline-run-20240822_044052-1r0pfywb\nwandb: Find logs at: saved/BART-xsum-2024-Aug-22_04-40-50/wandb/offline-run-20240822_044052-1r0pfywb/logs\nwandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python run_textbox.py --model=BART --dataset=xsum --model_path=facebook/bart-base --epochs=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342d86db",
   "metadata": {},
   "source": [
    "### Step 29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7de90f6",
   "metadata": {},
   "source": [
    "Execution Start time: 04:41:26, End time: 04:41:29 (duration: 3 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fa8ca8",
   "metadata": {},
   "source": [
    "**Thought**: It seems there was an error during the training process related to the `GradientState` object. This might be due to a version mismatch or an issue with the library. Let's try updating the `accelerate` library to see if it resolves the issue.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c6524e",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db05cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/site-packages (0.33.0)\nRequirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/site-packages (from accelerate) (1.25.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from accelerate) (24.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/site-packages (from accelerate) (5.9.8)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/site-packages (from accelerate) (2.2.1+cpu)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/site-packages (from accelerate) (0.23.3)\nRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/site-packages (from accelerate) (0.4.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2023.6.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.7)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.6.2)\nRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df658311",
   "metadata": {},
   "source": [
    "### Step 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c45e027",
   "metadata": {},
   "source": [
    "Execution Start time: 04:41:31, End time: 04:42:06 (duration: 35 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c29d9f7",
   "metadata": {},
   "source": [
    "**Thought**: The `accelerate` library is already up to date. Let's try running the model training and evaluation command again to see if the issue persists.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecc4a4b",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd11fafa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "W&B online. Running your script from this directory will now sync to the cloud.\nDetected kernel version 4.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n22 Aug 04:41    INFO 66 parameters found.\n================================================================================\n\n# General Hyper Parameters: \n\ngpu_id: 0\nuse_gpu: True\ndevice: cpu\nseed: 2020\nreproducibility: True\ncmd: run_textbox.py --model=BART --dataset=xsum --model_path=facebook/bart-base --epochs=1\nfilename: BART-xsum-2024-Aug-22_04-41-35\nsaved_dir: saved/\nstate: INFO\nwandb: online\n\n\n# Training Hyper Parameters: \n\ndo_train: True\ndo_valid: True\noptimizer: adamw\nadafactor_kwargs: {'lr': 0.001, 'scale_parameter': False, 'relative_step': False, 'warmup_init': False}\noptimizer_kwargs: {}\nvalid_steps: 1\nvalid_strategy: epoch\nstopping_steps: 2\nepochs: 1\nlearning_rate: 3e-05\ntrain_batch_size: 4\ngrad_clip: 0.1\naccumulation_steps: 48\ndisable_tqdm: False\nresume_training: True\n\n\n# Evaluation Hyper Parameters: \n\ndo_test: True\nlower_evaluation: True\nmultiref_strategy: max\nbleu_max_ngrams: 4\nbleu_type: nltk\nsmoothing_function: 0\ncorpus_bleu: False\nrouge_max_ngrams: 2\nrouge_type: files2rouge\nmeteor_type: pycocoevalcap\nchrf_type: m-popovic\ndistinct_max_ngrams: 4\ninter_distinct: True\nunique_max_ngrams: 4\nself_bleu_max_ngrams: 4\ntgt_lang: en\nmetrics: ['rouge']\neval_batch_size: 16\ncorpus_meteor: True\n\n\n# Model Hyper Parameters: \n\nmodel: BART\nmodel_name: bart\nmodel_path: facebook/bart-base\nconfig_kwargs: {}\ntokenizer_kwargs: {'use_fast': True}\ngeneration_kwargs: {'num_beams': 5, 'no_repeat_ngram_size': 3, 'early_stopping': True}\nefficient_kwargs: {}\nefficient_methods: []\nefficient_unfreeze_model: False\nlabel_smoothing: 0.1\n\n\n# Dataset Hyper Parameters: \n\ndataset: xsum\ndata_path: dataset/xsum\ntgt_lang: en\nsrc_len: 1024\ntgt_len: 128\ntruncate: tail\nprefix_prompt: Summarize: \nmetrics_for_best_model: ['rouge-1', 'rouge-2', 'rouge-l']\n\n\n# Unrecognized Hyper Parameters: \n\nfind_unused_parameters: False\ntokenizer_add_tokens: []\nload_type: from_pretrained\n\n================================================================================\n/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n22 Aug 04:41    INFO Pretrain type: pretrain disabled\nwandb: Tracking run with wandb version 0.17.7\nwandb: W&B syncing is set to `offline` in this directory.  \nwandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.\n/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n22 Aug 04:41    INFO Pretrained_Models(\n  (model): BartForConditionalGeneration(\n    (model): BartModel(\n      (shared): Embedding(50265, 768, padding_idx=1)\n      (encoder): BartEncoder(\n        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n        (layers): ModuleList(\n          (0-5): 6 x BartEncoderLayer(\n            (self_attn): BartSdpaAttention(\n            )\n            (activation_fn): GELUActivation()\n          )\n        )\n      )\n      (decoder): BartDecoder(\n        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n        (layers): ModuleList(\n          (0-5): 6 x BartDecoderLayer(\n            (self_attn): BartSdpaAttention(\n            )\n            (activation_fn): GELUActivation()\n            (encoder_attn): BartSdpaAttention(\n            )\n          )\n        )\n      )\n    )\n  )\n)\nTrainable parameters: 139420416\n22 Aug 04:41    INFO ====== Start training ======\n\ntrain    1:   0%|                            | 0/1 [00:00<?, ?step/s, loss=None]\ntrain    1: 100%|████████████████████| 1/1 [00:23<00:00, 24.00s/step, loss=None]\ntrain    1: 100%|███████████████████████| 1/1 [00:23<00:00, 24.00s/step, loss=5]22 Aug 04:42    ERROR Traceback (most recent call last):\n  File \"/content/TextBox/textbox/utils/dashboard.py\", line 311, in new_experiment\n    yield True\n  File \"/content/TextBox/textbox/quick_start/experiment.py\", line 140, in run\n    self._do_train_and_valid()\n  File \"/content/TextBox/textbox/quick_start/experiment.py\", line 115, in _do_train_and_valid\n    self.valid_result = self.trainer.fit(train_data, valid_data)\n  File \"/content/TextBox/textbox/trainer/trainer.py\", line 452, in fit\n    loss = self._train_epoch(train_data, epoch_idx, valid_data)['loss']\n  File \"/content/TextBox/textbox/trainer/trainer.py\", line 236, in _train_epoch\n    self.accelerator.gradient_state._set_end_of_dataloader(False)\nAttributeError: 'GradientState' object has no attribute '_set_end_of_dataloader'\n\n\ntrain    1: 100%|███████████████████████| 1/1 [00:24<00:00, 24.00s/step, loss=5]\nwandb: \nwandb: Run history:\nwandb: loss/train ▁\nwandb: \nwandb: Run summary:\nwandb: loss/train 4.58426\nwandb: \nwandb: You can sync this run to the cloud by running:\nwandb: wandb sync saved/BART-xsum-2024-Aug-22_04-41-35/wandb/offline-run-20240822_044137-2y7k1yv3\nwandb: Find logs at: saved/BART-xsum-2024-Aug-22_04-41-35/wandb/offline-run-20240822_044137-2y7k1yv3/logs\nwandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python run_textbox.py --model=BART --dataset=xsum --model_path=facebook/bart-base --epochs=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0347e64",
   "metadata": {},
   "source": [
    "### Step 31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7ec427",
   "metadata": {},
   "source": [
    "Execution Start time: 04:42:08, End time: 04:42:12 (duration: 3 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f5958e",
   "metadata": {},
   "source": [
    "**Thought**: The error persists, indicating an issue with the `GradientState` object in the `accelerate` library. This might be due to a version mismatch or an incompatibility with the current setup. Let's try downgrading the `accelerate` library to a previous version that might be more compatible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba560c7",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20463846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting accelerate==0.20.3\n  Downloading accelerate-0.20.3-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from accelerate==0.20.3) (1.25.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from accelerate==0.20.3) (24.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/site-packages (from accelerate==0.20.3) (5.9.8)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/site-packages (from accelerate==0.20.3) (6.0.1)\nRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/site-packages (from accelerate==0.20.3) (2.2.1+cpu)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (1.12)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (2023.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate==0.20.3) (2.1.5)\nRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate==0.20.3) (1.3.0)\nDownloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/227.6 kB ? eta -:--:--\n\n\u001b[?25hInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.33.0\n    Uninstalling accelerate-0.33.0:\n      Successfully uninstalled accelerate-0.33.0\nSuccessfully installed accelerate-0.20.3\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install accelerate==0.20.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34273653",
   "metadata": {},
   "source": [
    "### Step 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b68d1b",
   "metadata": {},
   "source": [
    "Execution Start time: 04:42:14, End time: 04:42:49 (duration: 34 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6fa760",
   "metadata": {},
   "source": [
    "**Thought**: The `accelerate` library has been downgraded to version 0.20.3. Let's try running the model training and evaluation command again to see if this resolves the issue.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b80345",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad10138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "W&B online. Running your script from this directory will now sync to the cloud.\n22 Aug 04:42    INFO 66 parameters found.\n================================================================================\n\n# General Hyper Parameters: \n\ngpu_id: 0\nuse_gpu: True\ndevice: cpu\nseed: 2020\nreproducibility: True\ncmd: run_textbox.py --model=BART --dataset=xsum --model_path=facebook/bart-base --epochs=1\nfilename: BART-xsum-2024-Aug-22_04-42-19\nsaved_dir: saved/\nstate: INFO\nwandb: online\n\n\n# Training Hyper Parameters: \n\ndo_train: True\ndo_valid: True\noptimizer: adamw\nadafactor_kwargs: {'lr': 0.001, 'scale_parameter': False, 'relative_step': False, 'warmup_init': False}\noptimizer_kwargs: {}\nvalid_steps: 1\nvalid_strategy: epoch\nstopping_steps: 2\nepochs: 1\nlearning_rate: 3e-05\ntrain_batch_size: 4\ngrad_clip: 0.1\naccumulation_steps: 48\ndisable_tqdm: False\nresume_training: True\n\n\n# Evaluation Hyper Parameters: \n\ndo_test: True\nlower_evaluation: True\nmultiref_strategy: max\nbleu_max_ngrams: 4\nbleu_type: nltk\nsmoothing_function: 0\ncorpus_bleu: False\nrouge_max_ngrams: 2\nrouge_type: files2rouge\nmeteor_type: pycocoevalcap\nchrf_type: m-popovic\ndistinct_max_ngrams: 4\ninter_distinct: True\nunique_max_ngrams: 4\nself_bleu_max_ngrams: 4\ntgt_lang: en\nmetrics: ['rouge']\neval_batch_size: 16\ncorpus_meteor: True\n\n\n# Model Hyper Parameters: \n\nmodel: BART\nmodel_name: bart\nmodel_path: facebook/bart-base\nconfig_kwargs: {}\ntokenizer_kwargs: {'use_fast': True}\ngeneration_kwargs: {'num_beams': 5, 'no_repeat_ngram_size': 3, 'early_stopping': True}\nefficient_kwargs: {}\nefficient_methods: []\nefficient_unfreeze_model: False\nlabel_smoothing: 0.1\n\n\n# Dataset Hyper Parameters: \n\ndataset: xsum\ndata_path: dataset/xsum\ntgt_lang: en\nsrc_len: 1024\ntgt_len: 128\ntruncate: tail\nprefix_prompt: Summarize: \nmetrics_for_best_model: ['rouge-1', 'rouge-2', 'rouge-l']\n\n\n# Unrecognized Hyper Parameters: \n\nfind_unused_parameters: False\ntokenizer_add_tokens: []\nload_type: from_pretrained\n\n================================================================================\n/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n22 Aug 04:42    INFO Pretrain type: pretrain disabled\nwandb: Tracking run with wandb version 0.17.7\nwandb: W&B syncing is set to `offline` in this directory.  \nwandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.\n/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n22 Aug 04:42    INFO Pretrained_Models(\n  (model): BartForConditionalGeneration(\n    (model): BartModel(\n      (shared): Embedding(50265, 768, padding_idx=1)\n      (encoder): BartEncoder(\n        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n        (layers): ModuleList(\n          (0-5): 6 x BartEncoderLayer(\n            (self_attn): BartSdpaAttention(\n            )\n            (activation_fn): GELUActivation()\n          )\n        )\n      )\n      (decoder): BartDecoder(\n        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n        (layers): ModuleList(\n          (0-5): 6 x BartDecoderLayer(\n            (self_attn): BartSdpaAttention(\n            )\n            (activation_fn): GELUActivation()\n            (encoder_attn): BartSdpaAttention(\n            )\n          )\n        )\n      )\n    )\n  )\n)\nTrainable parameters: 139420416\n22 Aug 04:42    INFO ====== Start training ======\n\ntrain    1:   0%|                            | 0/1 [00:00<?, ?step/s, loss=None]\ntrain    1: 100%|████████████████████| 1/1 [00:23<00:00, 23.91s/step, loss=None]\ntrain    1: 100%|███████████████████████| 1/1 [00:23<00:00, 23.91s/step, loss=5]22 Aug 04:42    ERROR Traceback (most recent call last):\n  File \"/content/TextBox/textbox/utils/dashboard.py\", line 311, in new_experiment\n    yield True\n  File \"/content/TextBox/textbox/quick_start/experiment.py\", line 140, in run\n    self._do_train_and_valid()\n  File \"/content/TextBox/textbox/quick_start/experiment.py\", line 115, in _do_train_and_valid\n    self.valid_result = self.trainer.fit(train_data, valid_data)\n  File \"/content/TextBox/textbox/trainer/trainer.py\", line 452, in fit\n    loss = self._train_epoch(train_data, epoch_idx, valid_data)['loss']\n  File \"/content/TextBox/textbox/trainer/trainer.py\", line 236, in _train_epoch\n    self.accelerator.gradient_state._set_end_of_dataloader(False)\nAttributeError: 'GradientState' object has no attribute '_set_end_of_dataloader'\n\n\ntrain    1: 100%|███████████████████████| 1/1 [00:23<00:00, 23.91s/step, loss=5]\nwandb: \nwandb: Run history:\nwandb: loss/train ▁\nwandb: \nwandb: Run summary:\nwandb: loss/train 4.58426\nwandb: \nwandb: You can sync this run to the cloud by running:\nwandb: wandb sync saved/BART-xsum-2024-Aug-22_04-42-19/wandb/offline-run-20240822_044221-v8dhg1aa\nwandb: Find logs at: saved/BART-xsum-2024-Aug-22_04-42-19/wandb/offline-run-20240822_044221-v8dhg1aa/logs\nwandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python run_textbox.py --model=BART --dataset=xsum --model_path=facebook/bart-base --epochs=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209f40bd",
   "metadata": {},
   "source": [
    "### Step 33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3060ff8",
   "metadata": {},
   "source": [
    "Execution Start time: 04:42:51, End time: 04:42:54 (duration: 3 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b455a33",
   "metadata": {},
   "source": [
    "**Thought**: The error persists, indicating an issue with the `GradientState` object in the `accelerate` library. This might be due to a version mismatch or an incompatibility with the current setup. Let's try downgrading the `accelerate` library to a previous version that might be more compatible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c53e96",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f456876e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting accelerate==0.19.0\n  Downloading accelerate-0.19.0-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from accelerate==0.19.0) (1.25.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from accelerate==0.19.0) (24.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/site-packages (from accelerate==0.19.0) (5.9.8)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/site-packages (from accelerate==0.19.0) (6.0.1)\nRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/site-packages (from accelerate==0.19.0) (2.2.1+cpu)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.19.0) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.19.0) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.19.0) (1.12)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.19.0) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.19.0) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.19.0) (2023.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate==0.19.0) (2.1.5)\nRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate==0.19.0) (1.3.0)\nDownloading accelerate-0.19.0-py3-none-any.whl (219 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/219.1 kB ? eta -:--:--\n\n\u001b[?25hInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.20.3\n    Uninstalling accelerate-0.20.3:\n      Successfully uninstalled accelerate-0.20.3\nSuccessfully installed accelerate-0.19.0\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install accelerate==0.19.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac411ea",
   "metadata": {},
   "source": [
    "### Step 34"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4c01a2",
   "metadata": {},
   "source": [
    "Execution Start time: 04:42:56, End time: 04:44:45 (duration: 109 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2128e058",
   "metadata": {},
   "source": [
    "**Thought**: The `accelerate` library has been downgraded to version 0.19.0. Let's try running the model training and evaluation command again to see if this resolves the issue.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2710491b",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b55fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "W&B online. Running your script from this directory will now sync to the cloud.\n22 Aug 04:43    INFO 66 parameters found.\n================================================================================\n\n# General Hyper Parameters: \n\ngpu_id: 0\nuse_gpu: True\ndevice: cpu\nseed: 2020\nreproducibility: True\ncmd: run_textbox.py --model=BART --dataset=xsum --model_path=facebook/bart-base --epochs=1\nfilename: BART-xsum-2024-Aug-22_04-43-00\nsaved_dir: saved/\nstate: INFO\nwandb: online\n\n\n# Training Hyper Parameters: \n\ndo_train: True\ndo_valid: True\noptimizer: adamw\nadafactor_kwargs: {'lr': 0.001, 'scale_parameter': False, 'relative_step': False, 'warmup_init': False}\noptimizer_kwargs: {}\nvalid_steps: 1\nvalid_strategy: epoch\nstopping_steps: 2\nepochs: 1\nlearning_rate: 3e-05\ntrain_batch_size: 4\ngrad_clip: 0.1\naccumulation_steps: 48\ndisable_tqdm: False\nresume_training: True\n\n\n# Evaluation Hyper Parameters: \n\ndo_test: True\nlower_evaluation: True\nmultiref_strategy: max\nbleu_max_ngrams: 4\nbleu_type: nltk\nsmoothing_function: 0\ncorpus_bleu: False\nrouge_max_ngrams: 2\nrouge_type: files2rouge\nmeteor_type: pycocoevalcap\nchrf_type: m-popovic\ndistinct_max_ngrams: 4\ninter_distinct: True\nunique_max_ngrams: 4\nself_bleu_max_ngrams: 4\ntgt_lang: en\nmetrics: ['rouge']\neval_batch_size: 16\ncorpus_meteor: True\n\n\n# Model Hyper Parameters: \n\nmodel: BART\nmodel_name: bart\nmodel_path: facebook/bart-base\nconfig_kwargs: {}\ntokenizer_kwargs: {'use_fast': True}\ngeneration_kwargs: {'num_beams': 5, 'no_repeat_ngram_size': 3, 'early_stopping': True}\nefficient_kwargs: {}\nefficient_methods: []\nefficient_unfreeze_model: False\nlabel_smoothing: 0.1\n\n\n# Dataset Hyper Parameters: \n\ndataset: xsum\ndata_path: dataset/xsum\ntgt_lang: en\nsrc_len: 1024\ntgt_len: 128\ntruncate: tail\nprefix_prompt: Summarize: \nmetrics_for_best_model: ['rouge-1', 'rouge-2', 'rouge-l']\n\n\n# Unrecognized Hyper Parameters: \n\nfind_unused_parameters: False\ntokenizer_add_tokens: []\nload_type: from_pretrained\n\n================================================================================\n/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n22 Aug 04:43    INFO Pretrain type: pretrain disabled\nwandb: Tracking run with wandb version 0.17.7\nwandb: W&B syncing is set to `offline` in this directory.  \nwandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.\n/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n22 Aug 04:43    INFO Pretrained_Models(\n  (model): BartForConditionalGeneration(\n    (model): BartModel(\n      (shared): Embedding(50265, 768, padding_idx=1)\n      (encoder): BartEncoder(\n        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n        (layers): ModuleList(\n          (0-5): 6 x BartEncoderLayer(\n            (self_attn): BartSdpaAttention(\n            )\n            (activation_fn): GELUActivation()\n          )\n        )\n      )\n      (decoder): BartDecoder(\n        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n        (layers): ModuleList(\n          (0-5): 6 x BartDecoderLayer(\n            (self_attn): BartSdpaAttention(\n            )\n            (activation_fn): GELUActivation()\n            (encoder_attn): BartSdpaAttention(\n            )\n          )\n        )\n      )\n    )\n  )\n)\nTrainable parameters: 139420416\n22 Aug 04:43    INFO ====== Start training ======\n\ntrain    1:   0%|                            | 0/1 [00:00<?, ?step/s, loss=None]\ntrain    1: 100%|████████████████████| 1/1 [00:23<00:00, 23.44s/step, loss=None]\ntrain    1: 100%|███████████████████████| 1/1 [00:23<00:00, 23.44s/step, loss=5]\ntrain    1: 100%|███████████████████████| 1/1 [00:23<00:00, 23.44s/step, loss=5]\n22 Aug 04:43    INFO Train epoch  1 [time: 23.44s, loss: 5.00]\n\ngenerating:   0%|                                         | 0/1 [00:00<?, ?it/s]\ngenerating: 100%|█████████████████████████████████| 1/1 [01:14<00:00, 74.75s/it]\ngenerating: 100%|█████████████████████████████████| 1/1 [01:14<00:00, 74.75s/it]\nCan't locate XML/Parser.pm in @INC (you may need to install the XML::Parser module) (@INC contains: /root/.files2rouge /etc/perl /usr/local/lib/x86_64-linux-gnu/perl/5.34.0 /usr/local/share/perl/5.34.0 /usr/lib/x86_64-linux-gnu/perl5/5.34 /usr/share/perl5 /usr/lib/x86_64-linux-gnu/perl-base /usr/lib/x86_64-linux-gnu/perl/5.34 /usr/share/perl/5.34 /usr/local/lib/site_perl) at /root/.files2rouge/XML/DOM.pm line 41.\nBEGIN failed--compilation aborted at /root/.files2rouge/XML/DOM.pm line 70.\nCompilation failed in require at /root/.files2rouge/ROUGE-1.5.5.pl line 177.\nBEGIN failed--compilation aborted at /root/.files2rouge/ROUGE-1.5.5.pl line 177.\n22 Aug 04:44    ERROR Traceback (most recent call last):\n  File \"/content/TextBox/textbox/utils/dashboard.py\", line 311, in new_experiment\n    yield True\n  File \"/content/TextBox/textbox/quick_start/experiment.py\", line 140, in run\n    self._do_train_and_valid()\n  File \"/content/TextBox/textbox/quick_start/experiment.py\", line 115, in _do_train_and_valid\n    self.valid_result = self.trainer.fit(train_data, valid_data)\n  File \"/content/TextBox/textbox/trainer/trainer.py\", line 457, in fit\n    self.stopped |= self._valid(valid_data, 'epoch')\n  File \"/usr/local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/content/TextBox/textbox/trainer/trainer.py\", line 295, in _valid\n    valid_results = self.evaluate(valid_data, is_valid=True)\n  File \"/usr/local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/content/TextBox/textbox/trainer/trainer.py\", line 569, in evaluate\n    result = self.evaluator.evaluate(generate_corpus, reference_dataset)\n  File \"/content/TextBox/textbox/evaluator/base_evaluator.py\", line 157, in evaluate\n    metric_result = evaluator.evaluate(generate_corpus, reference_corpus, avg=avg)\n  File \"/content/TextBox/textbox/evaluator/abstract_evaluator.py\", line 31, in evaluate\n    metric_dict = self._calc_metrics_info(generate_corpus=generate_corpus, reference_corpus=reference_corpus)\n  File \"/content/TextBox/textbox/evaluator/rouge_evaluator.py\", line 57, in _calc_metrics_info\n    output = r.convert_and_evaluate(rouge_args=rouge_args_str)\n  File \"/usr/local/lib/python3.10/site-packages/pyrouge/Rouge155.py\", line 368, in convert_and_evaluate\n    rouge_output = self.evaluate(system_id, rouge_args)\n  File \"/usr/local/lib/python3.10/site-packages/pyrouge/Rouge155.py\", line 343, in evaluate\n    rouge_output = check_output(command, env=env).decode(\"UTF-8\")\n  File \"/usr/local/lib/python3.10/subprocess.py\", line 421, in check_output\n    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n  File \"/usr/local/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['/root/.files2rouge/ROUGE-1.5.5.pl', '-e', '/root/.files2rouge/data', '-c', '95', '-r', '1000', '-n', '2', '-w', '1.2', '-a', '-m', '/tmp/tmp0eckn5cd/rouge_conf.xml']' returned non-zero exit status 2.\n\nwandb: \nwandb: Run history:\nwandb: loss/train ▁\nwandb: \nwandb: Run summary:\nwandb: loss/train 4.58426\nwandb: \nwandb: You can sync this run to the cloud by running:\nwandb: wandb sync saved/BART-xsum-2024-Aug-22_04-43-00/wandb/offline-run-20240822_044302-dc2xxspy\nwandb: Find logs at: saved/BART-xsum-2024-Aug-22_04-43-00/wandb/offline-run-20240822_044302-dc2xxspy/logs\nwandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python run_textbox.py --model=BART --dataset=xsum --model_path=facebook/bart-base --epochs=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f5a3f3",
   "metadata": {},
   "source": [
    "### Step 35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6ee0fb",
   "metadata": {},
   "source": [
    "Execution Start time: 04:44:47, End time: 04:44:50 (duration: 3 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e907737f",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the `XML::Parser` module is missing, which is required by `files2rouge`. This module is a Perl module, and we need to install it to proceed. Let's install the `XML::Parser` module.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d808b5",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ec83da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\nReading package lists... 0%\n\nReading package lists... 0%\n\nReading package lists... 0%\n\nReading package lists... 5%\n\nReading package lists... 5%\n\nReading package lists... 6%\n\nReading package lists... 6%\n\nReading package lists... 56%\n\nReading package lists... 56%\n\nReading package lists... 57%\n\nReading package lists... 57%\n\nReading package lists... 64%\n\nReading package lists... 64%\n\nReading package lists... 74%\n\nReading package lists... 74%\n\nReading package lists... 79%\n\nReading package lists... 79%\n\nReading package lists... 79%\n\nReading package lists... 79%\n\nReading package lists... 79%\n\nReading package lists... 79%\n\nReading package lists... 79%\n\nReading package lists... 79%\n\nReading package lists... 86%\n\nReading package lists... 86%\n\nReading package lists... 95%\n\nReading package lists... 95%\n\nReading package lists... 99%\n\nReading package lists... 99%\n\nReading package lists... 99%\n\nReading package lists... 99%\n\nReading package lists... Done\n\n\nBuilding dependency tree... 0%\n\nBuilding dependency tree... 0%\n\nBuilding dependency tree... 8%\n\nBuilding dependency tree... 50%\n\nBuilding dependency tree... 50%\n\nBuilding dependency tree... Done\n\n\nReading state information... 0% \n\nReading state information... 0%\n\nReading state information... Done\n\nThe following additional packages will be installed:\n  libauthen-sasl-perl libclone-perl libdata-dump-perl libencode-locale-perl\n  libfile-listing-perl libfont-afm-perl libhtml-form-perl libhtml-format-perl\n  libhtml-parser-perl libhtml-tagset-perl libhtml-tree-perl\n  libhttp-cookies-perl libhttp-daemon-perl libhttp-date-perl\n  libhttp-message-perl libhttp-negotiate-perl libio-html-perl\n  libio-socket-ssl-perl liblwp-mediatypes-perl liblwp-protocol-https-perl\n  libmailtools-perl libnet-http-perl libnet-smtp-ssl-perl libnet-ssleay-perl\n  libtimedate-perl libtry-tiny-perl liburi-perl libwww-perl\n  libwww-robotrules-perl perl-openssl-defaults\nSuggested packages:\n  libdigest-hmac-perl libgssapi-perl libcrypt-ssleay-perl libsub-name-perl\n  libbusiness-isbn-perl libauthen-ntlm-perl\nThe following NEW packages will be installed:\n  libauthen-sasl-perl libclone-perl libdata-dump-perl libencode-locale-perl\n  libfile-listing-perl libfont-afm-perl libhtml-form-perl libhtml-format-perl\n  libhtml-parser-perl libhtml-tagset-perl libhtml-tree-perl\n  libhttp-cookies-perl libhttp-daemon-perl libhttp-date-perl\n  libhttp-message-perl libhttp-negotiate-perl libio-html-perl\n  libio-socket-ssl-perl liblwp-mediatypes-perl liblwp-protocol-https-perl\n  libmailtools-perl libnet-http-perl libnet-smtp-ssl-perl libnet-ssleay-perl\n  libtimedate-perl libtry-tiny-perl liburi-perl libwww-perl\n  libwww-robotrules-perl libxml-parser-perl perl-openssl-defaults\n0 upgraded, 31 newly installed, 0 to remove and 0 not upgraded.\nNeed to get 1804 kB of archives.\nAfter this operation, 5646 kB of additional disk space will be used.\n\n0% [Working]\n            \nGet:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libclone-perl amd64 0.45-1build3 [11.0 kB]\n\n0% [1 libclone-perl 11.0 kB/11.0 kB 100%]\n                                         \n1% [Working]\n            \nGet:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libdata-dump-perl all 1.25-1 [25.9 kB]\n\n1% [2 libdata-dump-perl 2606 B/25.9 kB 10%]\n                                           \n3% [Waiting for headers]\n                        \nGet:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libencode-locale-perl all 1.05-1.1 [11.8 kB]\n\n3% [3 libencode-locale-perl 8248 B/11.8 kB 70%]\n                                               \n4% [Waiting for headers]\n                        \nGet:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtimedate-perl all 2.3300-2 [34.0 kB]\n\n4% [4 libtimedate-perl 1970 B/34.0 kB 6%]\n                                         \n6% [Waiting for headers]\n                        \nGet:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libhttp-date-perl all 6.05-1 [9920 B]\n\n6% [5 libhttp-date-perl 1403 B/9920 B 14%]\n                                          \n7% [Waiting for headers]\n                        \nGet:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfile-listing-perl all 6.14-1 [11.2 kB]\n\n8% [6 libfile-listing-perl 4225 B/11.2 kB 38%]\n                                              \n8% [Working]\n            \nGet:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfont-afm-perl all 1.20-3 [13.6 kB]\n\n9% [7 libfont-afm-perl 10.1 kB/13.6 kB 74%]\n                                           \n10% [Working]\n             \nGet:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libhtml-tagset-perl all 3.20-4 [12.5 kB]\n\n10% [8 libhtml-tagset-perl 1957 B/12.5 kB 16%]\n                                              \n11% [Working]\n             \nGet:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 liburi-perl all 5.10-1 [78.8 kB]\n\n11% [9 liburi-perl 1237 B/78.8 kB 2%]\n                                     \n15% [Working]\n             \nGet:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libhtml-parser-perl amd64 3.76-1build2 [88.4 kB]\n\n15% [10 libhtml-parser-perl 7183 B/88.4 kB 8%]\n                                              \n20% [Waiting for headers]\n                         \nGet:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libio-html-perl all 1.004-2 [15.4 kB]\n\n20% [11 libio-html-perl 5824 B/15.4 kB 38%]\n                                           \n21% [Waiting for headers]\n                         \nGet:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblwp-mediatypes-perl all 6.04-1 [19.5 kB]\n\n21% [12 liblwp-mediatypes-perl 1158 B/19.5 kB 6%]\n                                                 \n22% [Working]\n             \nGet:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 libhttp-message-perl all 6.36-1 [76.8 kB]\n\n23% [13 libhttp-message-perl 13.3 kB/76.8 kB 17%]\n                                                 \n27% [Waiting for headers]\n                         \nGet:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 libhtml-form-perl all 6.07-1 [22.2 kB]\n\n27% [14 libhtml-form-perl 21.8 kB/22.2 kB 98%]\n                                              \n28% [Waiting for headers]\n                         \nGet:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libhtml-tree-perl all 5.07-2 [200 kB]\n\n29% [15 libhtml-tree-perl 21.7 kB/200 kB 11%]\n                                             \n38% [Waiting for headers]\n                         \nGet:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 libhtml-format-perl all 2.12-1.1 [41.3 kB]\n\n39% [16 libhtml-format-perl 27.4 kB/41.3 kB 66%]\n                                                \n40% [Waiting for headers]\n                         \nGet:17 http://archive.ubuntu.com/ubuntu jammy/main amd64 libhttp-cookies-perl all 6.10-1 [18.4 kB]\n\n41% [17 libhttp-cookies-perl 18.4 kB/18.4 kB 100%]\n                                                  \n42% [Working]\n             \nGet:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libhttp-daemon-perl all 6.13-1ubuntu0.1 [22.9 kB]\n\n43% [18 libhttp-daemon-perl 22.9 kB/22.9 kB 100%]\n                                                 \n43% [Working]\n             \nGet:19 http://archive.ubuntu.com/ubuntu jammy/main amd64 libhttp-negotiate-perl all 6.01-1 [12.5 kB]\n\n44% [19 libhttp-negotiate-perl 12.5 kB/12.5 kB 100%]\n                                                    \n44% [Working]\n             \nGet:20 http://archive.ubuntu.com/ubuntu jammy/main amd64 perl-openssl-defaults amd64 5build2 [7542 B]\n\n45% [20 perl-openssl-defaults 6951 B/7542 B 92%]\n                                                \n45% [Waiting for headers]\n                         \nGet:21 http://archive.ubuntu.com/ubuntu jammy/main amd64 libnet-ssleay-perl amd64 1.92-1build2 [327 kB]\n\n47% [21 libnet-ssleay-perl 40.3 kB/327 kB 12%]\n                                              \n61% [Waiting for headers]\n                         \nGet:22 http://archive.ubuntu.com/ubuntu jammy/main amd64 libio-socket-ssl-perl all 2.074-2 [192 kB]\n\n63% [22 libio-socket-ssl-perl 47.7 kB/192 kB 25%]\n                                                 \n70% [Working]\n             \nGet:23 http://archive.ubuntu.com/ubuntu jammy/main amd64 libnet-http-perl all 6.22-1 [23.2 kB]\n\n71% [23 libnet-http-perl 23.2 kB/23.2 kB 100%]\n                                              \n71% [Waiting for headers]\n                         \nGet:24 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtry-tiny-perl all 0.31-1 [21.8 kB]\n\n72% [24 libtry-tiny-perl 2606 B/21.8 kB 12%]\n                                            \n73% [Waiting for headers]\n                         \nGet:25 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwww-robotrules-perl all 6.02-1 [12.6 kB]\n\n74% [Waiting for headers]\n                         \nGet:26 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwww-perl all 6.61-1 [141 kB]\n\n77% [26 libwww-perl 65.2 kB/141 kB 46%]\n                                       \n81% [Waiting for headers]\n                         \nGet:27 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblwp-protocol-https-perl all 6.10-1 [10.9 kB]\n\n82% [Waiting for headers]\n                         \nGet:28 http://archive.ubuntu.com/ubuntu jammy/main amd64 libnet-smtp-ssl-perl all 1.04-1 [5948 B]\n\n83% [Waiting for headers]\n                         \nGet:29 http://archive.ubuntu.com/ubuntu jammy/main amd64 libmailtools-perl all 2.21-1 [80.7 kB]\n\n86% [29 libmailtools-perl 65.2 kB/80.7 kB 81%]\n                                              \n87% [Waiting for headers]\n                         \nGet:30 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxml-parser-perl amd64 2.46-3build1 [212 kB]\n\n88% [30 libxml-parser-perl 4052 B/212 kB 2%]\n                                            \n97% [Waiting for headers]\n                         \nGet:31 http://archive.ubuntu.com/ubuntu jammy/main amd64 libauthen-sasl-perl all 2.1600-1.1 [43.1 kB]\n\n99% [31 libauthen-sasl-perl 43.1 kB/43.1 kB 100%]\n                                                 \n100% [Working]\n              \n\ndebconf: delaying package configuration, since apt-utils is not installed\nSelecting previously unselected package libclone-perl.\n(Reading database ... \n(Reading database ... 5%\n(Reading database ... 10%\n(Reading database ... 15%\n(Reading database ... 20%\n(Reading database ... 25%\n(Reading database ... 30%\n(Reading database ... 35%\n(Reading database ... 40%\n(Reading database ... 45%\n(Reading database ... 50%\n(Reading database ... 55%\n(Reading database ... 60%\n(Reading database ... 65%\n(Reading database ... 70%\n(Reading database ... 75%\n(Reading database ... 80%\n(Reading database ... 85%\n(Reading database ... 90%\n(Reading database ... 95%\n(Reading database ... 100%\n(Reading database ... 65058 files and directories currently installed.)\nPreparing to unpack .../00-libclone-perl_0.45-1build3_amd64.deb ...\nUnpacking libclone-perl (0.45-1build3) ...\nSelecting previously unselected package libdata-dump-perl.\nPreparing to unpack .../01-libdata-dump-perl_1.25-1_all.deb ...\nUnpacking libdata-dump-perl (1.25-1) ...\nSelecting previously unselected package libencode-locale-perl.\nPreparing to unpack .../02-libencode-locale-perl_1.05-1.1_all.deb ...\nUnpacking libencode-locale-perl (1.05-1.1) ...\nSelecting previously unselected package libtimedate-perl.\nPreparing to unpack .../03-libtimedate-perl_2.3300-2_all.deb ...\nUnpacking libtimedate-perl (2.3300-2) ...\nSelecting previously unselected package libhttp-date-perl.\nPreparing to unpack .../04-libhttp-date-perl_6.05-1_all.deb ...\nUnpacking libhttp-date-perl (6.05-1) ...\nSelecting previously unselected package libfile-listing-perl.\nPreparing to unpack .../05-libfile-listing-perl_6.14-1_all.deb ...\nUnpacking libfile-listing-perl (6.14-1) ...\nSelecting previously unselected package libfont-afm-perl.\nPreparing to unpack .../06-libfont-afm-perl_1.20-3_all.deb ...\nUnpacking libfont-afm-perl (1.20-3) ...\nSelecting previously unselected package libhtml-tagset-perl.\nPreparing to unpack .../07-libhtml-tagset-perl_3.20-4_all.deb ...\nUnpacking libhtml-tagset-perl (3.20-4) ...\nSelecting previously unselected package liburi-perl.\nPreparing to unpack .../08-liburi-perl_5.10-1_all.deb ...\nUnpacking liburi-perl (5.10-1) ...\nSelecting previously unselected package libhtml-parser-perl:amd64.\nPreparing to unpack .../09-libhtml-parser-perl_3.76-1build2_amd64.deb ...\nUnpacking libhtml-parser-perl:amd64 (3.76-1build2) ...\nSelecting previously unselected package libio-html-perl.\nPreparing to unpack .../10-libio-html-perl_1.004-2_all.deb ...\nUnpacking libio-html-perl (1.004-2) ...\nSelecting previously unselected package liblwp-mediatypes-perl.\nPreparing to unpack .../11-liblwp-mediatypes-perl_6.04-1_all.deb ...\nUnpacking liblwp-mediatypes-perl (6.04-1) ...\nSelecting previously unselected package libhttp-message-perl.\nPreparing to unpack .../12-libhttp-message-perl_6.36-1_all.deb ...\nUnpacking libhttp-message-perl (6.36-1) ...\nSelecting previously unselected package libhtml-form-perl.\nPreparing to unpack .../13-libhtml-form-perl_6.07-1_all.deb ...\nUnpacking libhtml-form-perl (6.07-1) ...\nSelecting previously unselected package libhtml-tree-perl.\nPreparing to unpack .../14-libhtml-tree-perl_5.07-2_all.deb ...\nUnpacking libhtml-tree-perl (5.07-2) ...\nSelecting previously unselected package libhtml-format-perl.\nPreparing to unpack .../15-libhtml-format-perl_2.12-1.1_all.deb ...\nUnpacking libhtml-format-perl (2.12-1.1) ...\nSelecting previously unselected package libhttp-cookies-perl.\nPreparing to unpack .../16-libhttp-cookies-perl_6.10-1_all.deb ...\nUnpacking libhttp-cookies-perl (6.10-1) ...\nSelecting previously unselected package libhttp-daemon-perl.\nPreparing to unpack .../17-libhttp-daemon-perl_6.13-1ubuntu0.1_all.deb ...\nUnpacking libhttp-daemon-perl (6.13-1ubuntu0.1) ...\nSelecting previously unselected package libhttp-negotiate-perl.\nPreparing to unpack .../18-libhttp-negotiate-perl_6.01-1_all.deb ...\nUnpacking libhttp-negotiate-perl (6.01-1) ...\nSelecting previously unselected package perl-openssl-defaults:amd64.\nPreparing to unpack .../19-perl-openssl-defaults_5build2_amd64.deb ...\nUnpacking perl-openssl-defaults:amd64 (5build2) ...\nSelecting previously unselected package libnet-ssleay-perl:amd64.\nPreparing to unpack .../20-libnet-ssleay-perl_1.92-1build2_amd64.deb ...\nUnpacking libnet-ssleay-perl:amd64 (1.92-1build2) ...\nSelecting previously unselected package libio-socket-ssl-perl.\nPreparing to unpack .../21-libio-socket-ssl-perl_2.074-2_all.deb ...\nUnpacking libio-socket-ssl-perl (2.074-2) ...\nSelecting previously unselected package libnet-http-perl.\nPreparing to unpack .../22-libnet-http-perl_6.22-1_all.deb ...\nUnpacking libnet-http-perl (6.22-1) ...\nSelecting previously unselected package libtry-tiny-perl.\nPreparing to unpack .../23-libtry-tiny-perl_0.31-1_all.deb ...\nUnpacking libtry-tiny-perl (0.31-1) ...\nSelecting previously unselected package libwww-robotrules-perl.\nPreparing to unpack .../24-libwww-robotrules-perl_6.02-1_all.deb ...\nUnpacking libwww-robotrules-perl (6.02-1) ...\nSelecting previously unselected package libwww-perl.\nPreparing to unpack .../25-libwww-perl_6.61-1_all.deb ...\nUnpacking libwww-perl (6.61-1) ...\nSelecting previously unselected package liblwp-protocol-https-perl.\nPreparing to unpack .../26-liblwp-protocol-https-perl_6.10-1_all.deb ...\nUnpacking liblwp-protocol-https-perl (6.10-1) ...\nSelecting previously unselected package libnet-smtp-ssl-perl.\nPreparing to unpack .../27-libnet-smtp-ssl-perl_1.04-1_all.deb ...\nUnpacking libnet-smtp-ssl-perl (1.04-1) ...\nSelecting previously unselected package libmailtools-perl.\nPreparing to unpack .../28-libmailtools-perl_2.21-1_all.deb ...\nUnpacking libmailtools-perl (2.21-1) ...\nSelecting previously unselected package libxml-parser-perl:amd64.\nPreparing to unpack .../29-libxml-parser-perl_2.46-3build1_amd64.deb ...\nUnpacking libxml-parser-perl:amd64 (2.46-3build1) ...\nSelecting previously unselected package libauthen-sasl-perl.\nPreparing to unpack .../30-libauthen-sasl-perl_2.1600-1.1_all.deb ...\nUnpacking libauthen-sasl-perl (2.1600-1.1) ...\nSetting up libfont-afm-perl (1.20-3) ...\nSetting up libclone-perl (0.45-1build3) ...\nSetting up libhtml-tagset-perl (3.20-4) ...\nSetting up libauthen-sasl-perl (2.1600-1.1) ...\nSetting up liblwp-mediatypes-perl (6.04-1) ...\nSetting up libtry-tiny-perl (0.31-1) ...\nSetting up perl-openssl-defaults:amd64 (5build2) ...\nSetting up libencode-locale-perl (1.05-1.1) ...\nSetting up libdata-dump-perl (1.25-1) ...\nSetting up libio-html-perl (1.004-2) ...\nSetting up libtimedate-perl (2.3300-2) ...\nSetting up liburi-perl (5.10-1) ...\nSetting up libnet-ssleay-perl:amd64 (1.92-1build2) ...\nSetting up libhttp-date-perl (6.05-1) ...\nSetting up libfile-listing-perl (6.14-1) ...\nSetting up libnet-http-perl (6.22-1) ...\nSetting up libwww-robotrules-perl (6.02-1) ...\nSetting up libhtml-parser-perl:amd64 (3.76-1build2) ...\nSetting up libio-socket-ssl-perl (2.074-2) ...\nSetting up libhttp-message-perl (6.36-1) ...\nSetting up libhtml-form-perl (6.07-1) ...\nSetting up libhttp-negotiate-perl (6.01-1) ...\nSetting up libhttp-cookies-perl (6.10-1) ...\nSetting up libhtml-tree-perl (5.07-2) ...\nSetting up libhtml-format-perl (2.12-1.1) ...\nSetting up libnet-smtp-ssl-perl (1.04-1) ...\nSetting up libmailtools-perl (2.21-1) ...\nSetting up libhttp-daemon-perl (6.13-1ubuntu0.1) ...\nSetting up liblwp-protocol-https-perl (6.10-1) ...\nSetting up libwww-perl (6.61-1) ...\nSetting up libxml-parser-perl:amd64 (2.46-3build1) ...\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!apt-get install -y libxml-parser-perl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ae5db3",
   "metadata": {},
   "source": [
    "### Step 36"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f2f547",
   "metadata": {},
   "source": [
    "Execution Start time: 04:44:53, End time: 04:48:02 (duration: 189 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c83bb38",
   "metadata": {},
   "source": [
    "**Thought**: The `XML::Parser` module has been successfully installed. Let's try running the model training and evaluation command again to see if this resolves the issue.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e78753",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8220c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "W&B online. Running your script from this directory will now sync to the cloud.\n22 Aug 04:44    INFO 66 parameters found.\n================================================================================\n\n# General Hyper Parameters: \n\ngpu_id: 0\nuse_gpu: True\ndevice: cpu\nseed: 2020\nreproducibility: True\ncmd: run_textbox.py --model=BART --dataset=xsum --model_path=facebook/bart-base --epochs=1\nfilename: BART-xsum-2024-Aug-22_04-44-57\nsaved_dir: saved/\nstate: INFO\nwandb: online\n\n\n# Training Hyper Parameters: \n\ndo_train: True\ndo_valid: True\noptimizer: adamw\nadafactor_kwargs: {'lr': 0.001, 'scale_parameter': False, 'relative_step': False, 'warmup_init': False}\noptimizer_kwargs: {}\nvalid_steps: 1\nvalid_strategy: epoch\nstopping_steps: 2\nepochs: 1\nlearning_rate: 3e-05\ntrain_batch_size: 4\ngrad_clip: 0.1\naccumulation_steps: 48\ndisable_tqdm: False\nresume_training: True\n\n\n# Evaluation Hyper Parameters: \n\ndo_test: True\nlower_evaluation: True\nmultiref_strategy: max\nbleu_max_ngrams: 4\nbleu_type: nltk\nsmoothing_function: 0\ncorpus_bleu: False\nrouge_max_ngrams: 2\nrouge_type: files2rouge\nmeteor_type: pycocoevalcap\nchrf_type: m-popovic\ndistinct_max_ngrams: 4\ninter_distinct: True\nunique_max_ngrams: 4\nself_bleu_max_ngrams: 4\ntgt_lang: en\nmetrics: ['rouge']\neval_batch_size: 16\ncorpus_meteor: True\n\n\n# Model Hyper Parameters: \n\nmodel: BART\nmodel_name: bart\nmodel_path: facebook/bart-base\nconfig_kwargs: {}\ntokenizer_kwargs: {'use_fast': True}\ngeneration_kwargs: {'num_beams': 5, 'no_repeat_ngram_size': 3, 'early_stopping': True}\nefficient_kwargs: {}\nefficient_methods: []\nefficient_unfreeze_model: False\nlabel_smoothing: 0.1\n\n\n# Dataset Hyper Parameters: \n\ndataset: xsum\ndata_path: dataset/xsum\ntgt_lang: en\nsrc_len: 1024\ntgt_len: 128\ntruncate: tail\nprefix_prompt: Summarize: \nmetrics_for_best_model: ['rouge-1', 'rouge-2', 'rouge-l']\n\n\n# Unrecognized Hyper Parameters: \n\nfind_unused_parameters: False\ntokenizer_add_tokens: []\nload_type: from_pretrained\n\n================================================================================\n/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n22 Aug 04:44    INFO Pretrain type: pretrain disabled\nwandb: Tracking run with wandb version 0.17.7\nwandb: W&B syncing is set to `offline` in this directory.  \nwandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.\n/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n22 Aug 04:45    INFO Pretrained_Models(\n  (model): BartForConditionalGeneration(\n    (model): BartModel(\n      (shared): Embedding(50265, 768, padding_idx=1)\n      (encoder): BartEncoder(\n        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n        (layers): ModuleList(\n          (0-5): 6 x BartEncoderLayer(\n            (self_attn): BartSdpaAttention(\n            )\n            (activation_fn): GELUActivation()\n          )\n        )\n      )\n      (decoder): BartDecoder(\n        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n        (layers): ModuleList(\n          (0-5): 6 x BartDecoderLayer(\n            (self_attn): BartSdpaAttention(\n            )\n            (activation_fn): GELUActivation()\n            (encoder_attn): BartSdpaAttention(\n            )\n          )\n        )\n      )\n    )\n  )\n)\nTrainable parameters: 139420416\n22 Aug 04:45    INFO ====== Start training ======\n\ntrain    1:   0%|                            | 0/1 [00:00<?, ?step/s, loss=None]\ntrain    1: 100%|████████████████████| 1/1 [00:23<00:00, 23.45s/step, loss=None]\ntrain    1: 100%|███████████████████████| 1/1 [00:23<00:00, 23.45s/step, loss=5]\ntrain    1: 100%|███████████████████████| 1/1 [00:23<00:00, 23.46s/step, loss=5]\n22 Aug 04:45    INFO Train epoch  1 [time: 23.46s, loss: 5.00]\n\ngenerating:   0%|                                         | 0/1 [00:00<?, ?it/s]\ngenerating: 100%|█████████████████████████████████| 1/1 [01:14<00:00, 74.83s/it]\ngenerating: 100%|█████████████████████████████████| 1/1 [01:14<00:00, 74.83s/it]\n22 Aug 04:46    INFO  Validation  1 (best) [time: 75.06s, score: 35.27, <rouge-1: 17.94>, <rouge-2: 2.84>, <rouge-l: 14.49>, rouge-w-1.2: 9.95]\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n22 Aug 04:46    INFO Soft link created: saved/BART-xsum-2024-Aug-22_04-44-57/checkpoint_best -> /content/TextBox/saved/BART-xsum-2024-Aug-22_04-44-57/checkpoint_epoch-1\n22 Aug 04:46    INFO ====== Finished training, best validation result at train epoch 1 ======\n22 Aug 04:46    INFO Best valid result: score: 35.27, <rouge-1: 17.94>, <rouge-2: 2.84>, <rouge-l: 14.49>, rouge-w-1.2: 9.95\n22 Aug 04:46    INFO Loading model structure and parameters from saved/BART-xsum-2024-Aug-22_04-44-57/checkpoint_best ...\n\ngenerating:   0%|                                         | 0/1 [00:00<?, ?it/s]\ngenerating: 100%|█████████████████████████████████| 1/1 [01:16<00:00, 76.74s/it]\ngenerating: 100%|█████████████████████████████████| 1/1 [01:16<00:00, 76.74s/it]\n22 Aug 04:47    INFO Evaluation result:\n score: 34.79,\n <rouge-1: 16.95>,\n <rouge-2: 3.16>,\n <rouge-l: 14.68>,\n rouge-w-1.2: 9.92\n22 Aug 04:47    INFO Epoch   [time: 77.28s, score: 34.79, <rouge-1: 16.95>, <rouge-2: 3.16>, <rouge-l: 14.68>, rouge-w-1.2: 9.92]\nwandb: - 0.000 MB of 0.000 MB uploaded\nwandb:                                                                                \nwandb: \nwandb: Run history:\nwandb:          loss/train ▁\nwandb:     metrics/rouge-1 ▁\nwandb:     metrics/rouge-2 ▁\nwandb:     metrics/rouge-l ▁\nwandb: metrics/rouge-w-1.2 ▁\nwandb:        test/rouge-1 ▁\nwandb:        test/rouge-2 ▁\nwandb:        test/rouge-l ▁\nwandb:    test/rouge-w-1.2 ▁\nwandb: \nwandb: Run summary:\nwandb:          loss/train 4.58426\nwandb:     metrics/rouge-1 17.94\nwandb:     metrics/rouge-2 2.84\nwandb:     metrics/rouge-l 14.49\nwandb: metrics/rouge-w-1.2 9.95\nwandb:        test/rouge-1 16.95\nwandb:        test/rouge-2 3.16\nwandb:        test/rouge-l 14.68\nwandb:    test/rouge-w-1.2 9.92\nwandb: \nwandb: You can sync this run to the cloud by running:\nwandb: wandb sync saved/BART-xsum-2024-Aug-22_04-44-57/wandb/offline-run-20240822_044459-r2v2q2oe\nwandb: Find logs at: saved/BART-xsum-2024-Aug-22_04-44-57/wandb/offline-run-20240822_044459-r2v2q2oe/logs\nwandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python run_textbox.py --model=BART --dataset=xsum --model_path=facebook/bart-base --epochs=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1031af11",
   "metadata": {},
   "source": [
    "### Step 37"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b203ad",
   "metadata": {},
   "source": [
    "Execution Start time: 04:48:05, End time: 04:48:05 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d88c6e",
   "metadata": {},
   "source": [
    "**Thought**: The model training and evaluation have completed successfully. The evaluation results are as follows:\n",
    "\n",
    "- rouge-1: 16.95\n",
    "- rouge-2: 3.16\n",
    "- rouge-L: 14.68\n",
    "- rouge-w-1.2: 9.92\n",
    "\n",
    "Let's format these results into the required JSON structure and submit the answer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e920acfc",
   "metadata": {},
   "source": [
    "**Action (submit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de65690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{rouge-1: 16.95, rouge-2: 3.16, rouge-L: 14.68, rouge-w-1.2: 9.92}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit {\"rouge-1\": 16.95, \"rouge-2\": 3.16, \"rouge-L\": 14.68, \"rouge-w-1.2\": 9.92}"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
