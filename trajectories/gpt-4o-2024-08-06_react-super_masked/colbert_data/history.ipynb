{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30a9be08",
   "metadata": {},
   "source": [
    "### Query\n",
    "Train a ColBERT model on my data, available on `https://drive.google.com/file/d/1xP0nIRu_aJ_LvQMW1cz3M4nYWIv2orTO/edit`. Report lass loss value as a json structured as follows: {\"last_loss\": 0.0} (replace 0.0 with the actual value).\n",
    "\n",
    "Additional instructions:\n",
    "1. Use the branch colbertv1.\n",
    "2. Use the following hyperparameters: * amp\n",
    "* mask_punctuation\n",
    "* batch size = 16\n",
    "* accumulation steps = 1\n",
    "* similarity function = 'l2'\n",
    "* document max length = 180\n",
    "\n",
    "Git repository: https://github.com/stanford-futuredata/ColBERT\n",
    "\n",
    "I have already executed some of the required steps. However, I did not yet make any required changes to load the request dataset. Your goal is to successfully load the dataset and complete the remaining steps to achieve the user request. You can re-run any of the pre-executed steps, and finally should successfully run\n",
    "```\n",
    "!python -m colbert.train --amp --doc_maxlen 180 --mask-punctuation --bsize 16 --accum 1 \\\n",
    "--triples /content/training_data.tsv \\\n",
    "--root experiments/ --experiment MSMARCO-psg --similarity l2 --run msmarco.psg.l2-b\n",
    "```\n",
    "to submit the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded0c483",
   "metadata": {},
   "source": [
    "### Prompt\n",
    "<details>\n",
    "[{'role': 'system', 'content': 'Interact in a jupyter notebook to solve the request.\\nOutput a thought and an action.\\nThought can reason about the current situation, and action allows you to interact with the Jupyter notebook. There are three types of actions:\\n(1) execute, which executes a notebook cell with the command under the \"content\" field. This could be a bash command (beginning with !), a python command, or magic command (beginning with %).\\n(2) edit, which allows you to replace existing lines in a file with new lines. To replace lines, you will need to provide the lines as they appear before the replacement, and the lines to replace them with.\\nIn the \"content\" field, format should be ```{filename}\\n<<<BEFORE_EDIT>>>\\nlines before edit\\nlines before edit\\n<<<AFTER_EDIT>>>\\nlines after edit\\nlines after edit```\\n  - Include the entire lines WITH indentation. After editing, make sure indentation is correct.\\n(3) submit, which submits the final answer (or null if there\\'s no answer to submit), formatted as JSON (make sure to use double quotes for keys and values).\\nAlways output exactly one \"thought\" and one \"action\", and encapsulate the content with <content>...</content> tags.\\nI will provide the cell output (observation) for you.\\n\\nTips:\\n- When a github repository is given, you should start by cloning it. You can read the readme file and browse the repo for installation and usage instructions.\\n- Avoid using conda, instead use pip for package management.\\n- Each execution has a 5-minute timeout. After this period, you can decide whether to interrupt the kernel or continue waiting.\\n  You should interrupt if: (1) you are prompted with an interactive message (e.g., \"Do you want to continue? [Y/n]\"). In that case, you can bypass it by executing echo \"y\" | command, or (2) the execution is taking longer than expected because the wrong task is being executed, such as a different model or incorrect number of epochs.\\n  You should continue to wait if the current execution is correct and proceeding as expected.\\n\\nHere are some examples.\\nGit repository: EleutherAI/lm-evaluation-harness\\nQuery: Evaluate EleutherAI/gpt-j-6B on hellaswag. What\\'s the score?\\nThought: First, I need to clone the GitHub repository \\'EleutherAI/lm-evaluation-harness\\'.\\nAction: execute\\n<content>\\n!git clone https://github.com/EleutherAI/lm-evaluation-harness.git\\n</content>\\nObservation: ```\\nCloning into \\'lm-evaluation-harness\\'...\\n```\\nThought: The repository has been cloned. Next, I should check the README file for installation and usage instructions to proceed with evaluating EleutherAI/gpt-j-6B on hellaswag.\\nAction: execute\\n<content>\\n!cat lm-evaluation-harness/README.md\\n</content>\\nObservation: ```# Language Model Evaluation Harness ... truncated ...\\n```\\nThought: The README provides detailed instructions for installing the `lm-eval` package and evaluating models like GPT-J-6B on tasks like hellaswag. I need to install the package and its dependencies first.\\nAction: execute\\n<content>\\n!pip install -e lm-evaluation-harness/.\\n</content>\\nObservation: ```\\nObtaining file:///home/nora/lm-evaluation-harness\\n  Installing build dependencies ... truncated ...\\n```\\nThought: The `lm-eval` package and its dependencies have been installed successfully. Now, I can proceed with evaluating the EleutherAI/gpt-j-6B model on the hellaswag task as per the instructions in the README.\\nAction: execute\\n<content>\\n!lm_eval --model hf --model_args pretrained=EleutherAI/gpt-j-6B --tasks hellaswag --device cuda:0 --batch_size 8\\n</content>\\nObservation: ```\\n{\\n  \"hellaswag\": {\\n    \"acc\": 0.24603658536585363,\\n    \"acc_norm\": 0.34436059706446426,\\n  },  ... truncated ...\\n```\\nThought: The `lm-eval` was executed successfully, providing the metrics I needed. The `acc` metric is 0.2460. The `acc_norm` metric is 0.3444\\nAction: submit\\n<content>\\n{\"acc\": 0.2460, \"acc_norm\": 0.3444}\\n</content>\\n\\nQuery: use pandas to read the file \\'data.csv\\' and display the first 2 rows.\\nThought: First, I need to read the file \\'data.csv\\' using the pandas library.\\nAction: execute\\n<content>\\nimport pandas as pd\\npd.read_csv(\\'data.csv\\').head()\\n</content>\\nObservation: ```\\n    name\\n  0  Alice\\n   1    Bob\\n  2  Charlie\\n```\\nThought: The file \\'data.csv\\' has been read successfully and the first 2 rows have been displayed.\\nAction: submit\\n<content>\\n[\"Alice\", \"Bob\"]\\n</content>\\n\\nQuery: Change the model name in the \\'config.json\\' file from \\'roberta-base\\' to \\'roberta-large\\'.\\nThought: First, I need to read the \\'config.json\\' file to find the location where I can replace the model name.\\nAction: execute\\n<content>\\n!cat config.json\\n</content>\\nObservation: ```\\n{\\n    \"model\": \"roberta-base\",\\n    \"batch_size\": 8,\\n    \"epochs\": 10\\n}\\n```\\nThought: The file has been read successfully. Now, I can replace the model name in the \\'config.json\\' file.\\nAction: edit\\n<content>\\nconfig.json\\n<<<BEFORE_EDIT>>>\\n{\\n    \"model\": \"roberta-base\",\\n    \"batch_size\": 8,\\n<<<AFTER_EDIT>>>\\n{\\n    \"model\": \"roberta-large\",\\n    \"batch_size\": 8,\\n</content>\\nObservation: \"editing file config.json\"\\nThought: The model name has been replaced in the \\'config.json\\' file.\\nAction: submit\\n<content>\\nnull\\n</content>\\n'}, {'role': 'user', 'content': 'Query: Train a ColBERT model on my data, available on `https://drive.google.com/file/d/1xP0nIRu_aJ_LvQMW1cz3M4nYWIv2orTO/edit`. Report lass loss value as a json structured as follows: {\"last_loss\": 0.0} (replace 0.0 with the actual value).\\n\\nAdditional instructions:\\n1. Use the branch colbertv1.\\n2. Use the following hyperparameters: * amp\\n* mask_punctuation\\n* batch size = 16\\n* accumulation steps = 1\\n* similarity function = \\'l2\\'\\n* document max length = 180\\n\\nGit repository: https://github.com/stanford-futuredata/ColBERT\\n\\nI have already executed some of the required steps. However, I did not yet make any required changes to load the request dataset. Your goal is to successfully load the dataset and complete the remaining steps to achieve the user request. You can re-run any of the pre-executed steps, and finally should successfully run\\n```\\n!python -m colbert.train --amp --doc_maxlen 180 --mask-punctuation --bsize 16 --accum 1 \\\\\\n--triples /content/training_data.tsv \\\\\\n--root experiments/ --experiment MSMARCO-psg --similarity l2 --run msmarco.psg.l2-b\\n```\\nto submit the answer.'}]\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d632a8",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3ebc59",
   "metadata": {},
   "source": [
    "Execution Start time: 22:33:56, End time: 22:33:56 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5597c4",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6188ab6",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3587cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I want to train a ColBERT model on my data, available on https://drive.google.com/file/d/1xP0nIRu_aJ_LvQMW1cz3M4nYWIv2orTO/edit.\n",
    "# \n",
    "# Use the `stanford-futuredata/ColBERT` repository, branch `colbertv1`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc209d71",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886bc895",
   "metadata": {},
   "source": [
    "Execution Start time: 22:33:57, End time: 22:33:57 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceaf869",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f42f14",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7ac88b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Package installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a993cb90",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba048c83",
   "metadata": {},
   "source": [
    "Execution Start time: 22:33:57, End time: 22:33:57 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf00b5c",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0185481",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a559a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cloning into 'ColBERT'...\nremote: Enumerating objects: 2810, done.\u001b[K\nremote: Counting objects: 100% (1321/1321)\u001b[K\nremote: Counting objects: 100% (1321/1321), done.\u001b[K\nremote: Compressing objects: 100% (427/427)\u001b[K\nremote: Compressing objects: 100% (427/427), done.\u001b[K\nremote: Total 2810 (delta 1023), reused 983 (delta 890), pack-reused 1489 (from 1)\u001b[K\nReceiving objects: 100% (2810/2810)\nReceiving objects: 100% (2810/2810), 2.06 MiB | 30.51 MiB/s, done.\nResolving deltas: 100% (1777/1777)\nResolving deltas: 100% (1777/1777), done.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!git clone https://github.com/stanford-futuredata/ColBERT.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408e0eeb",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a01276",
   "metadata": {},
   "source": [
    "Execution Start time: 22:33:57, End time: 22:33:57 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144c9112",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364e0a98",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb2a6a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/ColBERT\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd ColBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b045979",
   "metadata": {},
   "source": [
    "### Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83f80e5",
   "metadata": {},
   "source": [
    "Execution Start time: 22:33:57, End time: 22:33:58 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01267435",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34edcc85",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3979405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Branch 'colbertv1' set up to track remote branch 'colbertv1' from 'origin'.\nSwitched to a new branch 'colbertv1'\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!git checkout colbertv1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aa7ed0",
   "metadata": {},
   "source": [
    "### Step 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383d433b",
   "metadata": {},
   "source": [
    "Execution Start time: 22:33:58, End time: 22:33:58 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebcfa41",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e2b6fe",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4240e032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since conda is not natively supported, instead we install the requirements listed in the conda env file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1675cf41",
   "metadata": {},
   "source": [
    "### Step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14cebe8",
   "metadata": {},
   "source": [
    "Execution Start time: 22:33:58, End time: 22:36:04 (duration: 126 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5853b54",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f36578",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c97831c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting mlflow\n  Downloading mlflow-2.15.1-py3-none-any.whl.metadata (29 kB)\nRequirement already satisfied: tensorboard in /usr/local/lib/python3.10/site-packages (2.15.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (4.66.4)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/site-packages (4.40.2)\nCollecting ujson\n  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\nCollecting faiss-cpu\n  Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/site-packages (2.2.1+cpu)\nCollecting torchvision\n  Downloading torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.0 kB)\nCollecting mlflow-skinny==2.15.1 (from mlflow)\n  Downloading mlflow_skinny-2.15.1-py3-none-any.whl.metadata (30 kB)\nCollecting Flask<4 (from mlflow)\n  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\nCollecting alembic!=1.10.0,<2 (from mlflow)\n  Downloading alembic-1.13.2-py3-none-any.whl.metadata (7.4 kB)\nCollecting docker<8,>=4.0.0 (from mlflow)\n  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\nCollecting graphene<4 (from mlflow)\n  Downloading graphene-3.3-py2.py3-none-any.whl.metadata (7.7 kB)\nRequirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.10/site-packages (from mlflow) (3.6)\nRequirement already satisfied: matplotlib<4 in /usr/local/lib/python3.10/site-packages (from mlflow) (3.7.1)\nRequirement already satisfied: numpy<2 in /usr/local/lib/python3.10/site-packages (from mlflow) (1.25.2)\nRequirement already satisfied: pandas<3 in /usr/local/lib/python3.10/site-packages (from mlflow) (2.0.3)\nCollecting pyarrow<16,>=4.0.0 (from mlflow)\n  Downloading pyarrow-15.0.2-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\nCollecting querystring-parser<2 (from mlflow)\n  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl.metadata (559 bytes)\nRequirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.10/site-packages (from mlflow) (1.5.0)\nRequirement already satisfied: scipy<2 in /usr/local/lib/python3.10/site-packages (from mlflow) (1.11.4)\nCollecting sqlalchemy<3,>=1.4.0 (from mlflow)\n  Downloading SQLAlchemy-2.0.32-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\nRequirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.10/site-packages (from mlflow) (3.1.4)\nCollecting gunicorn<23 (from mlflow)\n  Downloading gunicorn-22.0.0-py3-none-any.whl.metadata (4.4 kB)\nRequirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.10/site-packages (from mlflow-skinny==2.15.1->mlflow) (5.3.3)\nRequirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/site-packages (from mlflow-skinny==2.15.1->mlflow) (8.1.7)\nRequirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.10/site-packages (from mlflow-skinny==2.15.1->mlflow) (2.0.0)\nCollecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.15.1->mlflow)\n  Downloading databricks_sdk-0.30.0-py3-none-any.whl.metadata (37 kB)\nCollecting entrypoints<1 (from mlflow-skinny==2.15.1->mlflow)\n  Downloading entrypoints-0.4-py3-none-any.whl.metadata (2.6 kB)\nCollecting gitpython<4,>=3.1.9 (from mlflow-skinny==2.15.1->mlflow)\n  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /usr/local/lib/python3.10/site-packages (from mlflow-skinny==2.15.1->mlflow) (4.8.1)\nCollecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==2.15.1->mlflow)\n  Downloading opentelemetry_api-1.26.0-py3-none-any.whl.metadata (1.4 kB)\nCollecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==2.15.1->mlflow)\n  Downloading opentelemetry_sdk-1.26.0-py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: packaging<25 in /usr/local/lib/python3.10/site-packages (from mlflow-skinny==2.15.1->mlflow) (24.1)\nRequirement already satisfied: protobuf<6,>=3.12.0 in /usr/local/lib/python3.10/site-packages (from mlflow-skinny==2.15.1->mlflow) (5.27.1)\nRequirement already satisfied: pytz<2025 in /usr/local/lib/python3.10/site-packages (from mlflow-skinny==2.15.1->mlflow) (2024.1)\nRequirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.10/site-packages (from mlflow-skinny==2.15.1->mlflow) (6.0.1)\nRequirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.10/site-packages (from mlflow-skinny==2.15.1->mlflow) (2.31.0)\nCollecting sqlparse<1,>=0.4.0 (from mlflow-skinny==2.15.1->mlflow)\n  Downloading sqlparse-0.5.1-py3-none-any.whl.metadata (3.9 kB)\nRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/site-packages (from tensorboard) (2.1.0)\nRequirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/site-packages (from tensorboard) (1.64.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/site-packages (from tensorboard) (2.30.0)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/site-packages (from tensorboard) (1.2.0)\nRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/site-packages (from tensorboard) (68.1.2)\nRequirement already satisfied: six>1.9 in /usr/local/lib/python3.10/site-packages (from tensorboard) (1.16.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/site-packages (from tensorboard) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/site-packages (from tensorboard) (3.0.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/site-packages (from transformers) (0.23.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/site-packages (from torch) (2023.6.0)\nCollecting torch\n  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/site-packages (from torchvision) (10.3.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.20.5 (from torch)\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==3.0.0 (from torch)\n  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n  Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting Mako (from alembic!=1.10.0,<2->mlflow)\n  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/site-packages (from docker<8,>=4.0.0->mlflow) (2.0.7)\nCollecting itsdangerous>=2.1.2 (from Flask<4->mlflow)\n  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\nCollecting blinker>=1.6.2 (from Flask<4->mlflow)\n  Downloading blinker-1.8.2-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.4.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\nCollecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n  Downloading graphql_core-3.2.3-py3-none-any.whl.metadata (10 kB)\nCollecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\nCollecting aniso8601<10,>=8 (from graphene<4->mlflow)\n  Downloading aniso8601-9.0.1-py2.py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from Jinja2<4,>=2.11->mlflow) (2.1.5)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/site-packages (from matplotlib<4->mlflow) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/site-packages (from matplotlib<4->mlflow) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.4.5)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/site-packages (from matplotlib<4->mlflow) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/site-packages (from matplotlib<4->mlflow) (2.9.0.post0)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/site-packages (from pandas<3->mlflow) (2024.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.15.1->mlflow) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.15.1->mlflow) (3.7)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.15.1->mlflow) (2024.6.2)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (3.5.0)\nCollecting greenlet!=0.4.17 (from sqlalchemy<3,>=1.4.0->mlflow)\n  Downloading greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\nRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nCollecting gitdb<5,>=4.0.1 (from gitpython<4,>=3.1.9->mlflow-skinny==2.15.1->mlflow)\n  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\nRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/site-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow-skinny==2.15.1->mlflow) (3.19.2)\nCollecting deprecated>=1.2.6 (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.15.1->mlflow)\n  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\nCollecting importlib-metadata!=4.7.0,<8,>=3.7.0 (from mlflow-skinny==2.15.1->mlflow)\n  Downloading importlib_metadata-7.2.1-py3-none-any.whl.metadata (4.6 kB)\nCollecting opentelemetry-semantic-conventions==0.47b0 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.15.1->mlflow)\n  Downloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.6.0)\nRequirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\nCollecting wrapt<2,>=1.10 (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.15.1->mlflow)\n  Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.15.1->mlflow)\n  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\nDownloading mlflow-2.15.1-py3-none-any.whl (26.3 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/26.3 MB ? eta -:--:--\n\n\n\n\u001b[?25hDownloading mlflow_skinny-2.15.1-py3-none-any.whl (5.5 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/5.5 MB ? eta -:--:--\n\n\u001b[?25hDownloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/53.6 kB ? eta -:--:--\n\n\u001b[?25hDownloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/27.0 MB ? eta -:--:--\n\n\n\n\n\u001b[?25hDownloading torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/7.0 MB ? eta -:--:--\n\n\u001b[?25hDownloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/797.2 MB ? eta -:--:--\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/410.6 MB ? eta -:--:--\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/14.1 MB ? eta -:--:--\n\n\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/23.7 MB ? eta -:--:--\n\n\n\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/823.6 kB ? eta -:--:--\n\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/664.8 MB ? eta -:--:--\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/121.6 MB ? eta -:--:--\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/56.5 MB ? eta -:--:--\n\n\n\n\n\n\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/124.2 MB ? eta -:--:--\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/196.0 MB ? eta -:--:--\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/176.2 MB ? eta -:--:--\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/99.1 kB ? eta -:--:--\n\n\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/209.4 MB ? eta -:--:--\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[?25hDownloading alembic-1.13.2-py3-none-any.whl (232 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/233.0 kB ? eta -:--:--\n\n\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/147.8 kB ? eta -:--:--\n\n\u001b[?25hDownloading flask-3.0.3-py3-none-any.whl (101 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/101.7 kB ? eta -:--:--\n\n\u001b[?25hDownloading graphene-3.3-py2.py3-none-any.whl (128 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/128.2 kB ? eta -:--:--\n\n\u001b[?25hDownloading gunicorn-22.0.0-py3-none-any.whl (84 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/84.4 kB ? eta -:--:--\n\n\u001b[?25hDownloading pyarrow-15.0.2-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/38.3 MB ? eta -:--:--\n\n\n\n\n\n\n\u001b[?25hDownloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\nDownloading SQLAlchemy-2.0.32-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/3.1 MB ? eta -:--:--\n\n\u001b[?25hDownloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/52.8 kB ? eta -:--:--\n\n\u001b[?25hDownloading blinker-1.8.2-py3-none-any.whl (9.5 kB)\nDownloading databricks_sdk-0.30.0-py3-none-any.whl (538 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/538.8 kB ? eta -:--:--\n\n\u001b[?25hDownloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\nDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/207.3 kB ? eta -:--:--\n\n\u001b[?25hDownloading graphql_core-3.2.3-py3-none-any.whl (202 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/202.9 kB ? eta -:--:--\n\n\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\nDownloading greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (616 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/616.0 kB ? eta -:--:--\n\n\u001b[?25hDownloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\nDownloading opentelemetry_api-1.26.0-py3-none-any.whl (61 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/61.5 kB ? eta -:--:--\n\n\u001b[?25hDownloading importlib_metadata-7.2.1-py3-none-any.whl (25 kB)\nDownloading opentelemetry_sdk-1.26.0-py3-none-any.whl (109 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/109.5 kB ? eta -:--:--\n\n\u001b[?25hDownloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl (138 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/138.0 kB ? eta -:--:--\n\n\u001b[?25hDownloading sqlparse-0.5.1-py3-none-any.whl (44 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/44.2 kB ? eta -:--:--\n\n\u001b[?25hDownloading Mako-1.3.5-py3-none-any.whl (78 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/78.6 kB ? eta -:--:--\n\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/19.7 MB ? eta -:--:--\n\n\n\n\u001b[?25hDownloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\nDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/62.7 kB ? eta -:--:--\n\n\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\nDownloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/80.3 kB ? eta -:--:--\n\n\u001b[?25hInstalling collected packages: aniso8601, wrapt, ujson, triton, sqlparse, smmap, querystring-parser, pyarrow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, Mako, itsdangerous, importlib-metadata, gunicorn, greenlet, graphql-core, faiss-cpu, entrypoints, blinker, sqlalchemy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, graphql-relay, gitdb, Flask, docker, deprecated, opentelemetry-api, nvidia-cusolver-cu12, graphene, gitpython, databricks-sdk, alembic, torch, opentelemetry-semantic-conventions, torchvision, opentelemetry-sdk, mlflow-skinny, mlflow\n  Attempting uninstall: importlib-metadata\n    Found existing installation: importlib-metadata 4.8.1\n    Uninstalling importlib-metadata-4.8.1:\n      Successfully uninstalled importlib-metadata-4.8.1\n  Attempting uninstall: torch\n    Found existing installation: torch 2.2.1+cpu\n    Uninstalling torch-2.2.1+cpu:\n      Successfully uninstalled torch-2.2.1+cpu\nSuccessfully installed Flask-3.0.3 Mako-1.3.5 alembic-1.13.2 aniso8601-9.0.1 blinker-1.8.2 databricks-sdk-0.30.0 deprecated-1.2.14 docker-7.1.0 entrypoints-0.4 faiss-cpu-1.8.0.post1 gitdb-4.0.11 gitpython-3.1.43 graphene-3.3 graphql-core-3.2.3 graphql-relay-3.2.0 greenlet-3.0.3 gunicorn-22.0.0 importlib-metadata-7.2.1 itsdangerous-2.2.0 mlflow-2.15.1 mlflow-skinny-2.15.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 opentelemetry-api-1.26.0 opentelemetry-sdk-1.26.0 opentelemetry-semantic-conventions-0.47b0 pyarrow-15.0.2 querystring-parser-1.2.4 smmap-5.0.1 sqlalchemy-2.0.32 sqlparse-0.5.1 torch-2.4.0 torchvision-0.19.0 triton-3.0.0 ujson-5.10.0 wrapt-1.16.0\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install mlflow tensorboard tqdm transformers ujson faiss-cpu torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae7be48",
   "metadata": {},
   "source": [
    "### Step 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8762fe85",
   "metadata": {},
   "source": [
    "Execution Start time: 22:36:04, End time: 22:36:04 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cef144",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11a9bab",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e1673e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79232d57",
   "metadata": {},
   "source": [
    "### Step 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5210a9",
   "metadata": {},
   "source": [
    "Execution Start time: 22:36:04, End time: 22:36:04 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8a7615",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92b3d47",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cc5a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/ColBERT\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/ColBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046b4db5",
   "metadata": {},
   "source": [
    "### Step 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf2e36c",
   "metadata": {},
   "source": [
    "Execution Start time: 22:36:04, End time: 22:36:13 (duration: 8 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80b94c5",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351de165",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf5b473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n\n[Aug 20, 22:36:10] #> Creating directory /content/ColBERT/experiments/MSMARCO-psg/train.py/msmarco.psg.l2 \n\n\n2024/08/20 22:36:10 INFO mlflow.tracking.fluent: Experiment with name 'MSMARCO-psg/train.py' does not exist. Creating a new experiment.\n\n\n[Aug 20, 22:36:11] #> Creating directory /content/ColBERT/experiments/MSMARCO-psg/train.py/msmarco.psg.l2/logs/ \n\n\n[Aug 20, 22:36:11] {'root': 'experiments/', 'experiment': 'MSMARCO-psg', 'run': 'msmarco.psg.l2', 'rank': -1, 'similarity': 'l2', 'dim': 128, 'query_maxlen': 32, 'doc_maxlen': 180, 'mask_punctuation': True, 'resume': False, 'resume_optimizer': False, 'checkpoint': None, 'lr': 3e-06, 'maxsteps': 400000, 'bsize': 2, 'accumsteps': 1, 'amp': True, 'triples': '/content/training_data.tsv', 'queries': None, 'collection': None} \n\n/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\ntokenizer_config.json:   0%|                         | 0.00/48.0 [00:00<?, ?B/s]\ntokenizer_config.json: 100%|██████████████████| 48.0/48.0 [00:00<00:00, 288kB/s]\n\nvocab.txt:   0%|                                     | 0.00/232k [00:00<?, ?B/s]\n\n\ntokenizer.json:   0%|                                | 0.00/466k [00:00<?, ?B/s]\n\n\nconfig.json:   0%|                                    | 0.00/570 [00:00<?, ?B/s]\n\n[Aug 20, 22:36:12] Traceback (most recent call last):\n  File \"/content/ColBERT/colbert/utils/runs.py\", line 70, in context\n    yield\n  File \"/content/ColBERT/colbert/train.py\", line 30, in main\n    train(args)\n  File \"/content/ColBERT/colbert/training/training.py\", line 38, in train\n    reader = EagerBatcher(args, (0 if args.rank == -1 else args.rank), args.nranks)\n  File \"/content/ColBERT/colbert/training/eager_batcher.py\", line 21, in __init__\n    self._reset_triples()\n  File \"/content/ColBERT/colbert/training/eager_batcher.py\", line 24, in _reset_triples\n    self.reader = open(self.triples_path, mode='r', encoding=\"utf-8\")\nFileNotFoundError: [Errno 2] No such file or directory: '/content/training_data.tsv'\n\n \n\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/local/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/content/ColBERT/colbert/train.py\", line 34, in <module>\n    main()\n  File \"/content/ColBERT/colbert/train.py\", line 30, in main\n    train(args)\n  File \"/content/ColBERT/colbert/training/training.py\", line 38, in train\n    reader = EagerBatcher(args, (0 if args.rank == -1 else args.rank), args.nranks)\n  File \"/content/ColBERT/colbert/training/eager_batcher.py\", line 21, in __init__\n    self._reset_triples()\n  File \"/content/ColBERT/colbert/training/eager_batcher.py\", line 24, in _reset_triples\n    self.reader = open(self.triples_path, mode='r', encoding=\"utf-8\")\nFileNotFoundError: [Errno 2] No such file or directory: '/content/training_data.tsv'\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python -m colbert.train --amp --doc_maxlen 180 --mask-punctuation --bsize 2 --accum 1 \\\n",
    "--triples /content/training_data.tsv \\\n",
    "--root experiments/ --experiment MSMARCO-psg --similarity l2 --run msmarco.psg.l2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326b21a6",
   "metadata": {},
   "source": [
    "### Step 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448185af",
   "metadata": {},
   "source": [
    "Execution Start time: 22:36:13, End time: 22:36:13 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf81bd6c",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fab941",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0548307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cuda is hard-coded, but we don't have a gpu, change to load cpu when cuda not available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f49b38",
   "metadata": {},
   "source": [
    "### Step 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1b16de",
   "metadata": {},
   "source": [
    "Execution Start time: 22:36:13, End time: 22:36:13 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036fa314",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeb873c",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf662ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\nimport torch\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nSAVED_CHECKPOINTS = [32*1000, 100*1000, 150*1000, 200*1000, 300*1000, 400*1000]\nSAVED_CHECKPOINTS += [10*1000, 20*1000, 30*1000, 40*1000, 50*1000, 60*1000, 70*1000, 80*1000, 90*1000]\nSAVED_CHECKPOINTS += [25*1000, 50*1000, 75*1000]\n\nSAVED_CHECKPOINTS = set(SAVED_CHECKPOINTS)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/ColBERT/colbert/parameters.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "import torch\n",
    "\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "\n",
    "SAVED_CHECKPOINTS = [32*1000, 100*1000, 150*1000, 200*1000, 300*1000, 400*1000]\n",
    "<<<AFTER_EDIT>>>\n",
    "import torch\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SAVED_CHECKPOINTS = [32*1000, 100*1000, 150*1000, 200*1000, 300*1000, 400*1000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335a5f49",
   "metadata": {},
   "source": [
    "### Step 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f956a891",
   "metadata": {},
   "source": [
    "Execution Start time: 22:36:15, End time: 22:36:19 (duration: 4 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0895a48d",
   "metadata": {},
   "source": [
    "**Thought**: The code has been successfully edited to use the CPU if CUDA is not available. Now, I need to download the dataset from the provided Google Drive link and save it as `/content/training_data.tsv` so that it can be used for training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d54afdb",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feef52a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/usr/local/lib/python3.10/site-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom: https://drive.google.com/uc?id=1xP0nIRu_aJ_LvQMW1cz3M4nYWIv2orTO\nTo: /content/training_data.tsv\n\n  0%|                                               | 0.00/37.4k [00:00<?, ?B/s]\n\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!gdown --id 1xP0nIRu_aJ_LvQMW1cz3M4nYWIv2orTO -O /content/training_data.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79cc899",
   "metadata": {},
   "source": [
    "### Step 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52941420",
   "metadata": {},
   "source": [
    "Execution Start time: 22:36:21, End time: 22:36:30 (duration: 8 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7984813c",
   "metadata": {},
   "source": [
    "**Thought**: The dataset has been successfully downloaded and saved as `/content/training_data.tsv`. Now, I can proceed with training the ColBERT model using the specified hyperparameters and the downloaded dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a6ac34",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a775cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n\n[Aug 20, 22:36:26] #> Creating directory /content/ColBERT/experiments/MSMARCO-psg/train.py/msmarco.psg.l2-b \n\n\n\n\n[Aug 20, 22:36:26] #> Creating directory /content/ColBERT/experiments/MSMARCO-psg/train.py/msmarco.psg.l2-b/logs/ \n\n\n[Aug 20, 22:36:26] {'root': 'experiments/', 'experiment': 'MSMARCO-psg', 'run': 'msmarco.psg.l2-b', 'rank': -1, 'similarity': 'l2', 'dim': 128, 'query_maxlen': 32, 'doc_maxlen': 180, 'mask_punctuation': True, 'resume': False, 'resume_optimizer': False, 'checkpoint': None, 'lr': 3e-06, 'maxsteps': 400000, 'bsize': 16, 'accumsteps': 1, 'amp': True, 'triples': '/content/training_data.tsv', 'queries': None, 'collection': None} \n\n/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\nmodel.safetensors:   0%|                             | 0.00/440M [00:00<?, ?B/s]\n\n\n\n\n\n\n\n\n\n\n\n\nSome weights of ColBERT were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['linear.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/content/ColBERT/colbert/utils/amp.py:17: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler()\n/usr/local/lib/python3.10/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n  warnings.warn(\n[Aug 20, 22:36:29] Traceback (most recent call last):\n  File \"/content/ColBERT/colbert/utils/runs.py\", line 70, in context\n    yield\n  File \"/content/ColBERT/colbert/train.py\", line 30, in main\n    train(args)\n  File \"/content/ColBERT/colbert/training/training.py\", line 91, in train\n    for batch_idx, BatchSteps in zip(range(start_batch_idx, args.maxsteps), reader):\n  File \"/content/ColBERT/colbert/training/eager_batcher.py\", line 33, in __next__\n    for line_idx, line in zip(range(self.bsize * self.nranks), self.reader):\n  File \"/usr/local/lib/python3.10/codecs.py\", line 322, in decode\n    (result, consumed) = self._buffer_decode(data, self.errors, final)\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xe7 in position 58: invalid continuation byte\n\n \n\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/local/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/content/ColBERT/colbert/train.py\", line 34, in <module>\n    main()\n  File \"/content/ColBERT/colbert/train.py\", line 30, in main\n    train(args)\n  File \"/content/ColBERT/colbert/training/training.py\", line 91, in train\n    for batch_idx, BatchSteps in zip(range(start_batch_idx, args.maxsteps), reader):\n  File \"/content/ColBERT/colbert/training/eager_batcher.py\", line 33, in __next__\n    for line_idx, line in zip(range(self.bsize * self.nranks), self.reader):\n  File \"/usr/local/lib/python3.10/codecs.py\", line 322, in decode\n    (result, consumed) = self._buffer_decode(data, self.errors, final)\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xe7 in position 58: invalid continuation byte\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python -m colbert.train --amp --doc_maxlen 180 --mask-punctuation --bsize 16 --accum 1 \\\n",
    "--triples /content/training_data.tsv \\\n",
    "--root experiments/ --experiment MSMARCO-psg --similarity l2 --run msmarco.psg.l2-b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7164352a",
   "metadata": {},
   "source": [
    "### Step 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1227504",
   "metadata": {},
   "source": [
    "Execution Start time: 22:36:31, End time: 22:36:32 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a7c9d0",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that there is an issue with decoding the dataset file using UTF-8 encoding. This suggests that the file might be encoded in a different format. I need to check the encoding of the file and possibly convert it to UTF-8 to resolve this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be533ad",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c09854c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/usr/bin/sh: 1: file: not found\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!file -i /content/training_data.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5459b4",
   "metadata": {},
   "source": [
    "### Step 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c251e535",
   "metadata": {},
   "source": [
    "Execution Start time: 22:36:34, End time: 22:36:34 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc041060",
   "metadata": {},
   "source": [
    "**Thought**: The `file` command is not available in this environment, which means I cannot directly check the file encoding using this command. Instead, I can try reading the file using Python and specifying different encodings to identify the correct one. Once identified, I can convert the file to UTF-8 encoding if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e29cb5",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a507d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Failed with encoding: utf-8, error: 'utf-8' codec can't decode byte 0xe7 in position 58: invalid continuation byte\nSuccessfully read with encoding: latin1\nSuccessfully read with encoding: iso-8859-1\nFailed with encoding: cp1252, error: 'charmap' codec can't decode byte 0x81 in position 137: character maps to <undefined>\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attempt to read the file with different encodings to identify the correct one\n",
    "encodings = ['utf-8', 'latin1', 'iso-8859-1', 'cp1252']\n",
    "for encoding in encodings:\n",
    "    try:\n",
    "        with open('/content/training_data.tsv', encoding=encoding) as f:\n",
    "            f.readline()\n",
    "        print(f\"Successfully read with encoding: {encoding}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed with encoding: {encoding}, error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3e48db",
   "metadata": {},
   "source": [
    "### Step 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb86d39",
   "metadata": {},
   "source": [
    "Execution Start time: 22:36:36, End time: 22:36:36 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d20ec3",
   "metadata": {},
   "source": [
    "**Thought**: The file can be successfully read using the `latin1` or `iso-8859-1` encoding. I will convert the file to UTF-8 encoding using one of these encodings, which should resolve the issue with reading the file during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e3bb6b",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8847edd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the file to UTF-8 encoding\n",
    "with open('/content/training_data.tsv', encoding='latin1') as f:\n",
    "    content = f.read()\n",
    "\n",
    "with open('/content/training_data.tsv', 'w', encoding='utf-8') as f:\n",
    "    f.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7c8e36",
   "metadata": {},
   "source": [
    "### Step 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aafbd00",
   "metadata": {},
   "source": [
    "Execution Start time: 22:36:38, End time: 22:41:38 (duration: 300 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d1ace2",
   "metadata": {},
   "source": [
    "**Thought**: The file has been successfully converted to UTF-8 encoding. Now, I can proceed with training the ColBERT model using the specified hyperparameters and the converted dataset file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4ee611",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8e2a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n\n\n[Aug 20, 22:36:42] It seems that  /content/ColBERT/experiments/MSMARCO-psg/train.py/msmarco.psg.l2-b  already exists.\n[Aug 20, 22:36:42] Do you want to overwrite it? \t yes/no \n\n\n\nYour command has already run for 300 seconds. It is still running. You can continue to wait or interrupt it with Thought: ... \nAction: interrupt\n```\n``` \nor:\nThought: ... \nAction: continue_wait\n```\n```"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python -m colbert.train --amp --doc_maxlen 180 --mask-punctuation --bsize 16 --accum 1 \\\n",
    "--triples /content/training_data.tsv \\\n",
    "--root experiments/ --experiment MSMARCO-psg --similarity l2 --run msmarco.psg.l2-b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79078e8a",
   "metadata": {},
   "source": [
    "### Step 19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75c4271",
   "metadata": {},
   "source": [
    "Execution Start time: 22:41:40, End time: 22:41:40 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb396a5",
   "metadata": {},
   "source": [
    "**Thought**: \n",
    "Thought: The training script is prompting whether to overwrite an existing directory. I will bypass this prompt by using `echo \"yes\" |` to automatically confirm the overwrite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33df1258",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31060141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Error/Traceback: SyntaxError: invalid syntax (3201547401.py, line 1)  Cell In[23], line 1\n    echo \"yes\" | python -m colbert.train --amp --doc_maxlen 180 --mask-punctuation --bsize 16 --accum 1 \\\n         ^\nSyntaxError: invalid syntax\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "echo \"yes\" | python -m colbert.train --amp --doc_maxlen 180 --mask-punctuation --bsize 16 --accum 1 \\\n",
    "--triples /content/training_data.tsv \\\n",
    "--root experiments/ --experiment MSMARCO-psg --similarity l2 --run msmarco.psg.l2-b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a80986",
   "metadata": {},
   "source": [
    "### Step 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d782812a",
   "metadata": {},
   "source": [
    "Execution Start time: 22:41:43, End time: 22:41:50 (duration: 7 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cbe767",
   "metadata": {},
   "source": [
    "**Thought**: The error occurred because I attempted to use shell syntax directly in a Python cell. I need to execute the command using the correct syntax for a shell command in a Jupyter notebook by prefixing it with `!`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d24565e",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8979dfca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n\n\n[Aug 20, 22:41:47] It seems that  /content/ColBERT/experiments/MSMARCO-psg/train.py/msmarco.psg.l2-b  already exists.\n[Aug 20, 22:41:47] Do you want to overwrite it? \t yes/no \n\n\n\n[Aug 20, 22:41:48] #> Note: Output directory /content/ColBERT/experiments/MSMARCO-psg/train.py/msmarco.psg.l2-b/logs/ already exists\n\n\n[Aug 20, 22:41:48] {'root': 'experiments/', 'experiment': 'MSMARCO-psg', 'run': 'msmarco.psg.l2-b', 'rank': -1, 'similarity': 'l2', 'dim': 128, 'query_maxlen': 32, 'doc_maxlen': 180, 'mask_punctuation': True, 'resume': False, 'resume_optimizer': False, 'checkpoint': None, 'lr': 3e-06, 'maxsteps': 400000, 'bsize': 16, 'accumsteps': 1, 'amp': True, 'triples': '/content/training_data.tsv', 'queries': None, 'collection': None} \n\n/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nSome weights of ColBERT were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['linear.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/content/ColBERT/colbert/utils/amp.py:17: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler()\n/usr/local/lib/python3.10/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n  warnings.warn(\n[Aug 20, 22:41:49] Traceback (most recent call last):\n  File \"/content/ColBERT/colbert/utils/runs.py\", line 70, in context\n    yield\n  File \"/content/ColBERT/colbert/train.py\", line 30, in main\n    train(args)\n  File \"/content/ColBERT/colbert/training/training.py\", line 91, in train\n    for batch_idx, BatchSteps in zip(range(start_batch_idx, args.maxsteps), reader):\n  File \"/content/ColBERT/colbert/training/eager_batcher.py\", line 37, in __next__\n    query, pos, neg = line.strip().split('\\t')\nValueError: not enough values to unpack (expected 3, got 1)\n\n \n\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/local/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/content/ColBERT/colbert/train.py\", line 34, in <module>\n    main()\n  File \"/content/ColBERT/colbert/train.py\", line 30, in main\n    train(args)\n  File \"/content/ColBERT/colbert/training/training.py\", line 91, in train\n    for batch_idx, BatchSteps in zip(range(start_batch_idx, args.maxsteps), reader):\n  File \"/content/ColBERT/colbert/training/eager_batcher.py\", line 37, in __next__\n    query, pos, neg = line.strip().split('\\t')\nValueError: not enough values to unpack (expected 3, got 1)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!echo \"yes\" | python -m colbert.train --amp --doc_maxlen 180 --mask-punctuation --bsize 16 --accum 1 \\\n",
    "--triples /content/training_data.tsv \\\n",
    "--root experiments/ --experiment MSMARCO-psg --similarity l2 --run msmarco.psg.l2-b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6014229",
   "metadata": {},
   "source": [
    "### Step 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03e0a3c",
   "metadata": {},
   "source": [
    "Execution Start time: 22:41:52, End time: 22:41:52 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ff0749",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the dataset file does not have the expected format, which should contain three tab-separated values per line (query, positive, negative). I need to inspect the dataset file to understand its structure and make necessary adjustments to ensure it meets the expected format for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6d7cd4",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35bbb61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PK\u0003\u0004\u0014\u0000\b\b\b\u0000\u0000\u0000!\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0015\u0000K\u0000ColBERT/triplets.jsonUT\u0005\u0000\u0001Oyçe\n\u0000 \u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0018\u0000q÷p7oÚ\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000up\u001a\u0000\u0001E*ë<ColBERT/triplets.jsonZMoì¸\u0011¼/°ÿxçE`úÛ¹\u0004ä°@°¹d\u001c\u001c(ªGâZ\u0012µ$e?!ÈO55öó\u001ejBû`{äñ4Õ]Ý]]­}ÿÁ×\u001fúõå·MÒþåæËëè\tÙ¤Î}ùáÃ\u001bÖC\t/÷\\}¼¾ÈàÎ×íqù¿?\\úxzÄ\nn1Îô2G\\aö®½\u0006{c|5S\\\u0006ÓG³ÇÍ,\"½9ÅdòÞ/²\u001b·ô&o)Åmé\u0003ÞæpªÌ\u000erK\u000er×p5\u0005/¦D\u0013\\Ü4\u0012&Á\u000bqDIÌä=1ùÐâëq7>./r]ôÛÑ.ö;3ùHL>5$êDSF1Ó¶\f\u0019î÷®\u001cfõbçü33m)´Z°åc.êáA\nPµÂèB1e\u0019¨l\u000bª:QKÉ\nÒ\u0007É&#\u001f`Î\fIÄ\u000bµÊÂiÛây¤¦Ó¨\u0013\u0007e!´-1DÜ\u0012f7eãõÆËW\u001aÖ5Å~ófwäÎàf¡ï \tÍÂ|Ý\u0012æ$³Ì$MW\u0005Ô#h\bôw·\u0011Éý:Fób9`èÕm\u0007øÆ\u000bU!âúSuÆÇø¬'ócðÏ²I\u0002oÐ\u000fAyá~a\u0005æº¥Â8$:\u0010ij\u001aÀ\u001f£àßqUPí~Û46§\u0018ùý3l^·`\u0013°QCYÃâÊÖE3»¾eH@\u0013\"°t.»\u001f¡öº\u0005µH\t\u0004vV×çê\u0003fç¡ï¦©i\u0012\"Â½Þ[Lè`!ÏGVv\u0013JéÝe¢Æi3kAâÚÇ° »\u0014Oë\\F\u0013©Uá¨\u000eú+µÍàuÓ\u0002¯·ê\u0013Ãqÿæä:´4j¡é¦\u0005MïÙ\u0004{¤\u00194aFâL¢]\u001bBÜøm2\u001cÝ´u°÷\"º\u0002VBiÐ-ÃÑm\u000b>\u0010\u0013Ü\u0015âVÜså\u0007þDûRF3<ÓfrË tÛ\u0006¥ã>\u0015G³û\u001aæmÆQrMÙ¼u[¯Ô0ÃÑm\u000bP\u0002\\ÚM\u001fNè`²ø\u0004i\n/âfjáé¶¹s*õ,BËÂ-Ím3ld4ìêÕ-\u001fg\u001dcy_àåÌ\u0013óEò®%¿ \u0019ízRRÒM1öæE2jPí; ½©Ò½\u001f\u000en6À\u000bòÕnA«ÞoéhÛ^? §'¤Ô÷3¥#¹u\u0005O@S¢vXïÃ\\ó\tÅ8\thõ)cMAÍê'á¯Ô4ÿ]KüßÓù5Û\u0007ÓUX1¯\u0001lß¸SA\u0017vðþ4¡Mô½öä-\nzÂÉñ^uÏjÌ}KQÐ«K\u0015T·Ìà«^£ÝÉ\u0010\u0016jÁñ¾\u0005?öq-:o\u0004\u0016uÔJ®ëB¡è¿gØºÿ\f¶0F\u0006×U.ú\u001ct´£æ\u0018Äî[g*Xû\u0015õ*+Å\tYïÔ!¿VNt§\\BÙÔ\u0001ô\u0000\fh÷Í¦È¼¾E\u0014/ëP«­C*ý¬/õ\u0007¨föS½=ÇN\u0007XQ>\u000eZç\u0016Ú¨\u001e\u0018\u001eZð1\u000bàAR \u0003ºH Sì)Å¹Bó\u001cubñ(\u0000¾%»pö\u0007=\u0012ÃÜC\u000bæPPB\u0012Ù9¢]ä\u000fto\"Ö¨ÏS^+puÙkZQS\fZ\u000f¡>J\t\u0000lÂ\f2¿c¾5\u0019â\u0012®\u001f\u0019²\u001e[5¸}rÇ,\u0007L/\u0017zÄ#Óc3ï9æÛ\u0003Èøä¼QÞñÈòØ\u0002²¯<Ð9\u0006 x\u0001vÖ.;\u0001æ±¹\tj\u0016 -¢ß\u000bGË#CËcL wëÅGÜP<² #H\u0007³úÄ`òÔ¤ñH*á\u0014ÿ\u0018°§ÞLá|_ÅÃ·\u0001Ößà3ËTíih/xbPzjÒ?ÕÓ\u000b*|ü\u0003.X\u0011Òlþé7]´É\u0010õôvWiû'¯5¾!\u0005j!ê©\u0005QÞÒ´Q\u0002þ*§aþ\u0002v¨zØ©>VA©j¢\u0019ü®÷\u0019R\u001dúPÞQ(í\u0015Õ\u000e¯Z\tS\u0007LWÉ{·\u001b?\u0001Ï)ô\u0007{EuÄ«ÏNagÈj±Õ$Rbñ\u001aÊ¨\\6D4YÌL¢ò\"?\u000bC½jRØªLW~ãÍ0£Q\"a¯¨yõ\u00195SQ\\ÔÝë\"[\u001b/Ä\nWíÕÓË\u0010½LÜ\u0017ÄèfªTõ¸ºä8ÊÕ\u0010M~\nó\\W\nÓ\u0005Îd¹@Ý¤PÿN¦ò\u0011³6\u0018DìóÁ\u001a{ÑYe\f:§þ\u001fâÉ¶*¤Ð\u0018USHÇ´Lm\u001aªEÖíG½ÌÍS|µ«åÚ\u0007'ü\u0017¦­5\u000fÉÍ­R5ç?\u0005e¡5\u000e>E}5¿Y0õò[¥¸mRÄßÇ:ÔÉ¾çJ:Vu³\u000fk%\u001bFf$7\u0015³,¿mþ]aµ\"7äòVtÚ­J\u001d``@¥\tIª¢*ß¶IúÖ\u0014Óí^ÕÐ&9\u00153êrbrKÔÐ\u0017Â0ÊÖã\u0006\"\u0006\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few lines of the dataset to understand its structure\n",
    "!head /content/training_data.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e69396",
   "metadata": {},
   "source": [
    "### Step 22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9e3c0e",
   "metadata": {},
   "source": [
    "Execution Start time: 22:41:56, End time: 22:41:57 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4b2ec0",
   "metadata": {},
   "source": [
    "**Thought**: The dataset file appears to be a binary file, possibly a compressed archive or a different format, rather than a plain text TSV file. This is likely why the training script is unable to parse it correctly. I need to inspect the file further to determine its actual format and extract the necessary data for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97345b41",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194b22b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The file is a zip archive.\nError/Traceback: BadZipFile: Bad magic number for central directory---------------------------------------------------------------------------BadZipFile                                Traceback (most recent call last)Cell In[26], line 10\n      8 if zipfile.is_zipfile(file_path):\n      9     print(\"The file is a zip archive.\")\n---> 10     with zipfile.ZipFile(file_path, 'r') as zip_ref:\n     11         zip_ref.extractall('/content/')\n     12 elif tarfile.is_tarfile(file_path):\nFile /usr/local/lib/python3.10/zipfile.py:1269, in ZipFile.__init__(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\n   1267 try:\n   1268     if mode == 'r':\n-> 1269         self._RealGetContents()\n   1270     elif mode in ('w', 'x'):\n   1271         # set the modified flag so central directory gets written\n   1272         # even if no files are added to the archive\n   1273         self._didModify = True\nFile /usr/local/lib/python3.10/zipfile.py:1366, in ZipFile._RealGetContents(self)\n   1364 centdir = struct.unpack(structCentralDir, centdir)\n   1365 if centdir[_CD_SIGNATURE] != stringCentralDir:\n-> 1366     raise BadZipFile(\"Bad magic number for central directory\")\n   1367 if self.debug > 2:\n   1368     print(centdir)\nBadZipFile: Bad magic number for central directory"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attempt to identify the file type using Python's built-in libraries\n",
    "import zipfile\n",
    "import tarfile\n",
    "\n",
    "file_path = '/content/training_data.tsv'\n",
    "\n",
    "# Check if the file is a zip archive\n",
    "if zipfile.is_zipfile(file_path):\n",
    "    print(\"The file is a zip archive.\")\n",
    "    with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall('/content/')\n",
    "elif tarfile.is_tarfile(file_path):\n",
    "    print(\"The file is a tar archive.\")\n",
    "    with tarfile.open(file_path, 'r') as tar_ref:\n",
    "        tar_ref.extractall('/content/')\n",
    "else:\n",
    "    print(\"The file is not a recognized archive format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a592e514",
   "metadata": {},
   "source": [
    "### Step 23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c51fff",
   "metadata": {},
   "source": [
    "Execution Start time: 22:41:58, End time: 22:41:58 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e4b7a5",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51d79b9",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8db1c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0e2830b",
   "metadata": {},
   "source": [
    "### Step 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ae24f2",
   "metadata": {},
   "source": [
    "Execution Start time: 22:42:01, End time: 22:42:05 (duration: 4 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726a3ee3",
   "metadata": {},
   "source": [
    "**Thought**: \n",
    "Thought: The file is not a recognized archive format, and attempting to read it as a zip file resulted in a \"BadZipFile\" error. This suggests that the file might be corrupted or in an unexpected format. I need to ensure that the file is correctly downloaded and in the expected TSV format. I will re-download the file using the correct method and verify its contents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7517c58e",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ea8dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/usr/local/lib/python3.10/site-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom: https://drive.google.com/uc?id=1xP0nIRu_aJ_LvQMW1cz3M4nYWIv2orTO\nTo: /content/training_data.tsv\n\n  0%|                                               | 0.00/37.4k [00:00<?, ?B/s]\n\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!gdown --id 1xP0nIRu_aJ_LvQMW1cz3M4nYWIv2orTO -O /content/training_data.tsv --fuzzy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68cf32d",
   "metadata": {},
   "source": [
    "### Step 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eea0507",
   "metadata": {},
   "source": [
    "Execution Start time: 22:42:07, End time: 22:42:07 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6b4409",
   "metadata": {},
   "source": [
    "**Thought**: The file has been re-downloaded. I will now inspect the first few lines of the file to verify its contents and ensure it is in the expected TSV format with three tab-separated values per line (query, positive, negative). If the format is correct, I can proceed with training the model. If not, I will need to address any issues with the file format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e52e63",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44d73ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PK\u0003\u0004\u0014\u0000\b\b\b\u0000\u0000\u0000!\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0015\u0000K\u0000ColBERT/triplets.jsonUT\u0005\u0000\u0001Oy�e\n\u0000 \u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0018\u0000�q�p7o�\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000up\u001a\u0000\u0001E*�<ColBERT/triplets.json�ZMo�\u0011�/���x�E`�۹\u0004��@��d�\u001c�\u001c(�G�Z\u0012�$e?!�O55��\u001ejB�`{��4�]�]]�}�����\u001f���M�������\t٤�}���\u001b֘C\t/��\\}�������q��?\\�x�����z��\nn1��2G�\\a�����\u0006{c|5S\\\u0006�G���,\"�9�d��/�\u001b��&o)�m�\u0003��p��\u000erK\u000er�p�5\u0005/�D\u0013�\\�4�\u0012&�\u000b�qDI��=1����q7>./�r���]��ۇ�.�;3�HL>5��$�DSF1Ӷ\f\u0019���\u001cf�b��33m)�Z��c.��A\nP���B1e\u0019�l\u000b�:QK�\n�\u0007�&��#\u001f`�\fI�\u000b���i��y��Ө�\u0013\u0007�e!�-1D��\u0012f7e����˄W\u001a�5�~�fw���f��\t��|�\u0012�$�̝$MW\u0005Ԃ#h\b�w��\u0011��:F�b9`��m\u0007��\u000bU�!��Su����'�c�ϲ�I�\u0002o�\u000f�Ay�~a\u0005溥�8$:\u0010ij\u001a�\u001f���q�UP�~�46�\u0018��3l^�`\u0013��QC�Y����E3���eH@\u0013\"��t.��\u001f����\u0005�H\t�\u0004vV���\u0003f熡囹�i\u0012\"½�[L�`!�GVv\u0013J��ݜe��i3kA���ǰ �\u0014O�\\F\u0013�U�\u000e�+���u�\u0002���\u0013Ôq���:�4j���\u0005M��\u0004{��\u00194aF�L��]�\u001bB��m2\u001cݴu��\"��\u0002VBi�-��m\u000b�>\u0010\u0013�\u0015�V�s�\u0007��D�RF�3<�frˠt�\u0006��>\u0015G��\u001a�m�QrMټu[���0��m\u000b�P\u0002\\�M\u001fN�`���\u0004i\n/�fj��鶹s*�,B��-��m3ld�4���-\u001f�g\u001dc�y_���\u0013�E�%�� �\u0019�zRR�M1��E2jP�; ��ҽ\u001f\u000en6�\u000b�ՏnA���o�h�^?��'���3�#�u\u0005O@S�vX���\\�\t�8\th��)cM�A��'��4��]K����5��\u0007�UX�1�\u0001l߸SA\u0017v��4�M����-\nz���^u�j�}K�QЫK�\u0015T���^���\u0010\u0016j���\u0005�?�q-:o�\u0004\u0016u�J��B��gغ�\f�0F\u0006�U.�\u001ct���\u0018��[g*X�\u0015�*+�\tY��!��VNt�\\B��\u0001�\u0000\fh�ͅ�ȼ�E\u0014/�P��C*��/��\u0007���f�S�=�N\u0007X�Q>\u000eZ�\u0016ڨ\u001e\u0018�\u001eZ��1\u000b�A�R \u0003�H��S�)Ź�B�\u001cub�(\u0000��%�p�\u0007=\u0012��C\u000b�PPB��\u0012��9��]�\u000ft�o\"֨ϓS^+pu�kZQS\fZ\u000f��>J\t\u0000l��\f2��c�5�\u0019�\u0012�\u001f\u0019�\u001e[�5�}r�,\u0007L/\u0017z�#��c3�9��\u0003���Q��Ȁ��\u0002����<�9�\u0006�x\u0001v֐.;�\u0001汹\tj\u0016 -��\u000bG�#C�c�L�w����G�P<����#H\u0007���`�Ԥ�H*�\u0014��\u0018���L�|_�÷\u0001�߆�3�T�i�h/xbPzj��?��\u000b*|�\u0003.X\u0011�l���7]��\u0010���vWi��'��5��!\u0005j�!�\u0005Q�Ҵ��Q\u0002�*��a�\u0002v��z؂�>VA�j�\u0019����\u0019R\u001d�P�Q�(��\u0015�\u000e�Z����\tS\u0007LW�{�\u001b?\u0001�)�\u0007{Euī�Nag�j��$Rb�\u001aʨ\\6D4Y�L��\"?\u000bC��j��RتLW~�͐0�Q\"a���y�\u00195SQ\\���\"[�\u001b/Ę\n�W������\u0010�Lܝ\u0017��f�T����8��\u0010M~\n�\\W\n�\u0005�d�@ݤP�N��\u0011�6\u0018D���\u001a{�Ye\f:���\u001f��ɶ�*��\u0018USHǴ�Lm\u001a�E��G���S|����\u0007'�\u0017��5�\u000f�ͭR�5��?\u0005e�5�\u000e>E�}5��Y0��[���mR���:�ɾ��J:Vu�\u000fk%\u001bFf$7\u0015�,��m��]a�\"�7��VtڭJ\u001d�``@�\tI����*߶I��\u0014��^��&9\u00153�rbrK��Б\u0017��0��օ�\u0006\"\u0006���\n�.S���ۅ}\u0011Ec�>�M��R\u001f]�:��n�+j}��_\u0004gB�O�\"�sP|6������TU����iC�\u0000�y�\u001eWK\u0000\u0017�\u000b\u0012���mW�Q���F�r�u\u0017St\u0001q\u0016R�\u001f��2\f��\"�u����O|�F\u0005s�)��\u0001���%C\u0014\\�����6t\u0019 \\gd@-\u001d9�\nS4}J,_'�\n$�\nj���I.���!j\u001bK�%�<]�8n��q�y�NT�Ӷ���Zwx�q��Ri�6i�ow�u|�x����mWī?щ{�����T\u001c�M�\u001aJ�R���I��'� �\n�Q��5��.��^ٖ\u0005e5��Z�̫\u001a\u0015�m�\u0002�a\u0010�\u0002Fqy\u0011�C\u0003U�m�����\f�ar�8�J�\u001e\u0018��<\u0006A\t8_�G��iR�_b�\b�G�\u0013ˊ\u0014�X�B�mR��s\u0004!6��G��\u0005�P�z�m�f\u0017��\u0017��c�X\u0011��m�x��RE�6I��\u0006\"�ү�%7}�f�[\u001f8��ʺm��=�֡[e�ʔB�����������&xbQ��6��*v�\u001c�3(�-����#�4�\u000b{5K�vۤ�+Q\t\u0013�߉���v0\n��e��\u001a�m\u0012ٵ�:��UV7˦�� �RR����Gn�\u0002�Ik��FL�K~�\u0003,i\u000e\u0017�4K\u0015u�$��\u0013�mp\u0018֢[�%�\u0010U\u000b��A���\u001e��iY���IO����\u001adE?�{��\u001b�,�\u0004^%��n���#��,�����n\u001fK\u0014ݲ����o[\fDy7��@~\u0018\n�v�]i\"\u0010�TNC��0��Q,5��\u001fE�Z*Λ�_+�{�\u0012�MSp5��8�J�\u0010���i���y�P\n�6��(\u0013~������=\u0001��*�Iz�F\u0019;���{A:���m\u0012�B�7?���ż?�\u0019�;��$����\u0015\u0019�\\�W#�L\u0014TM�{I[��Q�F�\u0016EQ�����g�@xy�Z�}\u0017ۿ����\u0003PK\u0007\b�����\t\u0000\u0000�-\u0000\u0000PK\u0003\u0004\u0014\u0000\b\b\b\u0000\u0000\u0000!\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0016\u0000L\u0000ColBERT/documents.jsonUT\u0005\u0000\u0001Oy�e\n\u0000 \u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0018\u0000�q�p7o�\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000up\u001b\u0000\u0001��0�ColBERT/documents.json��s\u001bI�&�����\u0010ݣl�� ��!QJ[\u001b�()�J]%2+�zzm�\u0011�\u0000�\u0018����\u0014rw��}���{\u0004(�\n��f{M\u001d\u0014\tD�����;��_���)�����\u000f��Y���<�Ǘ��d�i�v��?~�m_t��+��,�LӪ/;7ϋ���k|�|r���J���L\\�|yuy��m�,oӼ.�R�|p��Z$�*/��r��2K:���]^��뒴Z��2�r�&�+���IW%��n�{�u�\"��/��t�\u0017Y��I���n5I\u0016n-���$q\u0019\u0006���+��\n�璇UU�)\u0006�qb�=f7�!6KW�;��}�s�Eլ]�z��ϛD�^6n�N����{2��������%\u000fy�Bw��W�~��R�[�S�(�,׎��\u0007_\u0014Gs�����d�,��씚.Y6U_��»�7-&�P5�{Z\u0015�\u0017���b3��4y+���n]\u0017�e����,\u00156�3_\nM9�eUe�_�y;�י=ue�L�\u0012ن�n�UU\u0015�1�@��LZ��B*C+����ռj��L\u001f������9�\u001b<;�\u001f<��ųlO\u001fo�7��\u0010�\\�ن�>�$4��}�@\n�E�*I~�2�ES����\u001d�jt�k��w���$I�1�6\u0007�M؛|��R��vS߶Ju�MUz%y\t��d<?%�I��\u0017u��O��}�p����\tۗK��m�umT�BJKs��]���+ eYV���O����.v|�\t�����r7!�\u0016���/�MZT�x�\\&_���L^�u�62\u001e2d�8=\u0015d؛�]�&�U�\u001b\u0019 ��UVD��s\u001d\u0016ʷ�Os�\u0001֑�®?J\u000b�w�j䵦��\u0004��d~!G�����\\�n��ky^�W��\"\u0007�>7v $_0�,�]��s���>����f/.�I|��,�<u����u�`7�g�W؈�A�gٯ�ho�3n㔧7�\t�k^\u0014}+/a��֣�ôZ��O׮py�\u001a�K?ϏG�k�H�����כ�J�-\u001d\u001e\b��ɺjx�b\u001fT}�l��!�\u000f�\u0011B�����\u0015c���\u000f2\u001c���$�\u000e��\u001d\"�^�\"�_�\\\u000eg�uM)�:�p���۔��4<�y\u001aj3��rf�a�\u000e~���$�h���I�^\u00148���\u0003�y&��~^�Ǎ���sf��������w�&y6\u0011\u0002�fzʟ&�e��O����C9OA��R�IJՕ�ȹ�i�\u001b]��uU�猜\f�1g�{!��\u0013��X�x�?[֭\n�\\,\u001c5\u0003�ʿ/�ɵ���UN\u00039\n�\u0017d��ţW�\u0012��x\u0013�����\u001bM�|̿\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few lines of the dataset to verify its contents\n",
    "!head /content/training_data.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65db128e",
   "metadata": {},
   "source": [
    "### Step 26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0829467d",
   "metadata": {},
   "source": [
    "Execution Start time: 22:42:10, End time: 22:42:10 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675f6ca3",
   "metadata": {},
   "source": [
    "**Thought**: The file appears to be a compressed archive rather than a plain text TSV file. The content shown is indicative of a zip or similar archive format. I need to extract the contents of this archive to access the actual dataset files. I will attempt to extract the archive using Python's built-in libraries for handling zip files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6e139d",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0825a545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Extraction successful.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "# Attempt to extract the contents of the file assuming it's a zip archive\n",
    "file_path = '/content/training_data.tsv'\n",
    "\n",
    "try:\n",
    "    with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall('/content/')\n",
    "    print(\"Extraction successful.\")\n",
    "except zipfile.BadZipFile:\n",
    "    print(\"The file is not a zip archive or is corrupted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ea5689",
   "metadata": {},
   "source": [
    "### Step 27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb7c914",
   "metadata": {},
   "source": [
    "Execution Start time: 22:42:12, End time: 22:42:12 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585c843f",
   "metadata": {},
   "source": [
    "**Thought**: The extraction was successful, indicating that the file was indeed a zip archive. Now, I need to check the extracted contents to locate the actual dataset file and verify its format. Once I have the correct dataset file, I can proceed with training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee27a52",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43a2702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColBERT  training_data.tsv\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the contents of the extracted directory to locate the dataset file\n",
    "!ls /content/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10753aaf",
   "metadata": {},
   "source": [
    "### Step 28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f26f7c2",
   "metadata": {},
   "source": [
    "Execution Start time: 22:42:13, End time: 22:42:14 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d7b345",
   "metadata": {},
   "source": [
    "**Thought**: The extracted contents do not show any additional files, which suggests that the extraction might have overwritten the original file or the dataset was not extracted correctly. I need to re-check the extraction process and ensure that the dataset file is correctly extracted and available for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5d27c3",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4e6b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PK\u0003\u0004\u0014\u0000\b\b\b\u0000\u0000\u0000!\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0015\u0000K\u0000ColBERT/triplets.jsonUT\u0005\u0000\u0001Oy�e\n\u0000 \u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0018\u0000�q�p7o�\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000up\u001a\u0000\u0001E*�<ColBERT/triplets.json�ZMo�\u0011�/���x�E`�۹\u0004��@��d�\u001c�\u001c(�G�Z\u0012�$e?!�O55��\u001ejB�`{��4�]�]]�}�����\u001f���M�������\t٤�}���\u001b֘C\t/��\\}�������q��?\\�x�����z��\nn1��2G�\\a�����\u0006{c|5S\\\u0006�G���,\"�9�d��/�\u001b��&o)�m�\u0003��p��\u000erK\u000er�p�5\u0005/�D\u0013�\\�4�\u0012&�\u000b�qDI��=1����q7>./�r���]��ۇ�.�;3�HL>5��$�DSF1Ӷ\f\u0019���\u001cf�b��33m)�Z��c.��A\nP���B1e\u0019�l\u000b�:QK�\n�\u0007�&��#\u001f`�\fI�\u000b���i��y��Ө�\u0013\u0007�e!�-1D��\u0012f7e����˄W\u001a�5�~�fw���f��\t��|�\u0012�$�̝$MW\u0005Ԃ#h\b�w��\u0011��:F�b9`��m\u0007��\u000bU�!��Su����'�c�ϲ�I�\u0002o�\u000f�Ay�~a\u0005溥�8$:\u0010ij\u001a�\u001f���q�UP�~�46�\u0018��3l^�`\u0013��QC�Y����E3���eH@\u0013\"��t.��\u001f����\u0005�H\t�\u0004vV���\u0003f熡囹�i\u0012\"½�[L�`!�GVv\u0013J��ݜe��i3kA���ǰ �\u0014O�\\F\u0013�U�\u000e�+���u�\u0002���\u0013Ôq���:�4j���\u0005M��\u0004{��\u00194aF�L��]�\u001bB��m2\u001cݴu��\"��\u0002VBi�-��m\u000b�>\u0010\u0013�\u0015�V�s�\u0007��D�RF�3<�frˠt�\u0006��>\u0015G��\u001a�m�QrMټu[���0��m\u000b�P\u0002\\�M\u001fN�`���\u0004i\n/�fj��鶹s*�,B��-��m3ld�4���-\u001f�g\u001dc�y_���\u0013�E�%�� �\u0019�zRR�M1��E2jP�; ��ҽ\u001f\u000en6�\u000b�ՏnA���o�h�^?��'���3�#�u\u0005O@S�vX���\\�\t�8\th��)cM�A��'��4��]K����5��\u0007�UX�1�\u0001l߸SA\u0017v��4�M����-\nz���^u�j�}K�QЫK�\u0015T���^���\u0010\u0016j���\u0005�?�q-:o�\u0004\u0016u�J��B��gغ�\f�0F\u0006�U.�\u001ct���\u0018��[g*X�\u0015�*+�\tY��!��VNt�\\B��\u0001�\u0000\fh�ͅ�ȼ�E\u0014/�P��C*��/��\u0007���f�S�=�N\u0007X�Q>\u000eZ�\u0016ڨ\u001e\u0018�\u001eZ��1\u000b�A�R \u0003�H��S�)Ź�B�\u001cub�(\u0000��%�p�\u0007=\u0012��C\u000b�PPB��\u0012��9��]�\u000ft�o\"֨ϓS^+pu�kZQS\fZ\u000f��>J\t\u0000l��\f2��c�5�\u0019�\u0012�\u001f\u0019�\u001e[�5�}r�,\u0007L/\u0017z�#��c3�9��\u0003���Q��Ȁ��\u0002����<�9�\u0006�x\u0001v֐.;�\u0001汹\tj\u0016 -��\u000bG�#C�c�L�w����G�P<����#H\u0007���`�Ԥ�H*�\u0014��\u0018���L�|_�÷\u0001�߆�3�T�i�h/xbPzj��?��\u000b*|�\u0003.X\u0011�l���7]��\u0010���vWi��'��5��!\u0005j�!�\u0005Q�Ҵ��Q\u0002�*��a�\u0002v��z؂�>VA�j�\u0019����\u0019R\u001d�P�Q�(��\u0015�\u000e�Z����\tS\u0007LW�{�\u001b?\u0001�)�\u0007{Euī�Nag�j��$Rb�\u001aʨ\\6D4Y�L��\"?\u000bC��j��RتLW~�͐0�Q\"a���y�\u00195SQ\\���\"[�\u001b/Ę\n�W������\u0010�Lܝ\u0017��f�T����8��\u0010M~\n�\\W\n�\u0005�d�@ݤP�N��\u0011�6\u0018D���\u001a{�Ye\f:���\u001f��ɶ�*��\u0018USHǴ�Lm\u001a�E��G���S|����\u0007'�\u0017��5�\u000f�ͭR�5��?\u0005e�5�\u000e>E�}5��Y0��[���mR���:�ɾ��J:Vu�\u000fk%\u001bFf$7\u0015�,��m��]a�\"�7��VtڭJ\u001d�``@�\tI����*߶I��\u0014��^��&9\u00153�rbrK��Б\u0017��0��օ�\u0006\"\u0006���\n�.S���ۅ}\u0011Ec�>�M��R\u001f]�:��n�+j}��_\u0004gB�O�\"�sP|6������TU����iC�\u0000�y�\u001eWK\u0000\u0017�\u000b\u0012���mW�Q���F�r�u\u0017St\u0001q\u0016R�\u001f��2\f��\"�u����O|�F\u0005s�)��\u0001���%C\u0014\\�����6t\u0019 \\gd@-\u001d9�\nS4}J,_'�\n$�\nj���I.���!j\u001bK�%�<]�8n��q�y�NT�Ӷ���Zwx�q��Ri�6i�ow�u|�x����mWī?щ{�����T\u001c�M�\u001aJ�R���I��'� �\n�Q��5��.��^ٖ\u0005e5��Z�̫\u001a\u0015�m�\u0002�a\u0010�\u0002Fqy\u0011�C\u0003U�m�����\f�ar�8�J�\u001e\u0018��<\u0006A\t8_�G��iR�_b�\b�G�\u0013ˊ\u0014�X�B�mR��s\u0004!6��G��\u0005�P�z�m�f\u0017��\u0017��c�X\u0011��m�x��RE�6I��\u0006\"�ү�%7}�f�[\u001f8��ʺm��=�֡[e�ʔB�����������&xbQ��6��*v�\u001c�3(�-����#�4�\u000b{5K�vۤ�+Q\t\u0013�߉���v0\n��e��\u001a�m\u0012ٵ�:��UV7˦�� �RR����Gn�\u0002�Ik��FL�K~�\u0003,i\u000e\u0017�4K\u0015u�$��\u0013�mp\u0018֢[�%�\u0010U\u000b��A���\u001e��iY���IO����\u001adE?�{��\u001b�,�\u0004^%��n���#��,�����n\u001fK\u0014ݲ����o[\fDy7��@~\u0018\n�v�]i\"\u0010�TNC��0��Q,5��\u001fE�Z*Λ�_+�{�\u0012�MSp5��8�J�\u0010���i���y�P\n�6��(\u0013~������=\u0001��*�Iz�F\u0019;���{A:���m\u0012�B�7?���ż?�\u0019�;��$����\u0015\u0019�\\�W#�L\u0014TM�{I[��Q�F�\u0016EQ�����g�@xy�Z�}\u0017ۿ����\u0003PK\u0007\b�����\t\u0000\u0000�-\u0000\u0000PK\u0003\u0004\u0014\u0000\b\b\b\u0000\u0000\u0000!\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0016\u0000L\u0000ColBERT/documents.jsonUT\u0005\u0000\u0001Oy�e\n\u0000 \u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0018\u0000�q�p7o�\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000up\u001b\u0000\u0001��0�ColBERT/documents.json��s\u001bI�&�����\u0010ݣl�� ��!QJ[\u001b�()�J]%2+�zzm�\u0011�\u0000�\u0018����\u0014rw��}���{\u0004(�\n��f{M\u001d\u0014\tD�����;��_���)�����\u000f��Y���<�Ǘ��d�i�v��?~�m_t��+��,�LӪ/;7ϋ���k|�|r���J���L\\�|yuy��m�,oӼ.�R�|p��Z$�*/��r��2K:���]^��뒴Z��2�r�&�+���IW%��n�{�u�\"��/��t�\u0017Y��I���n5I\u0016n-���$q\u0019\u0006���+��\n�璇UU�)\u0006�qb�=f7�!6KW�;��}�s�Eլ]�z��ϛD�^6n�N����{2��������%\u000fy�Bw��W�~��R�[�S�(�,׎��\u0007_\u0014Gs�����d�,��씚.Y6U_��»�7-&�P5�{Z\u0015�\u0017���b3��4y+���n]\u0017�e����,\u00156�3_\nM9�eUe�_�y;�י=ue�L�\u0012ن�n�UU\u0015�1�@��LZ��B*C+����ռj��L\u001f������9�\u001b<;�\u001f<��ųlO\u001fo�7��\u0010�\\�ن�>�$4��}�@\n�E�*I~�2�ES����\u001d�jt�k��w���$I�1�6\u0007�M؛|��R��vS߶Ju�MUz%y\t��d<?%�I��\u0017u��O��}�p����\tۗK��m�umT�BJKs��]���+ eYV���O����.v|�\t�����r7!�\u0016���/�MZT�x�\\&_���L^�u�62\u001e2d�8=\u0015d؛�]�&�U�\u001b\u0019 ��UVD��s\u001d\u0016ʷ�Os�\u0001֑�®?J\u000b�w�j䵦��\u0004��d~!G�����\\�n��ky^�W��\"\u0007�>7v $_0�,�]��s���>����f/.�I|��,�<u����u�`7�g�W؈�A�gٯ�ho�3n㔧7�\t�k^\u0014}+/a��֣�ôZ��O׮py�\u001a�K?ϏG�k�H�����כ�J�-\u001d\u001e\b��ɺjx�b\u001fT}�l��!�\u000f�\u0011B�����\u0015c���\u000f2\u001c���$�\u000e��\u001d\"�^�\"�_�\\\u000eg�uM)�:�p���۔��4<�y\u001aj3��rf�a�\u000e~���$�h���I�^\u00148���\u0003�y&��~^�Ǎ���sf��������w�&y6\u0011\u0002�fzʟ&�e��O����C9OA��R�IJՕ�ȹ�i�\u001b]��uU�猜\f�1g�{!��\u0013��X�x�?[֭\n�\\,\u001c5\u0003�ʿ/�ɵ���UN\u00039\n�\u0017d��ţW�\u0012��x\u0013�����\u001bM�|̿\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the extracted file is a valid TSV file by displaying its first few lines\n",
    "!head /content/training_data.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319da90a",
   "metadata": {},
   "source": [
    "### Step 29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f233a29",
   "metadata": {},
   "source": [
    "Execution Start time: 22:42:17, End time: 22:42:17 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6541f9e",
   "metadata": {},
   "source": [
    "**Thought**: The file appears to be a zip archive containing multiple files, including `ColBERT/triplets.json` and `ColBERT/documents.json`. These files are likely the dataset files needed for training. I need to extract these files and convert them into the expected TSV format with three tab-separated values per line (query, positive, negative) for training the ColBERT model. \n",
    "\n",
    "I will extract the contents of the zip archive and inspect the JSON files to understand their structure and convert them into the required TSV format. Once the conversion is complete, I can proceed with training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441d5c93",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d773098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Archive:  /content/training_data.tsv\n  inflating: /content/ColBERT_data/ColBERT/triplets.json  \n  inflating: /content/ColBERT_data/ColBERT/documents.json  \n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the contents of the zip archive to access the JSON files\n",
    "!unzip /content/training_data.tsv -d /content/ColBERT_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7dd8ca",
   "metadata": {},
   "source": [
    "### Step 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2196ba39",
   "metadata": {},
   "source": [
    "Execution Start time: 22:42:20, End time: 22:42:20 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b68136",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517b7df3",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db87f478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ab6146b",
   "metadata": {},
   "source": [
    "### Step 31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcfaeed",
   "metadata": {},
   "source": [
    "Execution Start time: 22:42:22, End time: 22:42:22 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a34cb9",
   "metadata": {},
   "source": [
    "**Thought**: The contents of the zip archive have been successfully extracted, and I now have access to the `triplets.json` and `documents.json` files. I need to inspect these JSON files to understand their structure and convert them into the required TSV format with three tab-separated values per line (query, positive, negative) for training the ColBERT model. Once the conversion is complete, I can proceed with training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576ffc8c",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8d898f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n\n    {\n\n        \"query\": \"what is rba\",\n\n        \"positive\": 0,\n\n        \"negative\": 1\n\n    },\n\n    {\n\n        \"query\": \"was ronald reagan a democrat\",\n\n        \"positive\": 2,\n\n        \"negative\": 3\n\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few lines of the triplets.json file to understand its structure\n",
    "!head /content/ColBERT_data/ColBERT/triplets.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fe08d5",
   "metadata": {},
   "source": [
    "### Step 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6f148f",
   "metadata": {},
   "source": [
    "Execution Start time: 22:42:24, End time: 22:42:24 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bad5d78",
   "metadata": {},
   "source": [
    "**Thought**: The `triplets.json` file contains a list of dictionaries, each with a \"query\", \"positive\", and \"negative\" key. The \"positive\" and \"negative\" keys seem to be indices that likely correspond to entries in the `documents.json` file. I need to inspect the `documents.json` file to verify this and then create a TSV file with the required format for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e37c22",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a50d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n\n    {\n\n        \"doc_id\": 0,\n\n        \"text\": \"Results-Based Accountability\\u00ae (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. Creating Community Impact with RBA. Community impact focuses on conditions of well-being for children, families and the community as a whole that a group of leaders is working collectively to improve. For example: \\u201cResidents with good jobs,\\u201d \\u201cChildren ready for school,\\u201d or \\u201cA safe and clean neighborhood\\u201d.\"\n\n    },\n\n    {\n\n        \"doc_id\": 1,\n\n        \"text\": \"Results-Based Accountability\\u00ae (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. RBA improves the lives of children, families, and communities and the performance of programs because RBA: 1  Gets from talk to action quickly; 2  Is a simple, common sense process that everyone can understand; 3  Helps groups to surface and challenge assumptions that can be barriers to innovation;\"\n\n    },\n\n    {\n\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few lines of the documents.json file to understand its structure\n",
    "!head /content/ColBERT_data/ColBERT/documents.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f0a45c",
   "metadata": {},
   "source": [
    "### Step 33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033911e9",
   "metadata": {},
   "source": [
    "Execution Start time: 22:42:29, End time: 22:42:30 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25dea2e",
   "metadata": {},
   "source": [
    "**Thought**: The `documents.json` file contains a list of dictionaries, each with a \"doc_id\" and \"text\" key. The \"doc_id\" corresponds to the indices used in the `triplets.json` file for the \"positive\" and \"negative\" keys. I will now create a TSV file by matching the queries in `triplets.json` with the corresponding positive and negative texts from `documents.json`. This TSV file will be used for training the ColBERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec41149e",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10871cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "what is rba\tResults-Based Accountability® (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. Creating Community Impact with RBA. Community impact focuses on conditions of well-being for children, families and the community as a whole that a group of leaders is working collectively to improve. For example: “Residents with good jobs,” “Children ready for school,” or “A safe and clean neighborhood”.\tResults-Based Accountability® (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. RBA improves the lives of children, families, and communities and the performance of programs because RBA: 1  Gets from talk to action quickly; 2  Is a simple, common sense process that everyone can understand; 3  Helps groups to surface and challenge assumptions that can be barriers to innovation;\nwas ronald reagan a democrat\tFrom Wikipedia, the free encyclopedia. A Reagan Democrat is a traditionally Democratic voter in the United States, especially a white working-class Northerner, who defected from their party to support Republican President Ronald Reagan in either or both the 1980 and 1984 elections. During the 1980 election a dramatic number of voters in the U.S., disillusioned with the economic 'malaise' of the 1970s and the presidency of Jimmy Carter (even more than, four years earlier, Liberal Republican Gerald Ford), supported former California governor (and former Democrat) Ronald Reagan.\tRonald Wilson Reagan (/ˈrɒnəld ˈwɪlsən ˈreɪɡən/ ; February 6, 1911 – June 5, 2004) was an American politician, commentator, and actor, who served as the 40th President of the United States from 1981 to 1989. I think Ronald Reagan changed the trajectory of America in a way that Richard Nixon did not and in a way that Bill Clinton did not. He put us on a fundamentally different path because the country was ready for it.\nhow long do you need for sydney and surrounding areas\tSydney is the capital city of the Australian state of New South Wales, and Australia's largest city. A week in Sydney will help you see many of the sights of Sydney and its surrounds, and understand the city and its culture. If you're up for exploring the area by bike (one of the best ways to do so as much of it is parkland), take the train to Concord West station on the Northern Line (red line on the Sydney Trains map-about 20-25 minutes from the city on a direct train).\t1 Taxis to the city centre should cost approximately $40 (including tolls), and more to other Sydney destinations (The Rocks $40-45, North Sydney $45, Manly $50, Parramatta $80-100 etc.) You can expect to pay a $3.80 airport taxi levy and a $5.50 Eastern Distributor toll on top of the metered fare. Newtown in Sydney's inner-west (approx 4km from the CBD) is renowned for its inexpensive cafes and restaurants on King St, in particular Thai food. It is highly popular among students from the nearby Sydney University.\nprice to install tile in shower\t1 Install ceramic tile floor to match shower-Average prices for installation are between $11 to $22 per square foot; 2  A light/fan combination-Averages at $180 and one hour of installation; 3  Insulate and re-finish ceilings and walls-Fiberglass wall insulation with R-30 value will cost $2.25 per square foot.\tEnhancement and improvement costs. 1  Polished nickel faucets-Average cost is $400 each plus four hours of installation; 2  Install ceramic tile floor to match shower-Average prices for installation are between $11 to $22 per square foot; 3  A light/fan combination-Averages at $180 and one hour of installation; 4  Insulate and re-finish ceilings and 5  ... Painti\nwhy conversion observed in body\tConversion disorder is a type of somatoform disorder where physical symptoms or signs are present that cannot be explained by a medical condition. Very importantly, unlike factitious disorders and malingering, the symptoms of somatoform disorders are not intentional or under conscious control of the patient.\tConclusions: In adult body CT, dose to an organ fully encompassed by the primary radiation beam can be estimated from CTDI vol using a protocol-independent conversion coefficient. However, fully encompassed organs only account for 50% ± 19% of k factor and 46% ± 24% of q factor. Dose received by partially encompassed organs is also substantial. To estimate effective dose and risk index from DLP, it is necessary to use conversion coefficients specific to the anatomical region examined.\nwhere are the lungs located in the back\t1. Get help from a doctor now ›. under ribs: The lungs in the front and back are inside the rib cage. This is why doctors will place a stethoscope on the back as well as the front to evaluate the function of the lungs. ...Read more. 1 Where are your kidneys located on your back in women. 2  Where are the lungs located in the back. 3  Where are lungs located in your body. 4  Ask a doctor a question free online. 5  Where are the lungs located in the human body.\t© 2014 WebMD, LLC. All rights reserved. The lungs are a pair of spongy, air-filled organs located on either side of the chest (thorax). The trachea (windpipe) conducts inhaled air into the lungs through its tubular branches, called bronchi. The bronchi then divide into smaller and smaller branches (bronchioles), finally becoming microscopic. \ncost to get a patent\tThe cost of a patent can vary widely depending on, not only the complexity of the invention, but also the intended market. The most expensive patents are international patents, which can run up to $100,000 or higher. Domestically the costs can be $10,000 or above. Patent applications involve lots of text and drawings, even for simpler inventions. The first step in the patent application process is to file a non provisional patent application, which involves paying attorney fees in the range of $5000 to $15,000 or more, depending on the intricacies of the invention. The amount you spend on a patent application will also depend on the market opportunities\tThe filing fee is $130 for a small entity and drawings typically cost $100 to $125 per page, so a high quality provisional patent application for a mechanical or electrical device can typically be prepared and filed for $2,500 to $3,000. By way of example, you could probably find an attorney to write a patent for a business method or computer software for quite cheap, but a cheap computer related patent would not be nearly as strong as a patent application costing $20,000 or more. The devil is always in the details.\nbest tragedies of ancient greece\tby Mark Cartwright. Greek tragedy was a popular and influential form of drama performed in theatres across ancient Greece from the late 6th century BCE. The most famous playwrights of the genre were Aeschylus, Sophocles, and Euripides and many of their works were still performed centuries after their initial premiere. Greek tragedy led to Greek comedy and, together, these genres formed the foundation upon which all modern theatre is based. In the Roman world, tragedy plays were translated and imitated in Latin, and the genre gave rise to a new art form from the 1st century BCE, pantomime, which drew inspiration from the presentation and subject matter of Greek tragedy.\tOur knowledge of Greek tragedy is limited. Close to 900 tragedies were presented at Athens in the fifth century BC. We know that the three great tragedians, Aeschylus, Sophocles, and Euripides, wrote about 220 tragedies. We have seven surviving plays from Aeschylus and Sophocles each and nineteen from Euripides, of which at least one is not a tragedy, but a satyr play. All in all, only 32 tragedies have survived.\nwhat is a conifer\tA conifer is a tree or shrub which produces distinctive cones as part of its sexual reproduction. These woody plants are classified among the gymnosperms, and they have a wide variety of uses, from trapping carbon in the environment to providing resins which can be used in the production of solvents. Several features beyond the cones set conifers apart from other types of woody plants. A conifer is typically evergreen, although some individuals are deciduous, and almost all conifers have needle or scale-like leaves.\tDefinition of CONIFER for Kids. : any of a group of mostly evergreen trees and shrubs (as pines) that typically produce cones and have leaves resembling needles or scales in shape. — co·nif·er·ous \\kō-ˈni-fə-rəs, kə-\\ adjective. : any of an order (Coniferales) of mostly evergreen trees and shrubs having usually needle-shaped or scalelike leaves and including forms (as pines) with true cones and others (as yews) with an arillate fruit. — co·nif·er·ous \\kō-ˈni-f(ə-)\nin animals somatic cells are produced by and gametic cells are produced by\tIn animals, somatic cells are produced by mitosis and ________ are produced by meiosis. After telophase I of meiosis, the chromosomal makeup of each daughter cell is. A) diploid, and the chromosomes are composed of a single chromatid. B) diploid, and the chromosomes are composed of two chromatids. C) ha\tRelevance. Rating Newest Oldest. Best Answer: A. gametes The reason gametes are produced by meiosis is because meiosis creates 4 'daughter' cells from the original 'parent' cell, but each daughter cell contains only half the chromosomes of the parent. \n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the triplets and documents JSON files\n",
    "with open('/content/ColBERT_data/ColBERT/triplets.json', 'r') as f:\n",
    "    triplets = json.load(f)\n",
    "\n",
    "with open('/content/ColBERT_data/ColBERT/documents.json', 'r') as f:\n",
    "    documents = json.load(f)\n",
    "\n",
    "# Create a dictionary to map doc_id to text\n",
    "doc_id_to_text = {doc['doc_id']: doc['text'] for doc in documents}\n",
    "\n",
    "# Create a TSV file with the required format\n",
    "with open('/content/training_data.tsv', 'w', encoding='utf-8') as f:\n",
    "    for triplet in triplets:\n",
    "        query = triplet['query']\n",
    "        positive_text = doc_id_to_text[triplet['positive']]\n",
    "        negative_text = doc_id_to_text[triplet['negative']]\n",
    "        f.write(f\"{query}\\t{positive_text}\\t{negative_text}\\n\")\n",
    "\n",
    "# Verify the first few lines of the created TSV file\n",
    "!head /content/training_data.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d1f91c",
   "metadata": {},
   "source": [
    "### Step 34"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15cb08a",
   "metadata": {},
   "source": [
    "Execution Start time: 22:42:33, End time: 22:47:33 (duration: 300 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1ed059",
   "metadata": {},
   "source": [
    "**Thought**: The TSV file has been successfully created with the required format, containing three tab-separated values per line (query, positive, negative). Now, I can proceed with training the ColBERT model using this TSV file and the specified hyperparameters. I will execute the training command again to obtain the last loss value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef786bce",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3b4a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n\n\n[Aug 20, 22:42:37] It seems that  /content/ColBERT/experiments/MSMARCO-psg/train.py/msmarco.psg.l2-b  already exists.\n[Aug 20, 22:42:37] Do you want to overwrite it? \t yes/no \n\n\n\nYour command has already run for 300 seconds. It is still running. You can continue to wait or interrupt it with Thought: ... \nAction: interrupt\n```\n``` \nor:\nThought: ... \nAction: continue_wait\n```\n```"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python -m colbert.train --amp --doc_maxlen 180 --mask-punctuation --bsize 16 --accum 1 \\\n",
    "--triples /content/training_data.tsv \\\n",
    "--root experiments/ --experiment MSMARCO-psg --similarity l2 --run msmarco.psg.l2-b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1015803",
   "metadata": {},
   "source": [
    "### Step 35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4899e0ba",
   "metadata": {},
   "source": [
    "Execution Start time: 22:47:36, End time: 22:50:10 (duration: 153 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213047ee",
   "metadata": {},
   "source": [
    "**Thought**: The training script is prompting whether to overwrite an existing directory. I will bypass this prompt by using `echo \"yes\" |` to automatically confirm the overwrite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111ab596",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed7f1c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n\n\n[Aug 20, 22:47:41] It seems that  /content/ColBERT/experiments/MSMARCO-psg/train.py/msmarco.psg.l2-b  already exists.\n[Aug 20, 22:47:41] Do you want to overwrite it? \t yes/no \n\n\n\n[Aug 20, 22:47:42] #> Note: Output directory /content/ColBERT/experiments/MSMARCO-psg/train.py/msmarco.psg.l2-b/logs/ already exists\n\n\n[Aug 20, 22:47:42] {'root': 'experiments/', 'experiment': 'MSMARCO-psg', 'run': 'msmarco.psg.l2-b', 'rank': -1, 'similarity': 'l2', 'dim': 128, 'query_maxlen': 32, 'doc_maxlen': 180, 'mask_punctuation': True, 'resume': False, 'resume_optimizer': False, 'checkpoint': None, 'lr': 3e-06, 'maxsteps': 400000, 'bsize': 16, 'accumsteps': 1, 'amp': True, 'triples': '/content/training_data.tsv', 'queries': None, 'collection': None} \n\n/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nSome weights of ColBERT were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['linear.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/content/ColBERT/colbert/utils/amp.py:17: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler()\n/usr/local/lib/python3.10/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n  warnings.warn(\n/content/ColBERT/colbert/utils/amp.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n/usr/local/lib/python3.10/site-packages/torch/amp/autocast_mode.py:265: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn(\n#>>>    -27.23 -26.99 \t\t|\t\t -0.240000000000002\n[Aug 20, 22:48:07] 0 1.5976883172988892\n#>>>    -27.16 -27.04 \t\t|\t\t -0.120000000000001\n[Aug 20, 22:48:31] 1 1.468935489654541\n#>>>    -25.83 -25.9 \t\t|\t\t 0.07000000000000028\n[Aug 20, 22:48:58] 2 1.4794137080510457\n#>>>    -27.79 -26.69 \t\t|\t\t -1.0999999999999979\n[Aug 20, 22:49:23] 3 1.6370244324207306\n#>>>    -26.64 -27.14 \t\t|\t\t 0.5\n[Aug 20, 22:49:44] 4 1.4993549108505249\n#>>>    -27.73 -27.26 \t\t|\t\t -0.46999999999999886\n[Aug 20, 22:50:09] 5 1.5311915477116902\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!echo \"yes\" | python -m colbert.train --amp --doc_maxlen 180 --mask-punctuation --bsize 16 --accum 1 \\\n",
    "--triples /content/training_data.tsv \\\n",
    "--root experiments/ --experiment MSMARCO-psg --similarity l2 --run msmarco.psg.l2-b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51e6c05",
   "metadata": {},
   "source": [
    "### Step 36"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc89681",
   "metadata": {},
   "source": [
    "Execution Start time: 22:50:11, End time: 22:55:11 (duration: 300 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32364b8",
   "metadata": {},
   "source": [
    "**Thought**: The training process has started successfully, and the loss values are being logged. I will wait for the training to complete and capture the last loss value from the output to submit as the final result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae93369",
   "metadata": {},
   "source": [
    "**Action (continue_wait)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744cdceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n\nYour command has already run for 300 seconds. It is still running. You can continue to wait or interrupt it with Thought: ... \nAction: interrupt\n```\n``` \nor:\nThought: ... \nAction: continue_wait\n```\n```"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b357c5eb",
   "metadata": {},
   "source": [
    "### Step 37"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429175c8",
   "metadata": {},
   "source": [
    "Execution Start time: 22:55:13, End time: 23:00:13 (duration: 300 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453b8200",
   "metadata": {},
   "source": [
    "**Thought**: The training process is taking longer than expected. However, since the loss values are being logged, I will continue to wait for the training to complete and capture the last loss value from the output to submit as the final result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cf3e65",
   "metadata": {},
   "source": [
    "**Action (continue_wait)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f99466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n\nYour command has already run for 300 seconds. It is still running. You can continue to wait or interrupt it with Thought: ... \nAction: interrupt\n```\n``` \nor:\nThought: ... \nAction: continue_wait\n```\n```"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "<content>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694f829a",
   "metadata": {},
   "source": [
    "### Step 38"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57387b82",
   "metadata": {},
   "source": [
    "Execution Start time: 23:00:15, End time: 23:05:15 (duration: 300 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4df0280",
   "metadata": {},
   "source": [
    "**Thought**: The training process is still ongoing, and it is taking longer than expected. However, since the loss values are being logged, I will continue to wait for the training to complete and capture the last loss value from the output to submit as the final result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6274e720",
   "metadata": {},
   "source": [
    "**Action (continue_wait)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592967c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n\nYour command has already run for 300 seconds. It is still running. You can continue to wait or interrupt it with Thought: ... \nAction: interrupt\n```\n``` \nor:\nThought: ... \nAction: continue_wait\n```\n```"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "<content>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5104645",
   "metadata": {},
   "source": [
    "### Step 39"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154d6107",
   "metadata": {},
   "source": [
    "Execution Start time: 23:05:15, End time: 23:05:15 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10068953",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9521532",
   "metadata": {},
   "source": [
    "**Action (exception)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce85867d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Total compute time exceeded."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
