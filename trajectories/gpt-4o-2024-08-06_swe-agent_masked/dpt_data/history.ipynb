{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f737e81",
   "metadata": {},
   "source": [
    "### System Prompt\n",
    "<details>\n",
    "SETTING: You are an autonomous programmer, and you're working directly in the command line with a special Jupyter notebook interface.\n",
    "\n",
    "The special Jupyter notebook interface consists of a file editor that shows you 100 lines of a file at a time.\n",
    "You can execute commands in the notebook using:\n",
    "\n",
    "1. Bash commands: Commands starting with !.\n",
    "2. Python commands: Standard Python code.\n",
    "3. Magic commands: Commands starting with %, e.g., %cd <path>.\n",
    "\n",
    "Additionally, you can also use the following commands to help you navigate and edit files.\n",
    "\n",
    "COMMANDS:\n",
    "\n",
    "open:\n",
    "  docstring: opens the file at the given path in the editor. If line_number is provided, the window will be move to include that line\n",
    "  signature: open <path> [<line_number>]\n",
    "  arguments:\n",
    "    - path (string) [required]: the path to the file to open\n",
    "    - line_number (integer) [optional]: the line number to move the window to (if not provided, the window will start at the top of the file)\n",
    "\n",
    "create:\n",
    "  docstring: creates and opens a new file with the given name\n",
    "  signature: create <filename>\n",
    "  arguments:\n",
    "    - filename (string) [required]: the name of the file to create\n",
    "\n",
    "goto:\n",
    "  docstring: moves the window to show <line_number>\n",
    "  signature: goto <line_number>\n",
    "  arguments:\n",
    "    - line_number (integer) [required]: the line number to move the window to\n",
    "\n",
    "scroll_down:\n",
    "  docstring: moves the window down {WINDOW} lines\n",
    "  signature: scroll_down\n",
    "\n",
    "scroll_up:\n",
    "  docstring: moves the window down {WINDOW} lines\n",
    "  signature: scroll_up\n",
    "\n",
    "submit:\n",
    "  docstring: submits your final answer\n",
    "  signature: submit <answer>\n",
    "  arguments:\n",
    "    - answer (string) [required]: the answer to be submitted\n",
    "\n",
    "search_dir:\n",
    "  docstring: searches for search_term in all files in dir_path. If dir_path is not provided, searches in the current directory\n",
    "  signature: search_dir <search_term> [<dir_path>]\n",
    "  arguments:\n",
    "    - search_term (string) [required]: the term to search for\n",
    "    - dir_path (string) [optional]: the directory to search in (if not provided, searches in the current directory)\n",
    "\n",
    "search_file:\n",
    "  docstring: searches for search_term in file. If file is not provided, searches in the current open file\n",
    "  signature: search_file <search_term> [<file>]\n",
    "  arguments:\n",
    "    - search_term (string) [required]: the term to search for\n",
    "    - file (string) [optional]: the file to search in (if not provided, searches in the current open file)\n",
    "\n",
    "find_file:\n",
    "  docstring: finds all files with the given name in dir_path. If dir_path is not provided, searches in the current directory\n",
    "  signature: find_file <file_name> [<dir_path>]\n",
    "  arguments:\n",
    "    - file_name (string) [required]: the name of the file to search for\n",
    "    - dir_path (string) [optional]: the directory to search in (if not provided, searches in the current directory)\n",
    "\n",
    "edit:\n",
    "  docstring: replaces lines <start_line> through <end_line> (inclusive) with the given text in the open file. The replacement text is terminated by a line with only end_of_edit on it. All of the <replacement text> will be entered, so make sure your indentation is formatted properly. Python files will be checked for syntax errors after the edit. If the system detects a syntax error, the edit will not be executed. Simply try to edit the file again, but make sure to read the error message and modify the edit command you issue accordingly. Issuing the same command a second time will just lead to the same error message again.\n",
    "  signature: edit <start_line>:<end_line>\n",
    "<replacement_text>\n",
    "end_of_edit\n",
    "  arguments:\n",
    "    - start_line (integer) [required]: the line number to start the edit at\n",
    "    - end_line (integer) [required]: the line number to end the edit at (inclusive)\n",
    "    - replacement_text (string) [required]: the text to replace the current selection with\n",
    "\n",
    "\n",
    "\n",
    "Please note that THE EDIT COMMAND REQUIRES PROPER INDENTATION. \n",
    "If you'd like to add the line '        print(x)' you must fully write that out, with all those spaces before the code! Indentation is important and code that is not indented correctly will fail and require fixing before it can be run.\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "Your shell prompt is formatted as follows:\n",
    "(Open file: <path>)\n",
    "(Current directory: <cwd>)\n",
    "In [ ]\n",
    "\n",
    "You need to format your output using two fields: discussion and command.\n",
    "Your output should always include _one_ discussion and _one_ command field EXACTLY as in the following example:\n",
    "DISCUSSION\n",
    "First I'll start by using ls to see what files are in the current directory. Then maybe we can look at some relevant files to see what they look like.\n",
    "```\n",
    "!ls -a\n",
    "```\n",
    "\n",
    "You should only include a *SINGLE* command in the command section and then wait for a response from the shell before continuing with more discussion and commands. Everything you include in the DISCUSSION section will be saved for future reference.\n",
    "If you'd like to issue two commands at once, PLEASE DO NOT DO THAT! Please instead first submit just the first command, and then after receiving a response you'll be able to issue the second command. \n",
    "You're free to use any other bash commands you want (e.g. find, grep, cat, ls, cd) in addition to the special commands listed above.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a627ff31",
   "metadata": {},
   "source": [
    "### Query\n",
    "Apply the decomposed prompt tuning method to fine-tune the t5-small model on our sentence pair classification dataset, available at https://drive.google.com/file/d/1raGLX-2vdX1uizYx2DdqZ_JzAkVeyeDF/view?usp=drive_link. Report the resulting metrics using the `main_ddp.py` script. Report accuracy on the development set as a json structured as follows: {\"accuracy\": 0.0} (replace 0.0 with the actual value).\n",
    "\n",
    "Additional instructions:\n",
    "1. Run for only 1 epoch.\n",
    "2. Use the following hyperparameters: --enc_prompt_tokens 100 -ts 16 -e 1 --bottle_neck 10\n",
    "\n",
    "Git repository: https://github.com/xyaoooo/dpt\n",
    "\n",
    "I have already executed some of the required steps. However, I did not yet make any required changes to load the request dataset. Your goal is to successfully load the dataset and complete the remaining steps to achieve the user request. You can re-run any of the pre-executed steps, and finally should successfully run\n",
    "```\n",
    "!python main_ddp.py --datasets=mrpc  --model_name=t5-small --enc_prompt_tokens 100 -ts 16 -e 1 --bottle_neck 10\n",
    "```\n",
    "to submit the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db63bf0",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392d6971",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e89ff9d",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124535e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cloning into 'dpt'...\r\nremote: Enumerating objects: 45, done.\u001b[K\r\nremote: Counting objects:   2% (1/45)\u001b[K\rremote: Counting objects:   4% (2/45)\u001b[K\rremote: Counting objects:   6% (3/45)\u001b[K\rremote: Counting objects:   8% (4/45)\u001b[K\rremote: Counting objects:  11% (5/45)\u001b[K\rremote: Counting objects:  13% (6/45)\u001b[K\rremote: Counting objects:  15% (7/45)\u001b[K\rremote: Counting objects:  17% (8/45)\u001b[K\rremote: Counting objects:  20% (9/45)\u001b[K\rremote: Counting objects:  22% (10/45)\u001b[K\rremote: Counting objects:  24% (11/45)\u001b[K\rremote: Counting objects:  26% (12/45)\u001b[K\rremote: Counting objects:  28% (13/45)\u001b[K\rremote: Counting objects:  31% (14/45)\u001b[K\rremote: Counting objects:  33% (15/45)\u001b[K\rremote: Counting objects:  35% (16/45)\u001b[K\rremote: Counting objects:  37% (17/45)\u001b[K\rremote: Counting objects:  40% (18/45)\u001b[K\rremote: Counting objects:  42% (19/45)\u001b[K\rremote: Counting objects:  44% (20/45)\u001b[K\rremote: Counting objects:  46% (21/45)\u001b[K\rremote: Counting objects:  48% (22/45)\u001b[K\rremote: Counting objects:  51% (23/45)\u001b[K\rremote: Counting objects:  53% (24/45)\u001b[K\rremote: Counting objects:  55% (25/45)\u001b[K\rremote: Counting objects:  57% (26/45)\u001b[K\rremote: Counting objects:  60% (27/45)\u001b[K\rremote: Counting objects:  62% (28/45)\u001b[K\rremote: Counting objects:  64% (29/45)\u001b[K\rremote: Counting objects:  66% (30/45)\u001b[K\rremote: Counting objects:  68% (31/45)\u001b[K\rremote: Counting objects:  71% (32/45)\u001b[K\rremote: Counting objects:  73% (33/45)\u001b[K\rremote: Counting objects:  75% (34/45)\u001b[K\rremote: Counting objects:  77% (35/45)\u001b[K\rremote: Counting objects:  80% (36/45)\u001b[K\rremote: Counting objects:  82% (37/45)\u001b[K\rremote: Counting objects:  84% (38/45)\u001b[K\rremote: Counting objects:  86% (39/45)\u001b[K\rremote: Counting objects:  88% (40/45)\u001b[K\rremote: Counting objects:  91% (41/45)\u001b[K\rremote: Counting objects:  93% (42/45)\u001b[K\rremote: Counting objects:  95% (43/45)\u001b[K\rremote: Counting objects:  97% (44/45)\u001b[K\rremote: Counting objects: 100% (45/45)\u001b[K\rremote: Counting objects: 100% (45/45), done.\u001b[K\r\nremote: Compressing objects:   2% (1/41)\u001b[K\rremote: Compressing objects:   4% (2/41)\u001b[K\rremote: Compressing objects:   7% (3/41)\u001b[K\rremote: Compressing objects:   9% (4/41)\u001b[K\rremote: Compressing objects:  12% (5/41)\u001b[K\rremote: Compressing objects:  14% (6/41)\u001b[K\rremote: Compressing objects:  17% (7/41)\u001b[K\rremote: Compressing objects:  19% (8/41)\u001b[K\rremote: Compressing objects:  21% (9/41)\u001b[K\rremote: Compressing objects:  24% (10/41)\u001b[K\rremote: Compressing objects:  26% (11/41)\u001b[K\rremote: Compressing objects:  29% (12/41)\u001b[K\rremote: Compressing objects:  31% (13/41)\u001b[K\rremote: Compressing objects:  34% (14/41)\u001b[K\rremote: Compressing objects:  36% (15/41)\u001b[K\rremote: Compressing objects:  39% (16/41)\u001b[K\rremote: Compressing objects:  41% (17/41)\u001b[K\rremote: Compressing objects:  43% (18/41)\u001b[K\rremote: Compressing objects:  46% (19/41)\u001b[K\rremote: Compressing objects:  48% (20/41)\u001b[K\rremote: Compressing objects:  51% (21/41)\u001b[K\rremote: Compressing objects:  53% (22/41)\u001b[K\rremote: Compressing objects:  56% (23/41)\u001b[K\rremote: Compressing objects:  58% (24/41)\u001b[K\rremote: Compressing objects:  60% (25/41)\u001b[K\rremote: Compressing objects:  63% (26/41)\u001b[K\rremote: Compressing objects:  65% (27/41)\u001b[K\rremote: Compressing objects:  68% (28/41)\u001b[K\rremote: Compressing objects:  70% (29/41)\u001b[K\rremote: Compressing objects:  73% (30/41)\u001b[K\rremote: Compressing objects:  75% (31/41)\u001b[K\rremote: Compressing objects:  78% (32/41)\u001b[K\rremote: Compressing objects:  80% (33/41)\u001b[K\rremote: Compressing objects:  82% (34/41)\u001b[K\rremote: Compressing objects:  85% (35/41)\u001b[K\rremote: Compressing objects:  87% (36/41)\u001b[K\rremote: Compressing objects:  90% (37/41)\u001b[K\rremote: Compressing objects:  92% (38/41)\u001b[K\rremote: Compressing objects:  95% (39/41)\u001b[K\rremote: Compressing objects:  97% (40/41)\u001b[K\rremote: Compressing objects: 100% (41/41)\u001b[K\rremote: Compressing objects: 100% (41/41), done.\u001b[K\r\nReceiving objects:   2% (1/45)\rReceiving objects:   4% (2/45)\rReceiving objects:   6% (3/45)\rReceiving objects:   8% (4/45)\rReceiving objects:  11% (5/45)\rReceiving objects:  13% (6/45)\rReceiving objects:  15% (7/45)\rReceiving objects:  17% (8/45)\rReceiving objects:  20% (9/45)\rReceiving objects:  22% (10/45)\rReceiving objects:  24% (11/45)\rReceiving objects:  26% (12/45)\rReceiving objects:  28% (13/45)\rReceiving objects:  31% (14/45)\rReceiving objects:  33% (15/45)\rReceiving objects:  35% (16/45)\rReceiving objects:  37% (17/45)\rReceiving objects:  40% (18/45)\rReceiving objects:  42% (19/45)\rReceiving objects:  44% (20/45)\rReceiving objects:  46% (21/45)\rReceiving objects:  48% (22/45)\rReceiving objects:  51% (23/45)\rReceiving objects:  53% (24/45)\rremote: Total 45 (delta 18), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\r\nReceiving objects:  55% (25/45)\rReceiving objects:  57% (26/45)\rReceiving objects:  60% (27/45)\rReceiving objects:  62% (28/45)\rReceiving objects:  64% (29/45)\rReceiving objects:  66% (30/45)\rReceiving objects:  68% (31/45)\rReceiving objects:  71% (32/45)\rReceiving objects:  73% (33/45)\rReceiving objects:  75% (34/45)\rReceiving objects:  77% (35/45)\rReceiving objects:  80% (36/45)\rReceiving objects:  82% (37/45)\rReceiving objects:  84% (38/45)\rReceiving objects:  86% (39/45)\rReceiving objects:  88% (40/45)\rReceiving objects:  91% (41/45)\rReceiving objects:  93% (42/45)\rReceiving objects:  95% (43/45)\rReceiving objects:  97% (44/45)\rReceiving objects: 100% (45/45)\rReceiving objects: 100% (45/45), 39.40 KiB | 13.13 MiB/s, done.\r\nResolving deltas:   0% (0/18)\rResolving deltas:   5% (1/18)\rResolving deltas:  11% (2/18)\rResolving deltas:  16% (3/18)\rResolving deltas:  22% (4/18)\rResolving deltas:  27% (5/18)\rResolving deltas:  33% (6/18)\rResolving deltas:  38% (7/18)\rResolving deltas:  44% (8/18)\rResolving deltas:  50% (9/18)\rResolving deltas:  55% (10/18)\rResolving deltas:  61% (11/18)\rResolving deltas:  66% (12/18)\rResolving deltas:  72% (13/18)\rResolving deltas:  77% (14/18)\rResolving deltas:  83% (15/18)\rResolving deltas:  88% (16/18)\rResolving deltas:  94% (17/18)\rResolving deltas: 100% (18/18)\rResolving deltas: 100% (18/18), done.\r\n/content/dpt\n/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!git clone https://github.com/xyaoooo/dpt\n",
    "%cd dpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b31c704",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3221e9",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25470781",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf43526c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip3 install -q huggingface-hub numpy scikit-learn scipy sentencepiece spacy tabulate tokenizers torch torchvision tqdm transformers datasets===2.19.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e60017",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d4e9b3",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4b6af0",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd9b354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\ndef run(local_rank, args):\n\n    is_master = local_rank == 0\n    world_size = args.world_size\n    is_mp = world_size > 1\n    # set the device\n    device = 'cpu' if not torch.cuda.is_available() else 'cuda'\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed_all(args.seed)\n    g = torch.Generator()\n    g.manual_seed(args.seed)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/dpt/main_ddp.py. CHANGING:\n",
    "\n",
    "\n",
    "    world_size = args.world_size\n",
    "    is_mp = world_size > 1\n",
    "    # set the device\n",
    "    device = local_rank\n",
    "\n",
    "    random.seed(args.seed)\n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "    world_size = args.world_size\n",
    "    is_mp = world_size > 1\n",
    "    # set the device\n",
    "    device = 'cpu' if not torch.cuda.is_available() else 'cuda'\n",
    "\n",
    "    random.seed(args.seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c61b1b5",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0327d071",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69467fbe",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b037765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n\n                for step, batch in enumerate(TrainDataloader):\n                    global_step += 1\n\n                    if global_step <= resume_steps:\n\n                        if step % update_stride == 0:\n                            pbar.update(update_stride)                        \n                        continue\n                    if len(batch['source_ids'].shape) == 3:\n                        source_ids = batch['source_ids'].squeeze(0).to(device)\n                        source_mask = batch['source_mask'].squeeze(0).to(device)\n                        labels = batch['target_ids'].squeeze(0).to(device)\n                        task_ids = torch.tensor([x[0] for x in batch['task_ids']]).to(device)\n                    else:\n                        source_ids = batch['source_ids'].to(device)\n                        source_mask = batch['source_mask'].to(device)\n                        labels = batch['target_ids'].to(device)\n                        task_ids = batch['task_ids'].to(device)\n\n                    outputs = model(input_ids=source_ids, attention_mask=source_mask, labels=labels, task_ids=task_ids)\n                    loss = outputs['loss']\n\n                    loss = loss / args.accumulate_steps\n                    loss.backward()\n                    step_count += 1\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/dpt/main_ddp.py. CHANGING:\n",
    "\n",
    "\n",
    "                            pbar.update(update_stride)                        \n",
    "                        continue\n",
    "                    if len(batch['source_ids'].shape) == 3:\n",
    "                        source_ids = batch['source_ids'].squeeze(0).to(local_rank)\n",
    "                        source_mask = batch['source_mask'].squeeze(0).to(local_rank)\n",
    "                        labels = batch['target_ids'].squeeze(0).to(local_rank)\n",
    "                        task_ids = torch.tensor([x[0] for x in batch['task_ids']]).to(local_rank)\n",
    "                    else:\n",
    "                        source_ids = batch['source_ids'].to(local_rank)\n",
    "                        source_mask = batch['source_mask'].to(local_rank)\n",
    "                        labels = batch['target_ids'].to(local_rank)\n",
    "                        task_ids = batch['task_ids'].to(local_rank)\n",
    "\n",
    "                    outputs = model(input_ids=source_ids, attention_mask=source_mask, labels=labels, task_ids=task_ids)\n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "                            pbar.update(update_stride)                        \n",
    "                        continue\n",
    "                    if len(batch['source_ids'].shape) == 3:\n",
    "                        source_ids = batch['source_ids'].squeeze(0).to(device)\n",
    "                        source_mask = batch['source_mask'].squeeze(0).to(device)\n",
    "                        labels = batch['target_ids'].squeeze(0).to(device)\n",
    "                        task_ids = torch.tensor([x[0] for x in batch['task_ids']]).to(device)\n",
    "                    else:\n",
    "                        source_ids = batch['source_ids'].to(device)\n",
    "                        source_mask = batch['source_mask'].to(device)\n",
    "                        labels = batch['target_ids'].to(device)\n",
    "                        task_ids = batch['task_ids'].to(device)\n",
    "\n",
    "                    outputs = model(input_ids=source_ids, attention_mask=source_mask, labels=labels, task_ids=task_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a57f65",
   "metadata": {},
   "source": [
    "### Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6928282f",
   "metadata": {},
   "source": [
    "Execution Start time: 06:40:16, End time: 06:40:20 (duration: 4 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf4b44f",
   "metadata": {},
   "source": [
    "**Thought**: Now that the necessary changes have been made to the `main_ddp.py` script to ensure it runs on the correct device, the next step is to download the dataset from the provided Google Drive link. After downloading, we will need to load it into the script. Let's start by downloading the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a9531f",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b26e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Downloading...\nFrom: https://drive.google.com/uc?id=1raGLX-2vdX1uizYx2DdqZ_JzAkVeyeDF\nTo: /content/dpt/mrpc_csv.zip\n\n  0%|                                               | 0.00/3.96k [00:00<?, ?B/s]\n\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!gdown https://drive.google.com/uc?id=1raGLX-2vdX1uizYx2DdqZ_JzAkVeyeDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7bd0f0",
   "metadata": {},
   "source": [
    "### Step 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e066e0",
   "metadata": {},
   "source": [
    "Execution Start time: 06:40:21, End time: 06:40:21 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ea08f1",
   "metadata": {},
   "source": [
    "**Thought**: The dataset has been downloaded as a zip file named `mrpc_csv.zip`. The next step is to extract the contents of this zip file so that we can access the dataset files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8671abcf",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b109df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Archive:  mrpc_csv.zip\n   creating: mrpc_csv/\n  inflating: mrpc_csv/val.csv        \n  inflating: mrpc_csv/test.csv       \n  inflating: mrpc_csv/train.csv      \n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!unzip mrpc_csv.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954fb41e",
   "metadata": {},
   "source": [
    "### Step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719c07e8",
   "metadata": {},
   "source": [
    "Execution Start time: 06:40:23, End time: 06:40:23 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e146d6",
   "metadata": {},
   "source": [
    "**Thought**: The dataset has been successfully extracted into a directory named `mrpc_csv`, which contains `train.csv`, `val.csv`, and `test.csv`. Now, we need to ensure that the `main_ddp.py` script loads this dataset correctly. Let's open the script to check how datasets are currently being loaded and make any necessary adjustments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e531f511",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0971df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/dpt/main_ddp.py (569 lines total)]\n1:import os\n2:import sys\n3:import random\n4:import argparse\n5:import numpy as np\n6:from tqdm import tqdm\n7:from datetime import datetime\n8:\n9:import torch\n10:import torch.nn as nn\n11:import torch.nn.functional as F\n12:from torch.utils.data import Dataset, DataLoader\n13:from torch import optim\n14:\n15:import torch.distributed as dist\n16:import torch.multiprocessing as mp\n17:from torch.nn.parallel import DistributedDataParallel as DDP\n18:from torch.utils.data.distributed import DistributedSampler\n19:\n20:import transformers\n21:from transformers import (\n22:    AutoTokenizer,\n23:    T5TokenizerFast,\n24:    T5ForConditionalGeneration,\n25:    AutoConfig,\n26:    AdamW,\n27:    get_scheduler,\n28:    set_seed,\n29:)\n30:transformers.logging.set_verbosity_error()\n31:\n32:from modeling_t5 import T5PromptForConditionalGeneration_param\n33:from data_utils import AutoTask\n34:from eval_utils import AutoPostProcessor\n35:from metrics import *\n36:from options import *\n37:from utils import *\n38:from loader import *\n39:\n40:import datasets\n41:from datasets import concatenate_datasets\n42:from datasets.utils.logging import set_verbosity_error\n43:from eval_utils import *\n44:set_verbosity_error()\n45:\n46:import logging\n47:logging.disable(logging.WARNING)\n48:\n49:import warnings\n50:warnings.filterwarnings(\"ignore\")\n51:\n52:\n53:def run(local_rank, args):\n54:\n55:    is_master = local_rank == 0\n56:    world_size = args.world_size\n57:    is_mp = world_size > 1\n58:    # set the device\n59:    device = 'cpu' if not torch.cuda.is_available() else 'cuda'\n60:\n61:    random.seed(args.seed)\n62:    np.random.seed(args.seed)\n63:    torch.manual_seed(args.seed)\n64:    torch.cuda.manual_seed_all(args.seed)\n65:    g = torch.Generator()\n66:    g.manual_seed(args.seed)\n67:\n68:    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n69:\n70:    ###################################################################################################\n71:    #   Load data                                                                                     #\n72:    ###################################################################################################\n73:    # Can processing multiple datasets\n74:    train_datasets, val_datasets = [], {}\n75:    for dataset_name in args.datasets_names:\n76:        train_dataset = AutoTask.get(dataset_name).get(split=\"train\",\n77:                                    split_validation_test=True,\n78:                                    add_prefix=args.add_task_prefix,\n79:                                    add_vb=args.add_verbalizer,\n80:                                    file_prefix=args.local_file_prefix,\n81:                                    n_obs=100000 if dataset_name == 'yelp_polarity' else None)\n82:        train_datasets.append(train_dataset)\n83:\n84:        val_dataset = AutoTask.get(dataset_name).get(split=\"validation\",\n85:                                    split_validation_test=True,\n86:                                    add_prefix=args.add_task_prefix,\n87:                                    add_vb=args.add_verbalizer,\n88:                                    file_prefix=args.local_file_prefix,\n89:                                    n_obs=None)\n90:        val_datasets.update({dataset_name: val_dataset})\n91:\n92:        \n93:        if is_master:\n94:            print(local_rank, dataset_name, 'Train\\t', train_dataset[0])\n95:            print(local_rank, dataset_name, 'Val\\t', val_dataset[0])\n96:            \n97:    # merge all datasets if there are multiple ones\n98:    train_datasets = concatenate_datasets(train_datasets)\n99:\n100:    if is_master:\n(469 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open main_ddp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d9d413",
   "metadata": {},
   "source": [
    "### Step 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddb2852",
   "metadata": {},
   "source": [
    "Execution Start time: 06:40:24, End time: 06:40:24 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2341d381",
   "metadata": {},
   "source": [
    "**Thought**: The script uses the `AutoTask` class to load datasets. We need to ensure that our custom dataset is loaded correctly. Let's check the `data_utils.py` file to understand how `AutoTask` is implemented and how we can integrate our dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5b4969",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f3ee38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/dpt/data_utils.py (987 lines total)]\n1:# Handling dataset loading and preprocessing\n2:\n3:import os\n4:import re\n5:import sys\n6:import abc\n7:import random\n8:import logging\n9:import functools\n10:import numpy as np\n11:import collections\n12:from collections import OrderedDict\n13:from typing import Callable, List, Mapping\n14:\n15:import torch\n16:import datasets\n17:\n18:import metrics\n19:from utils import round_stsb_target, pad_punctuation\n20:\n21:from datasets.utils import disable_progress_bar\n22:disable_progress_bar()\n23:\n24:logger = logging.getLogger(__name__)\n25:\n26:class AbstractTask(abc.ABC):\n27:    name = NotImplemented\n28:    config = NotImplemented\n29:    prefix = NotImplemented\n30:    preprocessor: Callable = NotImplemented\n31:    metric = NotImplemented\n32:    metric_names = NotImplemented\n33:    split_map = None\n34:    labels_list = None\n35:    split_to_data_split: Mapping[str, str] = {\"train\": \"train\", \"validation\": \"validation\", \"test\": \"test\"}\n36:    small_datasets_without_all_splits = [\"cola\", \"wnli\", \"rte\", \"superglue-cb\", \"superglue-copa\", \"superglue-multirc\",\n37:                                         \"superglue-wic\", \"superglue-wsc.fixed\", \"superglue-rte\", \"mrpc\", \"stsb\",\n38:                                         \"superglue-boolq\", \"xsum\", \"scitail\"]\n39:    large_data_without_all_splits = [\"qqp\", \"qnli\", \"superglue-record\", \"sst2\", \"squad\", \"snli\", \"anli\",\n40:                                     \"amazon_polarity\", \"yelp_polarity\", \"winogrande\", \"newsqa\", \"searchqa\", \"triviaqa\", \"naturalquestions\", \"hotpotqa\"]\n41:\n42:    def __init__(self, config, seed=42):\n43:        self.config = config\n44:        self.seed = seed\n45:\n46:    def get_max_target_length(self, tokenizer, default_max_length):\n47:        if self.labels_list is not None:\n48:            return max([len(tokenizer.encode(label)) for label in self.labels_list])\n49:        return default_max_length\n50:\n51:    def seq2seq_format(self, sources: List[str],\n52:                       targets: List[str],\n53:                       add_prefix: bool = False,\n54:                       prefix: str = None,\n55:                       extra_fields={},\n56:                       verbalizer=\"\"):\n57:        \n58:        src_prefix = self.name if prefix is None else prefix\n59:        if verbalizer:\n60:            sources = [verbalizer] + sources\n61:        # sources = [src_prefix] + sources if add_prefix else sources\n62:        if len(extra_fields) == 0:\n63:            return {'source': ' '.join(sources),\n64:                    'target': ' '.join(targets),\n65:                    'task': self.name}\n66:        else:\n67:            return {'source': ' '.join(sources),\n68:                    'target': ' '.join(targets),\n69:                    'task': self.name,\n70:                    'extra_fields': extra_fields}\n71:\n72:    def check_n_obs(self, n_obs, total_size):\n73:        if n_obs is not None and n_obs > total_size:\n74:            n_obs = total_size\n75:            logger.warning(\"n_obs is set to %s\", n_obs)\n76:        return n_obs\n77:\n78:    def shuffled_indices(self, dataset):\n79:        num_samples = len(dataset)\n80:        generator = torch.Generator()\n81:        generator.manual_seed(self.seed)\n82:        return torch.randperm(num_samples, generator=generator).tolist()\n83:\n84:    def subsample(self, dataset, n_obs=None, indices=None):\n85:        \"\"\"\n86:        Given a dataset returns the subsampled dataset.\n87:        :param n_obs: the number of samples of the subsampled dataset.\n88:        :param indices: indices to select the samples from, if not given, indices are computed\n89:        from by shuffling the given dataset.\n90:        :return: subsampled dataset.\n91:        \"\"\"\n92:        num_samples = len(dataset)\n93:        n_obs = self.check_n_obs(n_obs, num_samples)\n94:        \n95:        if indices is None:\n96:            indices = self.shuffled_indices(dataset)\n97:            \n98:        indices = indices[:n_obs]\n99:        \n100:        return dataset.select(indices)\n(887 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open data_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa627018",
   "metadata": {},
   "source": [
    "### Step 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac87f715",
   "metadata": {},
   "source": [
    "Execution Start time: 06:40:26, End time: 06:40:26 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03291a8d",
   "metadata": {},
   "source": [
    "**Thought**: The `AbstractTask` class in `data_utils.py` seems to be a base class for handling datasets. To integrate our custom dataset, we need to create a new task class that inherits from `AbstractTask` and implements the necessary methods to load and preprocess the dataset. Let's scroll down to see if there are any existing task classes that we can use as a reference for creating our custom task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143fd1b2",
   "metadata": {},
   "source": [
    "**Action (goto)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04766913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/dpt/data_utils.py (987 lines total)]\n(84 more lines above)\n85:        \"\"\"\n86:        Given a dataset returns the subsampled dataset.\n87:        :param n_obs: the number of samples of the subsampled dataset.\n88:        :param indices: indices to select the samples from, if not given, indices are computed\n89:        from by shuffling the given dataset.\n90:        :return: subsampled dataset.\n91:        \"\"\"\n92:        num_samples = len(dataset)\n93:        n_obs = self.check_n_obs(n_obs, num_samples)\n94:        \n95:        if indices is None:\n96:            indices = self.shuffled_indices(dataset)\n97:            \n98:        indices = indices[:n_obs]\n99:        \n100:        return dataset.select(indices)\n101:\n102:    def load_dataset(self, split: int):\n103:        return datasets.load_dataset(self.name, self.config, split=split, script_version=\"master\")\n104:\n105:    def get_split_indices(self, dataset):\n106:        indices = self.shuffled_indices(dataset)\n107:        # change by Yao\n108:        return indices\n109:       \n110:    def map_dataset(self, dataset, add_prefix, add_vb):\n111:        return dataset.map(functools.partial(self.preprocessor, add_prefix=add_prefix, add_vb=add_vb),\n112:                           remove_columns=dataset.column_names, load_from_cache_file=False)\n113:\n114:    def get(self, split, add_prefix=True, n_obs=None, split_validation_test=False, lang=None, file_name=None, add_vb=False, file_prefix=None):\n115:        self.file_prefix = file_prefix  # path prefix for external dataset loading\n116:\n117:        if split_validation_test and split == \"train\":\n118:            mapped_split = self.split_to_data_split[\"train\"]\n119:            dataset = self.load_dataset(split=mapped_split)\n120:            indices = self.get_split_indices(dataset)\n121:            dataset = self.subsample(dataset, n_obs, indices)\n122:        else:\n123:            mapped_split = self.split_to_data_split[\"validation\"]\n124:            dataset = self.load_dataset(split=mapped_split)\n125:            indices = self.get_split_indices(dataset)\n126:            dataset = self.subsample(dataset, n_obs, indices)\n127:\n128:        return self.map_dataset(dataset, add_prefix, add_vb)\n129:    \n130:\n131:class Squad(AbstractTask):\n132:    name = \"squad\"\n133:    metric = [metrics.squad]\n134:\n135:    split_to_data_split = {\"train\": \"train\",\n136:                           \"validation\": \"validation\",\n137:                           \"test\": \"validation\"}\n138:\n139:    def load_dataset(self, split):\n140:        return datasets.load_dataset(\"squad\", split=split)\n141:\n142:    def preprocessor(self, example, add_prefix, add_vb=False):\n143:        answer = pad_punctuation(example['answers']['text'][0])\n144:        question = pad_punctuation(example['question'])\n145:        context = pad_punctuation(example['context'])\n146:        source = [\"question:\", question,\n147:                  \"context:\", context]\n148:        target = [answer] if type(answer) == str else answer\n149:        return self.seq2seq_format(source, target, add_prefix)\n150:\n151:\n152:class NewsQA(AbstractTask):\n153:    name = \"newsqa\"\n154:    metric = [metrics.squad]\n155:\n156:    split_to_data_split = {\"train\": \"train\",\n157:                           \"validation\": \"validation\",\n158:                           \"test\": \"validation\"}\n159:    split_to_file_name = {\n160:        'train': 'NewsQA_train',\n161:        'validation': 'NewsQA_dev'\n162:    }\n163:\n164:    def load_dataset(self, split):\n165:        return datasets.load_dataset('json', data_files={\n166:            split: os.path.join(self.file_prefix, f'{self.split_to_file_name[split]}.jsonl')\n167:        })[split]\n168:\n169:    def preprocessor(self, example, add_prefix, add_vb=False):\n170:        answer = pad_punctuation(example['answers']['text'][0])\n171:        question = pad_punctuation(example['question'])\n172:        context = pad_punctuation(example['context'])\n173:        source = [\"question:\", question,\n174:                  \"context:\", context]\n175:        target = [answer] if type(answer) == str else answer\n176:        return self.seq2seq_format(source, target, add_prefix)\n177:\n178:\n179:class SearchQA(AbstractTask):\n180:    name = \"searchqa\"\n181:    metric = [metrics.squad]\n182:    split_to_data_split = {\"train\": \"train\",\n183:                           \"validation\": \"validation\",\n184:                           \"test\": \"validation\"}\n(803 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goto 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a8777a",
   "metadata": {},
   "source": [
    "### Step 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d150c4",
   "metadata": {},
   "source": [
    "Execution Start time: 06:40:29, End time: 06:40:29 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac269e5",
   "metadata": {},
   "source": [
    "**Thought**: We can see that there are existing task classes like `Squad` and `NewsQA` that inherit from `AbstractTask`. These classes define methods to load datasets and preprocess them. We can create a similar class for our custom dataset. Let's scroll further to see if there are any other relevant classes or methods that might help us in creating our custom task class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924b560d",
   "metadata": {},
   "source": [
    "**Action (goto)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9a3009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/dpt/data_utils.py (987 lines total)]\n(184 more lines above)\n185:    split_to_file_name = {\n186:        'train': 'SearchQA_train',\n187:        'validation': 'SearchQA_dev'\n188:    }\n189:\n190:    def load_dataset(self, split):\n191:        return datasets.load_dataset('json', data_files={\n192:            split: os.path.join(self.file_prefix, f'{self.split_to_file_name[split]}.jsonl')\n193:        })[split]\n194:\n195:    def preprocessor(self, example, add_prefix, add_vb=False):\n196:        answer = pad_punctuation(example['answers']['text'][0])\n197:        question = pad_punctuation(example['question'])\n198:        context = pad_punctuation(example['context'])\n199:        source = [\"question:\", question,\n200:                  \"context:\", context]\n201:        target = [answer] if type(answer) == str else answer\n202:        return self.seq2seq_format(source, target, add_prefix)\n203:\n204:\n205:class NaturalQA(AbstractTask):\n206:    name = \"naturalquestions\"\n207:    metric = [metrics.squad]\n208:    split_to_data_split = {\"train\": \"train\",\n209:                           \"validation\": \"validation\",\n210:                           \"test\": \"validation\"}\n211:    split_to_file_name = {\n212:        'train': 'NaturalQuestionsShort_train',\n213:        'validation': 'NaturalQuestionsShort_dev'\n214:    }\n215:\n216:    def load_dataset(self, split):\n217:        return datasets.load_dataset('json', data_files={\n218:            split: os.path.join(self.file_prefix, f'{self.split_to_file_name[split]}.jsonl')\n219:        })[split]\n220:\n221:    def preprocessor(self, example, add_prefix, add_vb=False):\n222:        answer = pad_punctuation(example['answers']['text'][0])\n223:        question = pad_punctuation(example['question'])\n224:        context = pad_punctuation(example['context'])\n225:        source = [\"question:\", question,\n226:                  \"context:\", context]\n227:        target = [answer] if type(answer) == str else answer\n228:        return self.seq2seq_format(source, target, add_prefix)\n229:\n230:\n231:class HotpotQA(AbstractTask):\n232:    name = \"hotpotqa\"\n233:    metric = [metrics.squad]\n234:    split_to_data_split = {\"train\": \"train\",\n235:                           \"validation\": \"validation\",\n236:                           \"test\": \"validation\"}\n237:    split_to_file_name = {\n238:        'train': 'HotpotQA_train',\n239:        'validation': 'HotpotQA_dev'\n240:    }\n241:\n242:    def load_dataset(self, split):\n243:        return datasets.load_dataset('json', data_files={\n244:            split: os.path.join(self.file_prefix, f'{self.split_to_file_name[split]}.jsonl')\n245:        })[split]\n246:\n247:    def preprocessor(self, example, add_prefix, add_vb=False):\n248:        answer = pad_punctuation(example['answers']['text'][0])\n249:        question = pad_punctuation(example['question'])\n250:        context = pad_punctuation(example['context'])\n251:        source = [\"question:\", question,\n252:                  \"context:\", context]\n253:        target = [answer] if type(answer) == str else answer\n254:        return self.seq2seq_format(source, target, add_prefix)\n255:\n256:\n257:class SciTail(AbstractTask):\n258:    name = \"scitail\"\n259:    labels_list = [\"0\", \"1\"]\n260:    metric = [metrics.accuracy]\n261:    metric_names = [\"accuracy\"]\n262:    split_to_data_split = {\"train\": \"train\",\n263:                           \"validation\": \"validation\",\n264:                           \"test\": \"test\"}\n265:\n266:    def load_dataset(self, split):\n267:        return datasets.load_dataset('scitail', \"snli_format\", split=split)\n268:\n269:    def preprocessor(self, example, add_prefix=True, add_vb=False):\n270:        label2id = {\"entailment\": \"0\", \"neutral\": \"1\"}\n271:        src_texts = [\"sentence1:\", example['sentence1'],\n272:                     \"sentence2:\", example[\"sentence2\"]]\n273:        tgt_texts = [label2id[example[\"gold_label\"]]]\n274:        if add_vb:\n275:            verbalizer = \"{ 0 : entailment, 1 : neutral }\"\t\n276:        else:\n277:            verbalizer = \"\"\t\n278:        return self.seq2seq_format(src_texts, tgt_texts, add_prefix, verbalizer=verbalizer)\n279:\n280:\n281:class MRPC(AbstractTask):\n282:    name = \"mrpc\"\n283:    labels_list = [\"0\", \"1\"]\n284:    metric = [metrics.accuracy, metrics.f1_score_with_invalid]\n(703 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goto 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba23f7d",
   "metadata": {},
   "source": [
    "### Step 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9388f30",
   "metadata": {},
   "source": [
    "Execution Start time: 06:40:31, End time: 06:40:31 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb4f881",
   "metadata": {},
   "source": [
    "**Thought**: We have found the `MRPC` class, which seems to be relevant to our task since we are working with a sentence pair classification dataset. This class already handles loading and preprocessing for the MRPC dataset. We need to ensure that this class is set up to load our custom dataset from the `mrpc_csv` directory. Let's scroll further to see the implementation details of the `MRPC` class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb968d66",
   "metadata": {},
   "source": [
    "**Action (goto)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7860e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/dpt/data_utils.py (987 lines total)]\n(284 more lines above)\n285:    metric_names = [\"accuracy\", \"f1\"]\n286:    split_to_data_split = {\"train\": \"train\",\n287:                           \"validation\": \"validation\",\n288:                           \"test\": \"validation\"}\n289:\n290:    def load_dataset(self, split):\n291:        # return datasets.load_dataset('glue', 'mrpc', split=split, script_version=\"master\")\n292:        return datasets.load_dataset('glue', 'mrpc', split=split)\n293:\n294:    def preprocessor(self, example, add_prefix=True, add_vb=False):\n295:        src_texts = [\"sentence1:\", example['sentence1'],\n296:                     \"sentence2:\", example[\"sentence2\"]]\n297:        tgt_texts = [str(example['label'])]\n298:        if add_vb:\n299:            verbalizer = \"{ 0 : not equivalent, 1 : equivalent }\"\n300:        else:\n301:            verbalizer = \"\"\n302:        return self.seq2seq_format(src_texts, tgt_texts, add_prefix, verbalizer=verbalizer)\n303:\n304:\n305:class COLA(AbstractTask):\n306:    name = \"cola\"\n307:    labels_list = [\"0\", \"1\"]\n308:    metric = [metrics.matthews_corrcoef]\n309:    metric_names = [\"matthews_correlation\"]\n310:    split_to_data_split = {\"train\": \"train\",\n311:                           \"validation\": \"validation\",\n312:                           \"test\": \"validation\"}\n313:\n314:    def load_dataset(self, split):\n315:        return datasets.load_dataset('glue', 'cola', split=split)\n316:\n317:    def preprocessor(self, example, add_prefix=True, add_vb=False):\t\n318:        src_texts = [\"sentence:\", example['sentence']]\t\n319:        tgt_texts = [str(example['label'])]\t\n320:        if add_vb:\t\n321:            verbalizer = \"{ 0 : grammatically unacceptable, 1 : grammatically acceptable }\"\t\n322:        else:\t\n323:            verbalizer = \"\"\t\n324:        return self.seq2seq_format(src_texts, tgt_texts, add_prefix, verbalizer=verbalizer)\n325:\n326:class SST2(AbstractTask):\n327:    name = \"sst2\"\n328:    labels_list = [\"0\", \"1\"]\n329:    metric = [metrics.accuracy]\n330:    metric_names = [\"accuracy\"]\n331:    split_to_data_split = {\"train\": \"train\",\n332:                           \"validation\": \"validation\",\n333:                           \"test\": \"validation\"}\n334:\n335:    def load_dataset(self, split):\n336:        return datasets.load_dataset('glue', 'sst2', split=split)\n337:\n338:    def preprocessor(self, example, add_prefix=True, add_vb=False):\n339:        src_texts = [\"sentence:\", example['sentence']]\n340:        tgt_texts = [str(example['label'])]\n341:        if add_vb:\n342:            verbalizer = \"{ 0 : negative, 1 : positive }\"\n343:        else:\n344:            verbalizer = \"\"\n345:        return self.seq2seq_format(src_texts, tgt_texts, add_prefix, verbalizer=verbalizer)\n346:\n347:\n348:class YelpPolarity(AbstractTask):\n349:    name = \"yelp_polarity\"\n350:    labels_list = [\"0\", \"1\"]\n351:    metric = [metrics.accuracy]\n352:    metric_names = [\"accuracy\"]\n353:    split_to_data_split = {\"train\": \"train\", \"test\": \"test\"}\n354:\n355:    split_to_file_name = {\n356:        'train': 'yelp_train',\n357:        'test': 'yelp_test'\n358:    }\n359:\n360:    def load_dataset(self, split):\n361:        return datasets.load_dataset('csv', data_files={\n362:            split: os.path.join(self.file_prefix, f'{self.split_to_file_name[split]}.csv')},\n363:            column_names=['label', 'text']\n364:            )[split]\n365:\n366:    # def load_dataset(self, split):\n367:    #     return datasets.load_dataset('yelp_polarity')[split]\n368:\n369:    def preprocessor(self, example, add_prefix=True,  add_vb=False):\n370:        src_texts = [\"sentence:\", example['text']]\n371:        tgt_texts = [str(example['label'] - 1)]  # labels from local files\n372:        if add_vb:\n373:            verbalizer = \"{ 0 : negative, 1 : positive }\"\n374:        else:\n375:            verbalizer = \"\"\n376:        return self.seq2seq_format(src_texts, tgt_texts, add_prefix, verbalizer=verbalizer)\n377:\n378:\n379:class Amazon_Polarity(AbstractTask):\n380:    name = \"amazon_polarity\"\n381:    labels_list = [\"0\", \"1\"]\n382:    metric = [metrics.accuracy]\n383:    metric_names = [\"accuracy\"]\n384:    split_to_data_split = {\"train\": \"train\", \"test\": \"test\"}\n(603 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goto 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f30eac",
   "metadata": {},
   "source": [
    "### Step 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a043c99",
   "metadata": {},
   "source": [
    "Execution Start time: 06:40:34, End time: 06:40:35 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36abe40f",
   "metadata": {},
   "source": [
    "**Thought**: The `MRPC` class is already set up to load the MRPC dataset using the `datasets` library. However, since we have a custom dataset in CSV format, we need to modify the `load_dataset` method to load our CSV files instead. We can use the approach seen in the `YelpPolarity` class, which loads datasets from CSV files.\n",
    "\n",
    "Let's edit the `MRPC` class to load the dataset from the `mrpc_csv` directory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d40719",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8514ea1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/dpt/data_utils.py (986 lines total)]\n(239 more lines above)\n240:    }\n241:\n242:    def load_dataset(self, split):\n243:        return datasets.load_dataset('json', data_files={\n244:            split: os.path.join(self.file_prefix, f'{self.split_to_file_name[split]}.jsonl')\n245:        })[split]\n246:\n247:    def preprocessor(self, example, add_prefix, add_vb=False):\n248:        answer = pad_punctuation(example['answers']['text'][0])\n249:        question = pad_punctuation(example['question'])\n250:        context = pad_punctuation(example['context'])\n251:        source = [\"question:\", question,\n252:                  \"context:\", context]\n253:        target = [answer] if type(answer) == str else answer\n254:        return self.seq2seq_format(source, target, add_prefix)\n255:\n256:\n257:class SciTail(AbstractTask):\n258:    name = \"scitail\"\n259:    labels_list = [\"0\", \"1\"]\n260:    metric = [metrics.accuracy]\n261:    metric_names = [\"accuracy\"]\n262:    split_to_data_split = {\"train\": \"train\",\n263:                           \"validation\": \"validation\",\n264:                           \"test\": \"test\"}\n265:\n266:    def load_dataset(self, split):\n267:        return datasets.load_dataset('scitail', \"snli_format\", split=split)\n268:\n269:    def preprocessor(self, example, add_prefix=True, add_vb=False):\n270:        label2id = {\"entailment\": \"0\", \"neutral\": \"1\"}\n271:        src_texts = [\"sentence1:\", example['sentence1'],\n272:                     \"sentence2:\", example[\"sentence2\"]]\n273:        tgt_texts = [label2id[example[\"gold_label\"]]]\n274:        if add_vb:\n275:            verbalizer = \"{ 0 : entailment, 1 : neutral }\"\t\n276:        else:\n277:            verbalizer = \"\"\t\n278:        return self.seq2seq_format(src_texts, tgt_texts, add_prefix, verbalizer=verbalizer)\n279:\n280:\n281:class MRPC(AbstractTask):\n282:    name = \"mrpc\"\n283:    labels_list = [\"0\", \"1\"]\n284:    metric = [metrics.accuracy, metrics.f1_score_with_invalid]\n285:    metric_names = [\"accuracy\", \"f1\"]\n286:    split_to_data_split = {\"train\": \"train\",\n287:                           \"validation\": \"validation\",\n288:                           \"test\": \"validation\"}\n289:\n290:    def load_dataset(self, split):\n291:        return datasets.load_dataset('csv', data_files={split: f'mrpc_csv/{split}.csv'})[split]\n292:\n293:    def preprocessor(self, example, add_prefix=True, add_vb=False):\n294:        src_texts = [\"sentence1:\", example['sentence1'],\n295:                     \"sentence2:\", example[\"sentence2\"]]\n296:        tgt_texts = [str(example['label'])]\n297:        if add_vb:\n298:            verbalizer = \"{ 0 : not equivalent, 1 : equivalent }\"\n299:        else:\n300:            verbalizer = \"\"\n301:        return self.seq2seq_format(src_texts, tgt_texts, add_prefix, verbalizer=verbalizer)\n302:\n303:\n304:class COLA(AbstractTask):\n305:    name = \"cola\"\n306:    labels_list = [\"0\", \"1\"]\n307:    metric = [metrics.matthews_corrcoef]\n308:    metric_names = [\"matthews_correlation\"]\n309:    split_to_data_split = {\"train\": \"train\",\n310:                           \"validation\": \"validation\",\n311:                           \"test\": \"validation\"}\n312:\n313:    def load_dataset(self, split):\n314:        return datasets.load_dataset('glue', 'cola', split=split)\n315:\n316:    def preprocessor(self, example, add_prefix=True, add_vb=False):\t\n317:        src_texts = [\"sentence:\", example['sentence']]\t\n318:        tgt_texts = [str(example['label'])]\t\n319:        if add_vb:\t\n320:            verbalizer = \"{ 0 : grammatically unacceptable, 1 : grammatically acceptable }\"\t\n321:        else:\t\n322:            verbalizer = \"\"\t\n323:        return self.seq2seq_format(src_texts, tgt_texts, add_prefix, verbalizer=verbalizer)\n324:\n325:class SST2(AbstractTask):\n326:    name = \"sst2\"\n327:    labels_list = [\"0\", \"1\"]\n328:    metric = [metrics.accuracy]\n329:    metric_names = [\"accuracy\"]\n330:    split_to_data_split = {\"train\": \"train\",\n331:                           \"validation\": \"validation\",\n332:                           \"test\": \"validation\"}\n333:\n334:    def load_dataset(self, split):\n335:        return datasets.load_dataset('glue', 'sst2', split=split)\n336:\n337:    def preprocessor(self, example, add_prefix=True, add_vb=False):\n338:        src_texts = [\"sentence:\", example['sentence']]\n339:        tgt_texts = [str(example['label'])]\n(647 more lines below)\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 290:292\n",
    "    def load_dataset(self, split):\n",
    "        return datasets.load_dataset('csv', data_files={split: f'mrpc_csv/{split}.csv'})[split]\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd79bc98",
   "metadata": {},
   "source": [
    "### Step 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1570e52a",
   "metadata": {},
   "source": [
    "Execution Start time: 06:40:38, End time: 06:40:45 (duration: 6 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6c62a6",
   "metadata": {},
   "source": [
    "**Thought**: The `MRPC` class has been successfully updated to load the dataset from the `mrpc_csv` directory. Now, we should be ready to run the `main_ddp.py` script with the specified parameters to fine-tune the `t5-small` model using the decomposed prompt tuning method.\n",
    "\n",
    "Let's execute the script with the given hyperparameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bfb288",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d8f85d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stating time:  08/22/2024 06:40:43\nRaw Arguments:  Namespace(seed=42, from_checkpoint='', auto_resume=False, save_ckpt_dir='./saved_models', load_source_path='', saving_steps=1000, saving_each_epoch=True, latest_ckpt_name='latest_checkpoint.pt', n_epochs=1, train_batch_size=16, eval_batch_size=128, lr=0.3, max_grad_norm=1.0, weight_decay=1e-05, accumulate_steps=1, lr_scheduler_type='linear', num_warmup_steps=0.1, max_train_steps=0, min_training_epoch=20, early_stopping_patience=10, mode='train', close_tqdm=True, max_epochs_before_stop=2, model_name='t5-small', datasets=['mrpc'], max_source_length=256, max_target_length=128, prompt_type='param', is_dynamic_share=False, eval_in_train=True, enc_prompt_tokens=100, dec_prompt_tokens=0, sharing_ratio=1.0, share_seed=42, add_task_prefix=True, add_verbalizer=False, port=12355, gupids=None, sampling_method='uniform', size_limit=262144, stoch_task=False, bottle_neck=10, model_output_path='./saved_outputs', local_file_prefix='/gpfs/u/home/DPTV/DPTVhnwz/scratch/mrqa_datasets/datasets', datasets_names=['mrpc'])\nProcess ID:  170\nUse single gpu!\n\ntokenizer_config.json:   0%|                        | 0.00/2.32k [00:00<?, ?B/s]\n\n\nspiece.model:   0%|                                  | 0.00/792k [00:00<?, ?B/s]\n\n\ntokenizer.json:   0%|                               | 0.00/1.39M [00:00<?, ?B/s]\n\n\nTraceback (most recent call last):\n  File \"/content/dpt/main_ddp.py\", line 569, in <module>\n    main()\n  File \"/content/dpt/main_ddp.py\", line 562, in main\n    run(0, args)\n  File \"/content/dpt/main_ddp.py\", line 84, in run\n    val_dataset = AutoTask.get(dataset_name).get(split=\"validation\",\n  File \"/content/dpt/data_utils.py\", line 124, in get\n    dataset = self.load_dataset(split=mapped_split)\n  File \"/content/dpt/data_utils.py\", line 291, in load_dataset\n    return datasets.load_dataset('csv', data_files={split: f'mrpc_csv/{split}.csv'})[split]\n  File \"/usr/local/lib/python3.10/site-packages/datasets/load.py\", line 2587, in load_dataset\n    builder_instance = load_dataset_builder(\n  File \"/usr/local/lib/python3.10/site-packages/datasets/load.py\", line 2259, in load_dataset_builder\n    dataset_module = dataset_module_factory(\n  File \"/usr/local/lib/python3.10/site-packages/datasets/load.py\", line 1799, in dataset_module_factory\n    ).get_module()\n  File \"/usr/local/lib/python3.10/site-packages/datasets/load.py\", line 1141, in get_module\n    data_files = DataFilesDict.from_patterns(\n  File \"/usr/local/lib/python3.10/site-packages/datasets/data_files.py\", line 715, in from_patterns\n    DataFilesList.from_patterns(\n  File \"/usr/local/lib/python3.10/site-packages/datasets/data_files.py\", line 620, in from_patterns\n    resolve_pattern(\n  File \"/usr/local/lib/python3.10/site-packages/datasets/data_files.py\", line 407, in resolve_pattern\n    raise FileNotFoundError(error_msg)\nFileNotFoundError: Unable to find '/content/dpt/mrpc_csv/validation.csv'\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python main_ddp.py --datasets=mrpc --model_name=t5-small --enc_prompt_tokens 100 -ts 16 -e 1 --bottle_neck 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ec1853",
   "metadata": {},
   "source": [
    "### Step 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce7cdb7",
   "metadata": {},
   "source": [
    "Execution Start time: 06:40:47, End time: 06:40:48 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5c8376",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the script is trying to load a file named `validation.csv`, but our dataset directory contains `val.csv` instead. We need to update the `MRPC` class to correctly map the split name to the available file name.\n",
    "\n",
    "Let's edit the `MRPC` class to use `val.csv` instead of `validation.csv`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1138c3",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6772f0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/dpt/data_utils.py (986 lines total)]\n(233 more lines above)\n234:    split_to_data_split = {\"train\": \"train\",\n235:                           \"validation\": \"validation\",\n236:                           \"test\": \"validation\"}\n237:    split_to_file_name = {\n238:        'train': 'HotpotQA_train',\n239:        'validation': 'HotpotQA_dev'\n240:    }\n241:\n242:    def load_dataset(self, split):\n243:        return datasets.load_dataset('json', data_files={\n244:            split: os.path.join(self.file_prefix, f'{self.split_to_file_name[split]}.jsonl')\n245:        })[split]\n246:\n247:    def preprocessor(self, example, add_prefix, add_vb=False):\n248:        answer = pad_punctuation(example['answers']['text'][0])\n249:        question = pad_punctuation(example['question'])\n250:        context = pad_punctuation(example['context'])\n251:        source = [\"question:\", question,\n252:                  \"context:\", context]\n253:        target = [answer] if type(answer) == str else answer\n254:        return self.seq2seq_format(source, target, add_prefix)\n255:\n256:\n257:class SciTail(AbstractTask):\n258:    name = \"scitail\"\n259:    labels_list = [\"0\", \"1\"]\n260:    metric = [metrics.accuracy]\n261:    metric_names = [\"accuracy\"]\n262:    split_to_data_split = {\"train\": \"train\",\n263:                           \"validation\": \"validation\",\n264:                           \"test\": \"test\"}\n265:\n266:    def load_dataset(self, split):\n267:        return datasets.load_dataset('scitail', \"snli_format\", split=split)\n268:\n269:    def preprocessor(self, example, add_prefix=True, add_vb=False):\n270:        label2id = {\"entailment\": \"0\", \"neutral\": \"1\"}\n271:        src_texts = [\"sentence1:\", example['sentence1'],\n272:                     \"sentence2:\", example[\"sentence2\"]]\n273:        tgt_texts = [label2id[example[\"gold_label\"]]]\n274:        if add_vb:\n275:            verbalizer = \"{ 0 : entailment, 1 : neutral }\"\t\n276:        else:\n277:            verbalizer = \"\"\t\n278:        return self.seq2seq_format(src_texts, tgt_texts, add_prefix, verbalizer=verbalizer)\n279:\n280:\n281:class MRPC(AbstractTask):\n282:    name = \"mrpc\"\n283:    labels_list = [\"0\", \"1\"]\n284:    split_to_data_split = {\"train\": \"train\", \"validation\": \"val\", \"test\": \"test\"}\n285:    metric_names = [\"accuracy\", \"f1\"]\n286:    split_to_data_split = {\"train\": \"train\",\n287:                           \"validation\": \"validation\",\n288:                           \"test\": \"validation\"}\n289:\n290:    def load_dataset(self, split):\n291:        return datasets.load_dataset('csv', data_files={split: f'mrpc_csv/{split}.csv'})[split]\n292:\n293:    def preprocessor(self, example, add_prefix=True, add_vb=False):\n294:        src_texts = [\"sentence1:\", example['sentence1'],\n295:                     \"sentence2:\", example[\"sentence2\"]]\n296:        tgt_texts = [str(example['label'])]\n297:        if add_vb:\n298:            verbalizer = \"{ 0 : not equivalent, 1 : equivalent }\"\n299:        else:\n300:            verbalizer = \"\"\n301:        return self.seq2seq_format(src_texts, tgt_texts, add_prefix, verbalizer=verbalizer)\n302:\n303:\n304:class COLA(AbstractTask):\n305:    name = \"cola\"\n306:    labels_list = [\"0\", \"1\"]\n307:    metric = [metrics.matthews_corrcoef]\n308:    metric_names = [\"matthews_correlation\"]\n309:    split_to_data_split = {\"train\": \"train\",\n310:                           \"validation\": \"validation\",\n311:                           \"test\": \"validation\"}\n312:\n313:    def load_dataset(self, split):\n314:        return datasets.load_dataset('glue', 'cola', split=split)\n315:\n316:    def preprocessor(self, example, add_prefix=True, add_vb=False):\t\n317:        src_texts = [\"sentence:\", example['sentence']]\t\n318:        tgt_texts = [str(example['label'])]\t\n319:        if add_vb:\t\n320:            verbalizer = \"{ 0 : grammatically unacceptable, 1 : grammatically acceptable }\"\t\n321:        else:\t\n322:            verbalizer = \"\"\t\n323:        return self.seq2seq_format(src_texts, tgt_texts, add_prefix, verbalizer=verbalizer)\n324:\n325:class SST2(AbstractTask):\n326:    name = \"sst2\"\n327:    labels_list = [\"0\", \"1\"]\n328:    metric = [metrics.accuracy]\n329:    metric_names = [\"accuracy\"]\n330:    split_to_data_split = {\"train\": \"train\",\n331:                           \"validation\": \"validation\",\n332:                           \"test\": \"validation\"}\n333:\n(653 more lines below)\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 284:284\n",
    "    split_to_data_split = {\"train\": \"train\", \"validation\": \"val\", \"test\": \"test\"}\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b6bed2",
   "metadata": {},
   "source": [
    "### Step 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db678d4b",
   "metadata": {},
   "source": [
    "Execution Start time: 06:40:49, End time: 06:40:54 (duration: 5 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877dc5c4",
   "metadata": {},
   "source": [
    "**Thought**: The `MRPC` class has been updated to correctly map the split names to the available CSV files. Now, let's try running the `main_ddp.py` script again with the specified parameters to fine-tune the `t5-small` model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0d1307",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cd4431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stating time:  08/22/2024 06:40:53\nRaw Arguments:  Namespace(seed=42, from_checkpoint='', auto_resume=False, save_ckpt_dir='./saved_models', load_source_path='', saving_steps=1000, saving_each_epoch=True, latest_ckpt_name='latest_checkpoint.pt', n_epochs=1, train_batch_size=16, eval_batch_size=128, lr=0.3, max_grad_norm=1.0, weight_decay=1e-05, accumulate_steps=1, lr_scheduler_type='linear', num_warmup_steps=0.1, max_train_steps=0, min_training_epoch=20, early_stopping_patience=10, mode='train', close_tqdm=True, max_epochs_before_stop=2, model_name='t5-small', datasets=['mrpc'], max_source_length=256, max_target_length=128, prompt_type='param', is_dynamic_share=False, eval_in_train=True, enc_prompt_tokens=100, dec_prompt_tokens=0, sharing_ratio=1.0, share_seed=42, add_task_prefix=True, add_verbalizer=False, port=12355, gupids=None, sampling_method='uniform', size_limit=262144, stoch_task=False, bottle_neck=10, model_output_path='./saved_outputs', local_file_prefix='/gpfs/u/home/DPTV/DPTVhnwz/scratch/mrqa_datasets/datasets', datasets_names=['mrpc'])\nProcess ID:  206\nUse single gpu!\nTraceback (most recent call last):\n  File \"/content/dpt/main_ddp.py\", line 569, in <module>\n    main()\n  File \"/content/dpt/main_ddp.py\", line 562, in main\n    run(0, args)\n  File \"/content/dpt/main_ddp.py\", line 84, in run\n    val_dataset = AutoTask.get(dataset_name).get(split=\"validation\",\n  File \"/content/dpt/data_utils.py\", line 124, in get\n    dataset = self.load_dataset(split=mapped_split)\n  File \"/content/dpt/data_utils.py\", line 291, in load_dataset\n    return datasets.load_dataset('csv', data_files={split: f'mrpc_csv/{split}.csv'})[split]\n  File \"/usr/local/lib/python3.10/site-packages/datasets/load.py\", line 2587, in load_dataset\n    builder_instance = load_dataset_builder(\n  File \"/usr/local/lib/python3.10/site-packages/datasets/load.py\", line 2259, in load_dataset_builder\n    dataset_module = dataset_module_factory(\n  File \"/usr/local/lib/python3.10/site-packages/datasets/load.py\", line 1799, in dataset_module_factory\n    ).get_module()\n  File \"/usr/local/lib/python3.10/site-packages/datasets/load.py\", line 1141, in get_module\n    data_files = DataFilesDict.from_patterns(\n  File \"/usr/local/lib/python3.10/site-packages/datasets/data_files.py\", line 715, in from_patterns\n    DataFilesList.from_patterns(\n  File \"/usr/local/lib/python3.10/site-packages/datasets/data_files.py\", line 620, in from_patterns\n    resolve_pattern(\n  File \"/usr/local/lib/python3.10/site-packages/datasets/data_files.py\", line 407, in resolve_pattern\n    raise FileNotFoundError(error_msg)\nFileNotFoundError: Unable to find '/content/dpt/mrpc_csv/validation.csv'\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python main_ddp.py --datasets=mrpc --model_name=t5-small --enc_prompt_tokens 100 -ts 16 -e 1 --bottle_neck 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749be05c",
   "metadata": {},
   "source": [
    "### Step 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f04ddd",
   "metadata": {},
   "source": [
    "Execution Start time: 06:40:56, End time: 06:40:56 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b288c62e",
   "metadata": {},
   "source": [
    "**Thought**: It seems that the script is still trying to access a file named `validation.csv`. This indicates that there might be another place in the code where the split name is being used directly. Let's search the `main_ddp.py` file for any occurrences of the word \"validation\" to ensure all references are updated to use \"val\" instead.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8286f432",
   "metadata": {},
   "source": [
    "**Action (search_file)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6232759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Found 4 matches for \"validation\" in /content/dpt/main_ddp.py:\nLine 77:                                    split_validation_test=True,\nLine 84:        val_dataset = AutoTask.get(dataset_name).get(split=\"validation\",\nLine 85:                                    split_validation_test=True,\nLine 136:    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\nEnd of matches for \"validation\" in /content/dpt/main_ddp.py\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_file \"validation\" main_ddp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135c0d63",
   "metadata": {},
   "source": [
    "### Step 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba7676d",
   "metadata": {},
   "source": [
    "Execution Start time: 06:40:58, End time: 06:40:58 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015590d5",
   "metadata": {},
   "source": [
    "**Thought**: The `main_ddp.py` file contains references to the \"validation\" split on lines 77, 84, and 85. We need to update these references to use \"val\" instead, which matches the file name in our dataset directory.\n",
    "\n",
    "Let's edit the `main_ddp.py` file to make these changes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f67a86f",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95437457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/dpt/main_ddp.py (569 lines total)]\n(61 more lines above)\n62:    np.random.seed(args.seed)\n63:    torch.manual_seed(args.seed)\n64:    torch.cuda.manual_seed_all(args.seed)\n65:    g = torch.Generator()\n66:    g.manual_seed(args.seed)\n67:\n68:    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n69:\n70:    ###################################################################################################\n71:    #   Load data                                                                                     #\n72:    ###################################################################################################\n73:    # Can processing multiple datasets\n74:    train_datasets, val_datasets = [], {}\n75:    for dataset_name in args.datasets_names:\n76:        train_dataset = AutoTask.get(dataset_name).get(split=\"train\",\n77:                                    split_validation_test=True,\n78:                                    add_prefix=args.add_task_prefix,\n79:                                    add_vb=args.add_verbalizer,\n80:                                    file_prefix=args.local_file_prefix,\n81:                                    n_obs=100000 if dataset_name == 'yelp_polarity' else None)\n82:        train_datasets.append(train_dataset)\n83:\n84:        val_dataset = AutoTask.get(dataset_name).get(split=\"validation\",\n85:                                    split_validation_test=True,\n86:                                    add_prefix=args.add_task_prefix,\n87:                                    add_vb=args.add_verbalizer,\n88:                                    file_prefix=args.local_file_prefix,\n89:                                    n_obs=None)\n90:        val_datasets.update({dataset_name: val_dataset})\n91:\n92:        \n93:        if is_master:\n94:            print(local_rank, dataset_name, 'Train\\t', train_dataset[0])\n95:            print(local_rank, dataset_name, 'Val\\t', val_dataset[0])\n96:            \n97:    # merge all datasets if there are multiple ones\n98:    train_datasets = concatenate_datasets(train_datasets)\n99:\n100:    if is_master:\n101:        print(local_rank, len(train_dataset), len(val_dataset))\n102:        print('# all training samples:', len(train_datasets))\n103:        print(train_datasets[0])\n104:\n105:    # Data loader \n106:    # Creating the Training and Validation dataset for further creation of Dataloader\n107:    training_set = CustomT5Dataset(train_datasets, tokenizer, \n108:                                    args.max_source_length, \n109:                                    args.max_target_length, \n110:                                    args.datasets_names\n111:                                    )\n112:    \n113:\n114:    val_sets = {data_name: CustomT5Dataset(data_set, tokenizer, \n115:                                           args.max_source_length, \n116:                                           args.max_target_length, \n117:                                           args.datasets_names\n118:                                           )\n119:               for data_name, data_set in val_datasets.items()}\n120:\n121:    # Defining the parameters for creation of dataloaders\n122:    train_params = {\n123:        'batch_size': args.train_batch_size,\n124:        'shuffle': not is_mp,  # not shuffle in DDP\n125:        'num_workers': 4,\n126:        'worker_init_fn': seed_worker,\n127:        'generator': g,\n128:        }\n129:\n130:    eval_params = {\n131:        'batch_size': args.eval_batch_size,\n132:        'shuffle': False,\n133:        'num_workers': 0,\n134:        }\n135:\n136:    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n137:    if is_mp:\n138:        sampler = DistributedSampler(training_set, num_replicas=world_size, rank=local_rank, shuffle=True)\n139:        TrainDataloader = DataLoader(training_set, sampler=sampler, **train_params)\n140:    else:\n141:        TrainDataloader = DataLoader(training_set, **train_params)\n142:\n143:    ValDataloaders = {data_name: DataLoader(data_set, **eval_params) for data_name, data_set in val_sets.items()}\n144:    \n145:    ###################################################################################################\n146:    #   Build the model                                                                                   #\n147:    ###################################################################################################\n148:    config = AutoConfig.from_pretrained(args.model_name)\n149:    config.len_enc_prompt = args.enc_prompt_tokens\n150:    config.len_dec_prompt = args.dec_prompt_tokens\n151:    config.add_enc_prompt = args.enc_prompt_tokens > 0\n152:    config.add_dec_prompt = args.dec_prompt_tokens > 0\n153:    config.num_tasks = len(args.datasets_names)\n154:    \n155:    config.bottle_neck = args.bottle_neck\n156:\n157:    model = T5PromptForConditionalGeneration_param.from_pretrained(args.model_name, config=config)\n158:\n159:    # Freeze the backbone model\n160:    for name, param in model.named_parameters():\n161:        param.requires_grad = False if 'prefix' not in name else True\n(408 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open main_ddp.py 77"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847e5a29",
   "metadata": {},
   "source": [
    "### Step 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d87a24",
   "metadata": {},
   "source": [
    "Execution Start time: 06:41:01, End time: 06:41:02 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7536c1fb",
   "metadata": {},
   "source": [
    "**Thought**: Let's update the references to the \"validation\" split on lines 77, 84, and 85 to use \"val\" instead. This should align with the file names in our dataset directory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f696d63",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408b0892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\n\nERRORS:\n- E999 SyntaxError: invalid syntax. Perhaps you forgot a comma?\n\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /content/dpt/main_ddp.py (575 lines total)]\n(71 more lines above)\n71:    #   Load data                                                                                     #\n72:    ###################################################################################################\n73:    # Can processing multiple datasets\n74:    train_datasets, val_datasets = [], {}\n75:    for dataset_name in args.datasets_names:\n76:        train_dataset = AutoTask.get(dataset_name).get(split=\"train\",\n77:        train_dataset = AutoTask.get(dataset_name).get(split=\"train\",\n78:                                    split_validation_test=True,\n79:                                    add_prefix=args.add_task_prefix,\n80:                                    add_vb=args.add_verbalizer,\n81:                                    file_prefix=args.local_file_prefix,\n82:                                    n_obs=100000 if dataset_name == 'yelp_polarity' else None)\n83:        train_datasets.append(train_dataset)\n84:\n85:        val_dataset = AutoTask.get(dataset_name).get(split=\"val\",\n86:                                    split_validation_test=True,\n87:                                    add_prefix=args.add_task_prefix,\n88:                                    add_vb=args.add_verbalizer,\n89:                                    file_prefix=args.local_file_prefix,\n90:                                    n_obs=None)\n91:        val_datasets.update({dataset_name: val_dataset})\n92:                                    add_prefix=args.add_task_prefix,\n93:                                    add_vb=args.add_verbalizer,\n94:                                    file_prefix=args.local_file_prefix,\n95:                                    n_obs=None)\n(480 more lines below)\n-------------------------------------------------\n\nThis is the original code before your edit\n-------------------------------------------------\n[File: /content/dpt/main_ddp.py (569 lines total)]\n(72 more lines above)\n72:    ###################################################################################################\n73:    # Can processing multiple datasets\n74:    train_datasets, val_datasets = [], {}\n75:    for dataset_name in args.datasets_names:\n76:        train_dataset = AutoTask.get(dataset_name).get(split=\"train\",\n77:                                    split_validation_test=True,\n78:                                    add_prefix=args.add_task_prefix,\n79:                                    add_vb=args.add_verbalizer,\n80:                                    file_prefix=args.local_file_prefix,\n81:                                    n_obs=100000 if dataset_name == 'yelp_polarity' else None)\n82:        train_datasets.append(train_dataset)\n83:\n84:        val_dataset = AutoTask.get(dataset_name).get(split=\"validation\",\n85:                                    split_validation_test=True,\n86:                                    add_prefix=args.add_task_prefix,\n87:                                    add_vb=args.add_verbalizer,\n88:                                    file_prefix=args.local_file_prefix,\n89:                                    n_obs=None)\n90:        val_datasets.update({dataset_name: val_dataset})\n(479 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 77:85\n",
    "        train_dataset = AutoTask.get(dataset_name).get(split=\"train\",\n",
    "                                    split_validation_test=True,\n",
    "                                    add_prefix=args.add_task_prefix,\n",
    "                                    add_vb=args.add_verbalizer,\n",
    "                                    file_prefix=args.local_file_prefix,\n",
    "                                    n_obs=100000 if dataset_name == 'yelp_polarity' else None)\n",
    "        train_datasets.append(train_dataset)\n",
    "\n",
    "        val_dataset = AutoTask.get(dataset_name).get(split=\"val\",\n",
    "                                    split_validation_test=True,\n",
    "                                    add_prefix=args.add_task_prefix,\n",
    "                                    add_vb=args.add_verbalizer,\n",
    "                                    file_prefix=args.local_file_prefix,\n",
    "                                    n_obs=None)\n",
    "        val_datasets.update({dataset_name: val_dataset})\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab1537c",
   "metadata": {},
   "source": [
    "### Step 19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64e9144",
   "metadata": {},
   "source": [
    "Execution Start time: 06:41:04, End time: 06:41:05 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3ba967",
   "metadata": {},
   "source": [
    "**Thought**: It seems I mistakenly duplicated a line during the edit. Let's correct this by properly updating the references to the \"validation\" split to \"val\" without duplicating any lines.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f8b00a",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de217e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/dpt/main_ddp.py (569 lines total)]\n(33 more lines above)\n34:from eval_utils import AutoPostProcessor\n35:from metrics import *\n36:from options import *\n37:from utils import *\n38:from loader import *\n39:\n40:import datasets\n41:from datasets import concatenate_datasets\n42:from datasets.utils.logging import set_verbosity_error\n43:from eval_utils import *\n44:set_verbosity_error()\n45:\n46:import logging\n47:logging.disable(logging.WARNING)\n48:\n49:import warnings\n50:warnings.filterwarnings(\"ignore\")\n51:\n52:\n53:def run(local_rank, args):\n54:\n55:    is_master = local_rank == 0\n56:    world_size = args.world_size\n57:    is_mp = world_size > 1\n58:    # set the device\n59:    device = 'cpu' if not torch.cuda.is_available() else 'cuda'\n60:\n61:    random.seed(args.seed)\n62:    np.random.seed(args.seed)\n63:    torch.manual_seed(args.seed)\n64:    torch.cuda.manual_seed_all(args.seed)\n65:    g = torch.Generator()\n66:    g.manual_seed(args.seed)\n67:\n68:    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n69:\n70:    ###################################################################################################\n71:    #   Load data                                                                                     #\n72:    ###################################################################################################\n73:    # Can processing multiple datasets\n74:    train_datasets, val_datasets = [], {}\n75:    for dataset_name in args.datasets_names:\n76:        train_dataset = AutoTask.get(dataset_name).get(split=\"train\",\n77:                                    split_validation_test=True,\n78:                                    add_prefix=args.add_task_prefix,\n79:                                    add_vb=args.add_verbalizer,\n80:                                    file_prefix=args.local_file_prefix,\n81:                                    n_obs=100000 if dataset_name == 'yelp_polarity' else None)\n82:        train_datasets.append(train_dataset)\n83:\n84:        val_dataset = AutoTask.get(dataset_name).get(split=\"val\",\n85:                                    split_validation_test=True,\n86:                                    add_prefix=args.add_task_prefix,\n87:                                    add_vb=args.add_verbalizer,\n88:                                    file_prefix=args.local_file_prefix,\n89:                                    n_obs=None)\n90:        val_datasets.update({dataset_name: val_dataset})\n91:\n92:        \n93:        if is_master:\n94:            print(local_rank, dataset_name, 'Train\\t', train_dataset[0])\n95:            print(local_rank, dataset_name, 'Val\\t', val_dataset[0])\n96:            \n97:    # merge all datasets if there are multiple ones\n98:    train_datasets = concatenate_datasets(train_datasets)\n99:\n100:    if is_master:\n101:        print(local_rank, len(train_dataset), len(val_dataset))\n102:        print('# all training samples:', len(train_datasets))\n103:        print(train_datasets[0])\n104:\n105:    # Data loader \n106:    # Creating the Training and Validation dataset for further creation of Dataloader\n107:    training_set = CustomT5Dataset(train_datasets, tokenizer, \n108:                                    args.max_source_length, \n109:                                    args.max_target_length, \n110:                                    args.datasets_names\n111:                                    )\n112:    \n113:\n114:    val_sets = {data_name: CustomT5Dataset(data_set, tokenizer, \n115:                                           args.max_source_length, \n116:                                           args.max_target_length, \n117:                                           args.datasets_names\n118:                                           )\n119:               for data_name, data_set in val_datasets.items()}\n120:\n121:    # Defining the parameters for creation of dataloaders\n122:    train_params = {\n123:        'batch_size': args.train_batch_size,\n124:        'shuffle': not is_mp,  # not shuffle in DDP\n125:        'num_workers': 4,\n126:        'worker_init_fn': seed_worker,\n127:        'generator': g,\n128:        }\n129:\n130:    eval_params = {\n131:        'batch_size': args.eval_batch_size,\n132:        'shuffle': False,\n133:        'num_workers': 0,\n(436 more lines below)\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 84:85\n",
    "        val_dataset = AutoTask.get(dataset_name).get(split=\"val\",\n",
    "                                    split_validation_test=True,\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7565490b",
   "metadata": {},
   "source": [
    "### Step 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b8eb8d",
   "metadata": {},
   "source": [
    "Execution Start time: 06:41:07, End time: 06:41:12 (duration: 5 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bd2f2a",
   "metadata": {},
   "source": [
    "**Thought**: The references to the \"validation\" split have been successfully updated to \"val\". Now, let's try running the `main_ddp.py` script again with the specified parameters to fine-tune the `t5-small` model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59fea1f",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c0ade8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stating time:  08/22/2024 06:41:11\nRaw Arguments:  Namespace(seed=42, from_checkpoint='', auto_resume=False, save_ckpt_dir='./saved_models', load_source_path='', saving_steps=1000, saving_each_epoch=True, latest_ckpt_name='latest_checkpoint.pt', n_epochs=1, train_batch_size=16, eval_batch_size=128, lr=0.3, max_grad_norm=1.0, weight_decay=1e-05, accumulate_steps=1, lr_scheduler_type='linear', num_warmup_steps=0.1, max_train_steps=0, min_training_epoch=20, early_stopping_patience=10, mode='train', close_tqdm=True, max_epochs_before_stop=2, model_name='t5-small', datasets=['mrpc'], max_source_length=256, max_target_length=128, prompt_type='param', is_dynamic_share=False, eval_in_train=True, enc_prompt_tokens=100, dec_prompt_tokens=0, sharing_ratio=1.0, share_seed=42, add_task_prefix=True, add_verbalizer=False, port=12355, gupids=None, sampling_method='uniform', size_limit=262144, stoch_task=False, bottle_neck=10, model_output_path='./saved_outputs', local_file_prefix='/gpfs/u/home/DPTV/DPTVhnwz/scratch/mrqa_datasets/datasets', datasets_names=['mrpc'])\nProcess ID:  313\nUse single gpu!\nTraceback (most recent call last):\n  File \"/content/dpt/main_ddp.py\", line 569, in <module>\n    main()\n  File \"/content/dpt/main_ddp.py\", line 562, in main\n    run(0, args)\n  File \"/content/dpt/main_ddp.py\", line 84, in run\n    val_dataset = AutoTask.get(dataset_name).get(split=\"val\",\n  File \"/content/dpt/data_utils.py\", line 124, in get\n    dataset = self.load_dataset(split=mapped_split)\n  File \"/content/dpt/data_utils.py\", line 291, in load_dataset\n    return datasets.load_dataset('csv', data_files={split: f'mrpc_csv/{split}.csv'})[split]\n  File \"/usr/local/lib/python3.10/site-packages/datasets/load.py\", line 2587, in load_dataset\n    builder_instance = load_dataset_builder(\n  File \"/usr/local/lib/python3.10/site-packages/datasets/load.py\", line 2259, in load_dataset_builder\n    dataset_module = dataset_module_factory(\n  File \"/usr/local/lib/python3.10/site-packages/datasets/load.py\", line 1799, in dataset_module_factory\n    ).get_module()\n  File \"/usr/local/lib/python3.10/site-packages/datasets/load.py\", line 1141, in get_module\n    data_files = DataFilesDict.from_patterns(\n  File \"/usr/local/lib/python3.10/site-packages/datasets/data_files.py\", line 715, in from_patterns\n    DataFilesList.from_patterns(\n  File \"/usr/local/lib/python3.10/site-packages/datasets/data_files.py\", line 620, in from_patterns\n    resolve_pattern(\n  File \"/usr/local/lib/python3.10/site-packages/datasets/data_files.py\", line 407, in resolve_pattern\n    raise FileNotFoundError(error_msg)\nFileNotFoundError: Unable to find '/content/dpt/mrpc_csv/validation.csv'\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python main_ddp.py --datasets=mrpc --model_name=t5-small --enc_prompt_tokens 100 -ts 16 -e 1 --bottle_neck 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874cabf9",
   "metadata": {},
   "source": [
    "### Step 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b613d16e",
   "metadata": {},
   "source": [
    "Execution Start time: 06:41:14, End time: 06:41:14 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b20fb88",
   "metadata": {},
   "source": [
    "**Thought**: It seems that the error persists, indicating that the script is still attempting to access a file named `validation.csv`. This suggests that there might be a hardcoded reference elsewhere in the code or a misconfiguration in how the dataset splits are being handled.\n",
    "\n",
    "Let's search the entire directory for any occurrences of the word \"validation\" to identify any other places where this might be referenced.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef19446d",
   "metadata": {},
   "source": [
    "**Action (search_dir)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df03d67b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Found 73 matches for \"validation\" in /content/dpt:\n/content/dpt/data_utils.py (68 matches)\n/content/dpt/main_ddp.py (3 matches)\n/content/dpt/metrics.py (2 matches)\nEnd of matches for \"validation\" in /content/dpt\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_dir \"validation\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62025e37",
   "metadata": {},
   "source": [
    "### Step 22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb99f55",
   "metadata": {},
   "source": [
    "Execution Start time: 06:41:16, End time: 06:41:16 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41192f0",
   "metadata": {},
   "source": [
    "**Thought**: The majority of the occurrences are in `data_utils.py`, which suggests that there might be additional logic related to dataset splits that we need to adjust. Let's open `data_utils.py` and investigate these occurrences to ensure that all references to \"validation\" are correctly mapped to \"val\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9753accf",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a03e947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/dpt/data_utils.py (986 lines total)]\n(284 more lines above)\n285:    metric_names = [\"accuracy\", \"f1\"]\n286:    split_to_data_split = {\"train\": \"train\",\n287:                           \"validation\": \"validation\",\n288:                           \"test\": \"validation\"}\n289:\n290:    def load_dataset(self, split):\n291:        return datasets.load_dataset('csv', data_files={split: f'mrpc_csv/{split}.csv'})[split]\n292:\n293:    def preprocessor(self, example, add_prefix=True, add_vb=False):\n294:        src_texts = [\"sentence1:\", example['sentence1'],\n295:                     \"sentence2:\", example[\"sentence2\"]]\n296:        tgt_texts = [str(example['label'])]\n297:        if add_vb:\n298:            verbalizer = \"{ 0 : not equivalent, 1 : equivalent }\"\n299:        else:\n300:            verbalizer = \"\"\n301:        return self.seq2seq_format(src_texts, tgt_texts, add_prefix, verbalizer=verbalizer)\n302:\n303:\n304:class COLA(AbstractTask):\n305:    name = \"cola\"\n306:    labels_list = [\"0\", \"1\"]\n307:    metric = [metrics.matthews_corrcoef]\n308:    metric_names = [\"matthews_correlation\"]\n309:    split_to_data_split = {\"train\": \"train\",\n310:                           \"validation\": \"validation\",\n311:                           \"test\": \"validation\"}\n312:\n313:    def load_dataset(self, split):\n314:        return datasets.load_dataset('glue', 'cola', split=split)\n315:\n316:    def preprocessor(self, example, add_prefix=True, add_vb=False):\t\n317:        src_texts = [\"sentence:\", example['sentence']]\t\n318:        tgt_texts = [str(example['label'])]\t\n319:        if add_vb:\t\n320:            verbalizer = \"{ 0 : grammatically unacceptable, 1 : grammatically acceptable }\"\t\n321:        else:\t\n322:            verbalizer = \"\"\t\n323:        return self.seq2seq_format(src_texts, tgt_texts, add_prefix, verbalizer=verbalizer)\n324:\n325:class SST2(AbstractTask):\n326:    name = \"sst2\"\n327:    labels_list = [\"0\", \"1\"]\n328:    metric = [metrics.accuracy]\n329:    metric_names = [\"accuracy\"]\n330:    split_to_data_split = {\"train\": \"train\",\n331:                           \"validation\": \"validation\",\n332:                           \"test\": \"validation\"}\n333:\n334:    def load_dataset(self, split):\n335:        return datasets.load_dataset('glue', 'sst2', split=split)\n336:\n337:    def preprocessor(self, example, add_prefix=True, add_vb=False):\n338:        src_texts = [\"sentence:\", example['sentence']]\n339:        tgt_texts = [str(example['label'])]\n340:        if add_vb:\n341:            verbalizer = \"{ 0 : negative, 1 : positive }\"\n342:        else:\n343:            verbalizer = \"\"\n344:        return self.seq2seq_format(src_texts, tgt_texts, add_prefix, verbalizer=verbalizer)\n345:\n346:\n347:class YelpPolarity(AbstractTask):\n348:    name = \"yelp_polarity\"\n349:    labels_list = [\"0\", \"1\"]\n350:    metric = [metrics.accuracy]\n351:    metric_names = [\"accuracy\"]\n352:    split_to_data_split = {\"train\": \"train\", \"test\": \"test\"}\n353:\n354:    split_to_file_name = {\n355:        'train': 'yelp_train',\n356:        'test': 'yelp_test'\n357:    }\n358:\n359:    def load_dataset(self, split):\n360:        return datasets.load_dataset('csv', data_files={\n361:            split: os.path.join(self.file_prefix, f'{self.split_to_file_name[split]}.csv')},\n362:            column_names=['label', 'text']\n363:            )[split]\n364:\n365:    # def load_dataset(self, split):\n366:    #     return datasets.load_dataset('yelp_polarity')[split]\n367:\n368:    def preprocessor(self, example, add_prefix=True,  add_vb=False):\n369:        src_texts = [\"sentence:\", example['text']]\n370:        tgt_texts = [str(example['label'] - 1)]  # labels from local files\n371:        if add_vb:\n372:            verbalizer = \"{ 0 : negative, 1 : positive }\"\n373:        else:\n374:            verbalizer = \"\"\n375:        return self.seq2seq_format(src_texts, tgt_texts, add_prefix, verbalizer=verbalizer)\n376:\n377:\n378:class Amazon_Polarity(AbstractTask):\n379:    name = \"amazon_polarity\"\n380:    labels_list = [\"0\", \"1\"]\n381:    metric = [metrics.accuracy]\n382:    metric_names = [\"accuracy\"]\n383:    split_to_data_split = {\"train\": \"train\", \"test\": \"test\"}\n384:\n(602 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open data_utils.py 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2343139f",
   "metadata": {},
   "source": [
    "### Step 23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2a7de0",
   "metadata": {},
   "source": [
    "Execution Start time: 06:41:18, End time: 06:41:19 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd5c620",
   "metadata": {},
   "source": [
    "**Thought**: The `split_to_data_split` dictionary in the `MRPC` class still contains a reference to \"validation\". Let's update this dictionary to correctly map \"validation\" to \"val\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c3d36d",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d33afa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/dpt/data_utils.py (986 lines total)]\n(235 more lines above)\n236:                           \"test\": \"validation\"}\n237:    split_to_file_name = {\n238:        'train': 'HotpotQA_train',\n239:        'validation': 'HotpotQA_dev'\n240:    }\n241:\n242:    def load_dataset(self, split):\n243:        return datasets.load_dataset('json', data_files={\n244:            split: os.path.join(self.file_prefix, f'{self.split_to_file_name[split]}.jsonl')\n245:        })[split]\n246:\n247:    def preprocessor(self, example, add_prefix, add_vb=False):\n248:        answer = pad_punctuation(example['answers']['text'][0])\n249:        question = pad_punctuation(example['question'])\n250:        context = pad_punctuation(example['context'])\n251:        source = [\"question:\", question,\n252:                  \"context:\", context]\n253:        target = [answer] if type(answer) == str else answer\n254:        return self.seq2seq_format(source, target, add_prefix)\n255:\n256:\n257:class SciTail(AbstractTask):\n258:    name = \"scitail\"\n259:    labels_list = [\"0\", \"1\"]\n260:    metric = [metrics.accuracy]\n261:    metric_names = [\"accuracy\"]\n262:    split_to_data_split = {\"train\": \"train\",\n263:                           \"validation\": \"validation\",\n264:                           \"test\": \"test\"}\n265:\n266:    def load_dataset(self, split):\n267:        return datasets.load_dataset('scitail', \"snli_format\", split=split)\n268:\n269:    def preprocessor(self, example, add_prefix=True, add_vb=False):\n270:        label2id = {\"entailment\": \"0\", \"neutral\": \"1\"}\n271:        src_texts = [\"sentence1:\", example['sentence1'],\n272:                     \"sentence2:\", example[\"sentence2\"]]\n273:        tgt_texts = [label2id[example[\"gold_label\"]]]\n274:        if add_vb:\n275:            verbalizer = \"{ 0 : entailment, 1 : neutral }\"\t\n276:        else:\n277:            verbalizer = \"\"\t\n278:        return self.seq2seq_format(src_texts, tgt_texts, add_prefix, verbalizer=verbalizer)\n279:\n280:\n281:class MRPC(AbstractTask):\n282:    name = \"mrpc\"\n283:    labels_list = [\"0\", \"1\"]\n284:    split_to_data_split = {\"train\": \"train\", \"validation\": \"val\", \"test\": \"test\"}\n285:    metric_names = [\"accuracy\", \"f1\"]\n286:    split_to_data_split = {\"train\": \"train\",\n287:                           \"validation\": \"val\",\n288:                           \"test\": \"test\"}\n289:\n290:    def load_dataset(self, split):\n291:        return datasets.load_dataset('csv', data_files={split: f'mrpc_csv/{split}.csv'})[split]\n292:\n293:    def preprocessor(self, example, add_prefix=True, add_vb=False):\n294:        src_texts = [\"sentence1:\", example['sentence1'],\n295:                     \"sentence2:\", example[\"sentence2\"]]\n296:        tgt_texts = [str(example['label'])]\n297:        if add_vb:\n298:            verbalizer = \"{ 0 : not equivalent, 1 : equivalent }\"\n299:        else:\n300:            verbalizer = \"\"\n301:        return self.seq2seq_format(src_texts, tgt_texts, add_prefix, verbalizer=verbalizer)\n302:\n303:\n304:class COLA(AbstractTask):\n305:    name = \"cola\"\n306:    labels_list = [\"0\", \"1\"]\n307:    metric = [metrics.matthews_corrcoef]\n308:    metric_names = [\"matthews_correlation\"]\n309:    split_to_data_split = {\"train\": \"train\",\n310:                           \"validation\": \"validation\",\n311:                           \"test\": \"validation\"}\n312:\n313:    def load_dataset(self, split):\n314:        return datasets.load_dataset('glue', 'cola', split=split)\n315:\n316:    def preprocessor(self, example, add_prefix=True, add_vb=False):\t\n317:        src_texts = [\"sentence:\", example['sentence']]\t\n318:        tgt_texts = [str(example['label'])]\t\n319:        if add_vb:\t\n320:            verbalizer = \"{ 0 : grammatically unacceptable, 1 : grammatically acceptable }\"\t\n321:        else:\t\n322:            verbalizer = \"\"\t\n323:        return self.seq2seq_format(src_texts, tgt_texts, add_prefix, verbalizer=verbalizer)\n324:\n325:class SST2(AbstractTask):\n326:    name = \"sst2\"\n327:    labels_list = [\"0\", \"1\"]\n328:    metric = [metrics.accuracy]\n329:    metric_names = [\"accuracy\"]\n330:    split_to_data_split = {\"train\": \"train\",\n331:                           \"validation\": \"validation\",\n332:                           \"test\": \"validation\"}\n333:\n334:    def load_dataset(self, split):\n335:        return datasets.load_dataset('glue', 'sst2', split=split)\n(651 more lines below)\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 286:288\n",
    "    split_to_data_split = {\"train\": \"train\",\n",
    "                           \"validation\": \"val\",\n",
    "                           \"test\": \"test\"}\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4352c06",
   "metadata": {},
   "source": [
    "### Step 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1885bc79",
   "metadata": {},
   "source": [
    "Execution Start time: 06:41:24, End time: 06:42:39 (duration: 74 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2342fcfb",
   "metadata": {},
   "source": [
    "**Thought**: The `split_to_data_split` dictionary in the `MRPC` class has been updated to correctly map \"validation\" to \"val\". Let's try running the `main_ddp.py` script again with the specified parameters to fine-tune the `t5-small` model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4e5866",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a50a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stating time:  08/22/2024 06:41:28\nRaw Arguments:  Namespace(seed=42, from_checkpoint='', auto_resume=False, save_ckpt_dir='./saved_models', load_source_path='', saving_steps=1000, saving_each_epoch=True, latest_ckpt_name='latest_checkpoint.pt', n_epochs=1, train_batch_size=16, eval_batch_size=128, lr=0.3, max_grad_norm=1.0, weight_decay=1e-05, accumulate_steps=1, lr_scheduler_type='linear', num_warmup_steps=0.1, max_train_steps=0, min_training_epoch=20, early_stopping_patience=10, mode='train', close_tqdm=True, max_epochs_before_stop=2, model_name='t5-small', datasets=['mrpc'], max_source_length=256, max_target_length=128, prompt_type='param', is_dynamic_share=False, eval_in_train=True, enc_prompt_tokens=100, dec_prompt_tokens=0, sharing_ratio=1.0, share_seed=42, add_task_prefix=True, add_verbalizer=False, port=12355, gupids=None, sampling_method='uniform', size_limit=262144, stoch_task=False, bottle_neck=10, model_output_path='./saved_outputs', local_file_prefix='/gpfs/u/home/DPTV/DPTVhnwz/scratch/mrqa_datasets/datasets', datasets_names=['mrpc'])\nProcess ID:  382\nUse single gpu!\n0 mrpc Train\t {'source': 'sentence1: The companies uniformly declined to give specific numbers on customer turnover , saying they will release those figures only when they report overall company performance at year-end . sentence2: The companies , however , declined to give specifics on customer turnover , saying they would release figures only when they report their overall company performance .', 'target': '1', 'task': 'mrpc'}\n0 mrpc Val\t {'source': 'sentence1: The additional contribution brings total U.S. food aid to North Korea this year to 100,000 tonnes . sentence2: The donation of 60,000 tons brings the total of U.S. contributions for the year to 100,000 .', 'target': '1', 'task': 'mrpc'}\n0 10 10\n# all training samples: 10\n{'source': 'sentence1: The companies uniformly declined to give specific numbers on customer turnover , saying they will release those figures only when they report overall company performance at year-end . sentence2: The companies , however , declined to give specifics on customer turnover , saying they would release figures only when they report their overall company performance .', 'target': '1', 'task': 'mrpc'}\n\nconfig.json:   0%|                                  | 0.00/1.21k [00:00<?, ?B/s]\n\n\nmodel.safetensors:   0%|                             | 0.00/242M [00:00<?, ?B/s]\n\n\n\n\n\n\n\ngeneration_config.json:   0%|                         | 0.00/147 [00:00<?, ?B/s]\ngeneration_config.json: 100%|███████████████████| 147/147 [00:00<00:00, 701kB/s]\nParameters to optimize:  ['encoder.prefix_first', 'encoder.prefix_second']\nPrefix Parameters:  ['encoder.prefix_first', 'encoder.prefix_second']\nTrainable Parameters:  ['encoder.prefix_first', 'encoder.prefix_second']\n#Trainable Parameters:  6120\nAll trainable parameters: 6120; per task: 6120.0\n\n***** running training *****\n| batch_size: 16 | num_epochs: 1 | num_train: 1 |\nSaved soft prompts at: ./saved_models/mrpc.soft_prompts.epoch.0.pt\nTraceback (most recent call last):\n  File \"/content/dpt/main_ddp.py\", line 569, in <module>\n    main()\n  File \"/content/dpt/main_ddp.py\", line 562, in main\n    run(0, args)\n  File \"/content/dpt/main_ddp.py\", line 381, in run\n    res = task_evaluation(args, ValDataloaders, model, tokenizer, device, output_path)\n  File \"/content/dpt/main_ddp.py\", line 529, in task_evaluation\n    for i, metric in enumerate(AutoTask.get(dataset_name).metric):\nTypeError: 'NotImplementedType' object is not iterable\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python main_ddp.py --datasets=mrpc --model_name=t5-small --enc_prompt_tokens 100 -ts 16 -e 1 --bottle_neck 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49105f19",
   "metadata": {},
   "source": [
    "### Step 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76b83b4",
   "metadata": {},
   "source": [
    "Execution Start time: 06:42:41, End time: 06:42:41 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44543e5a",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that there is an issue with the evaluation metrics for the MRPC task. The `AutoTask.get(dataset_name).metric` is returning a `NotImplementedType`, which suggests that the metrics for the MRPC task might not be properly defined or implemented.\n",
    "\n",
    "Let's open the `metrics.py` file to investigate how metrics are defined and ensure that the MRPC task has the correct metrics implemented.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902e5537",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc550ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/dpt/metrics.py (301 lines total)]\n1:# several of the evaluation metrics are from https://github.com/google-research/text-to-text-transfer-transformer/blob/a1352e625db7ec114062f99d99b0565b9e45c155/t5/evaluation/metrics.py\n2:\"\"\"Defines different metrics used for evaluation of tasks.\"\"\"\n3:import numpy as np\n4:import math\n5:import scipy\n6:import sklearn\n7:import collections\n8:import sklearn.metrics\n9:from logging import getLogger\n10:\n11:import string\t\n12:import regex as re\n13:\n14:from utils import string_to_float\n15:\n16:logger = getLogger(__name__)\n17:\n18:\n19:TASK_VAL_REFERENCES = {}\n20:\n21:TASK_TEST_REFERENCES = {}\n22:\n23:def average_multi_task(res_dict):\n24:  return np.mean(sum([[vv for kk, vv in v.items()] for k, v in res_dict.items()], []))\n25:\n26:\n27:def multi_task_gain(res_dict, ref_dict):\n28:  res = []\n29:  for task in res_dict:\n30:    temp = []\n31:    if task not in ref_dict:\n32:      metrics = sum([list(res_dict[task].values()) for task in res_dict], [])\n33:      return np.mean(metrics)\n34:      \n35:    for metric in res_dict[task]:\n36:      temp.append((res_dict[task][metric] - ref_dict[task][metric]) / ref_dict[task][metric])\n37:\n38:    res.append(np.mean(temp))\n39:  return np.mean(res)\n40:\n41:\n42:def accuracy(predictions, targets) -> dict:\n43:    \"\"\"Computes the average accuracy.\"\"\"\n44:    return {\"accuracy\": 100 * ((np.array(predictions) == np.array(targets)).mean())}\n45:\n46:def pearson_corrcoef(predictions, targets) -> dict:\n47:    \"\"\"Computes Pearson correlation coefficient.\"\"\"\n48:    targets = [string_to_float(target) for target in targets]\n49:    predictions= [string_to_float(prediction) for prediction in predictions]\n50:    pearson_corrcoef = 100 * scipy.stats.pearsonr(targets, predictions)[0]\n51:\n52:    # Note that if all the predictions will be the same, spearman\n53:    # correlation is nan, to gaurad against this, we check the output\n54:    # and return 0 in this case.\n55:    if math.isnan(pearson_corrcoef):\n56:        pearson_corrcoef = 0\n57:    return {\"pearson\": pearson_corrcoef}\n58:\n59:ROUGE_KEYS = [\"rouge1\", \"rouge2\", \"rougeL\"]\n60:\n61:\n62:def calculate_rouge(output_lns, reference_lns, use_stemmer=True):\n63:    scorer = rouge_scorer.RougeScorer(ROUGE_KEYS, use_stemmer=use_stemmer)\n64:    aggregator = scoring.BootstrapAggregator()\n65:\n66:    for reference_ln, output_ln in zip(reference_lns, output_lns):\n67:        scores = scorer.score(reference_ln, output_ln)\n68:        aggregator.add_scores(scores)\n69:\n70:    result = aggregator.aggregate()\n71:    return {k: round(v.mid.fmeasure * 100, 4) for k, v in result.items()}\n72:\n73:def spearman_corrcoef(predictions, targets) -> dict:\n74:    \"\"\"Computes Spearman correlation coefficient.\"\"\"\n75:    # TODO: we need to do postprocessors in a clean way for each dataset.\n76:    targets = [string_to_float(target) for target in targets]\n77:    predictions= [string_to_float(prediction) for prediction in predictions]\n78:    spearman_corrcoef = 100 * scipy.stats.spearmanr(targets, predictions)[0]\n79:\n80:    # Note that if all the predictions will be the same, spearman\n81:    # correlation is nan, to gaurad against this, we check the output\n82:    # and return 0 in this case.\n83:    if math.isnan(spearman_corrcoef):\n84:        spearman_corrcoef = 0\n85:    return {\"spearmanr\": spearman_corrcoef}\n86:\n87:\n88:def f1_score_with_invalid(predictions, targets) -> dict:\n89:    \"\"\"Computes F1 score,  with any prediction != 0 or 1 is counted as incorrect.\n90:    Args:\n91:      targets: list of targets, either 0 or 1\n92:      predictions: list of predictions, any integer value\n93:    Returns:\n94:      F1 score, where any prediction != 0 or 1 is counted as wrong.\n95:    \"\"\"\n96:    def binary_reverse(labels):\n97:       return ['0' if label == '1' else '1' for label in labels]\n98:    targets, predictions = np.asarray(targets), np.asarray(predictions)\n99:    # Get indices of invalid predictions.\n100:    invalid_idx_mask = np.logical_and(predictions != '0', predictions != '1')\n(201 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open metrics.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2283669",
   "metadata": {},
   "source": [
    "### Step 26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7722ada8",
   "metadata": {},
   "source": [
    "Execution Start time: 06:42:43, End time: 06:42:44 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0179b268",
   "metadata": {},
   "source": [
    "**Thought**: The `metrics.py` file defines several evaluation metrics, including accuracy and F1 score, which are commonly used for classification tasks like MRPC. It seems that the MRPC task might not be correctly linked to these metrics.\n",
    "\n",
    "Let's search for the `MRPC` class in `data_utils.py` to ensure that it is correctly associated with the appropriate metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bdef1e",
   "metadata": {},
   "source": [
    "**Action (search_file)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e5d1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Found 2 matches for \"MRPC\" in /content/dpt/data_utils.py:\nLine 281:class MRPC(AbstractTask):\nLine 941:        ('mrpc', MRPC),\nEnd of matches for \"MRPC\" in /content/dpt/data_utils.py\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_file \"MRPC\" data_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5178fefd",
   "metadata": {},
   "source": [
    "### Step 27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56547e9f",
   "metadata": {},
   "source": [
    "Execution Start time: 06:42:45, End time: 06:42:45 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1a27a5",
   "metadata": {},
   "source": [
    "**Thought**: The `MRPC` class is defined on line 281 in `data_utils.py`, and it is associated with the `AutoTask` on line 941. Let's go back to the `MRPC` class definition to ensure that it is correctly linked to the appropriate metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98508e2",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cf7c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/dpt/data_utils.py (986 lines total)]\n(265 more lines above)\n266:    def load_dataset(self, split):\n267:        return datasets.load_dataset('scitail', \"snli_format\", split=split)\n268:\n269:    def preprocessor(self, example, add_prefix=True, add_vb=False):\n270:        label2id = {\"entailment\": \"0\", \"neutral\": \"1\"}\n271:        src_texts = [\"sentence1:\", example['sentence1'],\n272:                     \"sentence2:\", example[\"sentence2\"]]\n273:        tgt_texts = [label2id[example[\"gold_label\"]]]\n274:        if add_vb:\n275:            verbalizer = \"{ 0 : entailment, 1 : neutral }\"\t\n276:        else:\n277:            verbalizer = \"\"\t\n278:        return self.seq2seq_format(src_texts, tgt_texts, add_prefix, verbalizer=verbalizer)\n279:\n280:\n281:class MRPC(AbstractTask):\n282:    name = \"mrpc\"\n283:    labels_list = [\"0\", \"1\"]\n284:    split_to_data_split = {\"train\": \"train\", \"validation\": \"val\", \"test\": \"test\"}\n285:    metric_names = [\"accuracy\", \"f1\"]\n286:    split_to_data_split = {\"train\": \"train\",\n287:                           \"validation\": \"val\",\n288:                           \"test\": \"test\"}\n289:\n290:    def load_dataset(self, split):\n291:        return datasets.load_dataset('csv', data_files={split: f'mrpc_csv/{split}.csv'})[split]\n292:\n293:    def preprocessor(self, example, add_prefix=True, add_vb=False):\n294:        src_texts = [\"sentence1:\", example['sentence1'],\n295:                     \"sentence2:\", example[\"sentence2\"]]\n296:        tgt_texts = [str(example['label'])]\n297:        if add_vb:\n298:            verbalizer = \"{ 0 : not equivalent, 1 : equivalent }\"\n299:        else:\n300:            verbalizer = \"\"\n301:        return self.seq2seq_format(src_texts, tgt_texts, add_prefix, verbalizer=verbalizer)\n302:\n303:\n304:class COLA(AbstractTask):\n305:    name = \"cola\"\n306:    labels_list = [\"0\", \"1\"]\n307:    metric = [metrics.matthews_corrcoef]\n308:    metric_names = [\"matthews_correlation\"]\n309:    split_to_data_split = {\"train\": \"train\",\n310:                           \"validation\": \"validation\",\n311:                           \"test\": \"validation\"}\n312:\n313:    def load_dataset(self, split):\n314:        return datasets.load_dataset('glue', 'cola', split=split)\n315:\n316:    def preprocessor(self, example, add_prefix=True, add_vb=False):\t\n317:        src_texts = [\"sentence:\", example['sentence']]\t\n318:        tgt_texts = [str(example['label'])]\t\n319:        if add_vb:\t\n320:            verbalizer = \"{ 0 : grammatically unacceptable, 1 : grammatically acceptable }\"\t\n321:        else:\t\n322:            verbalizer = \"\"\t\n323:        return self.seq2seq_format(src_texts, tgt_texts, add_prefix, verbalizer=verbalizer)\n324:\n325:class SST2(AbstractTask):\n326:    name = \"sst2\"\n327:    labels_list = [\"0\", \"1\"]\n328:    metric = [metrics.accuracy]\n329:    metric_names = [\"accuracy\"]\n330:    split_to_data_split = {\"train\": \"train\",\n331:                           \"validation\": \"validation\",\n332:                           \"test\": \"validation\"}\n333:\n334:    def load_dataset(self, split):\n335:        return datasets.load_dataset('glue', 'sst2', split=split)\n336:\n337:    def preprocessor(self, example, add_prefix=True, add_vb=False):\n338:        src_texts = [\"sentence:\", example['sentence']]\n339:        tgt_texts = [str(example['label'])]\n340:        if add_vb:\n341:            verbalizer = \"{ 0 : negative, 1 : positive }\"\n342:        else:\n343:            verbalizer = \"\"\n344:        return self.seq2seq_format(src_texts, tgt_texts, add_prefix, verbalizer=verbalizer)\n345:\n346:\n347:class YelpPolarity(AbstractTask):\n348:    name = \"yelp_polarity\"\n349:    labels_list = [\"0\", \"1\"]\n350:    metric = [metrics.accuracy]\n351:    metric_names = [\"accuracy\"]\n352:    split_to_data_split = {\"train\": \"train\", \"test\": \"test\"}\n353:\n354:    split_to_file_name = {\n355:        'train': 'yelp_train',\n356:        'test': 'yelp_test'\n357:    }\n358:\n359:    def load_dataset(self, split):\n360:        return datasets.load_dataset('csv', data_files={\n361:            split: os.path.join(self.file_prefix, f'{self.split_to_file_name[split]}.csv')},\n362:            column_names=['label', 'text']\n363:            )[split]\n364:\n365:    # def load_dataset(self, split):\n(621 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open data_utils.py 281"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aba058",
   "metadata": {},
   "source": [
    "### Step 28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3a2599",
   "metadata": {},
   "source": [
    "Execution Start time: 06:42:47, End time: 06:42:48 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af929c84",
   "metadata": {},
   "source": [
    "**Thought**: The `MRPC` class has a `metric_names` attribute but lacks a `metric` attribute, which is likely causing the issue. We need to add the `metric` attribute to the `MRPC` class and link it to the appropriate metrics, such as accuracy and F1 score.\n",
    "\n",
    "Let's add the `metric` attribute to the `MRPC` class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8e9705",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11034dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/dpt/data_utils.py (986 lines total)]\n(234 more lines above)\n235:                           \"validation\": \"validation\",\n236:                           \"test\": \"validation\"}\n237:    split_to_file_name = {\n238:        'train': 'HotpotQA_train',\n239:        'validation': 'HotpotQA_dev'\n240:    }\n241:\n242:    def load_dataset(self, split):\n243:        return datasets.load_dataset('json', data_files={\n244:            split: os.path.join(self.file_prefix, f'{self.split_to_file_name[split]}.jsonl')\n245:        })[split]\n246:\n247:    def preprocessor(self, example, add_prefix, add_vb=False):\n248:        answer = pad_punctuation(example['answers']['text'][0])\n249:        question = pad_punctuation(example['question'])\n250:        context = pad_punctuation(example['context'])\n251:        source = [\"question:\", question,\n252:                  \"context:\", context]\n253:        target = [answer] if type(answer) == str else answer\n254:        return self.seq2seq_format(source, target, add_prefix)\n255:\n256:\n257:class SciTail(AbstractTask):\n258:    name = \"scitail\"\n259:    labels_list = [\"0\", \"1\"]\n260:    metric = [metrics.accuracy]\n261:    metric_names = [\"accuracy\"]\n262:    split_to_data_split = {\"train\": \"train\",\n263:                           \"validation\": \"validation\",\n264:                           \"test\": \"test\"}\n265:\n266:    def load_dataset(self, split):\n267:        return datasets.load_dataset('scitail', \"snli_format\", split=split)\n268:\n269:    def preprocessor(self, example, add_prefix=True, add_vb=False):\n270:        label2id = {\"entailment\": \"0\", \"neutral\": \"1\"}\n271:        src_texts = [\"sentence1:\", example['sentence1'],\n272:                     \"sentence2:\", example[\"sentence2\"]]\n273:        tgt_texts = [label2id[example[\"gold_label\"]]]\n274:        if add_vb:\n275:            verbalizer = \"{ 0 : entailment, 1 : neutral }\"\t\n276:        else:\n277:            verbalizer = \"\"\t\n278:        return self.seq2seq_format(src_texts, tgt_texts, add_prefix, verbalizer=verbalizer)\n279:\n280:\n281:class MRPC(AbstractTask):\n282:    name = \"mrpc\"\n283:    labels_list = [\"0\", \"1\"]\n284:    split_to_data_split = {\"train\": \"train\", \"validation\": \"val\", \"test\": \"test\"}\n285:    metric = [metrics.accuracy, metrics.f1_score_with_invalid]\n286:    split_to_data_split = {\"train\": \"train\",\n287:                           \"validation\": \"val\",\n288:                           \"test\": \"test\"}\n289:\n290:    def load_dataset(self, split):\n291:        return datasets.load_dataset('csv', data_files={split: f'mrpc_csv/{split}.csv'})[split]\n292:\n293:    def preprocessor(self, example, add_prefix=True, add_vb=False):\n294:        src_texts = [\"sentence1:\", example['sentence1'],\n295:                     \"sentence2:\", example[\"sentence2\"]]\n296:        tgt_texts = [str(example['label'])]\n297:        if add_vb:\n298:            verbalizer = \"{ 0 : not equivalent, 1 : equivalent }\"\n299:        else:\n300:            verbalizer = \"\"\n301:        return self.seq2seq_format(src_texts, tgt_texts, add_prefix, verbalizer=verbalizer)\n302:\n303:\n304:class COLA(AbstractTask):\n305:    name = \"cola\"\n306:    labels_list = [\"0\", \"1\"]\n307:    metric = [metrics.matthews_corrcoef]\n308:    metric_names = [\"matthews_correlation\"]\n309:    split_to_data_split = {\"train\": \"train\",\n310:                           \"validation\": \"validation\",\n311:                           \"test\": \"validation\"}\n312:\n313:    def load_dataset(self, split):\n314:        return datasets.load_dataset('glue', 'cola', split=split)\n315:\n316:    def preprocessor(self, example, add_prefix=True, add_vb=False):\t\n317:        src_texts = [\"sentence:\", example['sentence']]\t\n318:        tgt_texts = [str(example['label'])]\t\n319:        if add_vb:\t\n320:            verbalizer = \"{ 0 : grammatically unacceptable, 1 : grammatically acceptable }\"\t\n321:        else:\t\n322:            verbalizer = \"\"\t\n323:        return self.seq2seq_format(src_texts, tgt_texts, add_prefix, verbalizer=verbalizer)\n324:\n325:class SST2(AbstractTask):\n326:    name = \"sst2\"\n327:    labels_list = [\"0\", \"1\"]\n328:    metric = [metrics.accuracy]\n329:    metric_names = [\"accuracy\"]\n330:    split_to_data_split = {\"train\": \"train\",\n331:                           \"validation\": \"validation\",\n332:                           \"test\": \"validation\"}\n333:\n334:    def load_dataset(self, split):\n(652 more lines below)\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 285:285\n",
    "    metric = [metrics.accuracy, metrics.f1_score_with_invalid]\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0266989",
   "metadata": {},
   "source": [
    "### Step 29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaf61fc",
   "metadata": {},
   "source": [
    "Execution Start time: 06:42:50, End time: 06:44:06 (duration: 75 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904dedc5",
   "metadata": {},
   "source": [
    "**Thought**: The `MRPC` class now has the `metric` attribute correctly set to use accuracy and F1 score with invalid metrics. Let's try running the `main_ddp.py` script again with the specified parameters to fine-tune the `t5-small` model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0f1eca",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc6c068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stating time:  08/22/2024 06:42:55\nRaw Arguments:  Namespace(seed=42, from_checkpoint='', auto_resume=False, save_ckpt_dir='./saved_models', load_source_path='', saving_steps=1000, saving_each_epoch=True, latest_ckpt_name='latest_checkpoint.pt', n_epochs=1, train_batch_size=16, eval_batch_size=128, lr=0.3, max_grad_norm=1.0, weight_decay=1e-05, accumulate_steps=1, lr_scheduler_type='linear', num_warmup_steps=0.1, max_train_steps=0, min_training_epoch=20, early_stopping_patience=10, mode='train', close_tqdm=True, max_epochs_before_stop=2, model_name='t5-small', datasets=['mrpc'], max_source_length=256, max_target_length=128, prompt_type='param', is_dynamic_share=False, eval_in_train=True, enc_prompt_tokens=100, dec_prompt_tokens=0, sharing_ratio=1.0, share_seed=42, add_task_prefix=True, add_verbalizer=False, port=12355, gupids=None, sampling_method='uniform', size_limit=262144, stoch_task=False, bottle_neck=10, model_output_path='./saved_outputs', local_file_prefix='/gpfs/u/home/DPTV/DPTVhnwz/scratch/mrqa_datasets/datasets', datasets_names=['mrpc'])\nProcess ID:  496\nUse single gpu!\n0 mrpc Train\t {'source': 'sentence1: The companies uniformly declined to give specific numbers on customer turnover , saying they will release those figures only when they report overall company performance at year-end . sentence2: The companies , however , declined to give specifics on customer turnover , saying they would release figures only when they report their overall company performance .', 'target': '1', 'task': 'mrpc'}\n0 mrpc Val\t {'source': 'sentence1: The additional contribution brings total U.S. food aid to North Korea this year to 100,000 tonnes . sentence2: The donation of 60,000 tons brings the total of U.S. contributions for the year to 100,000 .', 'target': '1', 'task': 'mrpc'}\n0 10 10\n# all training samples: 10\n{'source': 'sentence1: The companies uniformly declined to give specific numbers on customer turnover , saying they will release those figures only when they report overall company performance at year-end . sentence2: The companies , however , declined to give specifics on customer turnover , saying they would release figures only when they report their overall company performance .', 'target': '1', 'task': 'mrpc'}\nParameters to optimize:  ['encoder.prefix_first', 'encoder.prefix_second']\nPrefix Parameters:  ['encoder.prefix_first', 'encoder.prefix_second']\nTrainable Parameters:  ['encoder.prefix_first', 'encoder.prefix_second']\n#Trainable Parameters:  6120\nAll trainable parameters: 6120; per task: 6120.0\n\n***** running training *****\n| batch_size: 16 | num_epochs: 1 | num_train: 1 |\nSaved soft prompts at: ./saved_models/mrpc.soft_prompts.epoch.0.pt\nEpoch 0 - Validation:  {'mrpc': {'accuracy': 0.0, 'f1': 0.0}}\nEpoch Best Val: 0.0 at Epoch 0.\n***** training ends *****\n\nbest dev acc: 0.00000 (at epoch 0)\nbest dev acc: -inf (at step 0)\n\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python main_ddp.py --datasets=mrpc --model_name=t5-small --enc_prompt_tokens 100 -ts 16 -e 1 --bottle_neck 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf500f8",
   "metadata": {},
   "source": [
    "### Step 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e22260",
   "metadata": {},
   "source": [
    "Execution Start time: 06:44:07, End time: 06:44:08 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c1c84e",
   "metadata": {},
   "source": [
    "**Thought**: The script has successfully run, but the resulting accuracy on the validation set is 0.0. This might be due to the small dataset size or other factors. However, we can still submit the result as requested.\n",
    "\n",
    "Let's submit the accuracy result in the required JSON format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee19714",
   "metadata": {},
   "source": [
    "**Action (submit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557abb89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{accuracy: 0.0}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit {\"accuracy\": 0.0}"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
