{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4605aa5b",
   "metadata": {},
   "source": [
    "### Query\n",
    "Apply the decomposed prompt tuning method to fine-tune the t5-small model on our sentence pair classification dataset, available at https://drive.google.com/file/d/1raGLX-2vdX1uizYx2DdqZ_JzAkVeyeDF/view?usp=drive_link. Report the resulting metrics using the `main_ddp.py` script. Report accuracy on the development set as a json structured as follows: {\"accuracy\": 0.0} (replace 0.0 with the actual value).\n",
    "\n",
    "Additional instructions:\n",
    "1. Run for only 1 epoch.\n",
    "2. Use the following hyperparameters: --enc_prompt_tokens 100 -ts 16 -e 1 --bottle_neck 10\n",
    "\n",
    "Git repository: https://github.com/xyaoooo/dpt\n",
    "\n",
    "I have already executed some of the required steps. Now, you should make the necessary changes to make sure the code runs on a CPU. Your goal is to successfully run\n",
    "```\n",
    "!python main_ddp.py --datasets=mrpc  --model_name=t5-small --enc_prompt_tokens 100 -ts 16 -e 1 --bottle_neck 10\n",
    "```\n",
    "and submit the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb26867",
   "metadata": {},
   "source": [
    "### Prompt\n",
    "<details>\n",
    "[{'role': 'system', 'content': 'Interact in a jupyter notebook to solve the request.\\nOutput a thought and an action.\\nThought can reason about the current situation, and action allows you to interact with the Jupyter notebook. There are three types of actions:\\n(1) execute, which executes a notebook cell with the command under the \"content\" field. This could be a bash command (beginning with !), a python command, or magic command (beginning with %).\\n(2) edit, which allows you to replace existing lines in a file with new lines. To replace lines, you will need to provide the lines as they appear before the replacement, and the lines to replace them with.\\nIn the \"content\" field, format should be ```{filename}\\n<<<BEFORE_EDIT>>>\\nlines before edit\\nlines before edit\\n<<<AFTER_EDIT>>>\\nlines after edit\\nlines after edit```\\n  - Include the entire lines WITH indentation. After editing, make sure indentation is correct.\\n(3) submit, which submits the final answer (or null if there\\'s no answer to submit), formatted as JSON (make sure to use double quotes for keys and values).\\nAlways output exactly one \"thought\" and one \"action\", and encapsulate the content with <content>...</content> tags.\\nI will provide the cell output (observation) for you.\\n\\nTips:\\n- When a github repository is given, you should start by cloning it. You can read the readme file and browse the repo for installation and usage instructions.\\n- Avoid using conda, instead use pip for package management.\\n- Each execution has a 5-minute timeout. After this period, you can decide whether to interrupt the kernel or continue waiting.\\n  You should interrupt if: (1) you are prompted with an interactive message (e.g., \"Do you want to continue? [Y/n]\"). In that case, you can bypass it by executing echo \"y\" | command, or (2) the execution is taking longer than expected because the wrong task is being executed, such as a different model or incorrect number of epochs.\\n  You should continue to wait if the current execution is correct and proceeding as expected.\\n\\nHere are some examples.\\nGit repository: EleutherAI/lm-evaluation-harness\\nQuery: Evaluate EleutherAI/gpt-j-6B on hellaswag. What\\'s the score?\\nThought: First, I need to clone the GitHub repository \\'EleutherAI/lm-evaluation-harness\\'.\\nAction: execute\\n<content>\\n!git clone https://github.com/EleutherAI/lm-evaluation-harness.git\\n</content>\\nObservation: ```\\nCloning into \\'lm-evaluation-harness\\'...\\n```\\nThought: The repository has been cloned. Next, I should check the README file for installation and usage instructions to proceed with evaluating EleutherAI/gpt-j-6B on hellaswag.\\nAction: execute\\n<content>\\n!cat lm-evaluation-harness/README.md\\n</content>\\nObservation: ```# Language Model Evaluation Harness ... truncated ...\\n```\\nThought: The README provides detailed instructions for installing the `lm-eval` package and evaluating models like GPT-J-6B on tasks like hellaswag. I need to install the package and its dependencies first.\\nAction: execute\\n<content>\\n!pip install -e lm-evaluation-harness/.\\n</content>\\nObservation: ```\\nObtaining file:///home/nora/lm-evaluation-harness\\n  Installing build dependencies ... truncated ...\\n```\\nThought: The `lm-eval` package and its dependencies have been installed successfully. Now, I can proceed with evaluating the EleutherAI/gpt-j-6B model on the hellaswag task as per the instructions in the README.\\nAction: execute\\n<content>\\n!lm_eval --model hf --model_args pretrained=EleutherAI/gpt-j-6B --tasks hellaswag --device cuda:0 --batch_size 8\\n</content>\\nObservation: ```\\n{\\n  \"hellaswag\": {\\n    \"acc\": 0.24603658536585363,\\n    \"acc_norm\": 0.34436059706446426,\\n  },  ... truncated ...\\n```\\nThought: The `lm-eval` was executed successfully, providing the metrics I needed. The `acc` metric is 0.2460. The `acc_norm` metric is 0.3444\\nAction: submit\\n<content>\\n{\"acc\": 0.2460, \"acc_norm\": 0.3444}\\n</content>\\n\\nQuery: use pandas to read the file \\'data.csv\\' and display the first 2 rows.\\nThought: First, I need to read the file \\'data.csv\\' using the pandas library.\\nAction: execute\\n<content>\\nimport pandas as pd\\npd.read_csv(\\'data.csv\\').head()\\n</content>\\nObservation: ```\\n    name\\n  0  Alice\\n   1    Bob\\n  2  Charlie\\n```\\nThought: The file \\'data.csv\\' has been read successfully and the first 2 rows have been displayed.\\nAction: submit\\n<content>\\n[\"Alice\", \"Bob\"]\\n</content>\\n\\nQuery: Change the model name in the \\'config.json\\' file from \\'roberta-base\\' to \\'roberta-large\\'.\\nThought: First, I need to read the \\'config.json\\' file to find the location where I can replace the model name.\\nAction: execute\\n<content>\\n!cat config.json\\n</content>\\nObservation: ```\\n{\\n    \"model\": \"roberta-base\",\\n    \"batch_size\": 8,\\n    \"epochs\": 10\\n}\\n```\\nThought: The file has been read successfully. Now, I can replace the model name in the \\'config.json\\' file.\\nAction: edit\\n<content>\\nconfig.json\\n<<<BEFORE_EDIT>>>\\n{\\n    \"model\": \"roberta-base\",\\n    \"batch_size\": 8,\\n<<<AFTER_EDIT>>>\\n{\\n    \"model\": \"roberta-large\",\\n    \"batch_size\": 8,\\n</content>\\nObservation: \"editing file config.json\"\\nThought: The model name has been replaced in the \\'config.json\\' file.\\nAction: submit\\n<content>\\nnull\\n</content>\\n'}, {'role': 'user', 'content': 'Query: Apply the decomposed prompt tuning method to fine-tune the t5-small model on our sentence pair classification dataset, available at https://drive.google.com/file/d/1raGLX-2vdX1uizYx2DdqZ_JzAkVeyeDF/view?usp=drive_link. Report the resulting metrics using the `main_ddp.py` script. Report accuracy on the development set as a json structured as follows: {\"accuracy\": 0.0} (replace 0.0 with the actual value).\\n\\nAdditional instructions:\\n1. Run for only 1 epoch.\\n2. Use the following hyperparameters: --enc_prompt_tokens 100 -ts 16 -e 1 --bottle_neck 10\\n\\nGit repository: https://github.com/xyaoooo/dpt\\n\\nI have already executed some of the required steps. Now, you should make the necessary changes to make sure the code runs on a CPU. Your goal is to successfully run\\n```\\n!python main_ddp.py --datasets=mrpc  --model_name=t5-small --enc_prompt_tokens 100 -ts 16 -e 1 --bottle_neck 10\\n```\\nand submit the answer.'}]\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9cf0b9",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba47fcd9",
   "metadata": {},
   "source": [
    "Execution Start time: 00:45:21, End time: 00:45:22 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd911199",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87987f41",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c183b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cloning into 'dpt'...\nremote: Enumerating objects: 45, done.\u001b[K\nremote: Counting objects: 100% (45/45)\u001b[K\nremote: Counting objects: 100% (45/45), done.\u001b[K\nremote: Compressing objects: 100% (41/41)\u001b[K\nremote: Compressing objects: 100% (41/41), done.\u001b[K\nremote: Total 45 (delta 18), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (45/45)\nReceiving objects: 100% (45/45), 39.40 KiB | 1.41 MiB/s, done.\nResolving deltas: 100% (18/18)\nResolving deltas: 100% (18/18), done.\n/content/dpt\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!git clone https://github.com/xyaoooo/dpt\n",
    "%cd dpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfe4fb2",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3690a753",
   "metadata": {},
   "source": [
    "Execution Start time: 00:45:22, End time: 00:45:22 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad389df",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985f62ab",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4f0c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n    name = \"mrpc\"\n    labels_list = [\"0\", \"1\"]\n    metric = [metrics.accuracy, metrics.f1_score_with_invalid]\n    metric_names = [\"accuracy\", \"f1\"]\n    split_to_data_split = {\"train\": \"train\",\n                           \"validation\": \"validation\",\n                           \"test\": \"validation\"}\n\n    def load_dataset(self, split):\n        # return datasets.load_dataset('glue', 'mrpc', split=split, script_version=\"master\")\n        #return datasets.load_dataset('glue', 'mrpc', split=split)\n        files = {\n        'train': 'mrpc_csv/train.csv',\n        'validation': 'mrpc_csv/val.csv',\n        'test': 'mrpc_csv/test.csv'\n        }\n\n        # Load the dataset\n        return datasets.load_dataset('csv', data_files=files, split=split)\n\n    def preprocessor(self, example, add_prefix=True, add_vb=False):\n        src_texts = [\"sentence1:\", example['sentence1'],\n                     \"sentence2:\", example[\"sentence2\"]]\n        tgt_texts = [str(example['label'])]\n        if add_vb:\n            verbalizer = \"{ 0 : not equivalent, 1 : equivalent }\"\n        else:"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/dpt/data_utils.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "\n",
    "    def load_dataset(self, split):\n",
    "        # return datasets.load_dataset('glue', 'mrpc', split=split, script_version=\"master\")\n",
    "        return datasets.load_dataset('glue', 'mrpc', split=split)\n",
    "\n",
    "    def preprocessor(self, example, add_prefix=True, add_vb=False):\n",
    "<<<AFTER_EDIT>>>\n",
    "\n",
    "    def load_dataset(self, split):\n",
    "        # return datasets.load_dataset('glue', 'mrpc', split=split, script_version=\"master\")\n",
    "        #return datasets.load_dataset('glue', 'mrpc', split=split)\n",
    "        files = {\n",
    "        'train': 'mrpc_csv/train.csv',\n",
    "        'validation': 'mrpc_csv/val.csv',\n",
    "        'test': 'mrpc_csv/test.csv'\n",
    "        }\n",
    "\n",
    "        # Load the dataset\n",
    "        return datasets.load_dataset('csv', data_files=files, split=split)\n",
    "\n",
    "    def preprocessor(self, example, add_prefix=True, add_vb=False):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6fa52c",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf942e14",
   "metadata": {},
   "source": [
    "Execution Start time: 00:45:22, End time: 00:47:17 (duration: 115 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072f9843",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bca6a13",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b27559a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip3 install -q huggingface-hub numpy scikit-learn scipy sentencepiece spacy tabulate tokenizers torch torchvision tqdm transformers datasets===2.19.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec6352a",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1bffa7",
   "metadata": {},
   "source": [
    "Execution Start time: 00:47:17, End time: 00:47:21 (duration: 4 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68787baf",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5017d02",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc01043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Downloading...\nFrom: https://drive.google.com/uc?id=1raGLX-2vdX1uizYx2DdqZ_JzAkVeyeDF\nTo: /content/dpt/mrpc_csv.zip\n\n  0%|          | 0.00/3.96k [00:00<?, ?B/s]\n\n   creating: mrpc_csv/\n  inflating: mrpc_csv/val.csv        \n  inflating: mrpc_csv/test.csv       \n  inflating: mrpc_csv/train.csv      \n\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# URL of the Google Drive file\n",
    "file_url = \"https://drive.google.com/uc?id=1raGLX-2vdX1uizYx2DdqZ_JzAkVeyeDF\"\n",
    "output_file = \"mrpc_csv.zip\"\n",
    "\n",
    "# Download the file from Google Driveraise ValueError(f'mutable default {type(f.default)} for field ' ValueError: mutable default <class 'datasets.utils.version.Version'> for field version is not allowed: use default_factory\n",
    "gdown.download(file_url, output_file, quiet=False)\n",
    "!unzip mrpc_csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98532c7",
   "metadata": {},
   "source": [
    "### Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12312299",
   "metadata": {},
   "source": [
    "Execution Start time: 00:47:23, End time: 00:47:23 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6291a4a",
   "metadata": {},
   "source": [
    "**Thought**: Now that the dataset is downloaded and extracted, I need to modify the code to ensure it runs on a CPU. I will check the `main_ddp.py` script for any GPU-specific code and modify it to use the CPU instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b19507a",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a72dc50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import os\nimport sys\nimport random\nimport argparse\nimport numpy as np\nfrom tqdm import tqdm\nfrom datetime import datetime\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import optim\n\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler\n\nimport transformers\nfrom transformers import (\n    AutoTokenizer,\n    T5TokenizerFast,\n    T5ForConditionalGeneration,\n    AutoConfig,\n    AdamW,\n    get_scheduler,\n    set_seed,\n)\ntransformers.logging.set_verbosity_error()\n\nfrom modeling_t5 import T5PromptForConditionalGeneration_param\nfrom data_utils import AutoTask\nfrom eval_utils import AutoPostProcessor\nfrom metrics import *\nfrom options import *\nfrom utils import *\nfrom loader import *\n\nimport datasets\nfrom datasets import concatenate_datasets\nfrom datasets.utils.logging import set_verbosity_error\nfrom eval_utils import *\nset_verbosity_error()\n\nimport logging\nlogging.disable(logging.WARNING)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\ndef run(local_rank, args):\n\n    is_master = local_rank == 0\n    world_size = args.world_size\n    is_mp = world_size > 1\n    # set the device\n    device = local_rank\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed_all(args.seed)\n    g = torch.Generator()\n    g.manual_seed(args.seed)\n\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n\n    ###################################################################################################\n    #   Load data                                                                                     #\n    ###################################################################################################\n    # Can processing multiple datasets\n    train_datasets, val_datasets = [], {}\n    for dataset_name in args.datasets_names:\n        train_dataset = AutoTask.get(dataset_name).get(split=\"train\",\n                                    split_validation_test=True,\n                                    add_prefix=args.add_task_prefix,\n                                    add_vb=args.add_verbalizer,\n                                    file_prefix=args.local_file_prefix,\n                                    n_obs=100000 if dataset_name == 'yelp_polarity' else None)\n        train_datasets.append(train_dataset)\n\n        val_dataset = AutoTask.get(dataset_name).get(split=\"validation\",\n                                    split_validation_test=True,\n                                    add_prefix=args.add_task_prefix,\n                                    add_vb=args.add_verbalizer,\n                                    file_prefix=args.local_file_prefix,\n                                    n_obs=None)\n        val_datasets.update({dataset_name: val_dataset})\n\n        \n        if is_master:\n            print(local_rank, dataset_name, 'Train\\t', train_dataset[0])\n            print(local_rank, dataset_name, 'Val\\t', val_dataset[0])\n            \n    # merge all datasets if there are multiple ones\n    train_datasets = concatenate_datasets(train_datasets)\n\n    if is_master:\n        print(local_rank, len(train_dataset), len(val_dataset))\n        print('# all training samples:', len(train_datasets))\n        print(train_datasets[0])\n\n    # Data loader \n    # Creating the Training and Validation dataset for further creation of Dataloader\n    training_set = CustomT5Dataset(train_datasets, tokenizer, \n                                    args.max_source_length, \n                                    args.max_target_length, \n                                    args.datasets_names\n                                    )\n    \n\n    val_sets = {data_name: CustomT5Dataset(data_set, tokenizer, \n                                           args.max_source_length, \n                                           args.max_target_length, \n                                           args.datasets_names\n                                           )\n               for data_name, data_set in val_datasets.items()}\n\n    # Defining the parameters for creation of dataloaders\n    train_params = {\n        'batch_size': args.train_batch_size,\n        'shuffle': not is_mp,  # not shuffle in DDP\n        'num_workers': 4,\n        'worker_init_fn': seed_worker,\n        'generator': g,\n        }\n\n    eval_params = {\n        'batch_size': args.eval_batch_size,\n        'shuffle': False,\n        'num_workers': 0,\n        }\n\n    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n    if is_mp:\n        sampler = DistributedSampler(training_set, num_replicas=world_size, rank=local_rank, shuffle=True)\n        TrainDataloader = DataLoader(training_set, sampler=sampler, **train_params)\n    else:\n        TrainDataloader = DataLoader(training_set, **train_params)\n\n    ValDataloaders = {data_name: DataLoader(data_set, **eval_params) for data_name, data_set in val_sets.items()}\n    \n    ###################################################################################################\n    #   Build the model                                                                                   #\n    ###################################################################################################\n    config = AutoConfig.from_pretrained(args.model_name)\n    config.len_enc_prompt = args.enc_prompt_tokens\n    config.len_dec_prompt = args.dec_prompt_tokens\n    config.add_enc_prompt = args.enc_prompt_tokens > 0\n    config.add_dec_prompt = args.dec_prompt_tokens > 0\n    config.num_tasks = len(args.datasets_names)\n    \n    config.bottle_neck = args.bottle_neck\n\n    model = T5PromptForConditionalGeneration_param.from_pretrained(args.model_name, config=config)\n\n    # Freeze the backbone model\n    for name, param in model.named_parameters():\n        param.requires_grad = False if 'prefix' not in name else True\n\n    if is_mp:\n        # initialize distributed data parallel (DDP)\n        dist.init_process_group(backend='nccl', rank=local_rank, world_size=world_size)\n\n        model = DDP(\n            model.to(local_rank),\n            device_ids=[local_rank],\n            find_unused_parameters=False\n        )\n    else:\n        model = model.to(device)\n\n    \n    if is_master:\n        print('Parameters to optimize: ', [n for n, p in model.named_parameters() if 'prefix' in n])\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if 'prefix' in n],\n            \"weight_decay\": args.weight_decay,\n        }\n    ]\n \n    optimizer = optim.AdamW(optimizer_grouped_parameters, lr=args.lr * np.sqrt(world_size))  # sacle the learning rate based on world_size\n\n    max_train_steps = args.max_train_steps if args.max_train_steps > 0 else args.n_epochs * len(TrainDataloader)\n    scheduler = get_scheduler(\n        name=args.lr_scheduler_type,\n        optimizer=optimizer,\n        num_warmup_steps=int(args.num_warmup_steps) if args.num_warmup_steps > 1 else int(args.num_warmup_steps * max_train_steps),\n        num_training_steps=max_train_steps\n    )\n\n    # Load the model or resume the training\n    resume_steps = 0\n    if args.from_checkpoint:\n        if is_mp:\n            checkpoint = torch.load(args.from_checkpoint, map_location=torch.device(f'cuda:{local_rank}'))\n        else:\n            checkpoint = torch.load(args.from_checkpoint)\n\n        resume_steps = checkpoint['global_step']\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        param_dict = checkpoint['params']\n        for n, p in model.named_parameters():\n            if n in param_dict:\n                p.data = param_dict[n].clone().detach().to(device)\n\n        if is_master:\n            print('Resume the training from the checkpoint: ', args.from_checkpoint)\n\n    if args.auto_resume and args.save_ckpt_dir:\n        checkpoint_path = os.path.join(args.save_ckpt_dir, args.latest_ckpt_name)\n\n        if os.path.exists(checkpoint_path) or len(os.listdir(args.save_ckpt_dir)) > 0:\n            if not os.path.exists(checkpoint_path):\n                list_files = os.listdir(args.save_ckpt_dir)\n                # little parsing to get the step number: format -> sst2.qqp.mnli.qnli.squad.record.soft_prompts.source.step.900.pt\n                list_steps = [x.strip().split('.')[-2] for x in list_files]\n                max_idx = list_steps.index(max(list_steps))\n                checkpoint_path = os.path.join(args.save_ckpt_dir, list_files[max_idx])    \n\n            assert os.path.exists(checkpoint_path)\n\n            if is_mp:\n                checkpoint = torch.load(checkpoint_path, map_location=torch.device(f'cuda:{local_rank}'))\n            else:\n                checkpoint = torch.load(checkpoint_path)\n\n            resume_steps = checkpoint['global_step']\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n            param_dict = checkpoint['params']\n            for n, p in model.named_parameters():\n                if n in param_dict:\n                    p.data = param_dict[n].clone().detach().to(device)\n\n            if is_master:\n                print(f'Auto-resume the training from the checkpoint: {checkpoint_path} from step {resume_steps}')\n        else:\n            if is_master:\n                print('No existing checkpoint; Start the training from scratch!')\n\n    \n    if is_master:\n        print('Prefix Parameters: ', [n for n, p in model.named_parameters() if 'prefix' in n])\n        print('Trainable Parameters: ', [n for n, p in model.named_parameters() if p.requires_grad])\n        print('#Trainable Parameters: ', sum(p.numel() for p in model.parameters() if p.requires_grad))\n\n        if args.prompt_type != 'dynamic':\n            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n            print(f'All trainable parameters: {trainable_params}; per task: {trainable_params / len(args.datasets_names)}')\n        else:\n            prompt_size = model.get_prompt_real_size()\n            trainable_params = prompt_size * 768\n            print(f'All trainable parameters: {trainable_params}; per task: {trainable_params / len(args.datasets_names)}')\n\n\n    ###################################################################################################\n    #   Training                                                                                      #\n    ###################################################################################################\n\n    if is_master:\n        print()\n        print('***** running training *****')\n        print(f'| batch_size: {args.train_batch_size} | num_epochs: {args.n_epochs} | num_train: {len(TrainDataloader)} |')\n\n    global_step = 0\n    best_dev_epoch = 0\n    best_dev_step = 0\n    best_epoch_dev = float('-inf')\n    best_step_dev = float('-inf')\n    val_res, test_res = 0, 0\n    final_test = 0\n    finetuned_checkpoint = None\n    # try:\n    while True:\n        for epoch in range(int(args.n_epochs)):\n            model.train()\n            step_count = 0\n\n            if epoch != 0 and len(args.datasets_names) > 1:\n                training_set.reset(epoch)\n\n            if is_mp:\n                sampler.set_epoch(epoch)\n\n            with tqdm(total=len(TrainDataloader), desc=f'Epoch {epoch}/{args.n_epochs}', unit='b', disable=args.close_tqdm) as pbar:\n                update_stride = len(TrainDataloader) // 100 if len(TrainDataloader) > 200 else 1\n\n                for step, batch in enumerate(TrainDataloader):\n                    global_step += 1\n\n                    if global_step <= resume_steps:\n\n                        if step % update_stride == 0:\n                            pbar.update(update_stride)                        \n                        continue\n                    if len(batch['source_ids'].shape) == 3:\n                        source_ids = batch['source_ids'].squeeze(0).to(local_rank)\n                        source_mask = batch['source_mask'].squeeze(0).to(local_rank)\n                        labels = batch['target_ids'].squeeze(0).to(local_rank)\n                        task_ids = torch.tensor([x[0] for x in batch['task_ids']]).to(local_rank)\n                    else:\n                        source_ids = batch['source_ids'].to(local_rank)\n                        source_mask = batch['source_mask'].to(local_rank)\n                        labels = batch['target_ids'].to(local_rank)\n                        task_ids = batch['task_ids'].to(local_rank)\n\n                    outputs = model(input_ids=source_ids, attention_mask=source_mask, labels=labels, task_ids=task_ids)\n                    loss = outputs['loss']\n\n                    loss = loss / args.accumulate_steps\n                    loss.backward()\n                    step_count += 1\n\n                    if step_count == args.accumulate_steps:\n\n                        if args.max_grad_norm > 0:\n                            nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n\n                        step_count = 0\n                        optimizer.step()\n                        scheduler.step()\n                        optimizer.zero_grad()\n\n                    if step % update_stride == 0:\n                        pbar.set_postfix(**{'loss': loss.item(), 'lr': scheduler.get_last_lr()[0]})\n                        pbar.update(update_stride)\n\n                    if is_master and args.save_ckpt_dir:  # save the prompts\n                        if global_step % args.saving_steps == 0:\n                            checkpoint = {\n                                'global_step': global_step,\n                                'params': {n: p for n, p in model.named_parameters() if 'prefix' in n},\n                                'optimizer_state_dict': optimizer.state_dict(),\n                                'scheduler_state_dict': scheduler.state_dict(),\n                            }\n                            file_name = '.'.join(args.datasets) + '.soft_prompts.step.{}.pt'.format(global_step)\n                            save_checkpoint(args.save_ckpt_dir, checkpoint, file_name)\n                            print(f\"Saved soft prompts at: {os.path.join(args.save_ckpt_dir, file_name)}\")\n\n                            # Track the latest checkpoint for resuming\n                            save_checkpoint(args.save_ckpt_dir, checkpoint, args.latest_ckpt_name)\n\n                            if args.eval_in_train:\n                                if \"wsc\" in args.datasets_names[0] or \"WSC\" in args.datasets_names[0]:\n                                    res = task_evaluation_wsc(args, ValDataloaders, model, tokenizer, device)\n                                else:\n                                    res = task_evaluation(args, ValDataloaders, model, tokenizer, device)\n                                if len(res) > 1:\n                                    val_res = average_multi_task(res)\n                                else:\n                                    val_res = res[args.datasets_names[0]][TASK_TO_METRICS[args.datasets_names[0]][0]]\n\n                                if val_res > best_step_dev:\n                                    best_step_dev = val_res\n                                    best_dev_step = global_step\n                                    print(f'Step Best Val: {best_step_dev} at Step {global_step}: {res}')\n\n            # Epoch saving dring the training\n            if is_master and args.saving_each_epoch and global_step > resume_steps: # save prompts at the end of every epoch\n                checkpoint = {\n                    'global_step': global_step,\n                    'params': {n: p for n, p in model.named_parameters() if 'prefix' in n},\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'scheduler_state_dict': scheduler.state_dict(),\n                }\n                file_name = '.'.join(args.datasets) + '.soft_prompts.epoch.{}.pt'.format(epoch)\n                save_checkpoint(args.save_ckpt_dir, checkpoint, file_name)\n                print(f\"Saved soft prompts at: {os.path.join(args.save_ckpt_dir, file_name)}\")\n\n            # Epoch evaluation dring the training\n            if is_master and args.eval_in_train and global_step > resume_steps:\n                output_path = os.path.join(args.model_output_path, f\"dev_ep_{epoch}\")\n                if \"wsc\" in args.datasets_names[0] or \"WSC\" in args.datasets_names[0]:\n                    res = task_evaluation_wsc(args, ValDataloaders, model, tokenizer, device, output_path)\n                else:\n                    res = task_evaluation(args, ValDataloaders, model, tokenizer, device, output_path)\n                print(f'Epoch {epoch} - Validation: ', res)\n                if len(res) > 1:\n                    val_res = average_multi_task(res)\n                else:\n                    val_res = res[args.datasets_names[0]][TASK_TO_METRICS[args.datasets_names[0]][0]]\n\n\n                if val_res > best_epoch_dev:\n                    best_epoch_dev = val_res\n                    \n                    best_dev_epoch = epoch\n                    print(f'Epoch Best Val: {best_epoch_dev} at Epoch {best_dev_epoch}.')\n\n        if is_master:\n            print('***** training ends *****')\n            print()\n            print('best dev acc: {:.5f} (at epoch {})'.format(best_epoch_dev, best_dev_epoch))\n            print('best dev acc: {:.5f} (at step {})'.format(best_step_dev, best_dev_step))\n            print()\n\n        exit()\n\n    return\n\n\ndef task_evaluation_wsc(args, dataloader_dict, model, tokenizer, device, output_path=None):\n        \n    model.eval()\n    results = {}  # tasks: {metrics}\n\n    wsc_acc = []\n    tag_labels = []\n    with torch.no_grad():\n        for dataset_name, data_loader in dataloader_dict.items():\n            results[dataset_name] = {}\n            raw_preds = []\n            task_preds = []\n            task_labels = []\n            for batch in data_loader:\n                \n                source_ids = batch['source_ids'].to(device)\n                source_mask = batch['source_mask'].to(device)\n                task_ids = batch['task_ids'].to(device)\n                labels = batch['target_ids']\n                raw_input = batch['raw_target']\n                tag_labels += [1 for i in batch[\"raw_target\"]]\n\n                try:\n                    preds = model.generate(\n                        input_ids=source_ids,\n                        attention_mask=source_mask, \n                        max_length=args.max_target_length,\n                        num_beams=1,\n                        task_ids=task_ids,  # model_kwargs\n                        ).cpu().detach()\n                except:\n                    preds = model.module.generate(\n                        input_ids=source_ids,\n                        attention_mask=source_mask, \n                        max_length=args.max_target_length,\n                        num_beams=1,\n                        task_ids=task_ids,  # model_kwargs\n                        ).cpu().detach()\n\n                raw_preds += preds\n                batch_extra_fields = [x[0] for x in batch['extra_fields']] if isinstance(batch['extra_fields'][0], tuple) else batch['extra_fields']\n                data_info = [eval(x) for x in batch_extra_fields]\n                post_processor = AutoPostProcessor.get(dataset_name, tokenizer, ignore_pad_token_for_loss=True)\n                decoded_preds, decoded_labels = post_processor.process(preds, labels, data_info)\n                # print(decoded_preds)\n                # print(decoded_labels)\n                \n                task_preds += decoded_preds\n                task_labels += decoded_labels\n\n                #return {\"accuracy\": 100 * ((np.array(predictions) == np.array(targets)).mean())}\n                print(task_preds)\n                print(raw_input)\n                for pred, truth in zip(task_preds, raw_input):\n                    flag = wsc_simple(pred, truth)\n                    wsc_acc.append(flag)\n\n            cnt = 0\n            for i,j in zip(tag_labels, wsc_acc):\n                if i == j:\n                    cnt += 1\n\n            for i, metric in enumerate(AutoTask.get(dataset_name).metric):\n                results[dataset_name].update({\"accuracy\": 100*cnt/len(wsc_acc)})   \n\n    model.train()\n    return results\n\n\ndef task_evaluation(args, dataloader_dict, model, tokenizer, device, output_path=None):\n    model.eval()\n    results = {}  # tasks: {metrics}\n    with torch.no_grad():\n        for dataset_name, data_loader in dataloader_dict.items():\n            results[dataset_name] = {}\n            raw_preds = []\n            task_preds = []\n            task_labels = []\n            for batch in data_loader:\n                source_ids = batch['source_ids'].to(device)\n                source_mask = batch['source_mask'].to(device)\n                task_ids = batch['task_ids'].to(device)\n                labels = batch['target_ids']\n\n                try:\n                    \n                    preds = model.generate(\n                        input_ids=source_ids,\n                        attention_mask=source_mask, \n                        max_length=args.max_target_length,\n                        num_beams=1,\n                        task_ids=task_ids,  # model_kwargs\n                        ).cpu().detach()\n                except:\n                    \n                    preds = model.module.generate(\n                        input_ids=source_ids,\n                        attention_mask=source_mask, \n                        max_length=args.max_target_length,\n                        num_beams=1,\n                        task_ids=task_ids,  # model_kwargs\n                        ).cpu().detach()\n\n                raw_preds += preds\n                batch_extra_fields = [x[0] for x in batch['extra_fields']] if isinstance(batch['extra_fields'][0], tuple) else batch['extra_fields']\n                data_info = [eval(x) for x in batch_extra_fields]\n                post_processor = AutoPostProcessor.get(dataset_name, tokenizer, ignore_pad_token_for_loss=True)\n                decoded_preds, decoded_labels = post_processor.process(preds, labels, data_info)\n\n                task_preds += decoded_preds\n                task_labels += decoded_labels\n\n            # store the results to files\n            if output_path:\n                file_path = output_path + f'_{dataset_name}.output'\n                with open(file_path, 'w') as f:\n                    for i in range(len(raw_preds)):\n                        f.write('raw output:\\t' + str(raw_preds[i]) + '\\n')\n                        f.write('predicted label:\\t' + str(task_preds[i]) + '\\n')\n                        f.write('golden label:\\t' + str(task_labels[i]) + '\\n')\n                        f.write('\\n')\n\n            for i, metric in enumerate(AutoTask.get(dataset_name).metric):\n                results[dataset_name].update(metric(task_preds, task_labels))\n\n    model.train()\n    return results\n\n\ndef main():\n    print('Stating time: ', datetime.now().strftime(\"%m/%d/%Y %X\"))\n    args = process_args()\n\n    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n    \n    # Use all gpus unless gpus ids are specified \n    args.gupids = args.gupids if args.gupids else list(range(torch.cuda.device_count()))\n\n    if len(args.gupids) > 1: \n\n        os.environ['MASTER_ADDR'] = 'localhost'\n\n        while is_port_in_use(args.port):\n            args.port += 1\n        \n        os.environ['MASTER_PORT'] = f'{args.port}'\n        print(\"Use port\", args.port)\n\n        print(\"Use gpus \", args.gupids)\n        args.world_size = len(args.gupids)\n        mp.spawn(run, nprocs=len(args.gupids), args=(args,), join=True)\n\n    else:\n        args.world_size = 1\n        print(\"Use single gpu!\")\t\n        run(0, args)\n    \n    print(\"Ending time: \", datetime.now().strftime(\"%m/%d/%Y %X\"))\n    return\n\n\nif __name__ == '__main__':\n    main()\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!cat main_ddp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfd51a3",
   "metadata": {},
   "source": [
    "### Step 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754f4ecb",
   "metadata": {},
   "source": [
    "Execution Start time: 00:47:25, End time: 00:47:25 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03330de8",
   "metadata": {},
   "source": [
    "**Thought**: The `main_ddp.py` script uses GPU by default. To run it on a CPU, I need to modify the script to ensure it doesn't attempt to use CUDA. Specifically, I need to change the device assignments and any GPU-specific code to use the CPU instead. I'll edit the script to set the device to 'cpu' and remove any distributed data parallel (DDP) setup since it's not needed for CPU execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c787cd55",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c41ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n\ndef run(local_rank, args):\n\n    is_master = local_rank == 0\n    world_size = args.world_size\n    is_mp = world_size > 1\n    # set the device\n    device = 'cpu'\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed_all(args.seed)\n    g = torch.Generator()"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddp.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "    device = local_rank\n",
    "<<<AFTER_EDIT>>>\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b5f932",
   "metadata": {},
   "source": [
    "### Step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6030a9a7",
   "metadata": {},
   "source": [
    "Execution Start time: 00:47:29, End time: 00:47:39 (duration: 10 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b67524",
   "metadata": {},
   "source": [
    "**Thought**: I have successfully modified the `main_ddp.py` script to use the CPU by setting the device to 'cpu'. Now, I can proceed to run the script with the specified parameters to fine-tune the `t5-small` model on the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5384058",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04567c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stating time:  08/21/2024 00:47:34\nRaw Arguments:  Namespace(seed=42, from_checkpoint='', auto_resume=False, save_ckpt_dir='./saved_models', load_source_path='', saving_steps=1000, saving_each_epoch=True, latest_ckpt_name='latest_checkpoint.pt', n_epochs=1, train_batch_size=16, eval_batch_size=128, lr=0.3, max_grad_norm=1.0, weight_decay=1e-05, accumulate_steps=1, lr_scheduler_type='linear', num_warmup_steps=0.1, max_train_steps=0, min_training_epoch=20, early_stopping_patience=10, mode='train', close_tqdm=True, max_epochs_before_stop=2, model_name='t5-small', datasets=['mrpc'], max_source_length=256, max_target_length=128, prompt_type='param', is_dynamic_share=False, eval_in_train=True, enc_prompt_tokens=100, dec_prompt_tokens=0, sharing_ratio=1.0, share_seed=42, add_task_prefix=True, add_verbalizer=False, port=12355, gupids=None, sampling_method='uniform', size_limit=262144, stoch_task=False, bottle_neck=10, model_output_path='./saved_outputs', local_file_prefix='/gpfs/u/home/DPTV/DPTVhnwz/scratch/mrqa_datasets/datasets', datasets_names=['mrpc'])\nProcess ID:  55\nUse single gpu!\n\ntokenizer_config.json:   0%|                        | 0.00/2.32k [00:00<?, ?B/s]\n\n\nspiece.model:   0%|                                  | 0.00/792k [00:00<?, ?B/s]\n\n\n\ntokenizer.json:   0%|                               | 0.00/1.39M [00:00<?, ?B/s]\n\n\n0 mrpc Train\t {'source': 'sentence1: The companies uniformly declined to give specific numbers on customer turnover , saying they will release those figures only when they report overall company performance at year-end . sentence2: The companies , however , declined to give specifics on customer turnover , saying they would release figures only when they report their overall company performance .', 'target': '1', 'task': 'mrpc'}\n0 mrpc Val\t {'source': 'sentence1: The additional contribution brings total U.S. food aid to North Korea this year to 100,000 tonnes . sentence2: The donation of 60,000 tons brings the total of U.S. contributions for the year to 100,000 .', 'target': '1', 'task': 'mrpc'}\n0 10 10\n# all training samples: 10\n{'source': 'sentence1: The companies uniformly declined to give specific numbers on customer turnover , saying they will release those figures only when they report overall company performance at year-end . sentence2: The companies , however , declined to give specifics on customer turnover , saying they would release figures only when they report their overall company performance .', 'target': '1', 'task': 'mrpc'}\n\nconfig.json:   0%|                                  | 0.00/1.21k [00:00<?, ?B/s]\n\n\nmodel.safetensors:   0%|                             | 0.00/242M [00:00<?, ?B/s]\n\n\n\n\n\n\n\n\n\ngeneration_config.json:   0%|                         | 0.00/147 [00:00<?, ?B/s]\n\nParameters to optimize:  ['encoder.prefix_first', 'encoder.prefix_second']\nPrefix Parameters:  ['encoder.prefix_first', 'encoder.prefix_second']\nTrainable Parameters:  ['encoder.prefix_first', 'encoder.prefix_second']\n#Trainable Parameters:  6120\nAll trainable parameters: 6120; per task: 6120.0\n\n***** running training *****\n| batch_size: 16 | num_epochs: 1 | num_train: 1 |\nTraceback (most recent call last):\n  File \"/content/dpt/main_ddp.py\", line 569, in <module>\n    main()\n  File \"/content/dpt/main_ddp.py\", line 562, in main\n    run(0, args)\n  File \"/content/dpt/main_ddp.py\", line 307, in run\n    source_ids = batch['source_ids'].to(local_rank)\n  File \"/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 314, in _lazy_init\n    torch._C._cuda_init()\nRuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python main_ddp.py --datasets=mrpc --model_name=t5-small --enc_prompt_tokens 100 -ts 16 -e 1 --bottle_neck 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be8f0d3",
   "metadata": {},
   "source": [
    "### Step 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753095b1",
   "metadata": {},
   "source": [
    "Execution Start time: 00:47:42, End time: 00:47:42 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb74ddb",
   "metadata": {},
   "source": [
    "**Thought**: It seems that there is still an attempt to use CUDA in the `main_ddp.py` script, specifically when moving tensors to the device. I need to ensure that all tensor operations are performed on the CPU by modifying the script to avoid any `.to(local_rank)` calls and replace them with `.to(device)` where `device` is set to 'cpu'. Let's make these changes in the script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0ab958",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41b8b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Could not find the following lines to replace in the file content:\n```\n            source_ids = batch['source_ids'].to(local_rank)\n            source_mask = batch['source_mask'].to(local_rank)\n            target_ids = batch['target_ids'].to(local_rank)\n            target_mask = batch['target_mask'].to(local_rank)\n```"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddp.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "            source_ids = batch['source_ids'].to(local_rank)\n",
    "            source_mask = batch['source_mask'].to(local_rank)\n",
    "            target_ids = batch['target_ids'].to(local_rank)\n",
    "            target_mask = batch['target_mask'].to(local_rank)\n",
    "<<<AFTER_EDIT>>>\n",
    "            source_ids = batch['source_ids'].to(device)\n",
    "            source_mask = batch['source_mask'].to(device)\n",
    "            target_ids = batch['target_ids'].to(device)\n",
    "            target_mask = batch['target_mask'].to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f49ebcc",
   "metadata": {},
   "source": [
    "### Step 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0181a896",
   "metadata": {},
   "source": [
    "Execution Start time: 00:47:44, End time: 00:47:44 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ace5f46",
   "metadata": {},
   "source": [
    "**Thought**: It seems that the lines I attempted to edit were not found in the `main_ddp.py` script. This might be due to a different implementation or structure in the script. I need to carefully inspect the script to identify where tensors are being moved to a device and ensure they are moved to the CPU instead. Let's search for any `.to(local_rank)` or similar calls and replace them with `.to(device)`, where `device` is set to 'cpu'. I'll review the script again to locate these lines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f419ac2",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3bdd71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168:            model.to(local_rank),\n173:        model = model.to(device)\n209:                p.data = param_dict[n].clone().detach().to(device)\n238:                    p.data = param_dict[n].clone().detach().to(device)\n302:                        source_ids = batch['source_ids'].squeeze(0).to(local_rank)\n303:                        source_mask = batch['source_mask'].squeeze(0).to(local_rank)\n304:                        labels = batch['target_ids'].squeeze(0).to(local_rank)\n305:                        task_ids = torch.tensor([x[0] for x in batch['task_ids']]).to(local_rank)\n307:                        source_ids = batch['source_ids'].to(local_rank)\n308:                        source_mask = batch['source_mask'].to(local_rank)\n309:                        labels = batch['target_ids'].to(local_rank)\n310:                        task_ids = batch['task_ids'].to(local_rank)\n422:                source_ids = batch['source_ids'].to(device)\n423:                source_mask = batch['source_mask'].to(device)\n424:                task_ids = batch['task_ids'].to(device)\n486:                source_ids = batch['source_ids'].to(device)\n487:                source_mask = batch['source_mask'].to(device)\n488:                task_ids = batch['task_ids'].to(device)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!grep -n \".to(\" main_ddp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99abec6",
   "metadata": {},
   "source": [
    "### Step 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbca972",
   "metadata": {},
   "source": [
    "Execution Start time: 00:47:50, End time: 00:47:50 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf443a6",
   "metadata": {},
   "source": [
    "**Thought**: The script contains several instances where tensors are moved to a device using `.to(local_rank)`. I need to replace these with `.to(device)` where `device` is set to 'cpu'. I'll edit the script to make these changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15b63de",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a466e596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Found multiple BEFORE_EDIT in the patch. Line 5: <<<BEFORE_EDIT>>>\nFormat of edit is:```\n{filename}\n[optional] preceding lines\n<<<BEFORE_EDIT>>>\nline to remove\nline to remove\n<<<AFTER_EDIT>>>\nline to add\nline to add\nline to add\n```"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddp.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "            model.to(local_rank),\n",
    "<<<AFTER_EDIT>>>\n",
    "            model.to(device),\n",
    "<<<BEFORE_EDIT>>>\n",
    "                        source_ids = batch['source_ids'].squeeze(0).to(local_rank)\n",
    "                        source_mask = batch['source_mask'].squeeze(0).to(local_rank)\n",
    "                        labels = batch['target_ids'].squeeze(0).to(local_rank)\n",
    "                        task_ids = torch.tensor([x[0] for x in batch['task_ids']]).to(local_rank)\n",
    "                        source_ids = batch['source_ids'].to(local_rank)\n",
    "                        source_mask = batch['source_mask'].to(local_rank)\n",
    "                        labels = batch['target_ids'].to(local_rank)\n",
    "                        task_ids = batch['task_ids'].to(local_rank)\n",
    "<<<AFTER_EDIT>>>\n",
    "                        source_ids = batch['source_ids'].squeeze(0).to(device)\n",
    "                        source_mask = batch['source_mask'].squeeze(0).to(device)\n",
    "                        labels = batch['target_ids'].squeeze(0).to(device)\n",
    "                        task_ids = torch.tensor([x[0] for x in batch['task_ids']]).to(device)\n",
    "                        source_ids = batch['source_ids'].to(device)\n",
    "                        source_mask = batch['source_mask'].to(device)\n",
    "                        labels = batch['target_ids'].to(device)\n",
    "                        task_ids = batch['task_ids'].to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1323e3ab",
   "metadata": {},
   "source": [
    "### Step 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e2a24e",
   "metadata": {},
   "source": [
    "Execution Start time: 00:47:53, End time: 00:47:53 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c29621",
   "metadata": {},
   "source": [
    "**Thought**: I attempted to edit multiple sections of the script at once, which caused an error. I need to edit each section separately to ensure the changes are applied correctly. Let's start by editing the first occurrence of `.to(local_rank)` and replace it with `.to(device)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6383fdcf",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a598cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        param.requires_grad = False if 'prefix' not in name else True\n\n    if is_mp:\n        # initialize distributed data parallel (DDP)\n        dist.init_process_group(backend='nccl', rank=local_rank, world_size=world_size)\n\n        model = DDP(\n            model.to(device),\n            device_ids=[local_rank],\n            find_unused_parameters=False\n        )\n    else:\n        model = model.to(device)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddp.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "            model.to(local_rank),\n",
    "<<<AFTER_EDIT>>>\n",
    "            model.to(device),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec40a81",
   "metadata": {},
   "source": [
    "### Step 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721f63ba",
   "metadata": {},
   "source": [
    "Execution Start time: 00:47:57, End time: 00:47:57 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a19762",
   "metadata": {},
   "source": [
    "**Thought**: I have successfully edited the first occurrence of `.to(local_rank)` to `.to(device)`. Now, I need to proceed with editing the remaining occurrences where tensors are moved to a device, ensuring they are moved to the CPU instead. I'll continue with the next set of lines that need editing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be15139a",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566d2782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n\n                    if global_step <= resume_steps:\n\n                        if step % update_stride == 0:\n                            pbar.update(update_stride)                        \n                        continue\n                    if len(batch['source_ids'].shape) == 3:\n                        source_ids = batch['source_ids'].squeeze(0).to(device)\n                        source_mask = batch['source_mask'].squeeze(0).to(device)\n                        labels = batch['target_ids'].squeeze(0).to(device)\n                        task_ids = torch.tensor([x[0] for x in batch['task_ids']]).to(device)\n                    else:\n                        source_ids = batch['source_ids'].to(local_rank)\n                        source_mask = batch['source_mask'].to(local_rank)\n                        labels = batch['target_ids'].to(local_rank)\n                        task_ids = batch['task_ids'].to(local_rank)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddp.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "                        source_ids = batch['source_ids'].squeeze(0).to(local_rank)\n",
    "                        source_mask = batch['source_mask'].squeeze(0).to(local_rank)\n",
    "                        labels = batch['target_ids'].squeeze(0).to(local_rank)\n",
    "                        task_ids = torch.tensor([x[0] for x in batch['task_ids']]).to(local_rank)\n",
    "<<<AFTER_EDIT>>>\n",
    "                        source_ids = batch['source_ids'].squeeze(0).to(device)\n",
    "                        source_mask = batch['source_mask'].squeeze(0).to(device)\n",
    "                        labels = batch['target_ids'].squeeze(0).to(device)\n",
    "                        task_ids = torch.tensor([x[0] for x in batch['task_ids']]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676cf973",
   "metadata": {},
   "source": [
    "### Step 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371cf602",
   "metadata": {},
   "source": [
    "Execution Start time: 00:48:02, End time: 00:48:03 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1182a94",
   "metadata": {},
   "source": [
    "**Thought**: I have successfully edited the second set of lines to move tensors to the CPU. Now, I need to edit the remaining occurrences where tensors are moved to a device, ensuring they are moved to the CPU instead. I'll continue with the next set of lines that need editing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd2bae5",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd3c25c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n                        continue\n                    if len(batch['source_ids'].shape) == 3:\n                        source_ids = batch['source_ids'].squeeze(0).to(device)\n                        source_mask = batch['source_mask'].squeeze(0).to(device)\n                        labels = batch['target_ids'].squeeze(0).to(device)\n                        task_ids = torch.tensor([x[0] for x in batch['task_ids']]).to(device)\n                    else:\n                        source_ids = batch['source_ids'].to(device)\n                        source_mask = batch['source_mask'].to(device)\n                        labels = batch['target_ids'].to(device)\n                        task_ids = batch['task_ids'].to(device)\n\n                    outputs = model(input_ids=source_ids, attention_mask=source_mask, labels=labels, task_ids=task_ids)\n                    loss = outputs['loss']\n\n                    loss = loss / args.accumulate_steps\n                    loss.backward()"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddp.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "                        source_ids = batch['source_ids'].to(local_rank)\n",
    "                        source_mask = batch['source_mask'].to(local_rank)\n",
    "                        labels = batch['target_ids'].to(local_rank)\n",
    "                        task_ids = batch['task_ids'].to(local_rank)\n",
    "<<<AFTER_EDIT>>>\n",
    "                        source_ids = batch['source_ids'].to(device)\n",
    "                        source_mask = batch['source_mask'].to(device)\n",
    "                        labels = batch['target_ids'].to(device)\n",
    "                        task_ids = batch['task_ids'].to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6751b369",
   "metadata": {},
   "source": [
    "### Step 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809e3f96",
   "metadata": {},
   "source": [
    "Execution Start time: 00:48:04, End time: 00:49:20 (duration: 75 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd76c13",
   "metadata": {},
   "source": [
    "**Thought**: I have successfully edited all occurrences where tensors were moved to a device, ensuring they are now moved to the CPU. Now, I can proceed to run the script with the specified parameters to fine-tune the `t5-small` model on the dataset using the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb02291",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59ac3be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stating time:  08/21/2024 00:48:09\nRaw Arguments:  Namespace(seed=42, from_checkpoint='', auto_resume=False, save_ckpt_dir='./saved_models', load_source_path='', saving_steps=1000, saving_each_epoch=True, latest_ckpt_name='latest_checkpoint.pt', n_epochs=1, train_batch_size=16, eval_batch_size=128, lr=0.3, max_grad_norm=1.0, weight_decay=1e-05, accumulate_steps=1, lr_scheduler_type='linear', num_warmup_steps=0.1, max_train_steps=0, min_training_epoch=20, early_stopping_patience=10, mode='train', close_tqdm=True, max_epochs_before_stop=2, model_name='t5-small', datasets=['mrpc'], max_source_length=256, max_target_length=128, prompt_type='param', is_dynamic_share=False, eval_in_train=True, enc_prompt_tokens=100, dec_prompt_tokens=0, sharing_ratio=1.0, share_seed=42, add_task_prefix=True, add_verbalizer=False, port=12355, gupids=None, sampling_method='uniform', size_limit=262144, stoch_task=False, bottle_neck=10, model_output_path='./saved_outputs', local_file_prefix='/gpfs/u/home/DPTV/DPTVhnwz/scratch/mrqa_datasets/datasets', datasets_names=['mrpc'])\nProcess ID:  107\nUse single gpu!\n0 mrpc Train\t {'source': 'sentence1: The companies uniformly declined to give specific numbers on customer turnover , saying they will release those figures only when they report overall company performance at year-end . sentence2: The companies , however , declined to give specifics on customer turnover , saying they would release figures only when they report their overall company performance .', 'target': '1', 'task': 'mrpc'}\n0 mrpc Val\t {'source': 'sentence1: The additional contribution brings total U.S. food aid to North Korea this year to 100,000 tonnes . sentence2: The donation of 60,000 tons brings the total of U.S. contributions for the year to 100,000 .', 'target': '1', 'task': 'mrpc'}\n0 10 10\n# all training samples: 10\n{'source': 'sentence1: The companies uniformly declined to give specific numbers on customer turnover , saying they will release those figures only when they report overall company performance at year-end . sentence2: The companies , however , declined to give specifics on customer turnover , saying they would release figures only when they report their overall company performance .', 'target': '1', 'task': 'mrpc'}\nParameters to optimize:  ['encoder.prefix_first', 'encoder.prefix_second']\nPrefix Parameters:  ['encoder.prefix_first', 'encoder.prefix_second']\nTrainable Parameters:  ['encoder.prefix_first', 'encoder.prefix_second']\n#Trainable Parameters:  6120\nAll trainable parameters: 6120; per task: 6120.0\n\n***** running training *****\n| batch_size: 16 | num_epochs: 1 | num_train: 1 |\nSaved soft prompts at: ./saved_models/mrpc.soft_prompts.epoch.0.pt\nEpoch 0 - Validation:  {'mrpc': {'accuracy': 0.0, 'f1': 0.0}}\nEpoch Best Val: 0.0 at Epoch 0.\n***** training ends *****\n\nbest dev acc: 0.00000 (at epoch 0)\nbest dev acc: -inf (at step 0)\n\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python main_ddp.py --datasets=mrpc --model_name=t5-small --enc_prompt_tokens 100 -ts 16 -e 1 --bottle_neck 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad88746",
   "metadata": {},
   "source": [
    "### Step 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d48353",
   "metadata": {},
   "source": [
    "Execution Start time: 00:49:21, End time: 00:49:21 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421a2a7c",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f020da",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8af835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bee3b179",
   "metadata": {},
   "source": [
    "### Step 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18613b73",
   "metadata": {},
   "source": [
    "Execution Start time: 00:49:22, End time: 00:49:22 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93b82b5",
   "metadata": {},
   "source": [
    "**Thought**: The script has been executed successfully, and the resulting accuracy on the development set is 0.0. I will now submit this result in the required JSON format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ad20d4",
   "metadata": {},
   "source": [
    "**Action (submit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2206bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'accuracy': 0.0}"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
