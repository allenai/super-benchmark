{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87ad1ef6",
   "metadata": {},
   "source": [
    "### System Prompt\n",
    "<details>\n",
    "SETTING: You are an autonomous programmer, and you're working directly in the command line with a special Jupyter notebook interface.\n",
    "\n",
    "The special Jupyter notebook interface consists of a file editor that shows you 100 lines of a file at a time.\n",
    "You can execute commands in the notebook using:\n",
    "\n",
    "1. Bash commands: Commands starting with !.\n",
    "2. Python commands: Standard Python code.\n",
    "3. Magic commands: Commands starting with %, e.g., %cd <path>.\n",
    "\n",
    "Additionally, you can also use the following commands to help you navigate and edit files.\n",
    "\n",
    "COMMANDS:\n",
    "\n",
    "open:\n",
    "  docstring: opens the file at the given path in the editor. If line_number is provided, the window will be move to include that line\n",
    "  signature: open <path> [<line_number>]\n",
    "  arguments:\n",
    "    - path (string) [required]: the path to the file to open\n",
    "    - line_number (integer) [optional]: the line number to move the window to (if not provided, the window will start at the top of the file)\n",
    "\n",
    "create:\n",
    "  docstring: creates and opens a new file with the given name\n",
    "  signature: create <filename>\n",
    "  arguments:\n",
    "    - filename (string) [required]: the name of the file to create\n",
    "\n",
    "goto:\n",
    "  docstring: moves the window to show <line_number>\n",
    "  signature: goto <line_number>\n",
    "  arguments:\n",
    "    - line_number (integer) [required]: the line number to move the window to\n",
    "\n",
    "scroll_down:\n",
    "  docstring: moves the window down {WINDOW} lines\n",
    "  signature: scroll_down\n",
    "\n",
    "scroll_up:\n",
    "  docstring: moves the window down {WINDOW} lines\n",
    "  signature: scroll_up\n",
    "\n",
    "submit:\n",
    "  docstring: submits your final answer\n",
    "  signature: submit <answer>\n",
    "  arguments:\n",
    "    - answer (string) [required]: the answer to be submitted\n",
    "\n",
    "search_dir:\n",
    "  docstring: searches for search_term in all files in dir_path. If dir_path is not provided, searches in the current directory\n",
    "  signature: search_dir <search_term> [<dir_path>]\n",
    "  arguments:\n",
    "    - search_term (string) [required]: the term to search for\n",
    "    - dir_path (string) [optional]: the directory to search in (if not provided, searches in the current directory)\n",
    "\n",
    "search_file:\n",
    "  docstring: searches for search_term in file. If file is not provided, searches in the current open file\n",
    "  signature: search_file <search_term> [<file>]\n",
    "  arguments:\n",
    "    - search_term (string) [required]: the term to search for\n",
    "    - file (string) [optional]: the file to search in (if not provided, searches in the current open file)\n",
    "\n",
    "find_file:\n",
    "  docstring: finds all files with the given name in dir_path. If dir_path is not provided, searches in the current directory\n",
    "  signature: find_file <file_name> [<dir_path>]\n",
    "  arguments:\n",
    "    - file_name (string) [required]: the name of the file to search for\n",
    "    - dir_path (string) [optional]: the directory to search in (if not provided, searches in the current directory)\n",
    "\n",
    "edit:\n",
    "  docstring: replaces lines <start_line> through <end_line> (inclusive) with the given text in the open file. The replacement text is terminated by a line with only end_of_edit on it. All of the <replacement text> will be entered, so make sure your indentation is formatted properly. Python files will be checked for syntax errors after the edit. If the system detects a syntax error, the edit will not be executed. Simply try to edit the file again, but make sure to read the error message and modify the edit command you issue accordingly. Issuing the same command a second time will just lead to the same error message again.\n",
    "  signature: edit <start_line>:<end_line>\n",
    "<replacement_text>\n",
    "end_of_edit\n",
    "  arguments:\n",
    "    - start_line (integer) [required]: the line number to start the edit at\n",
    "    - end_line (integer) [required]: the line number to end the edit at (inclusive)\n",
    "    - replacement_text (string) [required]: the text to replace the current selection with\n",
    "\n",
    "\n",
    "\n",
    "Please note that THE EDIT COMMAND REQUIRES PROPER INDENTATION. \n",
    "If you'd like to add the line '        print(x)' you must fully write that out, with all those spaces before the code! Indentation is important and code that is not indented correctly will fail and require fixing before it can be run.\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "Your shell prompt is formatted as follows:\n",
    "(Open file: <path>)\n",
    "(Current directory: <cwd>)\n",
    "In [ ]\n",
    "\n",
    "You need to format your output using two fields: discussion and command.\n",
    "Your output should always include _one_ discussion and _one_ command field EXACTLY as in the following example:\n",
    "DISCUSSION\n",
    "First I'll start by using ls to see what files are in the current directory. Then maybe we can look at some relevant files to see what they look like.\n",
    "```\n",
    "!ls -a\n",
    "```\n",
    "\n",
    "You should only include a *SINGLE* command in the command section and then wait for a response from the shell before continuing with more discussion and commands. Everything you include in the DISCUSSION section will be saved for future reference.\n",
    "If you'd like to issue two commands at once, PLEASE DO NOT DO THAT! Please instead first submit just the first command, and then after receiving a response you'll be able to issue the second command. \n",
    "You're free to use any other bash commands you want (e.g. find, grep, cat, ls, cd) in addition to the special commands listed above.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25f103c",
   "metadata": {},
   "source": [
    "### Query\n",
    "Finetune a text-to-AMR parser starting with \"facebook/bart-base\" on the train/val/test files provided in the examples directory. Report the last training loss as a json structured as follows: {\"last_loss\": 0.0} (replace 0.0 with the actual value).\n",
    "\n",
    "Additional instructions:\n",
    "1. Train only one epoch.\n",
    "2. Limit the max source and target length to 128.\n",
    "3. Limit the max generation tokens to 128.\n",
    "4. Limit the number of beams to 1.\n",
    "\n",
    "Git repository: https://github.com/goodbai-nlp/AMRBART\n",
    "\n",
    "I have already executed some of the required steps. Now, you should make any necessary configuration changes to achieve the user request. Your goal is to successfully make the required changes and successfully run\n",
    "```\n",
    "%cd /content/AMRBART/fine-tune\n",
    "!bash train-AMRBART-large-AMRParsing.sh \"facebook/bart-base\"\n",
    "%cd /content/AMRBART/fine-tune\n",
    "!bash inference-amr.sh \"outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp/checkpoint-1\"\n",
    "```\n",
    "then submit the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58061293",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c0a1c2",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1039f762",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae36890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Preparing code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7753fb",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df34199a",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d462624",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0b65e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Clone repository"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9930bd02",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1c398b",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad93119",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d738062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cloning into 'AMRBART'...\r\nremote: Enumerating objects: 400, done.\u001b[K\r\nremote: Counting objects:   1% (1/67)\u001b[K\rremote: Counting objects:   2% (2/67)\u001b[K\rremote: Counting objects:   4% (3/67)\u001b[K\rremote: Counting objects:   5% (4/67)\u001b[K\rremote: Counting objects:   7% (5/67)\u001b[K\rremote: Counting objects:   8% (6/67)\u001b[K\rremote: Counting objects:  10% (7/67)\u001b[K\rremote: Counting objects:  11% (8/67)\u001b[K\rremote: Counting objects:  13% (9/67)\u001b[K\rremote: Counting objects:  14% (10/67)\u001b[K\rremote: Counting objects:  16% (11/67)\u001b[K\rremote: Counting objects:  17% (12/67)\u001b[K\rremote: Counting objects:  19% (13/67)\u001b[K\rremote: Counting objects:  20% (14/67)\u001b[K\rremote: Counting objects:  22% (15/67)\u001b[K\rremote: Counting objects:  23% (16/67)\u001b[K\rremote: Counting objects:  25% (17/67)\u001b[K\rremote: Counting objects:  26% (18/67)\u001b[K\rremote: Counting objects:  28% (19/67)\u001b[K\rremote: Counting objects:  29% (20/67)\u001b[K\rremote: Counting objects:  31% (21/67)\u001b[K\rremote: Counting objects:  32% (22/67)\u001b[K\rremote: Counting objects:  34% (23/67)\u001b[K\rremote: Counting objects:  35% (24/67)\u001b[K\rremote: Counting objects:  37% (25/67)\u001b[K\rremote: Counting objects:  38% (26/67)\u001b[K\rremote: Counting objects:  40% (27/67)\u001b[K\rremote: Counting objects:  41% (28/67)\u001b[K\rremote: Counting objects:  43% (29/67)\u001b[K\rremote: Counting objects:  44% (30/67)\u001b[K\rremote: Counting objects:  46% (31/67)\u001b[K\rremote: Counting objects:  47% (32/67)\u001b[K\rremote: Counting objects:  49% (33/67)\u001b[K\rremote: Counting objects:  50% (34/67)\u001b[K\rremote: Counting objects:  52% (35/67)\u001b[K\rremote: Counting objects:  53% (36/67)\u001b[K\rremote: Counting objects:  55% (37/67)\u001b[K\rremote: Counting objects:  56% (38/67)\u001b[K\rremote: Counting objects:  58% (39/67)\u001b[K\rremote: Counting objects:  59% (40/67)\u001b[K\rremote: Counting objects:  61% (41/67)\u001b[K\rremote: Counting objects:  62% (42/67)\u001b[K\rremote: Counting objects:  64% (43/67)\u001b[K\rremote: Counting objects:  65% (44/67)\u001b[K\rremote: Counting objects:  67% (45/67)\u001b[K\rremote: Counting objects:  68% (46/67)\u001b[K\rremote: Counting objects:  70% (47/67)\u001b[K\rremote: Counting objects:  71% (48/67)\u001b[K\rremote: Counting objects:  73% (49/67)\u001b[K\rremote: Counting objects:  74% (50/67)\u001b[K\rremote: Counting objects:  76% (51/67)\u001b[K\rremote: Counting objects:  77% (52/67)\u001b[K\rremote: Counting objects:  79% (53/67)\u001b[K\rremote: Counting objects:  80% (54/67)\u001b[K\rremote: Counting objects:  82% (55/67)\u001b[K\rremote: Counting objects:  83% (56/67)\u001b[K\rremote: Counting objects:  85% (57/67)\u001b[K\rremote: Counting objects:  86% (58/67)\u001b[K\rremote: Counting objects:  88% (59/67)\u001b[K\rremote: Counting objects:  89% (60/67)\u001b[K\rremote: Counting objects:  91% (61/67)\u001b[K\rremote: Counting objects:  92% (62/67)\u001b[K\rremote: Counting objects:  94% (63/67)\u001b[K\rremote: Counting objects:  95% (64/67)\u001b[K\rremote: Counting objects:  97% (65/67)\u001b[K\rremote: Counting objects:  98% (66/67)\u001b[K\rremote: Counting objects: 100% (67/67)\u001b[K\rremote: Counting objects: 100% (67/67), done.\u001b[K\r\nremote: Compressing objects:   2% (1/40)\u001b[K\rremote: Compressing objects:   5% (2/40)\u001b[K\rremote: Compressing objects:   7% (3/40)\u001b[K\rremote: Compressing objects:  10% (4/40)\u001b[K\rremote: Compressing objects:  12% (5/40)\u001b[K\rremote: Compressing objects:  15% (6/40)\u001b[K\rremote: Compressing objects:  17% (7/40)\u001b[K\rremote: Compressing objects:  20% (8/40)\u001b[K\rremote: Compressing objects:  22% (9/40)\u001b[K\rremote: Compressing objects:  25% (10/40)\u001b[K\rremote: Compressing objects:  27% (11/40)\u001b[K\rremote: Compressing objects:  30% (12/40)\u001b[K\rremote: Compressing objects:  32% (13/40)\u001b[K\rremote: Compressing objects:  35% (14/40)\u001b[K\rremote: Compressing objects:  37% (15/40)\u001b[K\rremote: Compressing objects:  40% (16/40)\u001b[K\rremote: Compressing objects:  42% (17/40)\u001b[K\rremote: Compressing objects:  45% (18/40)\u001b[K\rremote: Compressing objects:  47% (19/40)\u001b[K\rremote: Compressing objects:  50% (20/40)\u001b[K\rremote: Compressing objects:  52% (21/40)\u001b[K\rremote: Compressing objects:  55% (22/40)\u001b[K\rremote: Compressing objects:  57% (23/40)\u001b[K\rremote: Compressing objects:  60% (24/40)\u001b[K\rremote: Compressing objects:  62% (25/40)\u001b[K\rremote: Compressing objects:  65% (26/40)\u001b[K\rremote: Compressing objects:  67% (27/40)\u001b[K\rremote: Compressing objects:  70% (28/40)\u001b[K\rremote: Compressing objects:  72% (29/40)\u001b[K\rremote: Compressing objects:  75% (30/40)\u001b[K\rremote: Compressing objects:  77% (31/40)\u001b[K\rremote: Compressing objects:  80% (32/40)\u001b[K\rremote: Compressing objects:  82% (33/40)\u001b[K\rremote: Compressing objects:  85% (34/40)\u001b[K\rremote: Compressing objects:  87% (35/40)\u001b[K\rremote: Compressing objects:  90% (36/40)\u001b[K\rremote: Compressing objects:  92% (37/40)\u001b[K\rremote: Compressing objects:  95% (38/40)\u001b[K\rremote: Compressing objects:  97% (39/40)\u001b[K\rremote: Compressing objects: 100% (40/40)\u001b[K\rremote: Compressing objects: 100% (40/40), done.\u001b[K\r\nReceiving objects:   0% (1/400)\rReceiving objects:   1% (4/400)\rReceiving objects:   2% (8/400)\rReceiving objects:   3% (12/400)\rReceiving objects:   4% (16/400)\rReceiving objects:   5% (20/400)\rReceiving objects:   6% (24/400)\rReceiving objects:   7% (28/400)\rReceiving objects:   8% (32/400)\rReceiving objects:   9% (36/400)\rReceiving objects:  10% (40/400)\rReceiving objects:  11% (44/400)\rReceiving objects:  12% (48/400)\rReceiving objects:  13% (52/400)\rReceiving objects:  14% (56/400)\rReceiving objects:  15% (60/400)\rReceiving objects:  16% (64/400)\rReceiving objects:  17% (68/400)\rReceiving objects:  18% (72/400)\rReceiving objects:  19% (76/400)\rReceiving objects:  20% (80/400)\rReceiving objects:  21% (84/400)\rReceiving objects:  22% (88/400)\rReceiving objects:  23% (92/400)\rReceiving objects:  24% (96/400)\rReceiving objects:  25% (100/400)\rReceiving objects:  26% (104/400)\rReceiving objects:  27% (108/400)\rReceiving objects:  28% (112/400)\rReceiving objects:  29% (116/400)\rReceiving objects:  30% (120/400)\rReceiving objects:  31% (124/400)\rReceiving objects:  32% (128/400)\rReceiving objects:  33% (132/400)\rReceiving objects:  34% (136/400)\rReceiving objects:  35% (140/400)\rReceiving objects:  36% (144/400)\rReceiving objects:  37% (148/400)\rReceiving objects:  38% (152/400)\rReceiving objects:  39% (156/400)\rReceiving objects:  40% (160/400)\rReceiving objects:  41% (164/400)\rReceiving objects:  42% (168/400)\rReceiving objects:  43% (172/400)\rReceiving objects:  44% (176/400)\rReceiving objects:  45% (180/400)\rReceiving objects:  46% (184/400)\rReceiving objects:  47% (188/400)\rReceiving objects:  48% (192/400)\rReceiving objects:  49% (196/400)\rReceiving objects:  50% (200/400)\rReceiving objects:  51% (204/400)\rReceiving objects:  52% (208/400)\rReceiving objects:  53% (212/400)\rReceiving objects:  54% (216/400)\rReceiving objects:  55% (220/400)\rReceiving objects:  56% (224/400)\rReceiving objects:  57% (228/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  58% (232/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  59% (236/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  60% (240/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  61% (244/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  62% (248/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  63% (252/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  64% (256/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  65% (260/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  66% (264/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  67% (268/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  68% (272/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  69% (276/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  70% (280/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  71% (284/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  72% (288/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  73% (292/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  74% (296/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  75% (300/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  76% (304/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  77% (308/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  78% (312/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  79% (316/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  80% (320/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  81% (324/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  82% (328/400), 4.44 MiB | 8.86 MiB/s\rremote: Total 400 (delta 43), reused 28 (delta 27), pack-reused 333 (from 1)\u001b[K\r\nReceiving objects:  83% (332/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  84% (336/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  85% (340/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  86% (344/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  87% (348/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  88% (352/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  89% (356/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  90% (360/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  91% (364/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  92% (368/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  93% (372/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  94% (376/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  95% (380/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  96% (384/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  97% (388/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  98% (392/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects:  99% (396/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects: 100% (400/400), 4.44 MiB | 8.86 MiB/s\rReceiving objects: 100% (400/400), 6.79 MiB | 10.81 MiB/s, done.\r\nResolving deltas:   0% (0/186)\rResolving deltas:   1% (2/186)\rResolving deltas:   2% (4/186)\rResolving deltas:   3% (6/186)\rResolving deltas:   4% (8/186)\rResolving deltas:   5% (10/186)\rResolving deltas:   6% (12/186)\rResolving deltas:   7% (14/186)\rResolving deltas:   8% (15/186)\rResolving deltas:   9% (17/186)\rResolving deltas:  10% (19/186)\rResolving deltas:  11% (21/186)\rResolving deltas:  12% (23/186)\rResolving deltas:  13% (25/186)\rResolving deltas:  14% (27/186)\rResolving deltas:  15% (28/186)\rResolving deltas:  16% (30/186)\rResolving deltas:  17% (32/186)\rResolving deltas:  18% (34/186)\rResolving deltas:  19% (36/186)\rResolving deltas:  20% (38/186)\rResolving deltas:  21% (40/186)\rResolving deltas:  22% (41/186)\rResolving deltas:  23% (43/186)\rResolving deltas:  24% (45/186)\rResolving deltas:  25% (47/186)\rResolving deltas:  26% (49/186)\rResolving deltas:  27% (51/186)\rResolving deltas:  28% (53/186)\rResolving deltas:  29% (54/186)\rResolving deltas:  30% (56/186)\rResolving deltas:  31% (58/186)\rResolving deltas:  32% (60/186)\rResolving deltas:  33% (62/186)\rResolving deltas:  34% (64/186)\rResolving deltas:  35% (66/186)\rResolving deltas:  36% (67/186)\rResolving deltas:  37% (69/186)\rResolving deltas:  38% (72/186)\rResolving deltas:  39% (73/186)\rResolving deltas:  40% (75/186)\rResolving deltas:  41% (77/186)\rResolving deltas:  42% (79/186)\rResolving deltas:  43% (80/186)\rResolving deltas:  44% (82/186)\rResolving deltas:  45% (84/186)\rResolving deltas:  46% (87/186)\rResolving deltas:  47% (88/186)\rResolving deltas:  48% (91/186)\rResolving deltas:  49% (92/186)\rResolving deltas:  50% (93/186)\rResolving deltas:  51% (95/186)\rResolving deltas:  52% (97/186)\rResolving deltas:  53% (99/186)\rResolving deltas:  54% (101/186)\rResolving deltas:  55% (103/186)\rResolving deltas:  56% (105/186)\rResolving deltas:  57% (107/186)\rResolving deltas:  58% (108/186)\rResolving deltas:  59% (110/186)\rResolving deltas:  60% (112/186)\rResolving deltas:  61% (114/186)\rResolving deltas:  62% (116/186)\rResolving deltas:  63% (119/186)\rResolving deltas:  64% (120/186)\rResolving deltas:  65% (122/186)\rResolving deltas:  66% (123/186)\rResolving deltas:  67% (125/186)\rResolving deltas:  68% (127/186)\rResolving deltas:  69% (129/186)\rResolving deltas:  70% (131/186)\rResolving deltas:  71% (133/186)\rResolving deltas:  72% (134/186)\rResolving deltas:  73% (136/186)\rResolving deltas:  74% (138/186)\rResolving deltas:  75% (140/186)\rResolving deltas:  76% (142/186)\rResolving deltas:  77% (144/186)\rResolving deltas:  79% (147/186)\rResolving deltas:  80% (149/186)\rResolving deltas:  81% (151/186)\rResolving deltas:  82% (153/186)\rResolving deltas:  83% (155/186)\rResolving deltas:  84% (157/186)\rResolving deltas:  85% (159/186)\rResolving deltas:  86% (160/186)\rResolving deltas:  87% (162/186)\rResolving deltas:  88% (164/186)\rResolving deltas:  89% (166/186)\rResolving deltas:  90% (169/186)\rResolving deltas:  91% (170/186)\rResolving deltas:  92% (172/186)\rResolving deltas:  93% (173/186)\rResolving deltas:  94% (175/186)\rResolving deltas:  95% (177/186)\rResolving deltas:  96% (179/186)\rResolving deltas:  97% (181/186)\rResolving deltas:  98% (183/186)\rResolving deltas:  99% (185/186)\rResolving deltas: 100% (186/186)\rResolving deltas: 100% (186/186), done.\r\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!git clone https://github.com/goodbai-nlp/AMRBART.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d0853c",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5429785f",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5c8eff",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e41e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Attempt to run example scirpt as it is\n",
    "# \n",
    "# The first part of the task is fine-tuning, so I base on the example command\n",
    "# \n",
    "# ```\n",
    "# bash train-AMRBART-large-AMRParsing.sh \"xfbai/AMRBART-large-v2\"\n",
    "# ```\n",
    "# \n",
    "# I attempt to run it as it is to check for missing dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05e0cd1",
   "metadata": {},
   "source": [
    "### Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40e6326",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97991c17",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd032a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/AMRBART/fine-tune\n/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\nTraceback (most recent call last):\r\n  File \"/content/AMRBART/fine-tune/main.py\", line 10, in <module>\r\n    import penman\r\nModuleNotFoundError: No module named 'penman'\r\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/AMRBART/fine-tune\n",
    "!bash train-AMRBART-large-AMRParsing.sh \"facebook/bart-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a100fa0",
   "metadata": {},
   "source": [
    "### Step 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3818c22d",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb449fd3",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7c4503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Install dependencies\n",
    "# \n",
    "# I install the minimum dependencies to avoid running-time errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15d3e9c",
   "metadata": {},
   "source": [
    "### Step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814775e3",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c3d009",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2ce77f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting penman\r\n  Downloading penman-1.3.1-py3-none-any.whl.metadata (7.7 kB)\r\nDownloading penman-1.3.1-py3-none-any.whl (43 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/43.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 kB\u001b[0m \u001b[31m884.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hInstalling collected packages: penman\r\nSuccessfully installed penman-1.3.1\r\n\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\nCollecting datasets==2.19.0\r\n  Downloading datasets-2.19.0-py3-none-any.whl.metadata (19 kB)\r\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (3.15.1)\r\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (1.25.2)\r\nCollecting pyarrow>=12.0.0 (from datasets==2.19.0)\r\n  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\r\nCollecting pyarrow-hotfix (from datasets==2.19.0)\r\n  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\r\nCollecting dill<0.3.9,>=0.3.0 (from datasets==2.19.0)\r\n  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\r\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (2.0.3)\r\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (2.31.0)\r\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (4.66.4)\r\nCollecting xxhash (from datasets==2.19.0)\r\n  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\r\nCollecting multiprocess (from datasets==2.19.0)\r\n  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\r\nRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets==2.19.0) (2023.6.0)\r\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (3.8.3)\r\nRequirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (0.23.3)\r\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (24.1)\r\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (6.0.1)\r\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0) (23.2.0)\r\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0) (2.1.1)\r\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0) (6.0.5)\r\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0) (4.0.3)\r\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0) (1.9.4)\r\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0) (1.4.1)\r\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0) (1.3.1)\r\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets==2.19.0) (4.12.2)\r\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.19.0) (3.7)\r\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.19.0) (2.0.7)\r\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.19.0) (2024.6.2)\r\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas->datasets==2.19.0) (2.9.0.post0)\r\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets==2.19.0) (2024.1)\r\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets==2.19.0) (2024.1)\r\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.19.0) (1.16.0)\r\nDownloading datasets-2.19.0-py3-none-any.whl (542 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/542.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m471.0/542.0 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/39.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/39.9 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/39.9 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/39.9 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/39.9 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/39.9 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/39.9 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.6/39.9 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/39.9 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.6/39.9 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m27.8/39.9 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m29.3/39.9 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m31.4/39.9 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m33.7/39.9 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m35.3/39.9 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m37.8/39.9 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/134.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\r\nDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/194.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m158.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hInstalling collected packages: xxhash, pyarrow-hotfix, pyarrow, dill, multiprocess, datasets\r\nSuccessfully installed datasets-2.19.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 pyarrow-hotfix-0.6 xxhash-3.5.0\r\n\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\nCollecting smatch\r\n  Downloading smatch-1.0.4.tar.gz (26 kB)\r\n  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\r\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\r\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \b\\\b \bdone\r\n\u001b[?25hBuilding wheels for collected packages: smatch\r\n  Building wheel for smatch (pyproject.toml) ... \u001b[?25l-\b \b\\\b \bdone\r\n\u001b[?25h  Created wheel for smatch: filename=smatch-1.0.4-py3-none-any.whl size=24052 sha256=819ec1703edf1f2dcfd6b3babd12bce7fbb4fd036101287a87ad7a915b2800fa\r\n  Stored in directory: /tmp/pip-ephem-wheel-cache-yvwzo7cj/wheels/2a/5e/2d/61b88bc74b337fbf3e998b9aa6b43bec72227e18a84a8335e8\r\nSuccessfully built smatch\r\nInstalling collected packages: smatch\r\nSuccessfully installed smatch-1.0.4\r\n\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\nCollecting rouge-score\r\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\r\n  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\r\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\r\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/site-packages (from rouge-score) (2.1.0)\r\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/site-packages (from rouge-score) (3.8.1)\r\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from rouge-score) (1.25.2)\r\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/site-packages (from rouge-score) (1.16.0)\r\nRequirement already satisfied: click in /usr/local/lib/python3.10/site-packages (from nltk->rouge-score) (8.1.7)\r\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/site-packages (from nltk->rouge-score) (1.4.2)\r\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/site-packages (from nltk->rouge-score) (2023.12.25)\r\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from nltk->rouge-score) (4.66.4)\r\nBuilding wheels for collected packages: rouge-score\r\n  Building wheel for rouge-score (pyproject.toml) ... \u001b[?25l-\b \b\\\b \bdone\r\n\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=c15ce7de762dbb8d15e553182c19cf2551cdba98d87be4d06649852dcca98abd\r\n  Stored in directory: /tmp/pip-ephem-wheel-cache-loy59btw/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\r\nSuccessfully built rouge-score\r\nInstalling collected packages: rouge-score\r\nSuccessfully installed rouge-score-0.1.2\r\n\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\nCollecting sacrebleu\r\n  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\r\n\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hCollecting portalocker (from sacrebleu)\r\n  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\r\nRequirement already satisfied: regex in /usr/local/lib/python3.10/site-packages (from sacrebleu) (2023.12.25)\r\nCollecting tabulate>=0.8.9 (from sacrebleu)\r\n  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\r\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from sacrebleu) (1.25.2)\r\nCollecting colorama (from sacrebleu)\r\n  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\r\nCollecting lxml (from sacrebleu)\r\n  Downloading lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\r\nDownloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/104.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading tabulate-0.9.0-py3-none-any.whl (35 kB)\r\nDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\r\nDownloading lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (5.0 MB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/5.0 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\r\nInstalling collected packages: tabulate, portalocker, lxml, colorama, sacrebleu\r\nSuccessfully installed colorama-0.4.6 lxml-5.3.0 portalocker-2.10.1 sacrebleu-2.4.3 tabulate-0.9.0\r\n\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\nCollecting transformers==4.21.3\r\n  Downloading transformers-4.21.3-py3-none-any.whl.metadata (81 kB)\r\n\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/82.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from transformers==4.21.3) (3.15.1)\r\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/site-packages (from transformers==4.21.3) (0.23.3)\r\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from transformers==4.21.3) (1.25.2)\r\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from transformers==4.21.3) (24.1)\r\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers==4.21.3) (6.0.1)\r\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers==4.21.3) (2023.12.25)\r\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers==4.21.3) (2.31.0)\r\nCollecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers==4.21.3)\r\n  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.5 kB)\r\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/site-packages (from transformers==4.21.3) (4.66.4)\r\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.21.3) (2023.6.0)\r\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.21.3) (4.12.2)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.21.3) (2.1.1)\r\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.21.3) (3.7)\r\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.21.3) (2.0.7)\r\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.21.3) (2024.6.2)\r\nDownloading transformers-4.21.3-py3-none-any.whl (4.7 MB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/4.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/4.7 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/4.7 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/4.7 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/4.7 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/4.7 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/4.7 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m3.8/4.7 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/6.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/6.6 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/6.6 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/6.6 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m6.2/6.6 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hInstalling collected packages: tokenizers, transformers\r\n  Attempting uninstall: tokenizers\r\n    Found existing installation: tokenizers 0.19.1\r\n    Uninstalling tokenizers-0.19.1:\r\n      Successfully uninstalled tokenizers-0.19.1\r\n  Attempting uninstall: transformers\r\n    Found existing installation: transformers 4.40.2\r\n    Uninstalling transformers-4.40.2:\r\n      Successfully uninstalled transformers-4.40.2\r\nSuccessfully installed tokenizers-0.12.1 transformers-4.21.3\r\n\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install penman\n",
    "!pip install datasets==2.19.0\n",
    "!pip install smatch\n",
    "!pip install rouge-score\n",
    "!pip install sacrebleu\n",
    "!pip install transformers==4.21.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125072c7",
   "metadata": {},
   "source": [
    "### Step 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2dddad",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782c8f07",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7f0c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before running the command, I patch the script to remove the input prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d59aba7",
   "metadata": {},
   "source": [
    "### Step 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edccece7",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743f181b",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f55875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\nModelCache=$BasePath/.cache\nDataCache=$DataPath/.cache/dump-amrparsing\n\nlr=1e-5\n\nOutputDir=${RootDir}/outputs/$Dataset-${ModelCate}-AMRParing-bsz16-lr-${lr}-UnifiedInp\n\nif [ ! -d ${OutputDir} ];then\n  mkdir -p ${OutputDir}\nelse\n  rm -rf ${OutputDir}; mkdir -p ${OutputDir};\nfi\nexport HF_DATASETS_CACHE=$DataCache\n\nif [ ! -d ${DataCache} ];then\n  mkdir -p ${DataCache}\nfi\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/AMRBART/fine-tune/train-AMRBART-large-AMRParsing.sh. CHANGING:\n",
    "\n",
    "\n",
    "if [ ! -d ${OutputDir} ];then\n",
    "  mkdir -p ${OutputDir}\n",
    "else\n",
    "  read -p \"${OutputDir} already exists, delete origin one [y/n]?\" yn\n",
    "  case $yn in\n",
    "    [Yy]* ) rm -rf ${OutputDir}; mkdir -p ${OutputDir};;\n",
    "    [Nn]* ) echo \"exiting...\"; exit;;\n",
    "    * ) echo \"Please answer yes or no.\";;\n",
    "  esac\n",
    "fi\n",
    "\n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "if [ ! -d ${OutputDir} ];then\n",
    "  mkdir -p ${OutputDir}\n",
    "else\n",
    "  rm -rf ${OutputDir}; mkdir -p ${OutputDir};\n",
    "fi\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e97530c",
   "metadata": {},
   "source": [
    "### Step 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7deec8",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553785ed",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e19b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/AMRBART/fine-tune\n/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\nTraceback (most recent call last):\r\n  File \"/content/AMRBART/fine-tune/main.py\", line 506, in <module>\r\n    main()\r\n  File \"/content/AMRBART/fine-tune/main.py\", line 79, in main\r\n    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\r\n  File \"/usr/local/lib/python3.10/site-packages/transformers/hf_argparser.py\", line 224, in parse_args_into_dataclasses\r\n    obj = dtype(**inputs)\r\n  File \"<string>\", line 110, in __init__\r\n  File \"/content/AMRBART/fine-tune/common/training_args.py\", line 1070, in __post_init__\r\n    raise ValueError(\r\nValueError: FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation (`--fp16_full_eval`) can only be used on CUDA devices.\r\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/AMRBART/fine-tune\n",
    "!bash train-AMRBART-large-AMRParsing.sh \"facebook/bart-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fc3910",
   "metadata": {},
   "source": [
    "### Step 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892d3d5f",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19a11bc",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5684116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/AMRBART\n/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/AMRBART/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045ab8b6",
   "metadata": {},
   "source": [
    "### Step 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1147c7db",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181256be",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e08b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Command-line argument `--fp16` is only available for GPU, so I patch the script again to remove it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344a6aa4",
   "metadata": {},
   "source": [
    "### Step 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eee2b8",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35356e94",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7771e0d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n    --max_steps -1 \\\n    --predict_with_generate \\\n    --smart_init False \\\n    --use_fast_tokenizer False \\\n    --logging_dir $OutputDir/logs \\\n    --logging_first_step True \\\n    --logging_steps 20 \\\n    --save_strategy \"epoch\" \\\n    --save_total_limit 1 \\\n    --seed 42 \\\n    --fp16_backend \"auto\" \\\n    --dataloader_num_workers 8 \\\n    --eval_dataloader_num_workers 2 \\\n    --load_best_model_at_end True \\\n    --metric_for_best_model \"eval_smatch\" \\\n    --include_inputs_for_metrics \\\n    --greater_is_better True \\\n    --do_train \\"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/AMRBART/fine-tune/train-AMRBART-large-AMRParsing.sh. CHANGING:\n",
    "\n",
    "\n",
    "    --save_strategy \"epoch\" \\\n",
    "    --save_total_limit 1 \\\n",
    "    --seed 42 \\\n",
    "    --fp16 \\\n",
    "    --fp16_backend \"auto\" \\\n",
    "    --dataloader_num_workers 8 \\\n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "    --save_strategy \"epoch\" \\\n",
    "    --save_total_limit 1 \\\n",
    "    --seed 42 \\\n",
    "    --fp16_backend \"auto\" \\\n",
    "    --dataloader_num_workers 8 \\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fd200f",
   "metadata": {},
   "source": [
    "### Step 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d15d507",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602590f3",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42d519c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/AMRBART/fine-tune\n/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n08/22/2024 04:45:11 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\r\n08/22/2024 04:45:11 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\r\n_n_gpu=0,\r\nadafactor=False,\r\nadam_beta1=0.9,\r\nadam_beta2=0.999,\r\nadam_epsilon=1e-08,\r\nauto_find_batch_size=False,\r\nbf16=False,\r\nbf16_full_eval=False,\r\ndata_seed=None,\r\ndataloader_drop_last=False,\r\ndataloader_num_workers=8,\r\ndataloader_pin_memory=True,\r\nddp_bucket_cap_mb=None,\r\nddp_find_unused_parameters=False,\r\ndebug=[],\r\ndeepspeed=None,\r\ndisable_tqdm=False,\r\ndo_eval=True,\r\ndo_predict=True,\r\ndo_train=True,\r\nearly_stopping=10,\r\neval_accumulation_steps=None,\r\neval_dataloader_num_workers=2,\r\neval_delay=0,\r\neval_lenpen=1.0,\r\neval_steps=None,\r\nevaluation_strategy=epoch,\r\nfp16=False,\r\nfp16_backend=auto,\r\nfp16_full_eval=False,\r\nfp16_opt_level=O1,\r\nfsdp=[],\r\nfsdp_min_num_params=0,\r\nfull_determinism=False,\r\ngeneration_max_length=1024,\r\ngeneration_num_beams=5,\r\ngradient_accumulation_steps=1,\r\ngradient_checkpointing=False,\r\ngreater_is_better=True,\r\ngroup_by_length=False,\r\nhalf_precision_backend=cuda_amp,\r\nhub_model_id=None,\r\nhub_private_repo=False,\r\nhub_strategy=every_save,\r\nhub_token=<HUB_TOKEN>,\r\nignore_data_skip=False,\r\ninclude_inputs_for_metrics=True,\r\njit_mode_eval=False,\r\nlabel_names=None,\r\nlabel_smoothing_factor=0.1,\r\nlearning_rate=1e-05,\r\nlength_column_name=length,\r\nload_best_model_at_end=True,\r\nlocal_rank=-1,\r\nlog_level=-1,\r\nlog_level_replica=-1,\r\nlog_on_each_node=True,\r\nlogging_dir=/content/AMRBART/fine-tune/outputs/LDC2020-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp/logs,\r\nlogging_first_step=True,\r\nlogging_nan_inf_filter=True,\r\nlogging_steps=20,\r\nlogging_strategy=steps,\r\nlr_scheduler_type=polynomial,\r\nmax_grad_norm=0.0,\r\nmax_steps=-1,\r\nmetric_for_best_model=eval_smatch,\r\nmp_parameters=,\r\nno_cuda=False,\r\nnum_train_epochs=30.0,\r\noptim=adamw_hf,\r\noutput_dir=/content/AMRBART/fine-tune/outputs/LDC2020-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp,\r\noverwrite_output_dir=True,\r\npast_index=-1,\r\nper_device_eval_batch_size=8,\r\nper_device_train_batch_size=16,\r\npredict_with_generate=True,\r\nprediction_loss_only=False,\r\npush_to_hub=False,\r\npush_to_hub_model_id=None,\r\npush_to_hub_organization=None,\r\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\r\nray_scope=last,\r\nremove_unused_columns=True,\r\nreport_to=['tensorboard'],\r\nresume_from_checkpoint=None,\r\nrun_name=/content/AMRBART/fine-tune/outputs/LDC2020-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp,\r\nsave_on_each_node=False,\r\nsave_steps=500,\r\nsave_strategy=epoch,\r\nsave_total_limit=1,\r\nseed=42,\r\nsharded_ddp=[],\r\nskip_memory_metrics=True,\r\nsmart_init=False,\r\nsortish_sampler=False,\r\ntask=text2amr,\r\ntf32=None,\r\ntorchdynamo=None,\r\ntpu_metrics_debug=False,\r\ntpu_num_cores=None,\r\nuse_ipex=False,\r\nuse_legacy_prediction_loop=False,\r\nwarmup_ratio=0.0,\r\nwarmup_steps=200,\r\nweight_decay=0.01,\r\nxpu_backend=None,\r\n)\r\n[INFO|hub.py:600] 2024-08-22 04:45:11,985 >> https://huggingface.co/facebook/bart-base/resolve/main/config.json not found in cache or force_download set to True, downloading to /mnt/nfs-storage/data/.cache/tmpzh9ubgh_\r\n\rDownloading config.json:   0%|          | 0.00/1.68k [00:00<?, ?B/s]\rDownloading config.json: 100%|██████████| 1.68k/1.68k [00:00<00:00, 4.61MB/s]\r\n[INFO|hub.py:613] 2024-08-22 04:45:12,153 >> storing https://huggingface.co/facebook/bart-base/resolve/main/config.json in cache at /mnt/nfs-storage/data/.cache/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\r\n[INFO|hub.py:621] 2024-08-22 04:45:12,154 >> creating metadata file for /mnt/nfs-storage/data/.cache/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\r\n[INFO|configuration_utils.py:681] 2024-08-22 04:45:12,158 >> loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /mnt/nfs-storage/data/.cache/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\r\n[INFO|configuration_utils.py:730] 2024-08-22 04:45:12,159 >> Model config BartConfig {\r\n  \"_name_or_path\": \"facebook/bart-base\",\r\n  \"activation_dropout\": 0.1,\r\n  \"activation_function\": \"gelu\",\r\n  \"add_bias_logits\": false,\r\n  \"add_final_layer_norm\": false,\r\n  \"architectures\": [\r\n    \"BartModel\"\r\n  ],\r\n  \"attention_dropout\": 0.1,\r\n  \"bos_token_id\": 0,\r\n  \"classif_dropout\": 0.1,\r\n  \"classifier_dropout\": 0.0,\r\n  \"d_model\": 768,\r\n  \"decoder_attention_heads\": 12,\r\n  \"decoder_ffn_dim\": 3072,\r\n  \"decoder_layerdrop\": 0.0,\r\n  \"decoder_layers\": 6,\r\n  \"decoder_start_token_id\": 2,\r\n  \"dropout\": 0.1,\r\n  \"early_stopping\": true,\r\n  \"encoder_attention_heads\": 12,\r\n  \"encoder_ffn_dim\": 3072,\r\n  \"encoder_layerdrop\": 0.0,\r\n  \"encoder_layers\": 6,\r\n  \"eos_token_id\": 2,\r\n  \"forced_bos_token_id\": 0,\r\n  \"forced_eos_token_id\": 2,\r\n  \"gradient_checkpointing\": false,\r\n  \"id2label\": {\r\n    \"0\": \"LABEL_0\",\r\n    \"1\": \"LABEL_1\",\r\n    \"2\": \"LABEL_2\"\r\n  },\r\n  \"init_std\": 0.02,\r\n  \"is_encoder_decoder\": true,\r\n  \"label2id\": {\r\n    \"LABEL_0\": 0,\r\n    \"LABEL_1\": 1,\r\n    \"LABEL_2\": 2\r\n  },\r\n  \"max_position_embeddings\": 1024,\r\n  \"model_type\": \"bart\",\r\n  \"no_repeat_ngram_size\": 3,\r\n  \"normalize_before\": false,\r\n  \"normalize_embedding\": true,\r\n  \"num_beams\": 4,\r\n  \"num_hidden_layers\": 6,\r\n  \"pad_token_id\": 1,\r\n  \"scale_embedding\": false,\r\n  \"task_specific_params\": {\r\n    \"summarization\": {\r\n      \"length_penalty\": 1.0,\r\n      \"max_length\": 128,\r\n      \"min_length\": 12,\r\n      \"num_beams\": 4\r\n    },\r\n    \"summarization_cnn\": {\r\n      \"length_penalty\": 2.0,\r\n      \"max_length\": 142,\r\n      \"min_length\": 56,\r\n      \"num_beams\": 4\r\n    },\r\n    \"summarization_xsum\": {\r\n      \"length_penalty\": 1.0,\r\n      \"max_length\": 62,\r\n      \"min_length\": 11,\r\n      \"num_beams\": 6\r\n    }\r\n  },\r\n  \"torch_dtype\": \"float32\",\r\n  \"transformers_version\": \"4.21.3\",\r\n  \"use_cache\": true,\r\n  \"vocab_size\": 50265\r\n}\r\n\r\n[INFO|hub.py:600] 2024-08-22 04:45:12,299 >> https://huggingface.co/facebook/bart-large/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /mnt/nfs-storage/data/.cache/tmp23wivgha\r\n\rDownloading vocab.json:   0%|          | 0.00/878k [00:00<?, ?B/s]\rDownloading vocab.json:  24%|██▍       | 210k/878k [00:00<00:00, 1.34MB/s]\rDownloading vocab.json: 100%|██████████| 878k/878k [00:00<00:00, 4.30MB/s]\r\n[INFO|hub.py:613] 2024-08-22 04:45:12,674 >> storing https://huggingface.co/facebook/bart-large/resolve/main/vocab.json in cache at /mnt/nfs-storage/data/.cache/0d6fc8b2ef1860c1f8f0baff4b021e3426cc7d11b153f98e563b799603ee2f25.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\r\n[INFO|hub.py:621] 2024-08-22 04:45:12,675 >> creating metadata file for /mnt/nfs-storage/data/.cache/0d6fc8b2ef1860c1f8f0baff4b021e3426cc7d11b153f98e563b799603ee2f25.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\r\n[INFO|hub.py:600] 2024-08-22 04:45:12,822 >> https://huggingface.co/facebook/bart-large/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /mnt/nfs-storage/data/.cache/tmpjkj8jyr3\r\n\rDownloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]\rDownloading merges.txt:  50%|█████     | 225k/446k [00:00<00:00, 1.80MB/s]\rDownloading merges.txt: 100%|██████████| 446k/446k [00:00<00:00, 2.95MB/s]\r\n[INFO|hub.py:613] 2024-08-22 04:45:13,122 >> storing https://huggingface.co/facebook/bart-large/resolve/main/merges.txt in cache at /mnt/nfs-storage/data/.cache/6e75e35f0bdd15870c98387e13b93a8e100237eb33ad99c36277a0562bd6d850.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\r\n[INFO|hub.py:621] 2024-08-22 04:45:13,124 >> creating metadata file for /mnt/nfs-storage/data/.cache/6e75e35f0bdd15870c98387e13b93a8e100237eb33ad99c36277a0562bd6d850.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\r\n[INFO|hub.py:600] 2024-08-22 04:45:13,536 >> https://huggingface.co/facebook/bart-large/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /mnt/nfs-storage/data/.cache/tmp7siqpl_s\r\n\rDownloading tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]\rDownloading tokenizer_config.json: 100%|██████████| 26.0/26.0 [00:00<00:00, 112kB/s]\r\n[INFO|hub.py:613] 2024-08-22 04:45:13,697 >> storing https://huggingface.co/facebook/bart-large/resolve/main/tokenizer_config.json in cache at /mnt/nfs-storage/data/.cache/1abf196c889c24daca2909359ca2090e5fcbfa21a9ea36d763f70adbafb500d7.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8\r\n[INFO|hub.py:621] 2024-08-22 04:45:13,698 >> creating metadata file for /mnt/nfs-storage/data/.cache/1abf196c889c24daca2909359ca2090e5fcbfa21a9ea36d763f70adbafb500d7.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8\r\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:45:13,701 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/vocab.json from cache at /mnt/nfs-storage/data/.cache/0d6fc8b2ef1860c1f8f0baff4b021e3426cc7d11b153f98e563b799603ee2f25.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\r\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:45:13,701 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/merges.txt from cache at /mnt/nfs-storage/data/.cache/6e75e35f0bdd15870c98387e13b93a8e100237eb33ad99c36277a0562bd6d850.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\r\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:45:13,701 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/added_tokens.json from cache at None\r\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:45:13,701 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/special_tokens_map.json from cache at None\r\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:45:13,701 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/tokenizer_config.json from cache at /mnt/nfs-storage/data/.cache/1abf196c889c24daca2909359ca2090e5fcbfa21a9ea36d763f70adbafb500d7.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8\r\n[INFO|hub.py:600] 2024-08-22 04:45:13,830 >> https://huggingface.co/facebook/bart-large/resolve/main/config.json not found in cache or force_download set to True, downloading to /mnt/nfs-storage/data/.cache/tmp9inv7cov\r\n\rDownloading config.json:   0%|          | 0.00/1.59k [00:00<?, ?B/s]\rDownloading config.json: 100%|██████████| 1.59k/1.59k [00:00<00:00, 6.52MB/s]\r\n[INFO|hub.py:613] 2024-08-22 04:45:14,099 >> storing https://huggingface.co/facebook/bart-large/resolve/main/config.json in cache at /mnt/nfs-storage/data/.cache/3f12fb71b844fcb7d591fdd4e55027da90d7b5dd6aa5430ad00ec6d76585f26c.bc22f15dc7ba074ee0a60bdd34c5f2fe3b6d746f89e765303376c51aff04e260\r\n[INFO|hub.py:621] 2024-08-22 04:45:14,100 >> creating metadata file for /mnt/nfs-storage/data/.cache/3f12fb71b844fcb7d591fdd4e55027da90d7b5dd6aa5430ad00ec6d76585f26c.bc22f15dc7ba074ee0a60bdd34c5f2fe3b6d746f89e765303376c51aff04e260\r\n[INFO|configuration_utils.py:681] 2024-08-22 04:45:14,108 >> loading configuration file https://huggingface.co/facebook/bart-large/resolve/main/config.json from cache at /mnt/nfs-storage/data/.cache/3f12fb71b844fcb7d591fdd4e55027da90d7b5dd6aa5430ad00ec6d76585f26c.bc22f15dc7ba074ee0a60bdd34c5f2fe3b6d746f89e765303376c51aff04e260\r\n[INFO|configuration_utils.py:730] 2024-08-22 04:45:14,122 >> Model config BartConfig {\r\n  \"_name_or_path\": \"facebook/bart-large\",\r\n  \"activation_dropout\": 0.1,\r\n  \"activation_function\": \"gelu\",\r\n  \"add_bias_logits\": false,\r\n  \"add_final_layer_norm\": false,\r\n  \"architectures\": [\r\n    \"BartModel\"\r\n  ],\r\n  \"attention_dropout\": 0.1,\r\n  \"bos_token_id\": 0,\r\n  \"classif_dropout\": 0.1,\r\n  \"classifier_dropout\": 0.0,\r\n  \"d_model\": 1024,\r\n  \"decoder_attention_heads\": 16,\r\n  \"decoder_ffn_dim\": 4096,\r\n  \"decoder_layerdrop\": 0.0,\r\n  \"decoder_layers\": 12,\r\n  \"decoder_start_token_id\": 2,\r\n  \"dropout\": 0.1,\r\n  \"early_stopping\": true,\r\n  \"encoder_attention_heads\": 16,\r\n  \"encoder_ffn_dim\": 4096,\r\n  \"encoder_layerdrop\": 0.0,\r\n  \"encoder_layers\": 12,\r\n  \"eos_token_id\": 2,\r\n  \"forced_bos_token_id\": 0,\r\n  \"forced_eos_token_id\": 2,\r\n  \"gradient_checkpointing\": false,\r\n  \"id2label\": {\r\n    \"0\": \"LABEL_0\",\r\n    \"1\": \"LABEL_1\",\r\n    \"2\": \"LABEL_2\"\r\n  },\r\n  \"init_std\": 0.02,\r\n  \"is_encoder_decoder\": true,\r\n  \"label2id\": {\r\n    \"LABEL_0\": 0,\r\n    \"LABEL_1\": 1,\r\n    \"LABEL_2\": 2\r\n  },\r\n  \"max_position_embeddings\": 1024,\r\n  \"model_type\": \"bart\",\r\n  \"no_repeat_ngram_size\": 3,\r\n  \"normalize_before\": false,\r\n  \"num_beams\": 4,\r\n  \"num_hidden_layers\": 12,\r\n  \"pad_token_id\": 1,\r\n  \"scale_embedding\": false,\r\n  \"task_specific_params\": {\r\n    \"summarization\": {\r\n      \"length_penalty\": 1.0,\r\n      \"max_length\": 128,\r\n      \"min_length\": 12,\r\n      \"num_beams\": 4\r\n    },\r\n    \"summarization_cnn\": {\r\n      \"length_penalty\": 2.0,\r\n      \"max_length\": 142,\r\n      \"min_length\": 56,\r\n      \"num_beams\": 4\r\n    },\r\n    \"summarization_xsum\": {\r\n      \"length_penalty\": 1.0,\r\n      \"max_length\": 62,\r\n      \"min_length\": 11,\r\n      \"num_beams\": 6\r\n    }\r\n  },\r\n  \"transformers_version\": \"4.21.3\",\r\n  \"use_cache\": true,\r\n  \"vocab_size\": 50265\r\n}\r\n\r\n[WARNING|tokenization_utils_base.py:1896] 2024-08-22 04:45:14,129 >> The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \r\nThe tokenizer class you load from this checkpoint is 'BartTokenizer'. \r\nThe class this function is called from is 'AMRBartTokenizer'.\r\nAdded 2965 AMR tokens\r\n[INFO|hub.py:600] 2024-08-22 04:45:14,573 >> https://huggingface.co/facebook/bart-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /mnt/nfs-storage/data/.cache/tmp8ks2_cn9\r\n\rDownloading pytorch_model.bin:   0%|          | 0.00/532M [00:00<?, ?B/s]\rDownloading pytorch_model.bin:   0%|          | 51.0k/532M [00:00<20:11, 460kB/s]\rDownloading pytorch_model.bin:   0%|          | 297k/532M [00:00<06:18, 1.47MB/s]\rDownloading pytorch_model.bin:   0%|          | 0.99M/532M [00:00<02:20, 3.96MB/s]\rDownloading pytorch_model.bin:   0%|          | 2.57M/532M [00:00<01:04, 8.67MB/s]\rDownloading pytorch_model.bin:   1%|          | 4.52M/532M [00:00<00:43, 12.8MB/s]\rDownloading pytorch_model.bin:   1%|          | 6.55M/532M [00:00<00:36, 15.1MB/s]\rDownloading pytorch_model.bin:   2%|▏         | 8.90M/532M [00:00<00:30, 18.1MB/s]\rDownloading pytorch_model.bin:   2%|▏         | 10.8M/532M [00:00<00:29, 18.8MB/s]\rDownloading pytorch_model.bin:   3%|▎         | 14.2M/532M [00:00<00:22, 23.9MB/s]\rDownloading pytorch_model.bin:   3%|▎         | 17.4M/532M [00:01<00:20, 26.8MB/s]\rDownloading pytorch_model.bin:   4%|▍         | 21.4M/532M [00:01<00:17, 31.5MB/s]\rDownloading pytorch_model.bin:   5%|▍         | 25.1M/532M [00:01<00:15, 33.4MB/s]\rDownloading pytorch_model.bin:   5%|▌         | 28.7M/532M [00:01<00:15, 34.8MB/s]\rDownloading pytorch_model.bin:   6%|▌         | 32.2M/532M [00:01<00:14, 35.4MB/s]\rDownloading pytorch_model.bin:   7%|▋         | 35.6M/532M [00:01<00:15, 34.5MB/s]\rDownloading pytorch_model.bin:   7%|▋         | 38.9M/532M [00:01<00:14, 34.6MB/s]\rDownloading pytorch_model.bin:   8%|▊         | 42.2M/532M [00:01<00:17, 29.5MB/s]\rDownloading pytorch_model.bin:   8%|▊         | 45.1M/532M [00:01<00:18, 27.4MB/s]\rDownloading pytorch_model.bin:   9%|▉         | 47.9M/532M [00:02<00:19, 26.5MB/s]\rDownloading pytorch_model.bin:   9%|▉         | 50.5M/532M [00:02<00:19, 25.8MB/s]\rDownloading pytorch_model.bin:  10%|▉         | 53.0M/532M [00:02<00:19, 25.6MB/s]\rDownloading pytorch_model.bin:  10%|█         | 55.4M/532M [00:02<00:20, 23.8MB/s]\rDownloading pytorch_model.bin:  11%|█         | 58.0M/532M [00:02<00:20, 24.7MB/s]\rDownloading pytorch_model.bin:  11%|█▏        | 61.1M/532M [00:02<00:18, 26.9MB/s]\rDownloading pytorch_model.bin:  12%|█▏        | 63.7M/532M [00:02<00:19, 25.4MB/s]\rDownloading pytorch_model.bin:  12%|█▏        | 66.2M/532M [00:02<00:20, 23.7MB/s]\rDownloading pytorch_model.bin:  13%|█▎        | 68.9M/532M [00:02<00:19, 24.8MB/s]\rDownloading pytorch_model.bin:  13%|█▎        | 71.3M/532M [00:03<00:19, 25.0MB/s]\rDownloading pytorch_model.bin:  14%|█▍        | 73.7M/532M [00:03<00:19, 24.7MB/s]\rDownloading pytorch_model.bin:  14%|█▍        | 76.1M/532M [00:03<00:20, 23.3MB/s]\rDownloading pytorch_model.bin:  15%|█▍        | 78.5M/532M [00:03<00:19, 24.0MB/s]\rDownloading pytorch_model.bin:  15%|█▌        | 81.0M/532M [00:03<00:19, 24.4MB/s]\rDownloading pytorch_model.bin:  16%|█▌        | 83.4M/532M [00:03<00:19, 24.6MB/s]\rDownloading pytorch_model.bin:  16%|█▌        | 86.0M/532M [00:03<00:18, 25.3MB/s]\rDownloading pytorch_model.bin:  17%|█▋        | 88.4M/532M [00:03<00:18, 25.4MB/s]\rDownloading pytorch_model.bin:  17%|█▋        | 90.9M/532M [00:03<00:18, 24.6MB/s]\rDownloading pytorch_model.bin:  18%|█▊        | 93.3M/532M [00:03<00:18, 24.9MB/s]\rDownloading pytorch_model.bin:  18%|█▊        | 95.7M/532M [00:04<00:18, 24.9MB/s]\rDownloading pytorch_model.bin:  19%|█▊        | 98.7M/532M [00:04<00:16, 26.8MB/s]\rDownloading pytorch_model.bin:  19%|█▉        | 102M/532M [00:04<00:16, 28.2MB/s] \rDownloading pytorch_model.bin:  20%|█▉        | 104M/532M [00:04<00:17, 25.3MB/s]\rDownloading pytorch_model.bin:  20%|██        | 108M/532M [00:04<00:15, 29.3MB/s]\rDownloading pytorch_model.bin:  21%|██        | 112M/532M [00:04<00:14, 31.4MB/s]\rDownloading pytorch_model.bin:  22%|██▏       | 115M/532M [00:04<00:13, 31.8MB/s]\rDownloading pytorch_model.bin:  22%|██▏       | 118M/532M [00:04<00:14, 30.5MB/s]\rDownloading pytorch_model.bin:  23%|██▎       | 121M/532M [00:04<00:13, 31.7MB/s]\rDownloading pytorch_model.bin:  23%|██▎       | 124M/532M [00:05<00:14, 30.2MB/s]\rDownloading pytorch_model.bin:  24%|██▍       | 127M/532M [00:05<00:14, 29.3MB/s]\rDownloading pytorch_model.bin:  25%|██▍       | 131M/532M [00:05<00:13, 30.7MB/s]\rDownloading pytorch_model.bin:  25%|██▌       | 134M/532M [00:05<00:12, 32.1MB/s]\rDownloading pytorch_model.bin:  26%|██▌       | 137M/532M [00:05<00:12, 32.9MB/s]\rDownloading pytorch_model.bin:  27%|██▋       | 141M/532M [00:05<00:11, 34.9MB/s]\rDownloading pytorch_model.bin:  27%|██▋       | 145M/532M [00:05<00:10, 37.2MB/s]\rDownloading pytorch_model.bin:  28%|██▊       | 149M/532M [00:05<00:11, 35.0MB/s]\rDownloading pytorch_model.bin:  29%|██▊       | 153M/532M [00:05<00:10, 36.9MB/s]\rDownloading pytorch_model.bin:  29%|██▉       | 156M/532M [00:06<00:10, 37.3MB/s]\rDownloading pytorch_model.bin:  30%|███       | 161M/532M [00:06<00:09, 39.5MB/s]\rDownloading pytorch_model.bin:  31%|███       | 165M/532M [00:06<00:09, 41.5MB/s]\rDownloading pytorch_model.bin:  32%|███▏      | 169M/532M [00:06<00:09, 42.2MB/s]\rDownloading pytorch_model.bin:  33%|███▎      | 173M/532M [00:06<00:09, 39.4MB/s]\rDownloading pytorch_model.bin:  33%|███▎      | 177M/532M [00:06<00:09, 39.5MB/s]\rDownloading pytorch_model.bin:  34%|███▍      | 182M/532M [00:06<00:08, 41.1MB/s]\rDownloading pytorch_model.bin:  35%|███▍      | 186M/532M [00:06<00:09, 38.2MB/s]\rDownloading pytorch_model.bin:  36%|███▌      | 189M/532M [00:06<00:10, 33.7MB/s]\rDownloading pytorch_model.bin:  36%|███▌      | 193M/532M [00:07<00:10, 33.2MB/s]\rDownloading pytorch_model.bin:  37%|███▋      | 196M/532M [00:07<00:11, 31.6MB/s]\rDownloading pytorch_model.bin:  37%|███▋      | 199M/532M [00:07<00:11, 30.2MB/s]\rDownloading pytorch_model.bin:  38%|███▊      | 202M/532M [00:07<00:11, 30.2MB/s]\rDownloading pytorch_model.bin:  38%|███▊      | 205M/532M [00:07<00:11, 29.8MB/s]\rDownloading pytorch_model.bin:  39%|███▉      | 208M/532M [00:07<00:10, 31.0MB/s]\rDownloading pytorch_model.bin:  40%|███▉      | 211M/532M [00:07<00:11, 28.2MB/s]\rDownloading pytorch_model.bin:  40%|████      | 214M/532M [00:07<00:11, 28.2MB/s]\rDownloading pytorch_model.bin:  41%|████      | 217M/532M [00:07<00:11, 29.1MB/s]\rDownloading pytorch_model.bin:  41%|████▏     | 220M/532M [00:08<00:11, 28.6MB/s]\rDownloading pytorch_model.bin:  42%|████▏     | 223M/532M [00:08<00:10, 30.9MB/s]\rDownloading pytorch_model.bin:  43%|████▎     | 227M/532M [00:08<00:09, 34.0MB/s]\rDownloading pytorch_model.bin:  43%|████▎     | 231M/532M [00:08<00:08, 35.9MB/s]\rDownloading pytorch_model.bin:  44%|████▍     | 234M/532M [00:08<00:09, 34.1MB/s]\rDownloading pytorch_model.bin:  45%|████▍     | 238M/532M [00:08<00:11, 27.3MB/s]\rDownloading pytorch_model.bin:  45%|████▌     | 240M/532M [00:08<00:11, 27.1MB/s]\rDownloading pytorch_model.bin:  46%|████▌     | 243M/532M [00:08<00:11, 26.9MB/s]\rDownloading pytorch_model.bin:  46%|████▌     | 246M/532M [00:08<00:11, 25.9MB/s]\rDownloading pytorch_model.bin:  47%|████▋     | 248M/532M [00:09<00:11, 25.8MB/s]\rDownloading pytorch_model.bin:  47%|████▋     | 251M/532M [00:09<00:11, 24.7MB/s]\rDownloading pytorch_model.bin:  48%|████▊     | 253M/532M [00:09<00:11, 24.5MB/s]\rDownloading pytorch_model.bin:  48%|████▊     | 256M/532M [00:09<00:12, 23.5MB/s]\rDownloading pytorch_model.bin:  48%|████▊     | 258M/532M [00:09<00:12, 23.1MB/s]\rDownloading pytorch_model.bin:  49%|████▉     | 261M/532M [00:09<00:11, 24.5MB/s]\rDownloading pytorch_model.bin:  50%|████▉     | 264M/532M [00:09<00:10, 26.7MB/s]\rDownloading pytorch_model.bin:  50%|█████     | 267M/532M [00:09<00:09, 29.1MB/s]\rDownloading pytorch_model.bin:  51%|█████     | 270M/532M [00:09<00:08, 30.7MB/s]\rDownloading pytorch_model.bin:  51%|█████▏    | 273M/532M [00:10<00:09, 30.0MB/s]\rDownloading pytorch_model.bin:  52%|█████▏    | 277M/532M [00:10<00:08, 31.9MB/s]\rDownloading pytorch_model.bin:  53%|█████▎    | 280M/532M [00:10<00:08, 31.8MB/s]\rDownloading pytorch_model.bin:  53%|█████▎    | 283M/532M [00:10<00:10, 26.1MB/s]\rDownloading pytorch_model.bin:  54%|█████▎    | 286M/532M [00:10<00:10, 24.6MB/s]\rDownloading pytorch_model.bin:  54%|█████▍    | 289M/532M [00:10<00:08, 28.6MB/s]\rDownloading pytorch_model.bin:  55%|█████▌    | 293M/532M [00:10<00:08, 30.7MB/s]\rDownloading pytorch_model.bin:  56%|█████▌    | 297M/532M [00:10<00:07, 32.9MB/s]\rDownloading pytorch_model.bin:  56%|█████▋    | 300M/532M [00:10<00:07, 33.3MB/s]\rDownloading pytorch_model.bin:  57%|█████▋    | 303M/532M [00:11<00:07, 33.0MB/s]\rDownloading pytorch_model.bin:  58%|█████▊    | 307M/532M [00:11<00:06, 36.4MB/s]\rDownloading pytorch_model.bin:  59%|█████▊    | 311M/532M [00:11<00:06, 37.3MB/s]\rDownloading pytorch_model.bin:  59%|█████▉    | 316M/532M [00:11<00:05, 39.7MB/s]\rDownloading pytorch_model.bin:  60%|██████    | 320M/532M [00:11<00:05, 41.6MB/s]\rDownloading pytorch_model.bin:  61%|██████    | 324M/532M [00:11<00:05, 37.9MB/s]\rDownloading pytorch_model.bin:  62%|██████▏   | 328M/532M [00:11<00:06, 35.7MB/s]\rDownloading pytorch_model.bin:  62%|██████▏   | 331M/532M [00:11<00:06, 34.1MB/s]\rDownloading pytorch_model.bin:  63%|██████▎   | 334M/532M [00:11<00:06, 34.2MB/s]\rDownloading pytorch_model.bin:  64%|██████▎   | 338M/532M [00:12<00:05, 35.1MB/s]\rDownloading pytorch_model.bin:  64%|██████▍   | 341M/532M [00:12<00:05, 34.7MB/s]\rDownloading pytorch_model.bin:  65%|██████▍   | 345M/532M [00:12<00:05, 33.8MB/s]\rDownloading pytorch_model.bin:  65%|██████▌   | 348M/532M [00:12<00:05, 34.6MB/s]\rDownloading pytorch_model.bin:  66%|██████▌   | 352M/532M [00:12<00:05, 31.6MB/s]\rDownloading pytorch_model.bin:  67%|██████▋   | 355M/532M [00:12<00:06, 30.1MB/s]\rDownloading pytorch_model.bin:  67%|██████▋   | 358M/532M [00:12<00:06, 28.7MB/s]\rDownloading pytorch_model.bin:  68%|██████▊   | 360M/532M [00:12<00:06, 28.9MB/s]\rDownloading pytorch_model.bin:  68%|██████▊   | 363M/532M [00:12<00:06, 28.5MB/s]\rDownloading pytorch_model.bin:  69%|██████▉   | 366M/532M [00:13<00:06, 28.3MB/s]\rDownloading pytorch_model.bin:  69%|██████▉   | 369M/532M [00:13<00:05, 28.8MB/s]\rDownloading pytorch_model.bin:  70%|██████▉   | 371M/532M [00:13<00:05, 28.2MB/s]\rDownloading pytorch_model.bin:  70%|███████   | 374M/532M [00:13<00:05, 28.9MB/s]\rDownloading pytorch_model.bin:  71%|███████   | 377M/532M [00:13<00:05, 29.8MB/s]\rDownloading pytorch_model.bin:  72%|███████▏  | 381M/532M [00:13<00:04, 31.7MB/s]\rDownloading pytorch_model.bin:  72%|███████▏  | 384M/532M [00:13<00:05, 30.8MB/s]\rDownloading pytorch_model.bin:  73%|███████▎  | 387M/532M [00:13<00:05, 29.7MB/s]\rDownloading pytorch_model.bin:  73%|███████▎  | 390M/532M [00:13<00:04, 30.3MB/s]\rDownloading pytorch_model.bin:  74%|███████▍  | 393M/532M [00:13<00:04, 32.4MB/s]\rDownloading pytorch_model.bin:  75%|███████▍  | 397M/532M [00:14<00:04, 32.8MB/s]\rDownloading pytorch_model.bin:  75%|███████▌  | 400M/532M [00:14<00:03, 34.8MB/s]\rDownloading pytorch_model.bin:  76%|███████▌  | 404M/532M [00:14<00:03, 36.0MB/s]\rDownloading pytorch_model.bin:  77%|███████▋  | 408M/532M [00:14<00:03, 38.1MB/s]\rDownloading pytorch_model.bin:  77%|███████▋  | 412M/532M [00:14<00:03, 38.1MB/s]\rDownloading pytorch_model.bin:  78%|███████▊  | 416M/532M [00:14<00:03, 36.6MB/s]\rDownloading pytorch_model.bin:  79%|███████▉  | 419M/532M [00:14<00:03, 33.4MB/s]\rDownloading pytorch_model.bin:  79%|███████▉  | 422M/532M [00:14<00:03, 32.8MB/s]\rDownloading pytorch_model.bin:  80%|████████  | 426M/532M [00:14<00:03, 33.6MB/s]\rDownloading pytorch_model.bin:  81%|████████  | 429M/532M [00:15<00:03, 32.5MB/s]\rDownloading pytorch_model.bin:  81%|████████▏ | 432M/532M [00:15<00:03, 32.0MB/s]\rDownloading pytorch_model.bin:  82%|████████▏ | 435M/532M [00:15<00:03, 31.4MB/s]\rDownloading pytorch_model.bin:  82%|████████▏ | 438M/532M [00:15<00:03, 29.7MB/s]\rDownloading pytorch_model.bin:  83%|████████▎ | 441M/532M [00:15<00:03, 25.8MB/s]\rDownloading pytorch_model.bin:  84%|████████▎ | 445M/532M [00:15<00:03, 30.2MB/s]\rDownloading pytorch_model.bin:  84%|████████▍ | 449M/532M [00:15<00:02, 32.9MB/s]\rDownloading pytorch_model.bin:  85%|████████▌ | 452M/532M [00:15<00:02, 33.5MB/s]\rDownloading pytorch_model.bin:  86%|████████▌ | 456M/532M [00:15<00:02, 35.2MB/s]\rDownloading pytorch_model.bin:  86%|████████▋ | 460M/532M [00:16<00:02, 35.9MB/s]\rDownloading pytorch_model.bin:  87%|████████▋ | 463M/532M [00:16<00:02, 34.8MB/s]\rDownloading pytorch_model.bin:  88%|████████▊ | 467M/532M [00:16<00:01, 34.8MB/s]\rDownloading pytorch_model.bin:  88%|████████▊ | 470M/532M [00:16<00:02, 32.3MB/s]\rDownloading pytorch_model.bin:  89%|████████▉ | 474M/532M [00:16<00:01, 34.3MB/s]\rDownloading pytorch_model.bin:  90%|████████▉ | 478M/532M [00:16<00:01, 36.5MB/s]\rDownloading pytorch_model.bin:  90%|█████████ | 481M/532M [00:16<00:01, 36.6MB/s]\rDownloading pytorch_model.bin:  91%|█████████ | 485M/532M [00:16<00:01, 37.0MB/s]\rDownloading pytorch_model.bin:  92%|█████████▏| 489M/532M [00:16<00:01, 38.7MB/s]\rDownloading pytorch_model.bin:  93%|█████████▎| 493M/532M [00:16<00:00, 40.5MB/s]\rDownloading pytorch_model.bin:  93%|█████████▎| 497M/532M [00:17<00:00, 39.0MB/s]\rDownloading pytorch_model.bin:  94%|█████████▍| 502M/532M [00:17<00:00, 43.2MB/s]\rDownloading pytorch_model.bin:  95%|█████████▌| 507M/532M [00:17<00:00, 45.9MB/s]\rDownloading pytorch_model.bin:  96%|█████████▌| 512M/532M [00:17<00:00, 45.6MB/s]\rDownloading pytorch_model.bin:  97%|█████████▋| 516M/532M [00:17<00:00, 43.5MB/s]\rDownloading pytorch_model.bin:  98%|█████████▊| 520M/532M [00:17<00:00, 39.7MB/s]\rDownloading pytorch_model.bin:  99%|█████████▊| 524M/532M [00:17<00:00, 35.1MB/s]\rDownloading pytorch_model.bin:  99%|█████████▉| 528M/532M [00:17<00:00, 33.6MB/s]\rDownloading pytorch_model.bin: 100%|█████████▉| 531M/532M [00:18<00:00, 32.5MB/s]\rDownloading pytorch_model.bin: 100%|██████████| 532M/532M [00:18<00:00, 30.8MB/s]\r\n[INFO|hub.py:613] 2024-08-22 04:45:33,121 >> storing https://huggingface.co/facebook/bart-base/resolve/main/pytorch_model.bin in cache at /mnt/nfs-storage/data/.cache/486355ec722ef05fd480e999d4c763be56549ae930f6a3742ee721a5d2a05647.f2f355ad2775769afc60592b43a46d72ca548375e3a1d65f381a751e711cbadd\r\n[INFO|hub.py:621] 2024-08-22 04:45:33,124 >> creating metadata file for /mnt/nfs-storage/data/.cache/486355ec722ef05fd480e999d4c763be56549ae930f6a3742ee721a5d2a05647.f2f355ad2775769afc60592b43a46d72ca548375e3a1d65f381a751e711cbadd\r\n[INFO|modeling_utils.py:2041] 2024-08-22 04:45:33,131 >> loading weights file https://huggingface.co/facebook/bart-base/resolve/main/pytorch_model.bin from cache at /mnt/nfs-storage/data/.cache/486355ec722ef05fd480e999d4c763be56549ae930f6a3742ee721a5d2a05647.f2f355ad2775769afc60592b43a46d72ca548375e3a1d65f381a751e711cbadd\r\n[INFO|modeling_utils.py:2435] 2024-08-22 04:45:38,971 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\r\n\r\n[INFO|modeling_utils.py:2443] 2024-08-22 04:45:38,973 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\r\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\r\nDataset cache dir: /mnt/nfs-storage/data/.cache\r\n/usr/local/lib/python3.10/site-packages/datasets/load.py:929: FutureWarning: The repository for data contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /content/AMRBART/fine-tune/data_interface/data.py\r\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\r\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\r\n  warnings.warn(\r\nTraceback (most recent call last):\r\n  File \"/content/AMRBART/fine-tune/main.py\", line 506, in <module>\r\n    main()\r\n  File \"/content/AMRBART/fine-tune/main.py\", line 194, in main\r\n    raw_datasets = DataSetCate(tokenizer, data_args, model_args)\r\n  File \"/content/AMRBART/fine-tune/data_interface/dataset.py\", line 63, in __init__\r\n    self.datasets = load_dataset(\r\n  File \"/usr/local/lib/python3.10/site-packages/datasets/load.py\", line 2587, in load_dataset\r\n    builder_instance = load_dataset_builder(\r\n  File \"/usr/local/lib/python3.10/site-packages/datasets/load.py\", line 2296, in load_dataset_builder\r\n    builder_instance: DatasetBuilder = builder_cls(\r\n  File \"/usr/local/lib/python3.10/site-packages/datasets/builder.py\", line 360, in __init__\r\n    data_files = DataFilesDict.from_patterns(\r\n  File \"/usr/local/lib/python3.10/site-packages/datasets/data_files.py\", line 715, in from_patterns\r\n    DataFilesList.from_patterns(\r\n  File \"/usr/local/lib/python3.10/site-packages/datasets/data_files.py\", line 620, in from_patterns\r\n    resolve_pattern(\r\n  File \"/usr/local/lib/python3.10/site-packages/datasets/data_files.py\", line 407, in resolve_pattern\r\n    raise FileNotFoundError(error_msg)\r\nFileNotFoundError: Unable to find '/content/AMRBART/fine-tune/data/LDC2020/train.jsonl'\r\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/AMRBART/fine-tune\n",
    "!bash train-AMRBART-large-AMRParsing.sh \"facebook/bart-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699b9d2c",
   "metadata": {},
   "source": [
    "### Step 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56749b50",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636ac68d",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b82f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now the code asks for the data, so let's work on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55591f54",
   "metadata": {},
   "source": [
    "### Step 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2918712a",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c53b2b4",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dcda5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00d7e4f",
   "metadata": {},
   "source": [
    "### Step 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df87e8e1",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36b58c6",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a0bbd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Download data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f758f6ec",
   "metadata": {},
   "source": [
    "### Step 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beba46d7",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c40da2",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e7b3ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Requirement already satisfied: gdown in /usr/local/lib/python3.10/site-packages (5.1.0)\r\nCollecting gdown\r\n  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\r\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/site-packages (from gdown) (4.12.3)\r\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from gdown) (3.15.1)\r\nRequirement already satisfied: requests[socks] in /usr/local/lib/python3.10/site-packages (from gdown) (2.31.0)\r\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from gdown) (4.66.4)\r\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown) (2.1.1)\r\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown) (3.7)\r\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown) (2.0.7)\r\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.6.2)\r\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\r\nDownloading gdown-5.2.0-py3-none-any.whl (18 kB)\r\nInstalling collected packages: gdown\r\n  Attempting uninstall: gdown\r\n    Found existing installation: gdown 5.1.0\r\n    Uninstalling gdown-5.1.0:\r\n      Successfully uninstalled gdown-5.1.0\r\nSuccessfully installed gdown-5.2.0\r\n\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!# use gdown to download files\n",
    "!pip install gdown --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444e0b11",
   "metadata": {},
   "source": [
    "### Step 19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abee13a",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6763155",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683430c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\nRetrieving folder contents\n/content/AMRBART/fine-tune/data\nProcessing file 141pCNafqMmjx6uZ5IlNSHQmmdfTCqNLW dev.jsonl\nProcessing file 1vSSHzz3ns6DQzWjBhZkI37-einZQuZXb infer.txt\nProcessing file 1jjka6n4pbdbih3LUuJII-UYTloNgv_hp test.jsonl\nProcessing file 1TDkzzq6xOthZDC-UKuyHwLdeaok8D6GQ train.jsonl\nRetrieving folder contents completed\nBuilding directory structure\nBuilding directory structure completed\nDownloading...\nFrom: https://drive.google.com/uc?id=141pCNafqMmjx6uZ5IlNSHQmmdfTCqNLW\nTo: /content/AMRBART/fine-tune/data/AMRBART/dev.jsonl\n\r  0%|          | 0.00/8.22k [00:00<?, ?B/s]\r100%|██████████| 8.22k/8.22k [00:00<00:00, 1.69MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1vSSHzz3ns6DQzWjBhZkI37-einZQuZXb\nTo: /content/AMRBART/fine-tune/data/AMRBART/infer.txt\n\r  0%|          | 0.00/1.17k [00:00<?, ?B/s]\r100%|██████████| 1.17k/1.17k [00:00<00:00, 1.83MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1jjka6n4pbdbih3LUuJII-UYTloNgv_hp\nTo: /content/AMRBART/fine-tune/data/AMRBART/test.jsonl\n\r  0%|          | 0.00/6.17k [00:00<?, ?B/s]\r100%|██████████| 6.17k/6.17k [00:00<00:00, 3.47MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1TDkzzq6xOthZDC-UKuyHwLdeaok8D6GQ\nTo: /content/AMRBART/fine-tune/data/AMRBART/train.jsonl\n\r  0%|          | 0.00/7.86k [00:00<?, ?B/s]\r100%|██████████| 7.86k/7.86k [00:00<00:00, 4.82MB/s]\nDownload completed\n['/content/AMRBART/fine-tune/data/AMRBART/dev.jsonl',\n '/content/AMRBART/fine-tune/data/AMRBART/infer.txt',\n '/content/AMRBART/fine-tune/data/AMRBART/test.jsonl',\n '/content/AMRBART/fine-tune/data/AMRBART/train.jsonl']\n\nYour command has already run for 300 seconds. It is still running. You can continue to wait or interrupt it with \"action\": {\"type\": \"interrupt\"} or \"action\": {\"type\": \"continue_wait\"}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "from pathlib import Path\n",
    "\n",
    "Path(\"/content/AMRBART/fine-tune/data\").mkdir(exist_ok=True)\n",
    "\n",
    "%cd /content/AMRBART/fine-tune/data\n",
    "gdown.download_folder(\"https://drive.google.com/drive/folders/1tgSf24gsTrSe6GRt-eSuz8vvB51Br9hG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7b9dd2",
   "metadata": {},
   "source": [
    "### Step 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af762913",
   "metadata": {},
   "source": [
    "**Thought**: [continued after timeout]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8a6dd8",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a790a532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Error: NameError: name 'continue_wait' is not defined\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcontinue_wait\u001b[49m\n\u001b[0;31mNameError\u001b[0m: name 'continue_wait' is not defined"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "continue_wait"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab8b643",
   "metadata": {},
   "source": [
    "### Step 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f14a85",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af23f43",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaeead3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The script looks for a validation dataset named \"val.jsnol\". Assuming \"dev.jsonl\" contains the this dataset, I rename it accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccef45da",
   "metadata": {},
   "source": [
    "### Step 22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76a792a",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102f61ce",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab67b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!mv AMRBART/dev.jsonl AMRBART/val.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef08d7c",
   "metadata": {},
   "source": [
    "### Step 23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2e79b8",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad3ba7e",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b08ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The model expects sentences to be under the `sent` key, but in the downloaded datasets the key is `sentence`, so I replace it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5c85b3",
   "metadata": {},
   "source": [
    "### Step 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c0ee4a",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62192cb3",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9ccf4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/AMRBART/fine-tune/data/AMRBART\n/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/AMRBART/fine-tune/data/AMRBART\n",
    "!sed -i 's/\"sentence\":/\"sent\":/' train.jsonl\n",
    "!sed -i 's/\"sentence\":/\"sent\":/' test.jsonl\n",
    "!sed -i 's/\"sentence\":/\"sent\":/' val.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b776c894",
   "metadata": {},
   "source": [
    "### Step 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f7418e",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a141ec5",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a65828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/AMRBART/fine-tune\n/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n08/22/2024 04:46:28 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\r\n08/22/2024 04:46:28 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\r\n_n_gpu=0,\r\nadafactor=False,\r\nadam_beta1=0.9,\r\nadam_beta2=0.999,\r\nadam_epsilon=1e-08,\r\nauto_find_batch_size=False,\r\nbf16=False,\r\nbf16_full_eval=False,\r\ndata_seed=None,\r\ndataloader_drop_last=False,\r\ndataloader_num_workers=8,\r\ndataloader_pin_memory=True,\r\nddp_bucket_cap_mb=None,\r\nddp_find_unused_parameters=False,\r\ndebug=[],\r\ndeepspeed=None,\r\ndisable_tqdm=False,\r\ndo_eval=True,\r\ndo_predict=True,\r\ndo_train=True,\r\nearly_stopping=10,\r\neval_accumulation_steps=None,\r\neval_dataloader_num_workers=2,\r\neval_delay=0,\r\neval_lenpen=1.0,\r\neval_steps=None,\r\nevaluation_strategy=epoch,\r\nfp16=False,\r\nfp16_backend=auto,\r\nfp16_full_eval=False,\r\nfp16_opt_level=O1,\r\nfsdp=[],\r\nfsdp_min_num_params=0,\r\nfull_determinism=False,\r\ngeneration_max_length=1024,\r\ngeneration_num_beams=5,\r\ngradient_accumulation_steps=1,\r\ngradient_checkpointing=False,\r\ngreater_is_better=True,\r\ngroup_by_length=False,\r\nhalf_precision_backend=cuda_amp,\r\nhub_model_id=None,\r\nhub_private_repo=False,\r\nhub_strategy=every_save,\r\nhub_token=<HUB_TOKEN>,\r\nignore_data_skip=False,\r\ninclude_inputs_for_metrics=True,\r\njit_mode_eval=False,\r\nlabel_names=None,\r\nlabel_smoothing_factor=0.1,\r\nlearning_rate=1e-05,\r\nlength_column_name=length,\r\nload_best_model_at_end=True,\r\nlocal_rank=-1,\r\nlog_level=-1,\r\nlog_level_replica=-1,\r\nlog_on_each_node=True,\r\nlogging_dir=/content/AMRBART/fine-tune/outputs/LDC2020-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp/logs,\r\nlogging_first_step=True,\r\nlogging_nan_inf_filter=True,\r\nlogging_steps=20,\r\nlogging_strategy=steps,\r\nlr_scheduler_type=polynomial,\r\nmax_grad_norm=0.0,\r\nmax_steps=-1,\r\nmetric_for_best_model=eval_smatch,\r\nmp_parameters=,\r\nno_cuda=False,\r\nnum_train_epochs=30.0,\r\noptim=adamw_hf,\r\noutput_dir=/content/AMRBART/fine-tune/outputs/LDC2020-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp,\r\noverwrite_output_dir=True,\r\npast_index=-1,\r\nper_device_eval_batch_size=8,\r\nper_device_train_batch_size=16,\r\npredict_with_generate=True,\r\nprediction_loss_only=False,\r\npush_to_hub=False,\r\npush_to_hub_model_id=None,\r\npush_to_hub_organization=None,\r\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\r\nray_scope=last,\r\nremove_unused_columns=True,\r\nreport_to=['tensorboard'],\r\nresume_from_checkpoint=None,\r\nrun_name=/content/AMRBART/fine-tune/outputs/LDC2020-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp,\r\nsave_on_each_node=False,\r\nsave_steps=500,\r\nsave_strategy=epoch,\r\nsave_total_limit=1,\r\nseed=42,\r\nsharded_ddp=[],\r\nskip_memory_metrics=True,\r\nsmart_init=False,\r\nsortish_sampler=False,\r\ntask=text2amr,\r\ntf32=None,\r\ntorchdynamo=None,\r\ntpu_metrics_debug=False,\r\ntpu_num_cores=None,\r\nuse_ipex=False,\r\nuse_legacy_prediction_loop=False,\r\nwarmup_ratio=0.0,\r\nwarmup_steps=200,\r\nweight_decay=0.01,\r\nxpu_backend=None,\r\n)\r\n[INFO|configuration_utils.py:681] 2024-08-22 04:46:28,851 >> loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /mnt/nfs-storage/data/.cache/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\r\n[INFO|configuration_utils.py:730] 2024-08-22 04:46:28,852 >> Model config BartConfig {\r\n  \"_name_or_path\": \"facebook/bart-base\",\r\n  \"activation_dropout\": 0.1,\r\n  \"activation_function\": \"gelu\",\r\n  \"add_bias_logits\": false,\r\n  \"add_final_layer_norm\": false,\r\n  \"architectures\": [\r\n    \"BartModel\"\r\n  ],\r\n  \"attention_dropout\": 0.1,\r\n  \"bos_token_id\": 0,\r\n  \"classif_dropout\": 0.1,\r\n  \"classifier_dropout\": 0.0,\r\n  \"d_model\": 768,\r\n  \"decoder_attention_heads\": 12,\r\n  \"decoder_ffn_dim\": 3072,\r\n  \"decoder_layerdrop\": 0.0,\r\n  \"decoder_layers\": 6,\r\n  \"decoder_start_token_id\": 2,\r\n  \"dropout\": 0.1,\r\n  \"early_stopping\": true,\r\n  \"encoder_attention_heads\": 12,\r\n  \"encoder_ffn_dim\": 3072,\r\n  \"encoder_layerdrop\": 0.0,\r\n  \"encoder_layers\": 6,\r\n  \"eos_token_id\": 2,\r\n  \"forced_bos_token_id\": 0,\r\n  \"forced_eos_token_id\": 2,\r\n  \"gradient_checkpointing\": false,\r\n  \"id2label\": {\r\n    \"0\": \"LABEL_0\",\r\n    \"1\": \"LABEL_1\",\r\n    \"2\": \"LABEL_2\"\r\n  },\r\n  \"init_std\": 0.02,\r\n  \"is_encoder_decoder\": true,\r\n  \"label2id\": {\r\n    \"LABEL_0\": 0,\r\n    \"LABEL_1\": 1,\r\n    \"LABEL_2\": 2\r\n  },\r\n  \"max_position_embeddings\": 1024,\r\n  \"model_type\": \"bart\",\r\n  \"no_repeat_ngram_size\": 3,\r\n  \"normalize_before\": false,\r\n  \"normalize_embedding\": true,\r\n  \"num_beams\": 4,\r\n  \"num_hidden_layers\": 6,\r\n  \"pad_token_id\": 1,\r\n  \"scale_embedding\": false,\r\n  \"task_specific_params\": {\r\n    \"summarization\": {\r\n      \"length_penalty\": 1.0,\r\n      \"max_length\": 128,\r\n      \"min_length\": 12,\r\n      \"num_beams\": 4\r\n    },\r\n    \"summarization_cnn\": {\r\n      \"length_penalty\": 2.0,\r\n      \"max_length\": 142,\r\n      \"min_length\": 56,\r\n      \"num_beams\": 4\r\n    },\r\n    \"summarization_xsum\": {\r\n      \"length_penalty\": 1.0,\r\n      \"max_length\": 62,\r\n      \"min_length\": 11,\r\n      \"num_beams\": 6\r\n    }\r\n  },\r\n  \"torch_dtype\": \"float32\",\r\n  \"transformers_version\": \"4.21.3\",\r\n  \"use_cache\": true,\r\n  \"vocab_size\": 50265\r\n}\r\n\r\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:46:29,492 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/vocab.json from cache at /mnt/nfs-storage/data/.cache/0d6fc8b2ef1860c1f8f0baff4b021e3426cc7d11b153f98e563b799603ee2f25.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\r\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:46:29,492 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/merges.txt from cache at /mnt/nfs-storage/data/.cache/6e75e35f0bdd15870c98387e13b93a8e100237eb33ad99c36277a0562bd6d850.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\r\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:46:29,492 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/added_tokens.json from cache at None\r\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:46:29,492 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/special_tokens_map.json from cache at None\r\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:46:29,492 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/tokenizer_config.json from cache at /mnt/nfs-storage/data/.cache/1abf196c889c24daca2909359ca2090e5fcbfa21a9ea36d763f70adbafb500d7.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8\r\n[INFO|configuration_utils.py:681] 2024-08-22 04:46:29,640 >> loading configuration file https://huggingface.co/facebook/bart-large/resolve/main/config.json from cache at /mnt/nfs-storage/data/.cache/3f12fb71b844fcb7d591fdd4e55027da90d7b5dd6aa5430ad00ec6d76585f26c.bc22f15dc7ba074ee0a60bdd34c5f2fe3b6d746f89e765303376c51aff04e260\r\n[INFO|configuration_utils.py:730] 2024-08-22 04:46:29,641 >> Model config BartConfig {\r\n  \"_name_or_path\": \"facebook/bart-large\",\r\n  \"activation_dropout\": 0.1,\r\n  \"activation_function\": \"gelu\",\r\n  \"add_bias_logits\": false,\r\n  \"add_final_layer_norm\": false,\r\n  \"architectures\": [\r\n    \"BartModel\"\r\n  ],\r\n  \"attention_dropout\": 0.1,\r\n  \"bos_token_id\": 0,\r\n  \"classif_dropout\": 0.1,\r\n  \"classifier_dropout\": 0.0,\r\n  \"d_model\": 1024,\r\n  \"decoder_attention_heads\": 16,\r\n  \"decoder_ffn_dim\": 4096,\r\n  \"decoder_layerdrop\": 0.0,\r\n  \"decoder_layers\": 12,\r\n  \"decoder_start_token_id\": 2,\r\n  \"dropout\": 0.1,\r\n  \"early_stopping\": true,\r\n  \"encoder_attention_heads\": 16,\r\n  \"encoder_ffn_dim\": 4096,\r\n  \"encoder_layerdrop\": 0.0,\r\n  \"encoder_layers\": 12,\r\n  \"eos_token_id\": 2,\r\n  \"forced_bos_token_id\": 0,\r\n  \"forced_eos_token_id\": 2,\r\n  \"gradient_checkpointing\": false,\r\n  \"id2label\": {\r\n    \"0\": \"LABEL_0\",\r\n    \"1\": \"LABEL_1\",\r\n    \"2\": \"LABEL_2\"\r\n  },\r\n  \"init_std\": 0.02,\r\n  \"is_encoder_decoder\": true,\r\n  \"label2id\": {\r\n    \"LABEL_0\": 0,\r\n    \"LABEL_1\": 1,\r\n    \"LABEL_2\": 2\r\n  },\r\n  \"max_position_embeddings\": 1024,\r\n  \"model_type\": \"bart\",\r\n  \"no_repeat_ngram_size\": 3,\r\n  \"normalize_before\": false,\r\n  \"num_beams\": 4,\r\n  \"num_hidden_layers\": 12,\r\n  \"pad_token_id\": 1,\r\n  \"scale_embedding\": false,\r\n  \"task_specific_params\": {\r\n    \"summarization\": {\r\n      \"length_penalty\": 1.0,\r\n      \"max_length\": 128,\r\n      \"min_length\": 12,\r\n      \"num_beams\": 4\r\n    },\r\n    \"summarization_cnn\": {\r\n      \"length_penalty\": 2.0,\r\n      \"max_length\": 142,\r\n      \"min_length\": 56,\r\n      \"num_beams\": 4\r\n    },\r\n    \"summarization_xsum\": {\r\n      \"length_penalty\": 1.0,\r\n      \"max_length\": 62,\r\n      \"min_length\": 11,\r\n      \"num_beams\": 6\r\n    }\r\n  },\r\n  \"transformers_version\": \"4.21.3\",\r\n  \"use_cache\": true,\r\n  \"vocab_size\": 50265\r\n}\r\n\r\n[WARNING|tokenization_utils_base.py:1896] 2024-08-22 04:46:29,641 >> The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \r\nThe tokenizer class you load from this checkpoint is 'BartTokenizer'. \r\nThe class this function is called from is 'AMRBartTokenizer'.\r\nAdded 2965 AMR tokens\r\n[INFO|modeling_utils.py:2041] 2024-08-22 04:46:30,015 >> loading weights file https://huggingface.co/facebook/bart-base/resolve/main/pytorch_model.bin from cache at /mnt/nfs-storage/data/.cache/486355ec722ef05fd480e999d4c763be56549ae930f6a3742ee721a5d2a05647.f2f355ad2775769afc60592b43a46d72ca548375e3a1d65f381a751e711cbadd\r\n[INFO|modeling_utils.py:2435] 2024-08-22 04:46:34,104 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\r\n\r\n[INFO|modeling_utils.py:2443] 2024-08-22 04:46:34,105 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\r\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\r\nDataset cache dir: /mnt/nfs-storage/data/.cache\r\n/usr/local/lib/python3.10/site-packages/datasets/load.py:929: FutureWarning: The repository for data contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /content/AMRBART/fine-tune/data_interface/data.py\r\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\r\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\r\n  warnings.warn(\r\nTraceback (most recent call last):\r\n  File \"/content/AMRBART/fine-tune/main.py\", line 506, in <module>\r\n    main()\r\n  File \"/content/AMRBART/fine-tune/main.py\", line 194, in main\r\n    raw_datasets = DataSetCate(tokenizer, data_args, model_args)\r\n  File \"/content/AMRBART/fine-tune/data_interface/dataset.py\", line 63, in __init__\r\n    self.datasets = load_dataset(\r\n  File \"/usr/local/lib/python3.10/site-packages/datasets/load.py\", line 2587, in load_dataset\r\n    builder_instance = load_dataset_builder(\r\n  File \"/usr/local/lib/python3.10/site-packages/datasets/load.py\", line 2296, in load_dataset_builder\r\n    builder_instance: DatasetBuilder = builder_cls(\r\n  File \"/usr/local/lib/python3.10/site-packages/datasets/builder.py\", line 360, in __init__\r\n    data_files = DataFilesDict.from_patterns(\r\n  File \"/usr/local/lib/python3.10/site-packages/datasets/data_files.py\", line 715, in from_patterns\r\n    DataFilesList.from_patterns(\r\n  File \"/usr/local/lib/python3.10/site-packages/datasets/data_files.py\", line 620, in from_patterns\r\n    resolve_pattern(\r\n  File \"/usr/local/lib/python3.10/site-packages/datasets/data_files.py\", line 407, in resolve_pattern\r\n    raise FileNotFoundError(error_msg)\r\nFileNotFoundError: Unable to find '/content/AMRBART/fine-tune/data/LDC2020/train.jsonl'\r\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/AMRBART/fine-tune\n",
    "!bash train-AMRBART-large-AMRParsing.sh \"facebook/bart-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee0c44e",
   "metadata": {},
   "source": [
    "### Step 26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e6bdad",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588a421d",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96dddda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There seems to be a bug in file `main.py` related to the attribute `pad_to_multple_of`. The script fails when it is\n",
    "# equal to `None`. Aparently, it is resolved by replacing `None` with `1`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98f603f",
   "metadata": {},
   "source": [
    "### Step 27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9be90fd",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d430b2cb",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53d2c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/AMRBART\n/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/AMRBART/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e1b2d7",
   "metadata": {},
   "source": [
    "### Step 28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42143f60",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c958e2",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6a932a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        else:\n            predict_dataset = load_from_disk(data_args.data_cache_dir + \"/test\", keep_in_memory=True)\n\n    # label_pad_token_id = -100 if data_args.ignore_pad_token_for_loss else tokenizer.pad_token_id\n    label_pad_token_id = tokenizer.pad_token_id\n    \n    DataCollatorCate = DataCollatorForAMR2Text if training_args.task == \"amr2text\" else DataCollatorForAMRParsing\n    data_collator = DataCollatorCate(\n        tokenizer,\n        label_pad_token_id=label_pad_token_id,\n        pad_to_multiple_of=8 if training_args.fp16 else 1,\n    )\n    \n    metric = load_metric(path=\"metric/sacrebleu.py\") if training_args.task == \"amr2text\" else None\n\n    def compute_metrics_parsing(eval_preds, global_step=0, prefix=\"val\"):\n        prefix = \"test\" if prefix == \"predict\" else \"val\"\n        preds, labels, inputs = eval_preds\n        # print(\"inputs:\", inputs)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/AMRBART/fine-tune/main.py. CHANGING:\n",
    "\n",
    "\n",
    "    data_collator = DataCollatorCate(\n",
    "        tokenizer,\n",
    "        label_pad_token_id=label_pad_token_id,\n",
    "        pad_to_multiple_of=8 if training_args.fp16 else None,\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "    data_collator = DataCollatorCate(\n",
    "        tokenizer,\n",
    "        label_pad_token_id=label_pad_token_id,\n",
    "        pad_to_multiple_of=8 if training_args.fp16 else 1,\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0316bf31",
   "metadata": {},
   "source": [
    "### Step 29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b44f34",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7318d7f",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3269d39d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfa8a21",
   "metadata": {},
   "source": [
    "### Step 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5144e7ac",
   "metadata": {},
   "source": [
    "Execution Start time: 04:46:38, End time: 04:46:57 (duration: 18 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e30e4f5",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ae32e2",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a60f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n/content/AMRBART/fine-tune\n08/22/2024 04:46:51 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n08/22/2024 04:46:51 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n_n_gpu=0,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=8,\ndataloader_pin_memory=True,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=False,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=True,\ndo_train=True,\nearly_stopping=10,\neval_accumulation_steps=None,\neval_dataloader_num_workers=2,\neval_delay=0,\neval_lenpen=1.0,\neval_steps=None,\nevaluation_strategy=epoch,\nfp16=False,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_min_num_params=0,\nfull_determinism=False,\ngeneration_max_length=1024,\ngeneration_num_beams=5,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngreater_is_better=True,\ngroup_by_length=False,\nhalf_precision_backend=cuda_amp,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=True,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.1,\nlearning_rate=1e-05,\nlength_column_name=length,\nload_best_model_at_end=True,\nlocal_rank=-1,\nlog_level=-1,\nlog_level_replica=-1,\nlog_on_each_node=True,\nlogging_dir=/content/AMRBART/fine-tune/outputs/LDC2020-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp/logs,\nlogging_first_step=True,\nlogging_nan_inf_filter=True,\nlogging_steps=20,\nlogging_strategy=steps,\nlr_scheduler_type=polynomial,\nmax_grad_norm=0.0,\nmax_steps=-1,\nmetric_for_best_model=eval_smatch,\nmp_parameters=,\nno_cuda=False,\nnum_train_epochs=30.0,\noptim=adamw_hf,\noutput_dir=/content/AMRBART/fine-tune/outputs/LDC2020-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp,\noverwrite_output_dir=True,\npast_index=-1,\nper_device_eval_batch_size=8,\nper_device_train_batch_size=16,\npredict_with_generate=True,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=/content/AMRBART/fine-tune/outputs/LDC2020-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp,\nsave_on_each_node=False,\nsave_steps=500,\nsave_strategy=epoch,\nsave_total_limit=1,\nseed=42,\nsharded_ddp=[],\nskip_memory_metrics=True,\nsmart_init=False,\nsortish_sampler=False,\ntask=text2amr,\ntf32=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nwarmup_ratio=0.0,\nwarmup_steps=200,\nweight_decay=0.01,\nxpu_backend=None,\n)\n[INFO|configuration_utils.py:681] 2024-08-22 04:46:51,542 >> loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /mnt/nfs-storage/data/.cache/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\n[INFO|configuration_utils.py:730] 2024-08-22 04:46:51,544 >> Model config BartConfig {\n  \"_name_or_path\": \"facebook/bart-base\",\n  \"activation_dropout\": 0.1,\n  \"activation_function\": \"gelu\",\n  \"add_bias_logits\": false,\n  \"add_final_layer_norm\": false,\n  \"architectures\": [\n    \"BartModel\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"bos_token_id\": 0,\n  \"classif_dropout\": 0.1,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 768,\n  \"decoder_attention_heads\": 12,\n  \"decoder_ffn_dim\": 3072,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 6,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"early_stopping\": true,\n  \"encoder_attention_heads\": 12,\n  \"encoder_ffn_dim\": 3072,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 6,\n  \"eos_token_id\": 2,\n  \"forced_bos_token_id\": 0,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\"\n  },\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2\n  },\n  \"max_position_embeddings\": 1024,\n  \"model_type\": \"bart\",\n  \"no_repeat_ngram_size\": 3,\n  \"normalize_before\": false,\n  \"normalize_embedding\": true,\n  \"num_beams\": 4,\n  \"num_hidden_layers\": 6,\n  \"pad_token_id\": 1,\n  \"scale_embedding\": false,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 128,\n      \"min_length\": 12,\n      \"num_beams\": 4\n    },\n    \"summarization_cnn\": {\n      \"length_penalty\": 2.0,\n      \"max_length\": 142,\n      \"min_length\": 56,\n      \"num_beams\": 4\n    },\n    \"summarization_xsum\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 62,\n      \"min_length\": 11,\n      \"num_beams\": 6\n    }\n  },\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.21.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:46:52,178 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/vocab.json from cache at /mnt/nfs-storage/data/.cache/0d6fc8b2ef1860c1f8f0baff4b021e3426cc7d11b153f98e563b799603ee2f25.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:46:52,179 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/merges.txt from cache at /mnt/nfs-storage/data/.cache/6e75e35f0bdd15870c98387e13b93a8e100237eb33ad99c36277a0562bd6d850.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:46:52,179 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:46:52,179 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:46:52,179 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/tokenizer_config.json from cache at /mnt/nfs-storage/data/.cache/1abf196c889c24daca2909359ca2090e5fcbfa21a9ea36d763f70adbafb500d7.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8\n[INFO|configuration_utils.py:681] 2024-08-22 04:46:52,300 >> loading configuration file https://huggingface.co/facebook/bart-large/resolve/main/config.json from cache at /mnt/nfs-storage/data/.cache/3f12fb71b844fcb7d591fdd4e55027da90d7b5dd6aa5430ad00ec6d76585f26c.bc22f15dc7ba074ee0a60bdd34c5f2fe3b6d746f89e765303376c51aff04e260\n[INFO|configuration_utils.py:730] 2024-08-22 04:46:52,301 >> Model config BartConfig {\n  \"_name_or_path\": \"facebook/bart-large\",\n  \"activation_dropout\": 0.1,\n  \"activation_function\": \"gelu\",\n  \"add_bias_logits\": false,\n  \"add_final_layer_norm\": false,\n  \"architectures\": [\n    \"BartModel\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"bos_token_id\": 0,\n  \"classif_dropout\": 0.1,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 12,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"early_stopping\": true,\n  \"encoder_attention_heads\": 16,\n  \"encoder_ffn_dim\": 4096,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 12,\n  \"eos_token_id\": 2,\n  \"forced_bos_token_id\": 0,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\"\n  },\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2\n  },\n  \"max_position_embeddings\": 1024,\n  \"model_type\": \"bart\",\n  \"no_repeat_ngram_size\": 3,\n  \"normalize_before\": false,\n  \"num_beams\": 4,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"scale_embedding\": false,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 128,\n      \"min_length\": 12,\n      \"num_beams\": 4\n    },\n    \"summarization_cnn\": {\n      \"length_penalty\": 2.0,\n      \"max_length\": 142,\n      \"min_length\": 56,\n      \"num_beams\": 4\n    },\n    \"summarization_xsum\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 62,\n      \"min_length\": 11,\n      \"num_beams\": 6\n    }\n  },\n  \"transformers_version\": \"4.21.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\n[WARNING|tokenization_utils_base.py:1896] 2024-08-22 04:46:52,302 >> The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'BartTokenizer'. \nThe class this function is called from is 'AMRBartTokenizer'.\nAdded 2965 AMR tokens\n[INFO|modeling_utils.py:2041] 2024-08-22 04:46:52,613 >> loading weights file https://huggingface.co/facebook/bart-base/resolve/main/pytorch_model.bin from cache at /mnt/nfs-storage/data/.cache/486355ec722ef05fd480e999d4c763be56549ae930f6a3742ee721a5d2a05647.f2f355ad2775769afc60592b43a46d72ca548375e3a1d65f381a751e711cbadd\n[INFO|modeling_utils.py:2435] 2024-08-22 04:46:55,452 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n\n[INFO|modeling_utils.py:2443] 2024-08-22 04:46:55,453 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\nDataset cache dir: /mnt/nfs-storage/data/.cache\n/usr/local/lib/python3.10/site-packages/datasets/load.py:929: FutureWarning: The repository for data contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /content/AMRBART/fine-tune/data_interface/data.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\nTraceback (most recent call last):\n  File \"/content/AMRBART/fine-tune/main.py\", line 506, in <module>\n    main()\n  File \"/content/AMRBART/fine-tune/main.py\", line 194, in main\n    raw_datasets = DataSetCate(tokenizer, data_args, model_args)\n  File \"/content/AMRBART/fine-tune/data_interface/dataset.py\", line 63, in __init__\n    self.datasets = load_dataset(\n  File \"/usr/local/lib/python3.10/site-packages/datasets/load.py\", line 2587, in load_dataset\n    builder_instance = load_dataset_builder(\n  File \"/usr/local/lib/python3.10/site-packages/datasets/load.py\", line 2296, in load_dataset_builder\n    builder_instance: DatasetBuilder = builder_cls(\n  File \"/usr/local/lib/python3.10/site-packages/datasets/builder.py\", line 360, in __init__\n    data_files = DataFilesDict.from_patterns(\n  File \"/usr/local/lib/python3.10/site-packages/datasets/data_files.py\", line 715, in from_patterns\n    DataFilesList.from_patterns(\n  File \"/usr/local/lib/python3.10/site-packages/datasets/data_files.py\", line 620, in from_patterns\n    resolve_pattern(\n  File \"/usr/local/lib/python3.10/site-packages/datasets/data_files.py\", line 407, in resolve_pattern\n    raise FileNotFoundError(error_msg)\nFileNotFoundError: Unable to find '/content/AMRBART/fine-tune/data/LDC2020/train.jsonl'\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/AMRBART/fine-tune\n",
    "!bash train-AMRBART-large-AMRParsing.sh \"facebook/bart-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b953203b",
   "metadata": {},
   "source": [
    "### Step 31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d2c522",
   "metadata": {},
   "source": [
    "Execution Start time: 04:46:59, End time: 04:46:59 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e403ef73",
   "metadata": {},
   "source": [
    "**Thought**: It seems like the script is looking for the dataset in a different directory. I need to adjust the script to point to the correct dataset directory. Let's open the script and make the necessary changes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5eb34e",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fd6254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/AMRBART/fine-tune/train-AMRBART-large-AMRParsing.sh (85 lines total)]\n1:export CUDA_VISIBLE_DEVICES=0\n2:RootDir=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" >/dev/null 2>&1 && pwd )\"\n3:\n4:Dataset=LDC2020\n5:#Dataset=LDC2017\n6:\n7:BasePath=/mnt/nfs-storage/data        # change dir here\n8:DataPath=$RootDir/data/$Dataset\n9:\n10:ModelCate=AMRBART-large\n11:\n12:MODEL=$1\n13:ModelCache=$BasePath/.cache\n14:DataCache=$DataPath/.cache/dump-amrparsing\n15:\n16:lr=1e-5\n17:\n18:OutputDir=${RootDir}/outputs/$Dataset-${ModelCate}-AMRParing-bsz16-lr-${lr}-UnifiedInp\n19:\n20:if [ ! -d ${OutputDir} ];then\n21:  mkdir -p ${OutputDir}\n22:else\n23:  rm -rf ${OutputDir}; mkdir -p ${OutputDir};\n24:fi\n25:export HF_DATASETS_CACHE=$DataCache\n26:\n27:if [ ! -d ${DataCache} ];then\n28:  mkdir -p ${DataCache}\n29:fi\n30:\n31:# torchrun --nnodes=1 --nproc_per_node=1 --max_restarts=0 --rdzv_id=1 --rdzv_backend=c10d main.py \\\n32:python -u main.py \\\n33:    --data_dir $DataPath \\\n34:    --task \"text2amr\" \\\n35:    --train_file $DataPath/train.jsonl \\\n36:    --validation_file $DataPath/val.jsonl \\\n37:    --test_file $DataPath/test.jsonl \\\n38:    --output_dir $OutputDir \\\n39:    --cache_dir $ModelCache \\\n40:    --data_cache_dir $DataCache \\\n41:    --tokenizer_name \"facebook/bart-large\" \\\n42:    --model_name_or_path $MODEL \\\n43:    --overwrite_output_dir \\\n44:    --unified_input True \\\n45:    --per_device_train_batch_size 16 \\\n46:    --per_device_eval_batch_size 8 \\\n47:    --gradient_accumulation_steps 1 \\\n48:    --learning_rate $lr \\\n49:    --optim \"adamw_hf\" \\\n50:    --lr_scheduler_type \"polynomial\" \\\n51:    --warmup_steps 200 \\\n52:    --num_train_epochs 30 \\\n53:    --early_stopping 10 \\\n54:    --max_source_length 400 \\\n55:    --max_target_length 1024 \\\n56:    --val_max_target_length 1024 \\\n57:    --generation_max_length 1024 \\\n58:    --generation_num_beams 5 \\\n59:    --label_smoothing_factor 0.1 \\\n60:    --evaluation_strategy \"epoch\" \\\n61:    --weight_decay 0.01 \\\n62:    --max_grad_norm 0 \\\n63:    --max_steps -1 \\\n64:    --predict_with_generate \\\n65:    --smart_init False \\\n66:    --use_fast_tokenizer False \\\n67:    --logging_dir $OutputDir/logs \\\n68:    --logging_first_step True \\\n69:    --logging_steps 20 \\\n70:    --save_strategy \"epoch\" \\\n71:    --save_total_limit 1 \\\n72:    --seed 42 \\\n73:    --fp16_backend \"auto\" \\\n74:    --dataloader_num_workers 8 \\\n75:    --eval_dataloader_num_workers 2 \\\n76:    --load_best_model_at_end True \\\n77:    --metric_for_best_model \"eval_smatch\" \\\n78:    --include_inputs_for_metrics \\\n79:    --greater_is_better True \\\n80:    --do_train \\\n81:    --do_eval \\\n82:    --do_predict \\\n83:    --ddp_find_unused_parameters False \\\n84:    --report_to \"tensorboard\" \\\n85:    --dataloader_pin_memory True 2>&1 | tee $OutputDir/run.log\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open train-AMRBART-large-AMRParsing.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde6b690",
   "metadata": {},
   "source": [
    "### Step 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02efae3e",
   "metadata": {},
   "source": [
    "Execution Start time: 04:47:01, End time: 04:47:01 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ded42e",
   "metadata": {},
   "source": [
    "**Thought**: The script is looking for the dataset in the `data/LDC2020` directory. However, our dataset is located in `data/AMRBART`. I will update the script to point to the correct directory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187705b1",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcb206b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Usage: edit <start_line>:<end_line>\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 4\n",
    "Dataset=AMRBART\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aac4ea7",
   "metadata": {},
   "source": [
    "### Step 33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b264688",
   "metadata": {},
   "source": [
    "Execution Start time: 04:47:02, End time: 04:47:02 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e2c0da",
   "metadata": {},
   "source": [
    "**Thought**: I need to specify the correct line range to edit the dataset path in the script. Let's search for the line where the dataset path is defined and update it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c221dfa",
   "metadata": {},
   "source": [
    "**Action (search_file)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c41b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Found 2 matches for \"Dataset=\" in /content/AMRBART/fine-tune/train-AMRBART-large-AMRParsing.sh:\nLine 4:Dataset=LDC2020\nLine 5:#Dataset=LDC2017\nEnd of matches for \"Dataset=\" in /content/AMRBART/fine-tune/train-AMRBART-large-AMRParsing.sh\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_file \"Dataset=\" train-AMRBART-large-AMRParsing.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3780501",
   "metadata": {},
   "source": [
    "### Step 34"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a58513",
   "metadata": {},
   "source": [
    "Execution Start time: 04:47:04, End time: 04:47:04 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84a6901",
   "metadata": {},
   "source": [
    "**Thought**: The dataset path is defined on line 4. I will update this line to point to the correct dataset directory, `AMRBART`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2f5448",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3849a415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/AMRBART/fine-tune/train-AMRBART-large-AMRParsing.sh (85 lines total)]\n1:export CUDA_VISIBLE_DEVICES=0\n2:RootDir=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" >/dev/null 2>&1 && pwd )\"\n3:\n4:Dataset=AMRBART\n5:#Dataset=LDC2017\n6:\n7:BasePath=/mnt/nfs-storage/data        # change dir here\n8:DataPath=$RootDir/data/$Dataset\n9:\n10:ModelCate=AMRBART-large\n11:\n12:MODEL=$1\n13:ModelCache=$BasePath/.cache\n14:DataCache=$DataPath/.cache/dump-amrparsing\n15:\n16:lr=1e-5\n17:\n18:OutputDir=${RootDir}/outputs/$Dataset-${ModelCate}-AMRParing-bsz16-lr-${lr}-UnifiedInp\n19:\n20:if [ ! -d ${OutputDir} ];then\n21:  mkdir -p ${OutputDir}\n22:else\n23:  rm -rf ${OutputDir}; mkdir -p ${OutputDir};\n24:fi\n25:export HF_DATASETS_CACHE=$DataCache\n26:\n27:if [ ! -d ${DataCache} ];then\n28:  mkdir -p ${DataCache}\n29:fi\n30:\n31:# torchrun --nnodes=1 --nproc_per_node=1 --max_restarts=0 --rdzv_id=1 --rdzv_backend=c10d main.py \\\n32:python -u main.py \\\n33:    --data_dir $DataPath \\\n34:    --task \"text2amr\" \\\n35:    --train_file $DataPath/train.jsonl \\\n36:    --validation_file $DataPath/val.jsonl \\\n37:    --test_file $DataPath/test.jsonl \\\n38:    --output_dir $OutputDir \\\n39:    --cache_dir $ModelCache \\\n40:    --data_cache_dir $DataCache \\\n41:    --tokenizer_name \"facebook/bart-large\" \\\n42:    --model_name_or_path $MODEL \\\n43:    --overwrite_output_dir \\\n44:    --unified_input True \\\n45:    --per_device_train_batch_size 16 \\\n46:    --per_device_eval_batch_size 8 \\\n47:    --gradient_accumulation_steps 1 \\\n48:    --learning_rate $lr \\\n49:    --optim \"adamw_hf\" \\\n50:    --lr_scheduler_type \"polynomial\" \\\n51:    --warmup_steps 200 \\\n52:    --num_train_epochs 30 \\\n53:    --early_stopping 10 \\\n54:    --max_source_length 400 \\\n55:    --max_target_length 1024 \\\n56:    --val_max_target_length 1024 \\\n57:    --generation_max_length 1024 \\\n58:    --generation_num_beams 5 \\\n59:    --label_smoothing_factor 0.1 \\\n60:    --evaluation_strategy \"epoch\" \\\n61:    --weight_decay 0.01 \\\n62:    --max_grad_norm 0 \\\n63:    --max_steps -1 \\\n64:    --predict_with_generate \\\n65:    --smart_init False \\\n66:    --use_fast_tokenizer False \\\n67:    --logging_dir $OutputDir/logs \\\n68:    --logging_first_step True \\\n69:    --logging_steps 20 \\\n70:    --save_strategy \"epoch\" \\\n71:    --save_total_limit 1 \\\n72:    --seed 42 \\\n73:    --fp16_backend \"auto\" \\\n74:    --dataloader_num_workers 8 \\\n75:    --eval_dataloader_num_workers 2 \\\n76:    --load_best_model_at_end True \\\n77:    --metric_for_best_model \"eval_smatch\" \\\n78:    --include_inputs_for_metrics \\\n79:    --greater_is_better True \\\n80:    --do_train \\\n81:    --do_eval \\\n82:    --do_predict \\\n83:    --ddp_find_unused_parameters False \\\n84:    --report_to \"tensorboard\" \\\n85:    --dataloader_pin_memory True 2>&1 | tee $OutputDir/run.log\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 4:4\n",
    "Dataset=AMRBART\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753d22fb",
   "metadata": {},
   "source": [
    "### Step 35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a1e669",
   "metadata": {},
   "source": [
    "Execution Start time: 04:47:06, End time: 04:52:06 (duration: 300 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81640e25",
   "metadata": {},
   "source": [
    "**Thought**: The dataset path has been updated in the script. Now, let's try running the training script again to see if it works with the correct dataset path.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43922bff",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac087b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "08/22/2024 04:47:17 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n08/22/2024 04:47:17 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n_n_gpu=0,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=8,\ndataloader_pin_memory=True,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=False,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=True,\ndo_train=True,\nearly_stopping=10,\neval_accumulation_steps=None,\neval_dataloader_num_workers=2,\neval_delay=0,\neval_lenpen=1.0,\neval_steps=None,\nevaluation_strategy=epoch,\nfp16=False,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_min_num_params=0,\nfull_determinism=False,\ngeneration_max_length=1024,\ngeneration_num_beams=5,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngreater_is_better=True,\ngroup_by_length=False,\nhalf_precision_backend=cuda_amp,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=True,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.1,\nlearning_rate=1e-05,\nlength_column_name=length,\nload_best_model_at_end=True,\nlocal_rank=-1,\nlog_level=-1,\nlog_level_replica=-1,\nlog_on_each_node=True,\nlogging_dir=/content/AMRBART/fine-tune/outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp/logs,\nlogging_first_step=True,\nlogging_nan_inf_filter=True,\nlogging_steps=20,\nlogging_strategy=steps,\nlr_scheduler_type=polynomial,\nmax_grad_norm=0.0,\nmax_steps=-1,\nmetric_for_best_model=eval_smatch,\nmp_parameters=,\nno_cuda=False,\nnum_train_epochs=30.0,\noptim=adamw_hf,\noutput_dir=/content/AMRBART/fine-tune/outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp,\noverwrite_output_dir=True,\npast_index=-1,\nper_device_eval_batch_size=8,\nper_device_train_batch_size=16,\npredict_with_generate=True,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=/content/AMRBART/fine-tune/outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp,\nsave_on_each_node=False,\nsave_steps=500,\nsave_strategy=epoch,\nsave_total_limit=1,\nseed=42,\nsharded_ddp=[],\nskip_memory_metrics=True,\nsmart_init=False,\nsortish_sampler=False,\ntask=text2amr,\ntf32=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nwarmup_ratio=0.0,\nwarmup_steps=200,\nweight_decay=0.01,\nxpu_backend=None,\n)\n[INFO|configuration_utils.py:681] 2024-08-22 04:47:17,514 >> loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /mnt/nfs-storage/data/.cache/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\n[INFO|configuration_utils.py:730] 2024-08-22 04:47:17,515 >> Model config BartConfig {\n  \"_name_or_path\": \"facebook/bart-base\",\n  \"activation_dropout\": 0.1,\n  \"activation_function\": \"gelu\",\n  \"add_bias_logits\": false,\n  \"add_final_layer_norm\": false,\n  \"architectures\": [\n    \"BartModel\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"bos_token_id\": 0,\n  \"classif_dropout\": 0.1,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 768,\n  \"decoder_attention_heads\": 12,\n  \"decoder_ffn_dim\": 3072,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 6,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"early_stopping\": true,\n  \"encoder_attention_heads\": 12,\n  \"encoder_ffn_dim\": 3072,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 6,\n  \"eos_token_id\": 2,\n  \"forced_bos_token_id\": 0,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\"\n  },\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2\n  },\n  \"max_position_embeddings\": 1024,\n  \"model_type\": \"bart\",\n  \"no_repeat_ngram_size\": 3,\n  \"normalize_before\": false,\n  \"normalize_embedding\": true,\n  \"num_beams\": 4,\n  \"num_hidden_layers\": 6,\n  \"pad_token_id\": 1,\n  \"scale_embedding\": false,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 128,\n      \"min_length\": 12,\n      \"num_beams\": 4\n    },\n    \"summarization_cnn\": {\n      \"length_penalty\": 2.0,\n      \"max_length\": 142,\n      \"min_length\": 56,\n      \"num_beams\": 4\n    },\n    \"summarization_xsum\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 62,\n      \"min_length\": 11,\n      \"num_beams\": 6\n    }\n  },\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.21.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:47:18,139 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/vocab.json from cache at /mnt/nfs-storage/data/.cache/0d6fc8b2ef1860c1f8f0baff4b021e3426cc7d11b153f98e563b799603ee2f25.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:47:18,140 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/merges.txt from cache at /mnt/nfs-storage/data/.cache/6e75e35f0bdd15870c98387e13b93a8e100237eb33ad99c36277a0562bd6d850.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:47:18,140 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:47:18,141 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:47:18,141 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/tokenizer_config.json from cache at /mnt/nfs-storage/data/.cache/1abf196c889c24daca2909359ca2090e5fcbfa21a9ea36d763f70adbafb500d7.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8\n[INFO|configuration_utils.py:681] 2024-08-22 04:47:18,267 >> loading configuration file https://huggingface.co/facebook/bart-large/resolve/main/config.json from cache at /mnt/nfs-storage/data/.cache/3f12fb71b844fcb7d591fdd4e55027da90d7b5dd6aa5430ad00ec6d76585f26c.bc22f15dc7ba074ee0a60bdd34c5f2fe3b6d746f89e765303376c51aff04e260\n[INFO|configuration_utils.py:730] 2024-08-22 04:47:18,268 >> Model config BartConfig {\n  \"_name_or_path\": \"facebook/bart-large\",\n  \"activation_dropout\": 0.1,\n  \"activation_function\": \"gelu\",\n  \"add_bias_logits\": false,\n  \"add_final_layer_norm\": false,\n  \"architectures\": [\n    \"BartModel\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"bos_token_id\": 0,\n  \"classif_dropout\": 0.1,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 12,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"early_stopping\": true,\n  \"encoder_attention_heads\": 16,\n  \"encoder_ffn_dim\": 4096,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 12,\n  \"eos_token_id\": 2,\n  \"forced_bos_token_id\": 0,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\"\n  },\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2\n  },\n  \"max_position_embeddings\": 1024,\n  \"model_type\": \"bart\",\n  \"no_repeat_ngram_size\": 3,\n  \"normalize_before\": false,\n  \"num_beams\": 4,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"scale_embedding\": false,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 128,\n      \"min_length\": 12,\n      \"num_beams\": 4\n    },\n    \"summarization_cnn\": {\n      \"length_penalty\": 2.0,\n      \"max_length\": 142,\n      \"min_length\": 56,\n      \"num_beams\": 4\n    },\n    \"summarization_xsum\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 62,\n      \"min_length\": 11,\n      \"num_beams\": 6\n    }\n  },\n  \"transformers_version\": \"4.21.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\n[WARNING|tokenization_utils_base.py:1896] 2024-08-22 04:47:18,268 >> The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'BartTokenizer'. \nThe class this function is called from is 'AMRBartTokenizer'.\nAdded 2965 AMR tokens\n[INFO|modeling_utils.py:2041] 2024-08-22 04:47:18,552 >> loading weights file https://huggingface.co/facebook/bart-base/resolve/main/pytorch_model.bin from cache at /mnt/nfs-storage/data/.cache/486355ec722ef05fd480e999d4c763be56549ae930f6a3742ee721a5d2a05647.f2f355ad2775769afc60592b43a46d72ca548375e3a1d65f381a751e711cbadd\n[INFO|modeling_utils.py:2435] 2024-08-22 04:47:21,281 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n\n[INFO|modeling_utils.py:2443] 2024-08-22 04:47:21,282 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\nDataset cache dir: /mnt/nfs-storage/data/.cache\n/usr/local/lib/python3.10/site-packages/datasets/load.py:929: FutureWarning: The repository for data contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /content/AMRBART/fine-tune/data_interface/data.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\nUsing custom data configuration default-b849ce260fee6cea\n08/22/2024 04:47:22 - INFO - datasets.builder - Using custom data configuration default-b849ce260fee6cea\nLoading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/data/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5\n08/22/2024 04:47:22 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/data/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5\nGenerating dataset data (/content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-b849ce260fee6cea/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5)\n08/22/2024 04:47:22 - INFO - datasets.builder - Generating dataset data (/content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-b849ce260fee6cea/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5)\nDownloading and preparing dataset data/default to /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-b849ce260fee6cea/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5...\n08/22/2024 04:47:22 - INFO - datasets.builder - Downloading and preparing dataset data/default to /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-b849ce260fee6cea/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5...\nGenerating train split\n08/22/2024 04:47:22 - INFO - datasets.builder - Generating train split\n\nGenerating train split: 0 examples [00:00, ? examples/s]\nGenerating train split: 10 examples [00:00, 398.79 examples/s]\nGenerating validation split\n08/22/2024 04:47:22 - INFO - datasets.builder - Generating validation split\n\nGenerating validation split: 0 examples [00:00, ? examples/s]\nGenerating validation split: 10 examples [00:00, 1665.13 examples/s]\nGenerating test split\n08/22/2024 04:47:22 - INFO - datasets.builder - Generating test split\n\nGenerating test split: 0 examples [00:00, ? examples/s]\nGenerating test split: 10 examples [00:00, 1953.38 examples/s]\nUnable to verify splits sizes.\n08/22/2024 04:47:22 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\nDataset data downloaded and prepared to /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-b849ce260fee6cea/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5. Subsequent calls will reuse this data.\n08/22/2024 04:47:22 - INFO - datasets.builder - Dataset data downloaded and prepared to /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-b849ce260fee6cea/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5. Subsequent calls will reuse this data.\ndatasets: DatasetDict({\n    train: Dataset({\n        features: ['src', 'tgt'],\n        num_rows: 10\n    })\n    validation: Dataset({\n        features: ['src', 'tgt'],\n        num_rows: 10\n    })\n    test: Dataset({\n        features: ['src', 'tgt'],\n        num_rows: 10\n    })\n})\ncolums: ['src', 'tgt']\n\nRunning tokenizer on train dataset:   0%|          | 0/10 [00:00<?, ? examples/s]Caching processed dataset at /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-b849ce260fee6cea/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5/cache-c3331aac1c3a5c5b.arrow\n08/22/2024 04:47:23 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-b849ce260fee6cea/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5/cache-c3331aac1c3a5c5b.arrow\n\nRunning tokenizer on train dataset: 100%|██████████| 10/10 [00:00<00:00, 272.73 examples/s]\nSaving cached train data ...\n\nSaving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s]\nSaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2477.88 examples/s]\nSaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2349.49 examples/s]\n\nRunning tokenizer on validation dataset:   0%|          | 0/10 [00:00<?, ? examples/s]Caching processed dataset at /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-b849ce260fee6cea/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5/cache-d67b3c899c63a4ef.arrow\n08/22/2024 04:47:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-b849ce260fee6cea/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5/cache-d67b3c899c63a4ef.arrow\n\nRunning tokenizer on validation dataset: 100%|██████████| 10/10 [00:00<00:00, 214.82 examples/s]\nSaving cached validation data ...\n\nSaving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s]\nSaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2720.04 examples/s]\nSaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2577.15 examples/s]\n\nRunning tokenizer on prediction dataset:   0%|          | 0/10 [00:00<?, ? examples/s]Caching processed dataset at /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-b849ce260fee6cea/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5/cache-07d173b48cb52365.arrow\n08/22/2024 04:47:27 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-b849ce260fee6cea/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5/cache-07d173b48cb52365.arrow\n\nRunning tokenizer on prediction dataset: 100%|██████████| 10/10 [00:00<00:00, 445.97 examples/s]\nSaving cached test data ...\n\nSaving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s]\nSaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2688.14 examples/s]\nSaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2509.01 examples/s]\n/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 5, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/usr/local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nAll 30 steps, warm_up steps: 200\n\n  0%|          | 0/30 [00:00<?, ?it/s]/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 5, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n\n  3%|▎         | 1/30 [00:33<16:05, 33.31s/it]\n                                              \n{'loss': 6.0523, 'learning_rate': 5.0000000000000004e-08, 'epoch': 1.0}\n\n  3%|▎         | 1/30 [00:33<16:05, 33.31s/it]Generation Kwargs:\n{'max_length': 1024, 'max_gen_length': 1024, 'num_beams': 5}\nSaving dummy inputs...\n\n\nYour command has already run for 300 seconds. It is still running. You can continue to wait or interrupt it with \"action\": {\"type\": \"interrupt\"} or \"action\": {\"type\": \"continue_wait\"}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!bash train-AMRBART-large-AMRParsing.sh \"facebook/bart-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ac3068",
   "metadata": {},
   "source": [
    "### Step 36"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809a7d9a",
   "metadata": {},
   "source": [
    "Execution Start time: 04:52:11, End time: 04:52:23 (duration: 12 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26b6ceb",
   "metadata": {},
   "source": [
    "**Thought**: The training script is running successfully, and it has completed one epoch. The last training loss is reported as `6.0523`. Now, I will proceed to run the inference script as instructed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff32bed",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3f5e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Traceback (most recent call last):\n  File \"/content/AMRBART/fine-tune/main.py\", line 506, in <module>\n    main()\n  File \"/content/AMRBART/fine-tune/main.py\", line 79, in main\n    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n  File \"/usr/local/lib/python3.10/site-packages/transformers/hf_argparser.py\", line 224, in parse_args_into_dataclasses\n    obj = dtype(**inputs)\n  File \"<string>\", line 110, in __init__\n  File \"/content/AMRBART/fine-tune/common/training_args.py\", line 1070, in __post_init__\n    raise ValueError(\nValueError: FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation (`--fp16_full_eval`) can only be used on CUDA devices.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!bash inference-amr.sh \"outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp/checkpoint-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4e89e8",
   "metadata": {},
   "source": [
    "### Step 37"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888ff9f4",
   "metadata": {},
   "source": [
    "Execution Start time: 04:52:24, End time: 04:52:25 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086915ab",
   "metadata": {},
   "source": [
    "**Thought**: It seems that the inference script is also trying to use FP16 precision, which is not supported on the current device. I need to modify the inference script to remove the `--fp16` argument as well.\n",
    "\n",
    "Let's open the inference script and make the necessary changes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5464c8",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa417bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/AMRBART/fine-tune/inference-amr.sh (67 lines total)]\n1:export CUDA_VISIBLE_DEVICES=0\n2:RootDir=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" >/dev/null 2>&1 && pwd )\"\n3:\n4:Dataset=examples\n5:\n6:BasePath=/mnt/nfs-storage/data                    # change dir here\n7:DataPath=$RootDir/../$Dataset\n8:\n9:ModelCate=AMRBART-large\n10:\n11:MODEL=$1\n12:ModelCache=$BasePath/.cache\n13:DataCache=$DataPath/.cache/dump-amrparsing\n14:\n15:lr=1e-5\n16:\n17:OutputDir=${RootDir}/outputs/Infer-$Dataset-${ModelCate}-AMRParing-bsz16-lr-${lr}-UnifiedInp\n18:\n19:if [ ! -d ${OutputDir} ];then\n20:  mkdir -p ${OutputDir}\n21:else\n22:  read -p \"${OutputDir} already exists, delete origin one [y/n]?\" yn\n23:  case $yn in\n24:    [Yy]* ) rm -rf ${OutputDir}; mkdir -p ${OutputDir};;\n25:    [Nn]* ) echo \"exiting...\"; exit;;\n26:    * ) echo \"Please answer yes or no.\";;\n27:  esac\n28:fi\n29:\n30:export HF_DATASETS_CACHE=$DataCache\n31:\n32:if [ ! -d ${DataCache} ];then\n33:  mkdir -p ${DataCache}\n34:fi\n35:\n36:# torchrun --nnodes=1 --nproc_per_node=1 --max_restarts=0 --rdzv_id=1 --rdzv_backend=c10d main.py \\\n37:python -u main.py \\\n38:    --data_dir $DataPath \\\n39:    --task \"text2amr\" \\\n40:    --test_file $DataPath/data4parsing.jsonl \\\n41:    --output_dir $OutputDir \\\n42:    --cache_dir $ModelCache \\\n43:    --data_cache_dir $DataCache \\\n44:    --overwrite_cache True \\\n45:    --model_name_or_path $MODEL \\\n46:    --overwrite_output_dir \\\n47:    --unified_input True \\\n48:    --per_device_eval_batch_size 16 \\\n49:    --max_source_length 400 \\\n50:    --max_target_length 1024 \\\n51:    --val_max_target_length 1024 \\\n52:    --generation_max_length 1024 \\\n53:    --generation_num_beams 5 \\\n54:    --predict_with_generate \\\n55:    --smart_init False \\\n56:    --use_fast_tokenizer False \\\n57:    --logging_dir $OutputDir/logs \\\n58:    --seed 42 \\\n59:    --fp16 \\\n60:    --fp16_backend \"auto\" \\\n61:    --dataloader_num_workers 8 \\\n62:    --eval_dataloader_num_workers 2 \\\n63:    --include_inputs_for_metrics \\\n64:    --do_predict \\\n65:    --ddp_find_unused_parameters False \\\n66:    --report_to \"tensorboard\" \\\n67:    --dataloader_pin_memory True 2>&1 | tee $OutputDir/run.log\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open inference-amr.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ac446d",
   "metadata": {},
   "source": [
    "### Step 38"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4902f2f2",
   "metadata": {},
   "source": [
    "Execution Start time: 04:52:26, End time: 04:52:27 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e657b474",
   "metadata": {},
   "source": [
    "**Thought**: The `--fp16` argument is present on line 59. I will remove this argument to ensure the script runs without attempting to use FP16 precision.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880b5612",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf3623e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/AMRBART/fine-tune/inference-amr.sh (67 lines total)]\n1:export CUDA_VISIBLE_DEVICES=0\n2:RootDir=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" >/dev/null 2>&1 && pwd )\"\n3:\n4:Dataset=examples\n5:\n6:BasePath=/mnt/nfs-storage/data                    # change dir here\n7:DataPath=$RootDir/../$Dataset\n8:\n9:ModelCate=AMRBART-large\n10:\n11:MODEL=$1\n12:ModelCache=$BasePath/.cache\n13:DataCache=$DataPath/.cache/dump-amrparsing\n14:\n15:lr=1e-5\n16:\n17:OutputDir=${RootDir}/outputs/Infer-$Dataset-${ModelCate}-AMRParing-bsz16-lr-${lr}-UnifiedInp\n18:\n19:if [ ! -d ${OutputDir} ];then\n20:  mkdir -p ${OutputDir}\n21:else\n22:  read -p \"${OutputDir} already exists, delete origin one [y/n]?\" yn\n23:  case $yn in\n24:    [Yy]* ) rm -rf ${OutputDir}; mkdir -p ${OutputDir};;\n25:    [Nn]* ) echo \"exiting...\"; exit;;\n26:    * ) echo \"Please answer yes or no.\";;\n27:  esac\n28:fi\n29:\n30:export HF_DATASETS_CACHE=$DataCache\n31:\n32:if [ ! -d ${DataCache} ];then\n33:  mkdir -p ${DataCache}\n34:fi\n35:\n36:# torchrun --nnodes=1 --nproc_per_node=1 --max_restarts=0 --rdzv_id=1 --rdzv_backend=c10d main.py \\\n37:python -u main.py \\\n38:    --data_dir $DataPath \\\n39:    --task \"text2amr\" \\\n40:    --test_file $DataPath/data4parsing.jsonl \\\n41:    --output_dir $OutputDir \\\n42:    --cache_dir $ModelCache \\\n43:    --data_cache_dir $DataCache \\\n44:    --overwrite_cache True \\\n45:    --model_name_or_path $MODEL \\\n46:    --overwrite_output_dir \\\n47:    --unified_input True \\\n48:    --per_device_eval_batch_size 16 \\\n49:    --max_source_length 400 \\\n50:    --max_target_length 1024 \\\n51:    --val_max_target_length 1024 \\\n52:    --generation_max_length 1024 \\\n53:    --generation_num_beams 5 \\\n54:    --predict_with_generate \\\n55:    --smart_init False \\\n56:    --use_fast_tokenizer False \\\n57:    --logging_dir $OutputDir/logs \\\n58:    --seed 42 \\\n59:# Removed --fp16 argument\n60:    --fp16_backend \"auto\" \\\n61:    --dataloader_num_workers 8 \\\n62:    --eval_dataloader_num_workers 2 \\\n63:    --include_inputs_for_metrics \\\n64:    --do_predict \\\n65:    --ddp_find_unused_parameters False \\\n66:    --report_to \"tensorboard\" \\\n67:    --dataloader_pin_memory True 2>&1 | tee $OutputDir/run.log\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 59:59\n",
    "# Removed --fp16 argument\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abf7a20",
   "metadata": {},
   "source": [
    "### Step 39"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e83df89",
   "metadata": {},
   "source": [
    "Execution Start time: 04:52:29, End time: 04:57:29 (duration: 300 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acce8b0e",
   "metadata": {},
   "source": [
    "**Thought**: The `--fp16` argument has been removed from the inference script. Now, let's try running the inference script again to see if it works without attempting to use FP16 precision.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f4634a",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ccd7a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/AMRBART/fine-tune/outputs/Infer-examples-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp already exists, delete origin one [y/n]?\n\nYour command has already run for 300 seconds. It is still running. You can continue to wait or interrupt it with \"action\": {\"type\": \"interrupt\"} or \"action\": {\"type\": \"continue_wait\"}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!bash inference-amr.sh \"outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp/checkpoint-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0cce58",
   "metadata": {},
   "source": [
    "### Step 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b05b99b",
   "metadata": {},
   "source": [
    "Execution Start time: 04:57:31, End time: 04:57:31 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baa62e2",
   "metadata": {},
   "source": [
    "**Thought**: The script is prompting to delete an existing directory. I need to modify the script to automatically handle this prompt by deleting the directory without asking.\n",
    "\n",
    "Let's open the inference script and make the necessary changes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eec569",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68174eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/AMRBART/fine-tune/inference-amr.sh (67 lines total)]\n1:export CUDA_VISIBLE_DEVICES=0\n2:RootDir=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" >/dev/null 2>&1 && pwd )\"\n3:\n4:Dataset=examples\n5:\n6:BasePath=/mnt/nfs-storage/data                    # change dir here\n7:DataPath=$RootDir/../$Dataset\n8:\n9:ModelCate=AMRBART-large\n10:\n11:MODEL=$1\n12:ModelCache=$BasePath/.cache\n13:DataCache=$DataPath/.cache/dump-amrparsing\n14:\n15:lr=1e-5\n16:\n17:OutputDir=${RootDir}/outputs/Infer-$Dataset-${ModelCate}-AMRParing-bsz16-lr-${lr}-UnifiedInp\n18:\n19:if [ ! -d ${OutputDir} ];then\n20:  mkdir -p ${OutputDir}\n21:else\n22:  read -p \"${OutputDir} already exists, delete origin one [y/n]?\" yn\n23:  case $yn in\n24:    [Yy]* ) rm -rf ${OutputDir}; mkdir -p ${OutputDir};;\n25:    [Nn]* ) echo \"exiting...\"; exit;;\n26:    * ) echo \"Please answer yes or no.\";;\n27:  esac\n28:fi\n29:\n30:export HF_DATASETS_CACHE=$DataCache\n31:\n32:if [ ! -d ${DataCache} ];then\n33:  mkdir -p ${DataCache}\n34:fi\n35:\n36:# torchrun --nnodes=1 --nproc_per_node=1 --max_restarts=0 --rdzv_id=1 --rdzv_backend=c10d main.py \\\n37:python -u main.py \\\n38:    --data_dir $DataPath \\\n39:    --task \"text2amr\" \\\n40:    --test_file $DataPath/data4parsing.jsonl \\\n41:    --output_dir $OutputDir \\\n42:    --cache_dir $ModelCache \\\n43:    --data_cache_dir $DataCache \\\n44:    --overwrite_cache True \\\n45:    --model_name_or_path $MODEL \\\n46:    --overwrite_output_dir \\\n47:    --unified_input True \\\n48:    --per_device_eval_batch_size 16 \\\n49:    --max_source_length 400 \\\n50:    --max_target_length 1024 \\\n51:    --val_max_target_length 1024 \\\n52:    --generation_max_length 1024 \\\n53:    --generation_num_beams 5 \\\n54:    --predict_with_generate \\\n55:    --smart_init False \\\n56:    --use_fast_tokenizer False \\\n57:    --logging_dir $OutputDir/logs \\\n58:    --seed 42 \\\n59:# Removed --fp16 argument\n60:    --fp16_backend \"auto\" \\\n61:    --dataloader_num_workers 8 \\\n62:    --eval_dataloader_num_workers 2 \\\n63:    --include_inputs_for_metrics \\\n64:    --do_predict \\\n65:    --ddp_find_unused_parameters False \\\n66:    --report_to \"tensorboard\" \\\n67:    --dataloader_pin_memory True 2>&1 | tee $OutputDir/run.log\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open inference-amr.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5a5090",
   "metadata": {},
   "source": [
    "### Step 41"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d56ca2",
   "metadata": {},
   "source": [
    "Execution Start time: 04:57:33, End time: 04:57:33 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1612ee5b",
   "metadata": {},
   "source": [
    "**Thought**: I will modify the script to automatically delete the existing directory without prompting the user.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb688eda",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41d6315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/AMRBART/fine-tune/inference-amr.sh (62 lines total)]\n1:export CUDA_VISIBLE_DEVICES=0\n2:RootDir=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" >/dev/null 2>&1 && pwd )\"\n3:\n4:Dataset=examples\n5:\n6:BasePath=/mnt/nfs-storage/data                    # change dir here\n7:DataPath=$RootDir/../$Dataset\n8:\n9:ModelCate=AMRBART-large\n10:\n11:MODEL=$1\n12:ModelCache=$BasePath/.cache\n13:DataCache=$DataPath/.cache/dump-amrparsing\n14:\n15:lr=1e-5\n16:\n17:OutputDir=${RootDir}/outputs/Infer-$Dataset-${ModelCate}-AMRParing-bsz16-lr-${lr}-UnifiedInp\n18:\n19:if [ ! -d ${OutputDir} ];then\n20:  mkdir -p ${OutputDir}\n21:else\n22:  rm -rf ${OutputDir}; mkdir -p ${OutputDir};\n23:fi\n24:\n25:export HF_DATASETS_CACHE=$DataCache\n26:\n27:if [ ! -d ${DataCache} ];then\n28:  mkdir -p ${DataCache}\n29:fi\n30:\n31:# torchrun --nnodes=1 --nproc_per_node=1 --max_restarts=0 --rdzv_id=1 --rdzv_backend=c10d main.py \\\n32:python -u main.py \\\n33:    --data_dir $DataPath \\\n34:    --task \"text2amr\" \\\n35:    --test_file $DataPath/data4parsing.jsonl \\\n36:    --output_dir $OutputDir \\\n37:    --cache_dir $ModelCache \\\n38:    --data_cache_dir $DataCache \\\n39:    --overwrite_cache True \\\n40:    --model_name_or_path $MODEL \\\n41:    --overwrite_output_dir \\\n42:    --unified_input True \\\n43:    --per_device_eval_batch_size 16 \\\n44:    --max_source_length 400 \\\n45:    --max_target_length 1024 \\\n46:    --val_max_target_length 1024 \\\n47:    --generation_max_length 1024 \\\n48:    --generation_num_beams 5 \\\n49:    --predict_with_generate \\\n50:    --smart_init False \\\n51:    --use_fast_tokenizer False \\\n52:    --logging_dir $OutputDir/logs \\\n53:    --seed 42 \\\n54:# Removed --fp16 argument\n55:    --fp16_backend \"auto\" \\\n56:    --dataloader_num_workers 8 \\\n57:    --eval_dataloader_num_workers 2 \\\n58:    --include_inputs_for_metrics \\\n59:    --do_predict \\\n60:    --ddp_find_unused_parameters False \\\n61:    --report_to \"tensorboard\" \\\n62:    --dataloader_pin_memory True 2>&1 | tee $OutputDir/run.log\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 22:27\n",
    "  rm -rf ${OutputDir}; mkdir -p ${OutputDir};\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5e7c8c",
   "metadata": {},
   "source": [
    "### Step 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45be81c8",
   "metadata": {},
   "source": [
    "Execution Start time: 04:57:35, End time: 04:57:45 (duration: 9 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f562e4bd",
   "metadata": {},
   "source": [
    "**Thought**: The script has been updated to automatically delete the existing directory without prompting. Now, let's try running the inference script again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01bd78f",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24cea83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "08/22/2024 04:57:44 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n08/22/2024 04:57:44 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n_n_gpu=0,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_pin_memory=True,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndo_eval=False,\ndo_predict=False,\ndo_train=False,\nearly_stopping=5,\neval_accumulation_steps=None,\neval_dataloader_num_workers=0,\neval_delay=0,\neval_lenpen=1.0,\neval_steps=None,\nevaluation_strategy=no,\nfp16=False,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_min_num_params=0,\nfull_determinism=False,\ngeneration_max_length=1024,\ngeneration_num_beams=5,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngreater_is_better=None,\ngroup_by_length=False,\nhalf_precision_backend=cuda_amp,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=5e-05,\nlength_column_name=length,\nload_best_model_at_end=False,\nlocal_rank=-1,\nlog_level=-1,\nlog_level_replica=-1,\nlog_on_each_node=True,\nlogging_dir=/content/AMRBART/fine-tune/outputs/Infer-examples-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp/logs,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=500,\nlogging_strategy=steps,\nlr_scheduler_type=linear,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=None,\nmp_parameters=,\nno_cuda=False,\nnum_train_epochs=3.0,\noptim=adamw_hf,\noutput_dir=/content/AMRBART/fine-tune/outputs/Infer-examples-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp,\noverwrite_output_dir=True,\npast_index=-1,\nper_device_eval_batch_size=16,\nper_device_train_batch_size=8,\npredict_with_generate=True,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=/content/AMRBART/fine-tune/outputs/Infer-examples-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp,\nsave_on_each_node=False,\nsave_steps=500,\nsave_strategy=steps,\nsave_total_limit=None,\nseed=42,\nsharded_ddp=[],\nskip_memory_metrics=True,\nsmart_init=False,\nsortish_sampler=False,\ntask=text2amr,\ntf32=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nwarmup_ratio=0.0,\nwarmup_steps=0,\nweight_decay=0.0,\nxpu_backend=None,\n)\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 623, in _get_config_dict\n    resolved_config_file = cached_path(\n  File \"/usr/local/lib/python3.10/site-packages/transformers/utils/hub.py\", line 284, in cached_path\n    output_path = get_from_cache(\n  File \"/usr/local/lib/python3.10/site-packages/transformers/utils/hub.py\", line 562, in get_from_cache\n    raise ValueError(\nValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/content/AMRBART/fine-tune/main.py\", line 506, in <module>\n    main()\n  File \"/content/AMRBART/fine-tune/main.py\", line 124, in main\n    config = AutoConfig.from_pretrained(\n  File \"/usr/local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\", line 725, in from_pretrained\n    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 561, in get_config_dict\n    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 656, in _get_config_dict\n    raise EnvironmentError(\nOSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp/checkpoint-1 is not the path to a directory containing a config.json file.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\ninference-amr.sh: line 55: --fp16_backend: command not found\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!bash inference-amr.sh \"outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp/checkpoint-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895dc282",
   "metadata": {},
   "source": [
    "### Step 43"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1bb87f",
   "metadata": {},
   "source": [
    "Execution Start time: 04:57:47, End time: 04:57:48 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461173aa",
   "metadata": {},
   "source": [
    "**Thought**: It seems there is an issue with connecting to Hugging Face to load the model configuration, and there is also a command not found error related to `--fp16_backend`. Let's address these issues.\n",
    "\n",
    "First, I'll check the internet connection to ensure we can access Hugging Face. Then, I'll correct the script to handle the `--fp16_backend` argument properly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1560b9a2",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c60b676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/usr/bin/sh: 1: ping: not found\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ping -c 4 huggingface.co"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41368706",
   "metadata": {},
   "source": [
    "### Step 44"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1564a28",
   "metadata": {},
   "source": [
    "Execution Start time: 04:57:50, End time: 04:57:50 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4616c17a",
   "metadata": {},
   "source": [
    "**Thought**: The `ping` command is not available in this environment. Instead, I'll check if we can access Hugging Face by attempting to download a small file using `wget`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57cb785",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cad003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Spider mode enabled. Check if remote file exists.\n--2024-08-22 04:57:50--  https://huggingface.co/\nResolving huggingface.co (huggingface.co)... 18.164.174.118, 18.164.174.55, 18.164.174.17, ...\nConnecting to huggingface.co (huggingface.co)|18.164.174.118|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 111245 (109K) [text/html]\nRemote file exists and could contain further links,\nbut recursion is disabled -- not retrieving.\n\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!wget --spider https://huggingface.co"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d432cf70",
   "metadata": {},
   "source": [
    "### Step 45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815e3251",
   "metadata": {},
   "source": [
    "Execution Start time: 04:57:52, End time: 04:57:52 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6145b929",
   "metadata": {},
   "source": [
    "**Thought**: The connection to Hugging Face is working fine. The issue might be related to the path or the script itself. Let's open the inference script and check the lines around the `--fp16_backend` argument to ensure it's correctly formatted.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1f03a0",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b7c5e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/AMRBART/fine-tune/inference-amr.sh (62 lines total)]\n1:export CUDA_VISIBLE_DEVICES=0\n2:RootDir=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" >/dev/null 2>&1 && pwd )\"\n3:\n4:Dataset=examples\n5:\n6:BasePath=/mnt/nfs-storage/data                    # change dir here\n7:DataPath=$RootDir/../$Dataset\n8:\n9:ModelCate=AMRBART-large\n10:\n11:MODEL=$1\n12:ModelCache=$BasePath/.cache\n13:DataCache=$DataPath/.cache/dump-amrparsing\n14:\n15:lr=1e-5\n16:\n17:OutputDir=${RootDir}/outputs/Infer-$Dataset-${ModelCate}-AMRParing-bsz16-lr-${lr}-UnifiedInp\n18:\n19:if [ ! -d ${OutputDir} ];then\n20:  mkdir -p ${OutputDir}\n21:else\n22:  rm -rf ${OutputDir}; mkdir -p ${OutputDir};\n23:fi\n24:\n25:export HF_DATASETS_CACHE=$DataCache\n26:\n27:if [ ! -d ${DataCache} ];then\n28:  mkdir -p ${DataCache}\n29:fi\n30:\n31:# torchrun --nnodes=1 --nproc_per_node=1 --max_restarts=0 --rdzv_id=1 --rdzv_backend=c10d main.py \\\n32:python -u main.py \\\n33:    --data_dir $DataPath \\\n34:    --task \"text2amr\" \\\n35:    --test_file $DataPath/data4parsing.jsonl \\\n36:    --output_dir $OutputDir \\\n37:    --cache_dir $ModelCache \\\n38:    --data_cache_dir $DataCache \\\n39:    --overwrite_cache True \\\n40:    --model_name_or_path $MODEL \\\n41:    --overwrite_output_dir \\\n42:    --unified_input True \\\n43:    --per_device_eval_batch_size 16 \\\n44:    --max_source_length 400 \\\n45:    --max_target_length 1024 \\\n46:    --val_max_target_length 1024 \\\n47:    --generation_max_length 1024 \\\n48:    --generation_num_beams 5 \\\n49:    --predict_with_generate \\\n50:    --smart_init False \\\n51:    --use_fast_tokenizer False \\\n52:    --logging_dir $OutputDir/logs \\\n53:    --seed 42 \\\n54:# Removed --fp16 argument\n55:    --fp16_backend \"auto\" \\\n56:    --dataloader_num_workers 8 \\\n57:    --eval_dataloader_num_workers 2 \\\n58:    --include_inputs_for_metrics \\\n59:    --do_predict \\\n60:    --ddp_find_unused_parameters False \\\n61:    --report_to \"tensorboard\" \\\n62:    --dataloader_pin_memory True 2>&1 | tee $OutputDir/run.log\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open inference-amr.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3fb234",
   "metadata": {},
   "source": [
    "### Step 46"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e958da5",
   "metadata": {},
   "source": [
    "Execution Start time: 04:57:57, End time: 04:57:57 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704d5329",
   "metadata": {},
   "source": [
    "**Thought**: The `--fp16_backend` argument is still present, but it should be removed along with the `--fp16` argument. I will remove the `--fp16_backend` argument to prevent any issues.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557d5ae1",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f243abbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/AMRBART/fine-tune/inference-amr.sh (62 lines total)]\n1:export CUDA_VISIBLE_DEVICES=0\n2:RootDir=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" >/dev/null 2>&1 && pwd )\"\n3:\n4:Dataset=examples\n5:\n6:BasePath=/mnt/nfs-storage/data                    # change dir here\n7:DataPath=$RootDir/../$Dataset\n8:\n9:ModelCate=AMRBART-large\n10:\n11:MODEL=$1\n12:ModelCache=$BasePath/.cache\n13:DataCache=$DataPath/.cache/dump-amrparsing\n14:\n15:lr=1e-5\n16:\n17:OutputDir=${RootDir}/outputs/Infer-$Dataset-${ModelCate}-AMRParing-bsz16-lr-${lr}-UnifiedInp\n18:\n19:if [ ! -d ${OutputDir} ];then\n20:  mkdir -p ${OutputDir}\n21:else\n22:  rm -rf ${OutputDir}; mkdir -p ${OutputDir};\n23:fi\n24:\n25:export HF_DATASETS_CACHE=$DataCache\n26:\n27:if [ ! -d ${DataCache} ];then\n28:  mkdir -p ${DataCache}\n29:fi\n30:\n31:# torchrun --nnodes=1 --nproc_per_node=1 --max_restarts=0 --rdzv_id=1 --rdzv_backend=c10d main.py \\\n32:python -u main.py \\\n33:    --data_dir $DataPath \\\n34:    --task \"text2amr\" \\\n35:    --test_file $DataPath/data4parsing.jsonl \\\n36:    --output_dir $OutputDir \\\n37:    --cache_dir $ModelCache \\\n38:    --data_cache_dir $DataCache \\\n39:    --overwrite_cache True \\\n40:    --model_name_or_path $MODEL \\\n41:    --overwrite_output_dir \\\n42:    --unified_input True \\\n43:    --per_device_eval_batch_size 16 \\\n44:    --max_source_length 400 \\\n45:    --max_target_length 1024 \\\n46:    --val_max_target_length 1024 \\\n47:    --generation_max_length 1024 \\\n48:    --generation_num_beams 5 \\\n49:    --predict_with_generate \\\n50:    --smart_init False \\\n51:    --use_fast_tokenizer False \\\n52:    --logging_dir $OutputDir/logs \\\n53:    --seed 42 \\\n54:# Removed --fp16 argument\n55:# Removed --fp16_backend argument\n56:    --dataloader_num_workers 8 \\\n57:    --eval_dataloader_num_workers 2 \\\n58:    --include_inputs_for_metrics \\\n59:    --do_predict \\\n60:    --ddp_find_unused_parameters False \\\n61:    --report_to \"tensorboard\" \\\n62:    --dataloader_pin_memory True 2>&1 | tee $OutputDir/run.log\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 55:55\n",
    "# Removed --fp16_backend argument\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fca7fcf",
   "metadata": {},
   "source": [
    "### Step 47"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed2f719",
   "metadata": {},
   "source": [
    "Execution Start time: 04:57:59, End time: 04:58:10 (duration: 10 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31348a89",
   "metadata": {},
   "source": [
    "**Thought**: The `--fp16_backend` argument has been removed from the inference script. Now, let's try running the inference script again to see if it works without attempting to use FP16 precision.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc8adbf",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3580cd77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "08/22/2024 04:58:08 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n08/22/2024 04:58:08 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n_n_gpu=0,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_pin_memory=True,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndo_eval=False,\ndo_predict=False,\ndo_train=False,\nearly_stopping=5,\neval_accumulation_steps=None,\neval_dataloader_num_workers=0,\neval_delay=0,\neval_lenpen=1.0,\neval_steps=None,\nevaluation_strategy=no,\nfp16=False,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_min_num_params=0,\nfull_determinism=False,\ngeneration_max_length=1024,\ngeneration_num_beams=5,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngreater_is_better=None,\ngroup_by_length=False,\nhalf_precision_backend=cuda_amp,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=5e-05,\nlength_column_name=length,\nload_best_model_at_end=False,\nlocal_rank=-1,\nlog_level=-1,\nlog_level_replica=-1,\nlog_on_each_node=True,\nlogging_dir=/content/AMRBART/fine-tune/outputs/Infer-examples-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp/logs,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=500,\nlogging_strategy=steps,\nlr_scheduler_type=linear,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=None,\nmp_parameters=,\nno_cuda=False,\nnum_train_epochs=3.0,\noptim=adamw_hf,\noutput_dir=/content/AMRBART/fine-tune/outputs/Infer-examples-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp,\noverwrite_output_dir=True,\npast_index=-1,\nper_device_eval_batch_size=16,\nper_device_train_batch_size=8,\npredict_with_generate=True,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=/content/AMRBART/fine-tune/outputs/Infer-examples-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp,\nsave_on_each_node=False,\nsave_steps=500,\nsave_strategy=steps,\nsave_total_limit=None,\nseed=42,\nsharded_ddp=[],\nskip_memory_metrics=True,\nsmart_init=False,\nsortish_sampler=False,\ntask=text2amr,\ntf32=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nwarmup_ratio=0.0,\nwarmup_steps=0,\nweight_decay=0.0,\nxpu_backend=None,\n)\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 623, in _get_config_dict\n    resolved_config_file = cached_path(\n  File \"/usr/local/lib/python3.10/site-packages/transformers/utils/hub.py\", line 284, in cached_path\n    output_path = get_from_cache(\n  File \"/usr/local/lib/python3.10/site-packages/transformers/utils/hub.py\", line 562, in get_from_cache\n    raise ValueError(\nValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/content/AMRBART/fine-tune/main.py\", line 506, in <module>\n    main()\n  File \"/content/AMRBART/fine-tune/main.py\", line 124, in main\n    config = AutoConfig.from_pretrained(\n  File \"/usr/local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\", line 725, in from_pretrained\n    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 561, in get_config_dict\n    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 656, in _get_config_dict\n    raise EnvironmentError(\nOSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp/checkpoint-1 is not the path to a directory containing a config.json file.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\ninference-amr.sh: line 56: --dataloader_num_workers: command not found\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!bash inference-amr.sh \"outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp/checkpoint-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def97519",
   "metadata": {},
   "source": [
    "### Step 48"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2c9907",
   "metadata": {},
   "source": [
    "Execution Start time: 04:58:12, End time: 04:58:12 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72fdf39",
   "metadata": {},
   "source": [
    "**Thought**: It seems there is still an issue with connecting to Hugging Face to load the model configuration, and there is also a command not found error related to `--dataloader_num_workers`. Let's address these issues.\n",
    "\n",
    "First, I'll ensure that the model path is correct and contains the necessary configuration files. Then, I'll correct the script to handle the `--dataloader_num_workers` argument properly.\n",
    "\n",
    "Let's open the inference script and check the lines around the `--dataloader_num_workers` argument to ensure it's correctly formatted.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e6de50",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab626e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/AMRBART/fine-tune/inference-amr.sh (62 lines total)]\n1:export CUDA_VISIBLE_DEVICES=0\n2:RootDir=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" >/dev/null 2>&1 && pwd )\"\n3:\n4:Dataset=examples\n5:\n6:BasePath=/mnt/nfs-storage/data                    # change dir here\n7:DataPath=$RootDir/../$Dataset\n8:\n9:ModelCate=AMRBART-large\n10:\n11:MODEL=$1\n12:ModelCache=$BasePath/.cache\n13:DataCache=$DataPath/.cache/dump-amrparsing\n14:\n15:lr=1e-5\n16:\n17:OutputDir=${RootDir}/outputs/Infer-$Dataset-${ModelCate}-AMRParing-bsz16-lr-${lr}-UnifiedInp\n18:\n19:if [ ! -d ${OutputDir} ];then\n20:  mkdir -p ${OutputDir}\n21:else\n22:  rm -rf ${OutputDir}; mkdir -p ${OutputDir};\n23:fi\n24:\n25:export HF_DATASETS_CACHE=$DataCache\n26:\n27:if [ ! -d ${DataCache} ];then\n28:  mkdir -p ${DataCache}\n29:fi\n30:\n31:# torchrun --nnodes=1 --nproc_per_node=1 --max_restarts=0 --rdzv_id=1 --rdzv_backend=c10d main.py \\\n32:python -u main.py \\\n33:    --data_dir $DataPath \\\n34:    --task \"text2amr\" \\\n35:    --test_file $DataPath/data4parsing.jsonl \\\n36:    --output_dir $OutputDir \\\n37:    --cache_dir $ModelCache \\\n38:    --data_cache_dir $DataCache \\\n39:    --overwrite_cache True \\\n40:    --model_name_or_path $MODEL \\\n41:    --overwrite_output_dir \\\n42:    --unified_input True \\\n43:    --per_device_eval_batch_size 16 \\\n44:    --max_source_length 400 \\\n45:    --max_target_length 1024 \\\n46:    --val_max_target_length 1024 \\\n47:    --generation_max_length 1024 \\\n48:    --generation_num_beams 5 \\\n49:    --predict_with_generate \\\n50:    --smart_init False \\\n51:    --use_fast_tokenizer False \\\n52:    --logging_dir $OutputDir/logs \\\n53:    --seed 42 \\\n54:# Removed --fp16 argument\n55:# Removed --fp16_backend argument\n56:    --dataloader_num_workers 8 \\\n57:    --eval_dataloader_num_workers 2 \\\n58:    --include_inputs_for_metrics \\\n59:    --do_predict \\\n60:    --ddp_find_unused_parameters False \\\n61:    --report_to \"tensorboard\" \\\n62:    --dataloader_pin_memory True 2>&1 | tee $OutputDir/run.log\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open inference-amr.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3b06f5",
   "metadata": {},
   "source": [
    "### Step 49"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a8b9a9",
   "metadata": {},
   "source": [
    "Execution Start time: 04:58:15, End time: 04:58:15 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c352ccc",
   "metadata": {},
   "source": [
    "**Thought**: The `--dataloader_num_workers` argument seems to be correctly formatted. The issue might be related to the model path. Let's ensure that the model path is correct and contains the necessary configuration files.\n",
    "\n",
    "I'll check the directory structure to verify the presence of the `config.json` file in the specified model path.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd7cc99",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e6017d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ls: cannot access 'outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp/checkpoint-1': No such file or directory\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ls outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp/checkpoint-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55146c7",
   "metadata": {},
   "source": [
    "### Step 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2fc699",
   "metadata": {},
   "source": [
    "Execution Start time: 04:58:17, End time: 04:58:17 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42538085",
   "metadata": {},
   "source": [
    "**Thought**: It seems that the specified model path does not exist. This might be due to an incorrect path or the model not being saved properly during training. Let's list the contents of the `outputs` directory to see what is available.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4ff453",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086c9eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp\nInfer-examples-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp\nLDC2020-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ls outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f78b67",
   "metadata": {},
   "source": [
    "### Step 51"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4326aa",
   "metadata": {},
   "source": [
    "Execution Start time: 04:58:20, End time: 04:58:20 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cb3b1f",
   "metadata": {},
   "source": [
    "**Thought**: The directory `AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp` exists, but it seems there might be an issue with the checkpoint directory. Let's check the contents of this directory to see if there are any checkpoints saved.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd527db",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96ff787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dummy_input.json  logs\trun.log\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ls outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46d6b0e",
   "metadata": {},
   "source": [
    "### Step 52"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a14176",
   "metadata": {},
   "source": [
    "Execution Start time: 04:58:22, End time: 04:58:22 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae1fca4",
   "metadata": {},
   "source": [
    "**Thought**: It seems that the expected checkpoint directory is missing. The training process might not have saved the model checkpoint correctly. Let's check the training log to see if there were any issues during the model saving process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36740ac8",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafecdae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "08/22/2024 04:47:17 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n08/22/2024 04:47:17 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n_n_gpu=0,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=8,\ndataloader_pin_memory=True,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=False,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=True,\ndo_train=True,\nearly_stopping=10,\neval_accumulation_steps=None,\neval_dataloader_num_workers=2,\neval_delay=0,\neval_lenpen=1.0,\neval_steps=None,\nevaluation_strategy=epoch,\nfp16=False,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_min_num_params=0,\nfull_determinism=False,\ngeneration_max_length=1024,\ngeneration_num_beams=5,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngreater_is_better=True,\ngroup_by_length=False,\nhalf_precision_backend=cuda_amp,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=True,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.1,\nlearning_rate=1e-05,\nlength_column_name=length,\nload_best_model_at_end=True,\nlocal_rank=-1,\nlog_level=-1,\nlog_level_replica=-1,\nlog_on_each_node=True,\nlogging_dir=/content/AMRBART/fine-tune/outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp/logs,\nlogging_first_step=True,\nlogging_nan_inf_filter=True,\nlogging_steps=20,\nlogging_strategy=steps,\nlr_scheduler_type=polynomial,\nmax_grad_norm=0.0,\nmax_steps=-1,\nmetric_for_best_model=eval_smatch,\nmp_parameters=,\nno_cuda=False,\nnum_train_epochs=30.0,\noptim=adamw_hf,\noutput_dir=/content/AMRBART/fine-tune/outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp,\noverwrite_output_dir=True,\npast_index=-1,\nper_device_eval_batch_size=8,\nper_device_train_batch_size=16,\npredict_with_generate=True,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=/content/AMRBART/fine-tune/outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp,\nsave_on_each_node=False,\nsave_steps=500,\nsave_strategy=epoch,\nsave_total_limit=1,\nseed=42,\nsharded_ddp=[],\nskip_memory_metrics=True,\nsmart_init=False,\nsortish_sampler=False,\ntask=text2amr,\ntf32=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nwarmup_ratio=0.0,\nwarmup_steps=200,\nweight_decay=0.01,\nxpu_backend=None,\n)\n[INFO|configuration_utils.py:681] 2024-08-22 04:47:17,514 >> loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /mnt/nfs-storage/data/.cache/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\n[INFO|configuration_utils.py:730] 2024-08-22 04:47:17,515 >> Model config BartConfig {\n  \"_name_or_path\": \"facebook/bart-base\",\n  \"activation_dropout\": 0.1,\n  \"activation_function\": \"gelu\",\n  \"add_bias_logits\": false,\n  \"add_final_layer_norm\": false,\n  \"architectures\": [\n    \"BartModel\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"bos_token_id\": 0,\n  \"classif_dropout\": 0.1,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 768,\n  \"decoder_attention_heads\": 12,\n  \"decoder_ffn_dim\": 3072,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 6,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"early_stopping\": true,\n  \"encoder_attention_heads\": 12,\n  \"encoder_ffn_dim\": 3072,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 6,\n  \"eos_token_id\": 2,\n  \"forced_bos_token_id\": 0,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\"\n  },\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2\n  },\n  \"max_position_embeddings\": 1024,\n  \"model_type\": \"bart\",\n  \"no_repeat_ngram_size\": 3,\n  \"normalize_before\": false,\n  \"normalize_embedding\": true,\n  \"num_beams\": 4,\n  \"num_hidden_layers\": 6,\n  \"pad_token_id\": 1,\n  \"scale_embedding\": false,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 128,\n      \"min_length\": 12,\n      \"num_beams\": 4\n    },\n    \"summarization_cnn\": {\n      \"length_penalty\": 2.0,\n      \"max_length\": 142,\n      \"min_length\": 56,\n      \"num_beams\": 4\n    },\n    \"summarization_xsum\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 62,\n      \"min_length\": 11,\n      \"num_beams\": 6\n    }\n  },\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.21.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:47:18,139 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/vocab.json from cache at /mnt/nfs-storage/data/.cache/0d6fc8b2ef1860c1f8f0baff4b021e3426cc7d11b153f98e563b799603ee2f25.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:47:18,140 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/merges.txt from cache at /mnt/nfs-storage/data/.cache/6e75e35f0bdd15870c98387e13b93a8e100237eb33ad99c36277a0562bd6d850.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:47:18,140 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:47:18,141 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:47:18,141 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/tokenizer_config.json from cache at /mnt/nfs-storage/data/.cache/1abf196c889c24daca2909359ca2090e5fcbfa21a9ea36d763f70adbafb500d7.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8\n[INFO|configuration_utils.py:681] 2024-08-22 04:47:18,267 >> loading configuration file https://huggingface.co/facebook/bart-large/resolve/main/config.json from cache at /mnt/nfs-storage/data/.cache/3f12fb71b844fcb7d591fdd4e55027da90d7b5dd6aa5430ad00ec6d76585f26c.bc22f15dc7ba074ee0a60bdd34c5f2fe3b6d746f89e765303376c51aff04e260\n[INFO|configuration_utils.py:730] 2024-08-22 04:47:18,268 >> Model config BartConfig {\n  \"_name_or_path\": \"facebook/bart-large\",\n  \"activation_dropout\": 0.1,\n  \"activation_function\": \"gelu\",\n  \"add_bias_logits\": false,\n  \"add_final_layer_norm\": false,\n  \"architectures\": [\n    \"BartModel\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"bos_token_id\": 0,\n  \"classif_dropout\": 0.1,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 12,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"early_stopping\": true,\n  \"encoder_attention_heads\": 16,\n  \"encoder_ffn_dim\": 4096,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 12,\n  \"eos_token_id\": 2,\n  \"forced_bos_token_id\": 0,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\"\n  },\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2\n  },\n  \"max_position_embeddings\": 1024,\n  \"model_type\": \"bart\",\n  \"no_repeat_ngram_size\": 3,\n  \"normalize_before\": false,\n  \"num_beams\": 4,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"scale_embedding\": false,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 128,\n      \"min_length\": 12,\n      \"num_beams\": 4\n    },\n    \"summarization_cnn\": {\n      \"length_penalty\": 2.0,\n      \"max_length\": 142,\n      \"min_length\": 56,\n      \"num_beams\": 4\n    },\n    \"summarization_xsum\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 62,\n      \"min_length\": 11,\n      \"num_beams\": 6\n    }\n  },\n  \"transformers_version\": \"4.21.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\n[WARNING|tokenization_utils_base.py:1896] 2024-08-22 04:47:18,268 >> The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'BartTokenizer'. \nThe class this function is called from is 'AMRBartTokenizer'.\nAdded 2965 AMR tokens\n[INFO|modeling_utils.py:2041] 2024-08-22 04:47:18,552 >> loading weights file https://huggingface.co/facebook/bart-base/resolve/main/pytorch_model.bin from cache at /mnt/nfs-storage/data/.cache/486355ec722ef05fd480e999d4c763be56549ae930f6a3742ee721a5d2a05647.f2f355ad2775769afc60592b43a46d72ca548375e3a1d65f381a751e711cbadd\n[INFO|modeling_utils.py:2435] 2024-08-22 04:47:21,281 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n\n[INFO|modeling_utils.py:2443] 2024-08-22 04:47:21,282 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\nDataset cache dir: /mnt/nfs-storage/data/.cache\n/usr/local/lib/python3.10/site-packages/datasets/load.py:929: FutureWarning: The repository for data contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /content/AMRBART/fine-tune/data_interface/data.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\nUsing custom data configuration default-b849ce260fee6cea\n08/22/2024 04:47:22 - INFO - datasets.builder - Using custom data configuration default-b849ce260fee6cea\nLoading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/data/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5\n08/22/2024 04:47:22 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/data/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5\nGenerating dataset data (/content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-b849ce260fee6cea/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5)\n08/22/2024 04:47:22 - INFO - datasets.builder - Generating dataset data (/content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-b849ce260fee6cea/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5)\nDownloading and preparing dataset data/default to /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-b849ce260fee6cea/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5...\n08/22/2024 04:47:22 - INFO - datasets.builder - Downloading and preparing dataset data/default to /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-b849ce260fee6cea/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5...\nGenerating train split\n08/22/2024 04:47:22 - INFO - datasets.builder - Generating train split\n\nGenerating train split: 0 examples [00:00, ? examples/s]\nGenerating train split: 10 examples [00:00, 398.79 examples/s]\nGenerating validation split\n08/22/2024 04:47:22 - INFO - datasets.builder - Generating validation split\n\nGenerating validation split: 0 examples [00:00, ? examples/s]\nGenerating validation split: 10 examples [00:00, 1665.13 examples/s]\nGenerating test split\n08/22/2024 04:47:22 - INFO - datasets.builder - Generating test split\n\nGenerating test split: 0 examples [00:00, ? examples/s]\nGenerating test split: 10 examples [00:00, 1953.38 examples/s]\nUnable to verify splits sizes.\n08/22/2024 04:47:22 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\nDataset data downloaded and prepared to /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-b849ce260fee6cea/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5. Subsequent calls will reuse this data.\n08/22/2024 04:47:22 - INFO - datasets.builder - Dataset data downloaded and prepared to /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-b849ce260fee6cea/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5. Subsequent calls will reuse this data.\ndatasets: DatasetDict({\n    train: Dataset({\n        features: ['src', 'tgt'],\n        num_rows: 10\n    })\n    validation: Dataset({\n        features: ['src', 'tgt'],\n        num_rows: 10\n    })\n    test: Dataset({\n        features: ['src', 'tgt'],\n        num_rows: 10\n    })\n})\ncolums: ['src', 'tgt']\n\nRunning tokenizer on train dataset:   0%|          | 0/10 [00:00<?, ? examples/s]Caching processed dataset at /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-b849ce260fee6cea/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5/cache-c3331aac1c3a5c5b.arrow\n08/22/2024 04:47:23 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-b849ce260fee6cea/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5/cache-c3331aac1c3a5c5b.arrow\n\nRunning tokenizer on train dataset: 100%|██████████| 10/10 [00:00<00:00, 272.73 examples/s]\nSaving cached train data ...\n\nSaving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s]\nSaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2477.88 examples/s]\nSaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2349.49 examples/s]\n\nRunning tokenizer on validation dataset:   0%|          | 0/10 [00:00<?, ? examples/s]Caching processed dataset at /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-b849ce260fee6cea/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5/cache-d67b3c899c63a4ef.arrow\n08/22/2024 04:47:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-b849ce260fee6cea/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5/cache-d67b3c899c63a4ef.arrow\n\nRunning tokenizer on validation dataset: 100%|██████████| 10/10 [00:00<00:00, 214.82 examples/s]\nSaving cached validation data ...\n\nSaving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s]\nSaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2720.04 examples/s]\nSaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2577.15 examples/s]\n\nRunning tokenizer on prediction dataset:   0%|          | 0/10 [00:00<?, ? examples/s]Caching processed dataset at /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-b849ce260fee6cea/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5/cache-07d173b48cb52365.arrow\n08/22/2024 04:47:27 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-b849ce260fee6cea/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5/cache-07d173b48cb52365.arrow\n\nRunning tokenizer on prediction dataset: 100%|██████████| 10/10 [00:00<00:00, 445.97 examples/s]\nSaving cached test data ...\n\nSaving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s]\nSaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2688.14 examples/s]\nSaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2509.01 examples/s]\n/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 5, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/usr/local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nAll 30 steps, warm_up steps: 200\n\n  0%|          | 0/30 [00:00<?, ?it/s]/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 5, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n\n  3%|▎         | 1/30 [00:33<16:05, 33.31s/it]\n                                              \n{'loss': 6.0523, 'learning_rate': 5.0000000000000004e-08, 'epoch': 1.0}\n\n  3%|▎         | 1/30 [00:33<16:05, 33.31s/it]Generation Kwargs:\n{'max_length': 1024, 'max_gen_length': 1024, 'num_beams': 5}\nSaving dummy inputs...\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!cat outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp/run.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7249adc6",
   "metadata": {},
   "source": [
    "### Step 53"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1dadb2",
   "metadata": {},
   "source": [
    "Execution Start time: 04:58:25, End time: 04:58:25 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fec685b",
   "metadata": {},
   "source": [
    "**Thought**: The training log indicates that the model was trained successfully, but it seems the checkpoint was not saved correctly. This might be due to the `save_steps` or `save_strategy` settings. Let's adjust these settings to ensure the model checkpoint is saved properly.\n",
    "\n",
    "I'll modify the training script to save the model checkpoint after each epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dc0310",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106d700b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/AMRBART/fine-tune/train-AMRBART-large-AMRParsing.sh (85 lines total)]\n1:export CUDA_VISIBLE_DEVICES=0\n2:RootDir=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" >/dev/null 2>&1 && pwd )\"\n3:\n4:Dataset=AMRBART\n5:#Dataset=LDC2017\n6:\n7:BasePath=/mnt/nfs-storage/data        # change dir here\n8:DataPath=$RootDir/data/$Dataset\n9:\n10:ModelCate=AMRBART-large\n11:\n12:MODEL=$1\n13:ModelCache=$BasePath/.cache\n14:DataCache=$DataPath/.cache/dump-amrparsing\n15:\n16:lr=1e-5\n17:\n18:OutputDir=${RootDir}/outputs/$Dataset-${ModelCate}-AMRParing-bsz16-lr-${lr}-UnifiedInp\n19:\n20:if [ ! -d ${OutputDir} ];then\n21:  mkdir -p ${OutputDir}\n22:else\n23:  rm -rf ${OutputDir}; mkdir -p ${OutputDir};\n24:fi\n25:export HF_DATASETS_CACHE=$DataCache\n26:\n27:if [ ! -d ${DataCache} ];then\n28:  mkdir -p ${DataCache}\n29:fi\n30:\n31:# torchrun --nnodes=1 --nproc_per_node=1 --max_restarts=0 --rdzv_id=1 --rdzv_backend=c10d main.py \\\n32:python -u main.py \\\n33:    --data_dir $DataPath \\\n34:    --task \"text2amr\" \\\n35:    --train_file $DataPath/train.jsonl \\\n36:    --validation_file $DataPath/val.jsonl \\\n37:    --test_file $DataPath/test.jsonl \\\n38:    --output_dir $OutputDir \\\n39:    --cache_dir $ModelCache \\\n40:    --data_cache_dir $DataCache \\\n41:    --tokenizer_name \"facebook/bart-large\" \\\n42:    --model_name_or_path $MODEL \\\n43:    --overwrite_output_dir \\\n44:    --unified_input True \\\n45:    --per_device_train_batch_size 16 \\\n46:    --per_device_eval_batch_size 8 \\\n47:    --gradient_accumulation_steps 1 \\\n48:    --learning_rate $lr \\\n49:    --optim \"adamw_hf\" \\\n50:    --lr_scheduler_type \"polynomial\" \\\n51:    --warmup_steps 200 \\\n52:    --num_train_epochs 30 \\\n53:    --early_stopping 10 \\\n54:    --max_source_length 400 \\\n55:    --max_target_length 1024 \\\n56:    --val_max_target_length 1024 \\\n57:    --generation_max_length 1024 \\\n58:    --generation_num_beams 5 \\\n59:    --label_smoothing_factor 0.1 \\\n60:    --evaluation_strategy \"epoch\" \\\n61:    --weight_decay 0.01 \\\n62:    --max_grad_norm 0 \\\n63:    --max_steps -1 \\\n64:    --predict_with_generate \\\n65:    --smart_init False \\\n66:    --use_fast_tokenizer False \\\n67:    --logging_dir $OutputDir/logs \\\n68:    --logging_first_step True \\\n69:    --logging_steps 20 \\\n70:    --save_strategy \"epoch\" \\\n71:    --save_total_limit 1 \\\n72:    --seed 42 \\\n73:    --fp16_backend \"auto\" \\\n74:    --dataloader_num_workers 8 \\\n75:    --eval_dataloader_num_workers 2 \\\n76:    --load_best_model_at_end True \\\n77:    --metric_for_best_model \"eval_smatch\" \\\n78:    --include_inputs_for_metrics \\\n79:    --greater_is_better True \\\n80:    --do_train \\\n81:    --do_eval \\\n82:    --do_predict \\\n83:    --ddp_find_unused_parameters False \\\n84:    --report_to \"tensorboard\" \\\n85:    --dataloader_pin_memory True 2>&1 | tee $OutputDir/run.log\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open train-AMRBART-large-AMRParsing.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc209c30",
   "metadata": {},
   "source": [
    "### Step 54"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc385c0",
   "metadata": {},
   "source": [
    "Execution Start time: 04:58:28, End time: 04:58:29 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656db098",
   "metadata": {},
   "source": [
    "**Thought**: The `save_strategy` is already set to \"epoch\", which should save the model after each epoch. However, the `num_train_epochs` is set to 30, which is more than needed. Let's ensure that the model is saved after the first epoch by setting `num_train_epochs` to 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c7e1da",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66853035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/AMRBART/fine-tune/train-AMRBART-large-AMRParsing.sh (85 lines total)]\n1:export CUDA_VISIBLE_DEVICES=0\n2:RootDir=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" >/dev/null 2>&1 && pwd )\"\n3:\n4:Dataset=AMRBART\n5:#Dataset=LDC2017\n6:\n7:BasePath=/mnt/nfs-storage/data        # change dir here\n8:DataPath=$RootDir/data/$Dataset\n9:\n10:ModelCate=AMRBART-large\n11:\n12:MODEL=$1\n13:ModelCache=$BasePath/.cache\n14:DataCache=$DataPath/.cache/dump-amrparsing\n15:\n16:lr=1e-5\n17:\n18:OutputDir=${RootDir}/outputs/$Dataset-${ModelCate}-AMRParing-bsz16-lr-${lr}-UnifiedInp\n19:\n20:if [ ! -d ${OutputDir} ];then\n21:  mkdir -p ${OutputDir}\n22:else\n23:  rm -rf ${OutputDir}; mkdir -p ${OutputDir};\n24:fi\n25:export HF_DATASETS_CACHE=$DataCache\n26:\n27:if [ ! -d ${DataCache} ];then\n28:  mkdir -p ${DataCache}\n29:fi\n30:\n31:# torchrun --nnodes=1 --nproc_per_node=1 --max_restarts=0 --rdzv_id=1 --rdzv_backend=c10d main.py \\\n32:python -u main.py \\\n33:    --data_dir $DataPath \\\n34:    --task \"text2amr\" \\\n35:    --train_file $DataPath/train.jsonl \\\n36:    --validation_file $DataPath/val.jsonl \\\n37:    --test_file $DataPath/test.jsonl \\\n38:    --output_dir $OutputDir \\\n39:    --cache_dir $ModelCache \\\n40:    --data_cache_dir $DataCache \\\n41:    --tokenizer_name \"facebook/bart-large\" \\\n42:    --model_name_or_path $MODEL \\\n43:    --overwrite_output_dir \\\n44:    --unified_input True \\\n45:    --per_device_train_batch_size 16 \\\n46:    --per_device_eval_batch_size 8 \\\n47:    --gradient_accumulation_steps 1 \\\n48:    --learning_rate $lr \\\n49:    --optim \"adamw_hf\" \\\n50:    --lr_scheduler_type \"polynomial\" \\\n51:    --warmup_steps 200 \\\n52:--num_train_epochs 1 \\\n53:    --early_stopping 10 \\\n54:    --max_source_length 400 \\\n55:    --max_target_length 1024 \\\n56:    --val_max_target_length 1024 \\\n57:    --generation_max_length 1024 \\\n58:    --generation_num_beams 5 \\\n59:    --label_smoothing_factor 0.1 \\\n60:    --evaluation_strategy \"epoch\" \\\n61:    --weight_decay 0.01 \\\n62:    --max_grad_norm 0 \\\n63:    --max_steps -1 \\\n64:    --predict_with_generate \\\n65:    --smart_init False \\\n66:    --use_fast_tokenizer False \\\n67:    --logging_dir $OutputDir/logs \\\n68:    --logging_first_step True \\\n69:    --logging_steps 20 \\\n70:    --save_strategy \"epoch\" \\\n71:    --save_total_limit 1 \\\n72:    --seed 42 \\\n73:    --fp16_backend \"auto\" \\\n74:    --dataloader_num_workers 8 \\\n75:    --eval_dataloader_num_workers 2 \\\n76:    --load_best_model_at_end True \\\n77:    --metric_for_best_model \"eval_smatch\" \\\n78:    --include_inputs_for_metrics \\\n79:    --greater_is_better True \\\n80:    --do_train \\\n81:    --do_eval \\\n82:    --do_predict \\\n83:    --ddp_find_unused_parameters False \\\n84:    --report_to \"tensorboard\" \\\n85:    --dataloader_pin_memory True 2>&1 | tee $OutputDir/run.log\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 52:52\n",
    "--num_train_epochs 1 \\\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9b9cff",
   "metadata": {},
   "source": [
    "### Step 55"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e85098",
   "metadata": {},
   "source": [
    "Execution Start time: 04:58:30, End time: 05:03:31 (duration: 300 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb7020a",
   "metadata": {},
   "source": [
    "**Thought**: The training script has been updated to run for only one epoch. Let's run the training script again to ensure the model checkpoint is saved correctly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0368458",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed97a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "08/22/2024 04:58:39 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n08/22/2024 04:58:39 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n_n_gpu=0,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=8,\ndataloader_pin_memory=True,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=False,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=True,\ndo_train=True,\nearly_stopping=10,\neval_accumulation_steps=None,\neval_dataloader_num_workers=2,\neval_delay=0,\neval_lenpen=1.0,\neval_steps=None,\nevaluation_strategy=epoch,\nfp16=False,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_min_num_params=0,\nfull_determinism=False,\ngeneration_max_length=1024,\ngeneration_num_beams=5,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngreater_is_better=True,\ngroup_by_length=False,\nhalf_precision_backend=cuda_amp,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=True,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.1,\nlearning_rate=1e-05,\nlength_column_name=length,\nload_best_model_at_end=True,\nlocal_rank=-1,\nlog_level=-1,\nlog_level_replica=-1,\nlog_on_each_node=True,\nlogging_dir=/content/AMRBART/fine-tune/outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp/logs,\nlogging_first_step=True,\nlogging_nan_inf_filter=True,\nlogging_steps=20,\nlogging_strategy=steps,\nlr_scheduler_type=polynomial,\nmax_grad_norm=0.0,\nmax_steps=-1,\nmetric_for_best_model=eval_smatch,\nmp_parameters=,\nno_cuda=False,\nnum_train_epochs=1.0,\noptim=adamw_hf,\noutput_dir=/content/AMRBART/fine-tune/outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp,\noverwrite_output_dir=True,\npast_index=-1,\nper_device_eval_batch_size=8,\nper_device_train_batch_size=16,\npredict_with_generate=True,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=/content/AMRBART/fine-tune/outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp,\nsave_on_each_node=False,\nsave_steps=500,\nsave_strategy=epoch,\nsave_total_limit=1,\nseed=42,\nsharded_ddp=[],\nskip_memory_metrics=True,\nsmart_init=False,\nsortish_sampler=False,\ntask=text2amr,\ntf32=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nwarmup_ratio=0.0,\nwarmup_steps=200,\nweight_decay=0.01,\nxpu_backend=None,\n)\n[INFO|configuration_utils.py:681] 2024-08-22 04:58:39,414 >> loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /mnt/nfs-storage/data/.cache/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\n[INFO|configuration_utils.py:730] 2024-08-22 04:58:39,416 >> Model config BartConfig {\n  \"_name_or_path\": \"facebook/bart-base\",\n  \"activation_dropout\": 0.1,\n  \"activation_function\": \"gelu\",\n  \"add_bias_logits\": false,\n  \"add_final_layer_norm\": false,\n  \"architectures\": [\n    \"BartModel\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"bos_token_id\": 0,\n  \"classif_dropout\": 0.1,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 768,\n  \"decoder_attention_heads\": 12,\n  \"decoder_ffn_dim\": 3072,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 6,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"early_stopping\": true,\n  \"encoder_attention_heads\": 12,\n  \"encoder_ffn_dim\": 3072,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 6,\n  \"eos_token_id\": 2,\n  \"forced_bos_token_id\": 0,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\"\n  },\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2\n  },\n  \"max_position_embeddings\": 1024,\n  \"model_type\": \"bart\",\n  \"no_repeat_ngram_size\": 3,\n  \"normalize_before\": false,\n  \"normalize_embedding\": true,\n  \"num_beams\": 4,\n  \"num_hidden_layers\": 6,\n  \"pad_token_id\": 1,\n  \"scale_embedding\": false,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 128,\n      \"min_length\": 12,\n      \"num_beams\": 4\n    },\n    \"summarization_cnn\": {\n      \"length_penalty\": 2.0,\n      \"max_length\": 142,\n      \"min_length\": 56,\n      \"num_beams\": 4\n    },\n    \"summarization_xsum\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 62,\n      \"min_length\": 11,\n      \"num_beams\": 6\n    }\n  },\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.21.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:58:40,111 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/vocab.json from cache at /mnt/nfs-storage/data/.cache/0d6fc8b2ef1860c1f8f0baff4b021e3426cc7d11b153f98e563b799603ee2f25.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:58:40,111 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/merges.txt from cache at /mnt/nfs-storage/data/.cache/6e75e35f0bdd15870c98387e13b93a8e100237eb33ad99c36277a0562bd6d850.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:58:40,111 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:58:40,111 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:58:40,111 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/tokenizer_config.json from cache at /mnt/nfs-storage/data/.cache/1abf196c889c24daca2909359ca2090e5fcbfa21a9ea36d763f70adbafb500d7.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8\n[INFO|configuration_utils.py:681] 2024-08-22 04:58:40,252 >> loading configuration file https://huggingface.co/facebook/bart-large/resolve/main/config.json from cache at /mnt/nfs-storage/data/.cache/3f12fb71b844fcb7d591fdd4e55027da90d7b5dd6aa5430ad00ec6d76585f26c.bc22f15dc7ba074ee0a60bdd34c5f2fe3b6d746f89e765303376c51aff04e260\n[INFO|configuration_utils.py:730] 2024-08-22 04:58:40,253 >> Model config BartConfig {\n  \"_name_or_path\": \"facebook/bart-large\",\n  \"activation_dropout\": 0.1,\n  \"activation_function\": \"gelu\",\n  \"add_bias_logits\": false,\n  \"add_final_layer_norm\": false,\n  \"architectures\": [\n    \"BartModel\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"bos_token_id\": 0,\n  \"classif_dropout\": 0.1,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 12,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"early_stopping\": true,\n  \"encoder_attention_heads\": 16,\n  \"encoder_ffn_dim\": 4096,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 12,\n  \"eos_token_id\": 2,\n  \"forced_bos_token_id\": 0,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\"\n  },\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2\n  },\n  \"max_position_embeddings\": 1024,\n  \"model_type\": \"bart\",\n  \"no_repeat_ngram_size\": 3,\n  \"normalize_before\": false,\n  \"num_beams\": 4,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"scale_embedding\": false,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 128,\n      \"min_length\": 12,\n      \"num_beams\": 4\n    },\n    \"summarization_cnn\": {\n      \"length_penalty\": 2.0,\n      \"max_length\": 142,\n      \"min_length\": 56,\n      \"num_beams\": 4\n    },\n    \"summarization_xsum\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 62,\n      \"min_length\": 11,\n      \"num_beams\": 6\n    }\n  },\n  \"transformers_version\": \"4.21.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\n[WARNING|tokenization_utils_base.py:1896] 2024-08-22 04:58:40,253 >> The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'BartTokenizer'. \nThe class this function is called from is 'AMRBartTokenizer'.\nAdded 2965 AMR tokens\n[INFO|modeling_utils.py:2041] 2024-08-22 04:58:40,519 >> loading weights file https://huggingface.co/facebook/bart-base/resolve/main/pytorch_model.bin from cache at /mnt/nfs-storage/data/.cache/486355ec722ef05fd480e999d4c763be56549ae930f6a3742ee721a5d2a05647.f2f355ad2775769afc60592b43a46d72ca548375e3a1d65f381a751e711cbadd\n[INFO|modeling_utils.py:2435] 2024-08-22 04:58:42,959 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n\n[INFO|modeling_utils.py:2443] 2024-08-22 04:58:42,959 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\nDataset cache dir: /mnt/nfs-storage/data/.cache\n/usr/local/lib/python3.10/site-packages/datasets/load.py:929: FutureWarning: The repository for data contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /content/AMRBART/fine-tune/data_interface/data.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\nUsing custom data configuration default-b849ce260fee6cea\n08/22/2024 04:58:43 - INFO - datasets.builder - Using custom data configuration default-b849ce260fee6cea\nLoading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/data/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5\n08/22/2024 04:58:43 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/data/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5\nOverwrite dataset info from restored data version if exists.\n08/22/2024 04:58:43 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\nLoading Dataset info from /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-b849ce260fee6cea/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5\n08/22/2024 04:58:43 - INFO - datasets.info - Loading Dataset info from /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-b849ce260fee6cea/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5\nFound cached dataset data (/content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-b849ce260fee6cea/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5)\n08/22/2024 04:58:43 - INFO - datasets.builder - Found cached dataset data (/content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-b849ce260fee6cea/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5)\nLoading Dataset info from /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-b849ce260fee6cea/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5\n08/22/2024 04:58:43 - INFO - datasets.info - Loading Dataset info from /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-b849ce260fee6cea/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5\ndatasets: DatasetDict({\n    train: Dataset({\n        features: ['src', 'tgt'],\n        num_rows: 10\n    })\n    validation: Dataset({\n        features: ['src', 'tgt'],\n        num_rows: 10\n    })\n    test: Dataset({\n        features: ['src', 'tgt'],\n        num_rows: 10\n    })\n})\ncolums: ['src', 'tgt']\n/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 5, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/usr/local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nAll 1 steps, warm_up steps: 200\n\n  0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 5, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n\n100%|██████████| 1/1 [00:25<00:00, 25.13s/it]\n                                             \n{'loss': 6.0523, 'learning_rate': 5.0000000000000004e-08, 'epoch': 1.0}\n\n100%|██████████| 1/1 [00:25<00:00, 25.13s/it]Generation Kwargs:\n{'max_length': 1024, 'max_gen_length': 1024, 'num_beams': 5}\nSaving dummy inputs...\n\n\nYour command has already run for 300 seconds. It is still running. You can continue to wait or interrupt it with \"action\": {\"type\": \"interrupt\"} or \"action\": {\"type\": \"continue_wait\"}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!bash train-AMRBART-large-AMRParsing.sh \"facebook/bart-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c218f7c",
   "metadata": {},
   "source": [
    "### Step 56"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11c09c7",
   "metadata": {},
   "source": [
    "Execution Start time: 05:03:34, End time: 05:03:34 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa680eb",
   "metadata": {},
   "source": [
    "**Thought**: The training script has completed successfully, and the model checkpoint should now be saved correctly. Let's check the contents of the `outputs` directory to verify the presence of the checkpoint.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe49706",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7860664a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dummy_input.json  logs\trun.log\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ls outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3332e02b",
   "metadata": {},
   "source": [
    "### Step 57"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495d7676",
   "metadata": {},
   "source": [
    "Execution Start time: 05:03:36, End time: 05:03:38 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9070db6b",
   "metadata": {},
   "source": [
    "**Thought**: It seems the checkpoint is still not being saved correctly. Let's check the training script to ensure that the model saving logic is correctly implemented.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f56752",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58225665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/AMRBART/fine-tune/train-AMRBART-large-AMRParsing.sh (85 lines total)]\n1:export CUDA_VISIBLE_DEVICES=0\n2:RootDir=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" >/dev/null 2>&1 && pwd )\"\n3:\n4:Dataset=AMRBART\n5:#Dataset=LDC2017\n6:\n7:BasePath=/mnt/nfs-storage/data        # change dir here\n8:DataPath=$RootDir/data/$Dataset\n9:\n10:ModelCate=AMRBART-large\n11:\n12:MODEL=$1\n13:ModelCache=$BasePath/.cache\n14:DataCache=$DataPath/.cache/dump-amrparsing\n15:\n16:lr=1e-5\n17:\n18:OutputDir=${RootDir}/outputs/$Dataset-${ModelCate}-AMRParing-bsz16-lr-${lr}-UnifiedInp\n19:\n20:if [ ! -d ${OutputDir} ];then\n21:  mkdir -p ${OutputDir}\n22:else\n23:  rm -rf ${OutputDir}; mkdir -p ${OutputDir};\n24:fi\n25:export HF_DATASETS_CACHE=$DataCache\n26:\n27:if [ ! -d ${DataCache} ];then\n28:  mkdir -p ${DataCache}\n29:fi\n30:\n31:# torchrun --nnodes=1 --nproc_per_node=1 --max_restarts=0 --rdzv_id=1 --rdzv_backend=c10d main.py \\\n32:python -u main.py \\\n33:    --data_dir $DataPath \\\n34:    --task \"text2amr\" \\\n35:    --train_file $DataPath/train.jsonl \\\n36:    --validation_file $DataPath/val.jsonl \\\n37:    --test_file $DataPath/test.jsonl \\\n38:    --output_dir $OutputDir \\\n39:    --cache_dir $ModelCache \\\n40:    --data_cache_dir $DataCache \\\n41:    --tokenizer_name \"facebook/bart-large\" \\\n42:    --model_name_or_path $MODEL \\\n43:    --overwrite_output_dir \\\n44:    --unified_input True \\\n45:    --per_device_train_batch_size 16 \\\n46:    --per_device_eval_batch_size 8 \\\n47:    --gradient_accumulation_steps 1 \\\n48:    --learning_rate $lr \\\n49:    --optim \"adamw_hf\" \\\n50:    --lr_scheduler_type \"polynomial\" \\\n51:    --warmup_steps 200 \\\n52:--num_train_epochs 1 \\\n53:    --early_stopping 10 \\\n54:    --max_source_length 400 \\\n55:    --max_target_length 1024 \\\n56:    --val_max_target_length 1024 \\\n57:    --generation_max_length 1024 \\\n58:    --generation_num_beams 5 \\\n59:    --label_smoothing_factor 0.1 \\\n60:    --evaluation_strategy \"epoch\" \\\n61:    --weight_decay 0.01 \\\n62:    --max_grad_norm 0 \\\n63:    --max_steps -1 \\\n64:    --predict_with_generate \\\n65:    --smart_init False \\\n66:    --use_fast_tokenizer False \\\n67:    --logging_dir $OutputDir/logs \\\n68:    --logging_first_step True \\\n69:    --logging_steps 20 \\\n70:    --save_strategy \"epoch\" \\\n71:    --save_total_limit 1 \\\n72:    --seed 42 \\\n73:    --fp16_backend \"auto\" \\\n74:    --dataloader_num_workers 8 \\\n75:    --eval_dataloader_num_workers 2 \\\n76:    --load_best_model_at_end True \\\n77:    --metric_for_best_model \"eval_smatch\" \\\n78:    --include_inputs_for_metrics \\\n79:    --greater_is_better True \\\n80:    --do_train \\\n81:    --do_eval \\\n82:    --do_predict \\\n83:    --ddp_find_unused_parameters False \\\n84:    --report_to \"tensorboard\" \\\n85:    --dataloader_pin_memory True 2>&1 | tee $OutputDir/run.log\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open train-AMRBART-large-AMRParsing.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb498734",
   "metadata": {},
   "source": [
    "### Step 58"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c5a8b1",
   "metadata": {},
   "source": [
    "Execution Start time: 05:03:40, End time: 05:03:40 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefccfaa",
   "metadata": {},
   "source": [
    "**Thought**: The script seems to be set up correctly to save checkpoints after each epoch. However, the `--output_dir` might not be correctly specified or there might be an issue with the saving mechanism. Let's ensure that the `--output_dir` is correctly set and that the script has permissions to write to this directory.\n",
    "\n",
    "I'll check the permissions of the `outputs` directory to ensure the script can write to it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bde03a",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f203b035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "drwxr-xr-x 5 root root 4096 Aug 22 04:58 outputs\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ls -ld outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdbd064",
   "metadata": {},
   "source": [
    "### Step 59"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ccf211",
   "metadata": {},
   "source": [
    "Execution Start time: 05:03:42, End time: 05:03:43 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29290e84",
   "metadata": {},
   "source": [
    "**Thought**: The `outputs` directory has the correct permissions for writing. The issue might be related to the script's logic for saving checkpoints. Let's check the script for any conditions or logic that might prevent the checkpoint from being saved.\n",
    "\n",
    "I'll search for any conditions related to saving checkpoints in the script.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8776122",
   "metadata": {},
   "source": [
    "**Action (search_file)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83e9cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Found 2 matches for \"save\" in /content/AMRBART/fine-tune/train-AMRBART-large-AMRParsing.sh:\nLine 70:    --save_strategy \"epoch\" \\\nLine 71:    --save_total_limit 1 \\\nEnd of matches for \"save\" in /content/AMRBART/fine-tune/train-AMRBART-large-AMRParsing.sh\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_file \"save\" train-AMRBART-large-AMRParsing.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964c4221",
   "metadata": {},
   "source": [
    "### Step 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc1ada3",
   "metadata": {},
   "source": [
    "Execution Start time: 05:03:45, End time: 05:03:45 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8410d2",
   "metadata": {},
   "source": [
    "**Thought**: The script is set to save checkpoints with the strategy \"epoch\" and a total limit of 1. This should be sufficient to save the checkpoint after one epoch. However, it seems the checkpoint is not being saved as expected.\n",
    "\n",
    "Let's check the training script for any additional logic or conditions that might affect the saving process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e353711e",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2d6dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/AMRBART/fine-tune/train-AMRBART-large-AMRParsing.sh (85 lines total)]\n1:export CUDA_VISIBLE_DEVICES=0\n2:RootDir=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" >/dev/null 2>&1 && pwd )\"\n3:\n4:Dataset=AMRBART\n5:#Dataset=LDC2017\n6:\n7:BasePath=/mnt/nfs-storage/data        # change dir here\n8:DataPath=$RootDir/data/$Dataset\n9:\n10:ModelCate=AMRBART-large\n11:\n12:MODEL=$1\n13:ModelCache=$BasePath/.cache\n14:DataCache=$DataPath/.cache/dump-amrparsing\n15:\n16:lr=1e-5\n17:\n18:OutputDir=${RootDir}/outputs/$Dataset-${ModelCate}-AMRParing-bsz16-lr-${lr}-UnifiedInp\n19:\n20:if [ ! -d ${OutputDir} ];then\n21:  mkdir -p ${OutputDir}\n22:else\n23:  rm -rf ${OutputDir}; mkdir -p ${OutputDir};\n24:fi\n25:export HF_DATASETS_CACHE=$DataCache\n26:\n27:if [ ! -d ${DataCache} ];then\n28:  mkdir -p ${DataCache}\n29:fi\n30:\n31:# torchrun --nnodes=1 --nproc_per_node=1 --max_restarts=0 --rdzv_id=1 --rdzv_backend=c10d main.py \\\n32:python -u main.py \\\n33:    --data_dir $DataPath \\\n34:    --task \"text2amr\" \\\n35:    --train_file $DataPath/train.jsonl \\\n36:    --validation_file $DataPath/val.jsonl \\\n37:    --test_file $DataPath/test.jsonl \\\n38:    --output_dir $OutputDir \\\n39:    --cache_dir $ModelCache \\\n40:    --data_cache_dir $DataCache \\\n41:    --tokenizer_name \"facebook/bart-large\" \\\n42:    --model_name_or_path $MODEL \\\n43:    --overwrite_output_dir \\\n44:    --unified_input True \\\n45:    --per_device_train_batch_size 16 \\\n46:    --per_device_eval_batch_size 8 \\\n47:    --gradient_accumulation_steps 1 \\\n48:    --learning_rate $lr \\\n49:    --optim \"adamw_hf\" \\\n50:    --lr_scheduler_type \"polynomial\" \\\n51:    --warmup_steps 200 \\\n52:--num_train_epochs 1 \\\n53:    --early_stopping 10 \\\n54:    --max_source_length 400 \\\n55:    --max_target_length 1024 \\\n56:    --val_max_target_length 1024 \\\n57:    --generation_max_length 1024 \\\n58:    --generation_num_beams 5 \\\n59:    --label_smoothing_factor 0.1 \\\n60:    --evaluation_strategy \"epoch\" \\\n61:    --weight_decay 0.01 \\\n62:    --max_grad_norm 0 \\\n63:    --max_steps -1 \\\n64:    --predict_with_generate \\\n65:    --smart_init False \\\n66:    --use_fast_tokenizer False \\\n67:    --logging_dir $OutputDir/logs \\\n68:    --logging_first_step True \\\n69:    --logging_steps 20 \\\n70:    --save_strategy \"epoch\" \\\n71:    --save_total_limit 1 \\\n72:    --seed 42 \\\n73:    --fp16_backend \"auto\" \\\n74:    --dataloader_num_workers 8 \\\n75:    --eval_dataloader_num_workers 2 \\\n76:    --load_best_model_at_end True \\\n77:    --metric_for_best_model \"eval_smatch\" \\\n78:    --include_inputs_for_metrics \\\n79:    --greater_is_better True \\\n80:    --do_train \\\n81:    --do_eval \\\n82:    --do_predict \\\n83:    --ddp_find_unused_parameters False \\\n84:    --report_to \"tensorboard\" \\\n85:    --dataloader_pin_memory True 2>&1 | tee $OutputDir/run.log\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open train-AMRBART-large-AMRParsing.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e5c419",
   "metadata": {},
   "source": [
    "### Step 61"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ea5adc",
   "metadata": {},
   "source": [
    "Execution Start time: 05:03:47, End time: 05:03:48 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15b64ac",
   "metadata": {},
   "source": [
    "**Thought**: The script seems to be set up correctly to save checkpoints after each epoch. However, the `--overwrite_output_dir` argument might be causing the existing checkpoints to be deleted. Let's remove this argument to ensure that the checkpoints are not overwritten.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b105e39a",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1745d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/AMRBART/fine-tune/train-AMRBART-large-AMRParsing.sh (85 lines total)]\n1:export CUDA_VISIBLE_DEVICES=0\n2:RootDir=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" >/dev/null 2>&1 && pwd )\"\n3:\n4:Dataset=AMRBART\n5:#Dataset=LDC2017\n6:\n7:BasePath=/mnt/nfs-storage/data        # change dir here\n8:DataPath=$RootDir/data/$Dataset\n9:\n10:ModelCate=AMRBART-large\n11:\n12:MODEL=$1\n13:ModelCache=$BasePath/.cache\n14:DataCache=$DataPath/.cache/dump-amrparsing\n15:\n16:lr=1e-5\n17:\n18:OutputDir=${RootDir}/outputs/$Dataset-${ModelCate}-AMRParing-bsz16-lr-${lr}-UnifiedInp\n19:\n20:if [ ! -d ${OutputDir} ];then\n21:  mkdir -p ${OutputDir}\n22:else\n23:  rm -rf ${OutputDir}; mkdir -p ${OutputDir};\n24:fi\n25:export HF_DATASETS_CACHE=$DataCache\n26:\n27:if [ ! -d ${DataCache} ];then\n28:  mkdir -p ${DataCache}\n29:fi\n30:\n31:# torchrun --nnodes=1 --nproc_per_node=1 --max_restarts=0 --rdzv_id=1 --rdzv_backend=c10d main.py \\\n32:python -u main.py \\\n33:    --data_dir $DataPath \\\n34:    --task \"text2amr\" \\\n35:    --train_file $DataPath/train.jsonl \\\n36:    --validation_file $DataPath/val.jsonl \\\n37:    --test_file $DataPath/test.jsonl \\\n38:    --output_dir $OutputDir \\\n39:    --cache_dir $ModelCache \\\n40:    --data_cache_dir $DataCache \\\n41:    --tokenizer_name \"facebook/bart-large\" \\\n42:    --model_name_or_path $MODEL \\\n43:# Removed --overwrite_output_dir argument\n44:    --unified_input True \\\n45:    --per_device_train_batch_size 16 \\\n46:    --per_device_eval_batch_size 8 \\\n47:    --gradient_accumulation_steps 1 \\\n48:    --learning_rate $lr \\\n49:    --optim \"adamw_hf\" \\\n50:    --lr_scheduler_type \"polynomial\" \\\n51:    --warmup_steps 200 \\\n52:--num_train_epochs 1 \\\n53:    --early_stopping 10 \\\n54:    --max_source_length 400 \\\n55:    --max_target_length 1024 \\\n56:    --val_max_target_length 1024 \\\n57:    --generation_max_length 1024 \\\n58:    --generation_num_beams 5 \\\n59:    --label_smoothing_factor 0.1 \\\n60:    --evaluation_strategy \"epoch\" \\\n61:    --weight_decay 0.01 \\\n62:    --max_grad_norm 0 \\\n63:    --max_steps -1 \\\n64:    --predict_with_generate \\\n65:    --smart_init False \\\n66:    --use_fast_tokenizer False \\\n67:    --logging_dir $OutputDir/logs \\\n68:    --logging_first_step True \\\n69:    --logging_steps 20 \\\n70:    --save_strategy \"epoch\" \\\n71:    --save_total_limit 1 \\\n72:    --seed 42 \\\n73:    --fp16_backend \"auto\" \\\n74:    --dataloader_num_workers 8 \\\n75:    --eval_dataloader_num_workers 2 \\\n76:    --load_best_model_at_end True \\\n77:    --metric_for_best_model \"eval_smatch\" \\\n78:    --include_inputs_for_metrics \\\n79:    --greater_is_better True \\\n80:    --do_train \\\n81:    --do_eval \\\n82:    --do_predict \\\n83:    --ddp_find_unused_parameters False \\\n84:    --report_to \"tensorboard\" \\\n85:    --dataloader_pin_memory True 2>&1 | tee $OutputDir/run.log\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 43:43\n",
    "# Removed --overwrite_output_dir argument\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5756e97b",
   "metadata": {},
   "source": [
    "### Step 62"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c792ad0d",
   "metadata": {},
   "source": [
    "Execution Start time: 05:03:50, End time: 05:04:12 (duration: 22 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e6b42c",
   "metadata": {},
   "source": [
    "**Thought**: The `--overwrite_output_dir` argument has been removed from the training script. Let's run the training script again to ensure the model checkpoint is saved correctly without being overwritten.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9985abb",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01b4865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "08/22/2024 05:04:04 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n08/22/2024 05:04:04 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n_n_gpu=0,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_pin_memory=True,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndo_eval=False,\ndo_predict=False,\ndo_train=False,\nearly_stopping=5,\neval_accumulation_steps=None,\neval_dataloader_num_workers=0,\neval_delay=0,\neval_lenpen=1.0,\neval_steps=None,\nevaluation_strategy=no,\nfp16=False,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_min_num_params=0,\nfull_determinism=False,\ngeneration_max_length=None,\ngeneration_num_beams=None,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngreater_is_better=None,\ngroup_by_length=False,\nhalf_precision_backend=cuda_amp,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=5e-05,\nlength_column_name=length,\nload_best_model_at_end=False,\nlocal_rank=-1,\nlog_level=-1,\nlog_level_replica=-1,\nlog_on_each_node=True,\nlogging_dir=/content/AMRBART/fine-tune/outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp/runs/Aug22_05-04-04_modal,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=500,\nlogging_strategy=steps,\nlr_scheduler_type=linear,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=None,\nmp_parameters=,\nno_cuda=False,\nnum_train_epochs=3.0,\noptim=adamw_hf,\noutput_dir=/content/AMRBART/fine-tune/outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=8,\nper_device_train_batch_size=8,\npredict_with_generate=False,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=/content/AMRBART/fine-tune/outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp,\nsave_on_each_node=False,\nsave_steps=500,\nsave_strategy=steps,\nsave_total_limit=None,\nseed=42,\nsharded_ddp=[],\nskip_memory_metrics=True,\nsmart_init=False,\nsortish_sampler=False,\ntask=text2amr,\ntf32=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nwarmup_ratio=0.0,\nwarmup_steps=0,\nweight_decay=0.0,\nxpu_backend=None,\n)\n[INFO|configuration_utils.py:681] 2024-08-22 05:04:04,781 >> loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /mnt/nfs-storage/data/.cache/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\n[INFO|configuration_utils.py:730] 2024-08-22 05:04:04,782 >> Model config BartConfig {\n  \"_name_or_path\": \"facebook/bart-base\",\n  \"activation_dropout\": 0.1,\n  \"activation_function\": \"gelu\",\n  \"add_bias_logits\": false,\n  \"add_final_layer_norm\": false,\n  \"architectures\": [\n    \"BartModel\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"bos_token_id\": 0,\n  \"classif_dropout\": 0.1,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 768,\n  \"decoder_attention_heads\": 12,\n  \"decoder_ffn_dim\": 3072,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 6,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"early_stopping\": true,\n  \"encoder_attention_heads\": 12,\n  \"encoder_ffn_dim\": 3072,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 6,\n  \"eos_token_id\": 2,\n  \"forced_bos_token_id\": 0,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\"\n  },\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2\n  },\n  \"max_position_embeddings\": 1024,\n  \"model_type\": \"bart\",\n  \"no_repeat_ngram_size\": 3,\n  \"normalize_before\": false,\n  \"normalize_embedding\": true,\n  \"num_beams\": 4,\n  \"num_hidden_layers\": 6,\n  \"pad_token_id\": 1,\n  \"scale_embedding\": false,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 128,\n      \"min_length\": 12,\n      \"num_beams\": 4\n    },\n    \"summarization_cnn\": {\n      \"length_penalty\": 2.0,\n      \"max_length\": 142,\n      \"min_length\": 56,\n      \"num_beams\": 4\n    },\n    \"summarization_xsum\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 62,\n      \"min_length\": 11,\n      \"num_beams\": 6\n    }\n  },\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.21.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 05:04:05,494 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/vocab.json from cache at /mnt/nfs-storage/data/.cache/0d6fc8b2ef1860c1f8f0baff4b021e3426cc7d11b153f98e563b799603ee2f25.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 05:04:05,500 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/merges.txt from cache at /mnt/nfs-storage/data/.cache/6e75e35f0bdd15870c98387e13b93a8e100237eb33ad99c36277a0562bd6d850.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 05:04:05,503 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 05:04:05,507 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 05:04:05,508 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/tokenizer_config.json from cache at /mnt/nfs-storage/data/.cache/1abf196c889c24daca2909359ca2090e5fcbfa21a9ea36d763f70adbafb500d7.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8\n[INFO|configuration_utils.py:681] 2024-08-22 05:04:05,637 >> loading configuration file https://huggingface.co/facebook/bart-large/resolve/main/config.json from cache at /mnt/nfs-storage/data/.cache/3f12fb71b844fcb7d591fdd4e55027da90d7b5dd6aa5430ad00ec6d76585f26c.bc22f15dc7ba074ee0a60bdd34c5f2fe3b6d746f89e765303376c51aff04e260\n[INFO|configuration_utils.py:730] 2024-08-22 05:04:05,638 >> Model config BartConfig {\n  \"_name_or_path\": \"facebook/bart-large\",\n  \"activation_dropout\": 0.1,\n  \"activation_function\": \"gelu\",\n  \"add_bias_logits\": false,\n  \"add_final_layer_norm\": false,\n  \"architectures\": [\n    \"BartModel\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"bos_token_id\": 0,\n  \"classif_dropout\": 0.1,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 12,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"early_stopping\": true,\n  \"encoder_attention_heads\": 16,\n  \"encoder_ffn_dim\": 4096,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 12,\n  \"eos_token_id\": 2,\n  \"forced_bos_token_id\": 0,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\"\n  },\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2\n  },\n  \"max_position_embeddings\": 1024,\n  \"model_type\": \"bart\",\n  \"no_repeat_ngram_size\": 3,\n  \"normalize_before\": false,\n  \"num_beams\": 4,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"scale_embedding\": false,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 128,\n      \"min_length\": 12,\n      \"num_beams\": 4\n    },\n    \"summarization_cnn\": {\n      \"length_penalty\": 2.0,\n      \"max_length\": 142,\n      \"min_length\": 56,\n      \"num_beams\": 4\n    },\n    \"summarization_xsum\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 62,\n      \"min_length\": 11,\n      \"num_beams\": 6\n    }\n  },\n  \"transformers_version\": \"4.21.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\n[WARNING|tokenization_utils_base.py:1896] 2024-08-22 05:04:05,639 >> The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'BartTokenizer'. \nThe class this function is called from is 'AMRBartTokenizer'.\nAdded 2965 AMR tokens\n[INFO|modeling_utils.py:2041] 2024-08-22 05:04:06,062 >> loading weights file https://huggingface.co/facebook/bart-base/resolve/main/pytorch_model.bin from cache at /mnt/nfs-storage/data/.cache/486355ec722ef05fd480e999d4c763be56549ae930f6a3742ee721a5d2a05647.f2f355ad2775769afc60592b43a46d72ca548375e3a1d65f381a751e711cbadd\n[INFO|modeling_utils.py:2435] 2024-08-22 05:04:09,909 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n\n[INFO|modeling_utils.py:2443] 2024-08-22 05:04:09,909 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\nDataset cache dir: /mnt/nfs-storage/data/.cache\n/usr/local/lib/python3.10/site-packages/datasets/load.py:929: FutureWarning: The repository for data contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /content/AMRBART/fine-tune/data_interface/data.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\nUsing custom data configuration default-b849ce260fee6cea\n08/22/2024 05:04:10 - INFO - datasets.builder - Using custom data configuration default-b849ce260fee6cea\nLoading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/data/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5\n08/22/2024 05:04:10 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/data/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5\nOverwrite dataset info from restored data version if exists.\n08/22/2024 05:04:10 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\nLoading Dataset info from /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-b849ce260fee6cea/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5\n08/22/2024 05:04:10 - INFO - datasets.info - Loading Dataset info from /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-b849ce260fee6cea/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5\nFound cached dataset data (/content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-b849ce260fee6cea/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5)\n08/22/2024 05:04:10 - INFO - datasets.builder - Found cached dataset data (/content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-b849ce260fee6cea/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5)\nLoading Dataset info from /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-b849ce260fee6cea/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5\n08/22/2024 05:04:10 - INFO - datasets.info - Loading Dataset info from /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-b849ce260fee6cea/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5\ndatasets: DatasetDict({\n    train: Dataset({\n        features: ['src', 'tgt'],\n        num_rows: 10\n    })\n    validation: Dataset({\n        features: ['src', 'tgt'],\n        num_rows: 10\n    })\n    test: Dataset({\n        features: ['src', 'tgt'],\n        num_rows: 10\n    })\n})\ncolums: ['src', 'tgt']\ntrain-AMRBART-large-AMRParsing.sh: line 44: --unified_input: command not found\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!bash train-AMRBART-large-AMRParsing.sh \"facebook/bart-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f2ff99",
   "metadata": {},
   "source": [
    "### Step 63"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b387f0",
   "metadata": {},
   "source": [
    "Execution Start time: 05:04:20, End time: 05:04:20 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bd1c5a",
   "metadata": {},
   "source": [
    "**Thought**: Exit due to context window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221ef71b",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1442760b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Exited"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exit_context"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
