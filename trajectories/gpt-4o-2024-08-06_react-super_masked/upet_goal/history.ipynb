{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf996a27",
   "metadata": {},
   "source": [
    "### Query\n",
    "Train a roberta-base model on the RTE dataset using the UPET method with the `run.py` script. Report eval accuracy as a json structured as follows: {\"eval_accuracy\": 0.0} (replace 0.0 with the actual value).\n",
    "\n",
    "Additional instructions:\n",
    "1. Run for only 1 epoch. 2. Set 5 examples per label, seed=42, and other hyperparameters taken from the example in the repository readme file.\n",
    "\n",
    "Git repository: https://github.com/wjn1996/UPET\n",
    "\n",
    "I have already executed some of the required steps. Now, your goal is to complete the remaining steps and submit the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de5048b",
   "metadata": {},
   "source": [
    "### Prompt\n",
    "<details>\n",
    "[{'role': 'system', 'content': 'Interact in a jupyter notebook to solve the request.\\nOutput a thought and an action.\\nThought can reason about the current situation, and action allows you to interact with the Jupyter notebook. There are three types of actions:\\n(1) execute, which executes a notebook cell with the command under the \"content\" field. This could be a bash command (beginning with !), a python command, or magic command (beginning with %).\\n(2) edit, which allows you to replace existing lines in a file with new lines. To replace lines, you will need to provide the lines as they appear before the replacement, and the lines to replace them with.\\nIn the \"content\" field, format should be ```{filename}\\n<<<BEFORE_EDIT>>>\\nlines before edit\\nlines before edit\\n<<<AFTER_EDIT>>>\\nlines after edit\\nlines after edit```\\n  - Include the entire lines WITH indentation. After editing, make sure indentation is correct.\\n(3) submit, which submits the final answer (or null if there\\'s no answer to submit), formatted as JSON (make sure to use double quotes for keys and values).\\nAlways output exactly one \"thought\" and one \"action\", and encapsulate the content with <content>...</content> tags.\\nI will provide the cell output (observation) for you.\\n\\nTips:\\n- When a github repository is given, you should start by cloning it. You can read the readme file and browse the repo for installation and usage instructions.\\n- Avoid using conda, instead use pip for package management.\\n- Each execution has a 5-minute timeout. After this period, you can decide whether to interrupt the kernel or continue waiting.\\n  You should interrupt if: (1) you are prompted with an interactive message (e.g., \"Do you want to continue? [Y/n]\"). In that case, you can bypass it by executing echo \"y\" | command, or (2) the execution is taking longer than expected because the wrong task is being executed, such as a different model or incorrect number of epochs.\\n  You should continue to wait if the current execution is correct and proceeding as expected.\\n\\nHere are some examples.\\nGit repository: EleutherAI/lm-evaluation-harness\\nQuery: Evaluate EleutherAI/gpt-j-6B on hellaswag. What\\'s the score?\\nThought: First, I need to clone the GitHub repository \\'EleutherAI/lm-evaluation-harness\\'.\\nAction: execute\\n<content>\\n!git clone https://github.com/EleutherAI/lm-evaluation-harness.git\\n</content>\\nObservation: ```\\nCloning into \\'lm-evaluation-harness\\'...\\n```\\nThought: The repository has been cloned. Next, I should check the README file for installation and usage instructions to proceed with evaluating EleutherAI/gpt-j-6B on hellaswag.\\nAction: execute\\n<content>\\n!cat lm-evaluation-harness/README.md\\n</content>\\nObservation: ```# Language Model Evaluation Harness ... truncated ...\\n```\\nThought: The README provides detailed instructions for installing the `lm-eval` package and evaluating models like GPT-J-6B on tasks like hellaswag. I need to install the package and its dependencies first.\\nAction: execute\\n<content>\\n!pip install -e lm-evaluation-harness/.\\n</content>\\nObservation: ```\\nObtaining file:///home/nora/lm-evaluation-harness\\n  Installing build dependencies ... truncated ...\\n```\\nThought: The `lm-eval` package and its dependencies have been installed successfully. Now, I can proceed with evaluating the EleutherAI/gpt-j-6B model on the hellaswag task as per the instructions in the README.\\nAction: execute\\n<content>\\n!lm_eval --model hf --model_args pretrained=EleutherAI/gpt-j-6B --tasks hellaswag --device cuda:0 --batch_size 8\\n</content>\\nObservation: ```\\n{\\n  \"hellaswag\": {\\n    \"acc\": 0.24603658536585363,\\n    \"acc_norm\": 0.34436059706446426,\\n  },  ... truncated ...\\n```\\nThought: The `lm-eval` was executed successfully, providing the metrics I needed. The `acc` metric is 0.2460. The `acc_norm` metric is 0.3444\\nAction: submit\\n<content>\\n{\"acc\": 0.2460, \"acc_norm\": 0.3444}\\n</content>\\n\\nQuery: use pandas to read the file \\'data.csv\\' and display the first 2 rows.\\nThought: First, I need to read the file \\'data.csv\\' using the pandas library.\\nAction: execute\\n<content>\\nimport pandas as pd\\npd.read_csv(\\'data.csv\\').head()\\n</content>\\nObservation: ```\\n    name\\n  0  Alice\\n   1    Bob\\n  2  Charlie\\n```\\nThought: The file \\'data.csv\\' has been read successfully and the first 2 rows have been displayed.\\nAction: submit\\n<content>\\n[\"Alice\", \"Bob\"]\\n</content>\\n\\nQuery: Change the model name in the \\'config.json\\' file from \\'roberta-base\\' to \\'roberta-large\\'.\\nThought: First, I need to read the \\'config.json\\' file to find the location where I can replace the model name.\\nAction: execute\\n<content>\\n!cat config.json\\n</content>\\nObservation: ```\\n{\\n    \"model\": \"roberta-base\",\\n    \"batch_size\": 8,\\n    \"epochs\": 10\\n}\\n```\\nThought: The file has been read successfully. Now, I can replace the model name in the \\'config.json\\' file.\\nAction: edit\\n<content>\\nconfig.json\\n<<<BEFORE_EDIT>>>\\n{\\n    \"model\": \"roberta-base\",\\n    \"batch_size\": 8,\\n<<<AFTER_EDIT>>>\\n{\\n    \"model\": \"roberta-large\",\\n    \"batch_size\": 8,\\n</content>\\nObservation: \"editing file config.json\"\\nThought: The model name has been replaced in the \\'config.json\\' file.\\nAction: submit\\n<content>\\nnull\\n</content>\\n'}, {'role': 'user', 'content': 'Query: Train a roberta-base model on the RTE dataset using the UPET method with the `run.py` script. Report eval accuracy as a json structured as follows: {\"eval_accuracy\": 0.0} (replace 0.0 with the actual value).\\n\\nAdditional instructions:\\n1. Run for only 1 epoch. 2. Set 5 examples per label, seed=42, and other hyperparameters taken from the example in the repository readme file.\\n\\nGit repository: https://github.com/wjn1996/UPET\\n\\nI have already executed some of the required steps. Now, your goal is to complete the remaining steps and submit the answer.'}]\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87fa8f8",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d30884",
   "metadata": {},
   "source": [
    "Execution Start time: 00:42:28, End time: 00:42:28 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae19ccd2",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f5a252",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462d169d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cloning into 'UPET'...\nremote: Enumerating objects: 384, done.\u001b[K\nremote: Counting objects: 100% (19/19)\u001b[K\nremote: Counting objects: 100% (19/19), done.\u001b[K\nremote: Compressing objects: 100% (17/17)\u001b[K\nremote: Compressing objects: 100% (17/17), done.\u001b[K\nremote: Total 384 (delta 5), reused 2 (delta 2), pack-reused 365 (from 1)\u001b[K\nReceiving objects: 100% (384/384)\nReceiving objects: 100% (384/384), 576.99 KiB | 8.61 MiB/s, done.\nResolving deltas: 100% (72/72)\nResolving deltas: 100% (72/72), done.\n/content/UPET\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!git clone https://github.com/wjn1996/UPET\n",
    "%cd UPET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eed327",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c72cd9b",
   "metadata": {},
   "source": [
    "Execution Start time: 00:42:28, End time: 00:43:06 (duration: 37 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a6afe2",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8abc44",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979e5669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#becuase installing the packages from the requirements txt generates dependencies issues\n",
    "!pip3 install -q datasets==1.16.1 tqdm==4.64.1 transformers==4.27.4 seqeval==1.2.2  accelerate==0.20.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5548731",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7958fce6",
   "metadata": {},
   "source": [
    "Execution Start time: 00:43:06, End time: 00:43:07 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2653d9",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d372448",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008533a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        elif config.model_type == \"bert\":\n            self.embeddings = self.lm_model.bert.embeddings\n        elif config.model_type in [\"deberta\", \"deberta-v2\"]:\n            self.embeddings = self.lm_model.deberta.embeddings\n        elif config.model_type == \"t5\":\n            self.embeddings = self.lm_model.T5.embeddings\n\n\n        # Pass dataset and argument information to the model\n        if model_args.prompt_prefix or model_args.prompt_ptuning or model_args.prompt_adapter or model_args.prompt_only:\n            self.lm_model.label_word_list = torch.tensor(data_args.label_word_list).long()\n        else:\n            raise RuntimeError(\"You must choose prompt_prefix or prompt_ptuning or prompt_adapter or prompt_only.\")\n        \n        if output_modes_mapping[data_args.dataset_name] == 'regression':\n            # lower / upper bounds\n            self.lm_model.lb, self.lm_model.ub = bound_mapping[data_args.dataset_name]\n        \n        self.lm_model.model_args = model_args"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/UPET/model/prompt_for_sequence_classification.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "\n",
    "        # Pass dataset and argument information to the model\n",
    "        if model_args.prompt_prefix or model_args.prompt_ptuning or model_args.prompt_adapter or model_args.prompt_only:\n",
    "            self.lm_model.label_word_list = torch.tensor(data_args.label_word_list).long().cuda()\n",
    "        else:\n",
    "            raise RuntimeError(\"You must choose prompt_prefix or prompt_ptuning or prompt_adapter or prompt_only.\")\n",
    "<<<AFTER_EDIT>>>\n",
    "\n",
    "        # Pass dataset and argument information to the model\n",
    "        if model_args.prompt_prefix or model_args.prompt_ptuning or model_args.prompt_adapter or model_args.prompt_only:\n",
    "            self.lm_model.label_word_list = torch.tensor(data_args.label_word_list).long()\n",
    "        else:\n",
    "            raise RuntimeError(\"You must choose prompt_prefix or prompt_ptuning or prompt_adapter or prompt_only.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dae9a40",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed37a542",
   "metadata": {},
   "source": [
    "Execution Start time: 00:43:07, End time: 00:43:08 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bdcb36",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6441eeab",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb6d319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n            output = (torch.exp(logits[..., 1].unsqueeze(-1)) * (self.ub - self.lb) + self.lb,)\n\n        return ((loss,) + output) if loss is not None else output\n\n\n    def from_pretrained(self, pretrained_model_name_or_path, *model_args, **kwargs):\n\n        self.lm_model = self.lm_model.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n\n        if self.data_args.prompt:\n            self.lm_model.label_word_list = torch.tensor(self.data_args.label_word_list).long()\n        if output_modes_mapping[self.data_args.dataset_name] == 'regression':\n            # lower / upper bounds\n            self.lm_model.lb, self.lm_model.ub = bound_mapping[self.data_args.dataset_name]\n        self.lm_model.model_args = self.model_args\n        self.lm_model.data_args = self.data_args\n\n        return self\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/UPET/model/prompt_for_sequence_classification.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "        self.lm_model = self.lm_model.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n",
    "\n",
    "        if self.data_args.prompt:\n",
    "            self.lm_model.label_word_list = torch.tensor(self.data_args.label_word_list).long().cuda()\n",
    "        if output_modes_mapping[self.data_args.dataset_name] == 'regression':\n",
    "            # lower / upper bounds\n",
    "<<<AFTER_EDIT>>>\n",
    "        self.lm_model = self.lm_model.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n",
    "\n",
    "        if self.data_args.prompt:\n",
    "            self.lm_model.label_word_list = torch.tensor(self.data_args.label_word_list).long()\n",
    "        if output_modes_mapping[self.data_args.dataset_name] == 'regression':\n",
    "            # lower / upper bounds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f6572b",
   "metadata": {},
   "source": [
    "### Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d1f645",
   "metadata": {},
   "source": [
    "Execution Start time: 00:43:09, End time: 00:43:09 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff84cd6d",
   "metadata": {},
   "source": [
    "**Thought**: The repository has been cloned, and necessary packages have been installed. The code has been edited to remove CUDA dependencies. Now, I need to check the README file for instructions on how to run the `run.py` script with the specified parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea8c574",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea85719f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# UPET: Uncertainty-aware Parameter-Efficient Tuning for Semi-supervised Language Understanding\n\n\nHead Tuning: Training the model with CLS head, whith or whitout prefix / adapter\nPrompt Tuning: Training the model with prompt and verbalizer (MLM head), whith or whitout prefix / adapter\n\naugment definition：\ne.g.,\n--prefix -> --head-prefix or --prompt-prefix\n--prompt -> --head-ptuning or --prompt-ptuning\n\n### Setup\nWe conduct our experiment with Anaconda3. If you have installed Anaconda3, then create the environment for P-tuning v2:\n\n```shell\nconda create -n pt2 python=3.8.5\nconda activate pt2\n```\n\nAfter we setup basic conda environment, install pytorch related packages via:\n\n```shell\nconda install -n pt2 pytorch==1.7.1 torchvision==0.8.2 torchaudio==0.7.2 cudatoolkit=11.0 -c pytorch\n```\n\nFinally, install other python packages we need:\n\n```shell\npip install -r requirements.txt\n```\n\n### Data\nFor SuperGLUE and SQuAD datasets, we download them from the Huggingface Datasets APIs (embedded in our codes).\n\nFor sequence tagging (NER, SRL) datasets, we prepare a non-official packup [here](https://zenodo.org/record/6318701/files/P-tuning-v2_data.tar.gz?download=1). \nAfter downloading, unzip the packup to the project root.\nPlease use at your own risk.\n\n### Training\nRun training scripts in [run_script](run_script) (e.g., RoBERTa for RTE):\n\nYou can change the augments and run:\n```shell\nbash run_script/run_rte_roberta.sh\n```\nor\n\n```shell\nexport TASK_NAME=superglue\nexport DATASET_NAME=rte\nexport CUDA_VISIBLE_DEVICES=0\nbs=32\nlr=5e-3\ndropout=0.1\npsl=128\nepoch=100\npython3 run.py \\\n  --model_name_or_path /wjn/pre-trained-lm/roberta-large \\\n  --task_name $TASK_NAME \\\n  --dataset_name $DATASET_NAME \\\n  --do_train \\\n  --do_eval \\\n  --max_seq_length 128 \\\n  --per_device_train_batch_size $bs \\\n  --learning_rate $lr \\\n  --num_train_epochs $epoch \\\n  --output_dir checkpoints/$DATASET_NAME-roberta/ \\\n  --overwrite_output_dir \\\n  --hidden_dropout_prob $dropout \\\n  --seed 11 \\\n  --save_strategy no \\\n  --evaluation_strategy epoch \\\n  --prefix\n```\nThis script is run for Full-data Full-supervised Pre-fix Tuning.\n\nWe provide the following kinds of settings:\n- Full-data v.s. Few-shot: The training data is full / few-shot\n- Full-supervised v.s. Semi-supervised: We use full-supervised / self-training\n- Full-Tuning v.s. Patameter-efficient Tuning: Only tuning the full parameters / Tuning the few parameters\n- One-stage v.s. Two-stage: directly tuning / tuning the few paraemters and then tuning the full\n- Head-Tuning v.s. Prompt-Tuning: Prefix/Adapter + CLS head / Prefix/Adapter + Prompt + Vaberlizer\n\nThe specific augments for different scenarios:\n\n\n#### Few-shot Head Tuning\n\n### Implemented Results\nCurrently we have released our reimplementation on following tasks and datasets. More implementation will be released soon.\n\nReleased results on BERT-large\n\n|              | BoolQ | COPA | RTE  | WiC  | WSC  | CoNLL04 | OntoNotes 5.0 | CoNLL12 |\n|--------------|-------|------|------|------|------|---------|---------------|---------|\n| Result       | 74.3  | 77.0 | 80.1 | 75.1 | 68.3 | 84.5    | 86.4          | 85.3    |\n| Total Epochs | 100   | 80   | 60   | 80   | 80   | 40      | 30            | 45      |\n| Best Epoch   | 58    | 12   | 30   | 56   | 17   | 33      | 24            | 43      |\n\nReleased results on RoBERTa-large\n\n|              | BoolQ | COPA | RTE  | WiC  | WSC  | CoNLL03 | CoNLL04 | OntoNotes 5.0 | CoNLL12 | CoNLL05 WSJ | CoNLL05 Brown | SQuAD 1.1 | SQuAD 2.0 |\n|--------------|-------|------|------|------|------|---------|---------|---------------|---------|-------------|---------------|-----------|-----------|\n| Results      | 84.0  | 92.0 | 86.6 | 73.7 | 64.4 | 91.8    | 88.4    | 90.1          | 84.7    | 89.4        | 83.9          | 88.1/94.2 | 81.3/84.7 |\n| Total Epochs | 100   | 120  | 100  | 50   | 10   | 30      | 80      | 60            | 45      | 15          | -             | 30        | 10        |\n| Best Epoch   | 86    | 78   | 65   | 31   | 3    | 28      | 45      | 59            | 37      | 13          | -             | 24        | 9         |\n\nFor other hyper-parameters, please refer to the training scripts. \nIf you can not achieve the reported results at the best epoch, there is probably an environmental mismatch and hyper-parameter search is needed.\n\n## Citation\n\nIf you find our work useful, please kindly cite our paper:\n\n\n\n\n\n\n---\n\nuse_prompt\nsst-2,mr,cr,mnli,snli,qnli,rte,mrpc,qqp,cola,trec\n\n\n\nsuperglue\nBoolQ\tCB\tCOPA\tMultiRC\tReCoRD\tRTE\tWiC\tWSC\tAX-b\tAX-g\nglue\nCoLA\tSST-2\tMRPC\tSTS-B\tQQP\tMNLI-m\tMNLI-mm\tQNLI\tRTE\tWNLI\tAX\n\n\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!cat README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1577c287",
   "metadata": {},
   "source": [
    "### Step 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6cb984",
   "metadata": {},
   "source": [
    "Execution Start time: 00:43:13, End time: 00:43:22 (duration: 9 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0127247b",
   "metadata": {},
   "source": [
    "**Thought**: The README file provides an example command for running the `run.py` script with RoBERTa on the RTE dataset. I need to modify this command to use `roberta-base`, set the number of epochs to 1, use 5 examples per label, and set the seed to 42. I will execute the modified command to train the model and evaluate its accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d880bebd",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad58ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "usage: run.py [-h] --model_name_or_path MODEL_NAME_OR_PATH\n              [--config_name CONFIG_NAME] [--tokenizer_name TOKENIZER_NAME]\n              [--cache_dir CACHE_DIR]\n              [--use_fast_tokenizer [USE_FAST_TOKENIZER]]\n              [--no_use_fast_tokenizer] [--model_revision MODEL_REVISION]\n              [--use_auth_token [USE_AUTH_TOKEN]] [--use_pe [USE_PE]]\n              [--head_prefix [HEAD_PREFIX]] [--prompt_prefix [PROMPT_PREFIX]]\n              [--head_only [HEAD_ONLY]] [--prompt_only [PROMPT_ONLY]]\n              [--head_ptuning [HEAD_PTUNING]]\n              [--prompt_ptuning [PROMPT_PTUNING]]\n              [--head_adapter [HEAD_ADAPTER]]\n              [--prompt_adapter [PROMPT_ADAPTER]]\n              [--adapter_choice ADAPTER_CHOICE] [--adapter_dim ADAPTER_DIM]\n              [--pre_seq_len PRE_SEQ_LEN]\n              [--prefix_projection [PREFIX_PROJECTION]]\n              [--prefix_hidden_size PREFIX_HIDDEN_SIZE]\n              [--hidden_dropout_prob HIDDEN_DROPOUT_PROB] --task_name\n              {glue,superglue,ner,srl,qa,other_cls} --dataset_name\n              {cola,mnli,mrpc,qnli,qqp,rte,sst2,stsb,wnli,boolq,cb,rte,wic,wsc,copa,record,multirc,conll2003,conll2004,ontonotes,conll2005,conll2012,squad,squad_v2,movie_rationales,cr,snli,trec,ag_news,yelp_polarity}\n              [--num_examples_per_label NUM_EXAMPLES_PER_LABEL]\n              [--dataset_config_name DATASET_CONFIG_NAME]\n              [--max_seq_length MAX_SEQ_LENGTH]\n              [--overwrite_cache [OVERWRITE_CACHE]]\n              [--pad_to_max_length [PAD_TO_MAX_LENGTH]]\n              [--no_pad_to_max_length] [--max_train_samples MAX_TRAIN_SAMPLES]\n              [--max_eval_samples MAX_EVAL_SAMPLES]\n              [--max_predict_samples MAX_PREDICT_SAMPLES]\n              [--train_file TRAIN_FILE] [--validation_file VALIDATION_FILE]\n              [--test_file TEST_FILE] [--template_id TEMPLATE_ID] --output_dir\n              OUTPUT_DIR [--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]]\n              [--do_train [DO_TRAIN]] [--do_eval [DO_EVAL]]\n              [--do_predict [DO_PREDICT]]\n              [--evaluation_strategy {no,steps,epoch}]\n              [--prediction_loss_only [PREDICTION_LOSS_ONLY]]\n              [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\n              [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\n              [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n              [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n              [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n              [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]\n              [--eval_delay EVAL_DELAY] [--learning_rate LEARNING_RATE]\n              [--weight_decay WEIGHT_DECAY] [--adam_beta1 ADAM_BETA1]\n              [--adam_beta2 ADAM_BETA2] [--adam_epsilon ADAM_EPSILON]\n              [--max_grad_norm MAX_GRAD_NORM]\n              [--num_train_epochs NUM_TRAIN_EPOCHS] [--max_steps MAX_STEPS]\n              [--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup,inverse_sqrt}]\n              [--warmup_ratio WARMUP_RATIO] [--warmup_steps WARMUP_STEPS]\n              [--log_level {debug,info,warning,error,critical,passive}]\n              [--log_level_replica {debug,info,warning,error,critical,passive}]\n              [--log_on_each_node [LOG_ON_EACH_NODE]] [--no_log_on_each_node]\n              [--logging_dir LOGGING_DIR]\n              [--logging_strategy {no,steps,epoch}]\n              [--logging_first_step [LOGGING_FIRST_STEP]]\n              [--logging_steps LOGGING_STEPS]\n              [--logging_nan_inf_filter [LOGGING_NAN_INF_FILTER]]\n              [--no_logging_nan_inf_filter] [--save_strategy {no,steps,epoch}]\n              [--save_steps SAVE_STEPS] [--save_total_limit SAVE_TOTAL_LIMIT]\n              [--save_on_each_node [SAVE_ON_EACH_NODE]] [--no_cuda [NO_CUDA]]\n              [--use_mps_device [USE_MPS_DEVICE]] [--seed SEED]\n              [--data_seed DATA_SEED] [--jit_mode_eval [JIT_MODE_EVAL]]\n              [--use_ipex [USE_IPEX]] [--bf16 [BF16]] [--fp16 [FP16]]\n              [--fp16_opt_level FP16_OPT_LEVEL]\n              [--half_precision_backend {auto,cuda_amp,apex,cpu_amp}]\n              [--bf16_full_eval [BF16_FULL_EVAL]]\n              [--fp16_full_eval [FP16_FULL_EVAL]] [--tf32 TF32]\n              [--local_rank LOCAL_RANK] [--xpu_backend {mpi,ccl,gloo}]\n              [--tpu_num_cores TPU_NUM_CORES]\n              [--tpu_metrics_debug [TPU_METRICS_DEBUG]] [--debug DEBUG]\n              [--dataloader_drop_last [DATALOADER_DROP_LAST]]\n              [--eval_steps EVAL_STEPS]\n              [--dataloader_num_workers DATALOADER_NUM_WORKERS]\n              [--past_index PAST_INDEX] [--run_name RUN_NAME]\n              [--disable_tqdm DISABLE_TQDM]\n              [--remove_unused_columns [REMOVE_UNUSED_COLUMNS]]\n              [--no_remove_unused_columns]\n              [--label_names LABEL_NAMES [LABEL_NAMES ...]]\n              [--load_best_model_at_end [LOAD_BEST_MODEL_AT_END]]\n              [--metric_for_best_model METRIC_FOR_BEST_MODEL]\n              [--greater_is_better GREATER_IS_BETTER]\n              [--ignore_data_skip [IGNORE_DATA_SKIP]]\n              [--sharded_ddp SHARDED_DDP] [--fsdp FSDP]\n              [--fsdp_min_num_params FSDP_MIN_NUM_PARAMS]\n              [--fsdp_config FSDP_CONFIG]\n              [--fsdp_transformer_layer_cls_to_wrap FSDP_TRANSFORMER_LAYER_CLS_TO_WRAP]\n              [--deepspeed DEEPSPEED]\n              [--label_smoothing_factor LABEL_SMOOTHING_FACTOR]\n              [--optim {adamw_hf,adamw_torch,adamw_torch_fused,adamw_torch_xla,adamw_apex_fused,adafactor,adamw_bnb_8bit,adamw_anyprecision,sgd,adagrad}]\n              [--optim_args OPTIM_ARGS] [--adafactor [ADAFACTOR]]\n              [--group_by_length [GROUP_BY_LENGTH]]\n              [--length_column_name LENGTH_COLUMN_NAME]\n              [--report_to REPORT_TO [REPORT_TO ...]]\n              [--ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS]\n              [--ddp_bucket_cap_mb DDP_BUCKET_CAP_MB]\n              [--dataloader_pin_memory [DATALOADER_PIN_MEMORY]]\n              [--no_dataloader_pin_memory]\n              [--skip_memory_metrics [SKIP_MEMORY_METRICS]]\n              [--no_skip_memory_metrics]\n              [--use_legacy_prediction_loop [USE_LEGACY_PREDICTION_LOOP]]\n              [--push_to_hub [PUSH_TO_HUB]]\n              [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n              [--hub_model_id HUB_MODEL_ID]\n              [--hub_strategy {end,every_save,checkpoint,all_checkpoints}]\n              [--hub_token HUB_TOKEN] [--hub_private_repo [HUB_PRIVATE_REPO]]\n              [--gradient_checkpointing [GRADIENT_CHECKPOINTING]]\n              [--include_inputs_for_metrics [INCLUDE_INPUTS_FOR_METRICS]]\n              [--fp16_backend {auto,cuda_amp,apex,cpu_amp}]\n              [--push_to_hub_model_id PUSH_TO_HUB_MODEL_ID]\n              [--push_to_hub_organization PUSH_TO_HUB_ORGANIZATION]\n              [--push_to_hub_token PUSH_TO_HUB_TOKEN]\n              [--mp_parameters MP_PARAMETERS]\n              [--auto_find_batch_size [AUTO_FIND_BATCH_SIZE]]\n              [--full_determinism [FULL_DETERMINISM]]\n              [--torchdynamo TORCHDYNAMO] [--ray_scope RAY_SCOPE]\n              [--ddp_timeout DDP_TIMEOUT] [--torch_compile [TORCH_COMPILE]]\n              [--torch_compile_backend TORCH_COMPILE_BACKEND]\n              [--torch_compile_mode TORCH_COMPILE_MODE]\n              [--use_semi [USE_SEMI]]\n              [--unlabeled_data_num UNLABELED_DATA_NUM]\n              [--unlabeled_data_batch_size UNLABELED_DATA_BATCH_SIZE]\n              [--pseudo_sample_num_or_ratio PSEUDO_SAMPLE_NUM_OR_RATIO]\n              [--teacher_training_epoch TEACHER_TRAINING_EPOCH]\n              [--teacher_tuning_epoch TEACHER_TUNING_EPOCH]\n              [--student_training_epoch STUDENT_TRAINING_EPOCH]\n              [--student_learning_rate STUDENT_LEARNING_RATE]\n              [--self_training_epoch SELF_TRAINING_EPOCH]\n              [--post_student_train [POST_STUDENT_TRAIN]]\n              [--student_pre_seq_len STUDENT_PRE_SEQ_LEN]\n              [--n_best_size N_BEST_SIZE]\n              [--max_answer_length MAX_ANSWER_LENGTH]\n              [--version_2_with_negative [VERSION_2_WITH_NEGATIVE]]\n              [--null_score_diff_threshold NULL_SCORE_DIFF_THRESHOLD]\nrun.py: error: ambiguous option: --prefix could match --prefix_projection, --prefix_hidden_size\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python3 run.py \\\n",
    "  --model_name_or_path roberta-base \\\n",
    "  --task_name superglue \\\n",
    "  --dataset_name rte \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --max_seq_length 128 \\\n",
    "  --per_device_train_batch_size 32 \\\n",
    "  --learning_rate 5e-3 \\\n",
    "  --num_train_epochs 1 \\\n",
    "  --output_dir checkpoints/rte-roberta/ \\\n",
    "  --overwrite_output_dir \\\n",
    "  --hidden_dropout_prob 0.1 \\\n",
    "  --seed 42 \\\n",
    "  --save_strategy no \\\n",
    "  --evaluation_strategy epoch \\\n",
    "  --prefix \\\n",
    "  --k_shot 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2759127",
   "metadata": {},
   "source": [
    "### Step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d90386a",
   "metadata": {},
   "source": [
    "Execution Start time: 00:43:25, End time: 00:43:33 (duration: 7 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cdddcb",
   "metadata": {},
   "source": [
    "**Thought**: \n",
    "Thought: The error indicates that the `--prefix` option is ambiguous and could match multiple options. I need to specify the correct option for using the prefix method. Based on the options listed, it seems like I should use `--prompt_prefix` instead of `--prefix`. I will modify the command accordingly and re-run it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc10b86",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d11fce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n╭───────────────────── Traceback (most recent call last) ──────────────────────╮\n│ /content/UPET/run.py:71 in <module>                                          │\n│                                                                              │\n│    68                                                                        │\n│    69 if __name__ == '__main__':                                             │\n│    70 │                                                                      │\n│ ❱  71 │   args = get_args()                                                  │\n│    72 │                                                                      │\n│    73 │   _, data_args, training_args, semi_training_args, _ = args          │\n│    74                                                                        │\n│                                                                              │\n│ /content/UPET/arguments.py:326 in get_args                                   │\n│                                                                              │\n│   323 │   \"\"\"Parse all the args.\"\"\"                                          │\n│   324 │   parser = HfArgumentParser((ModelArguments, DataTrainingArguments,  │\n│   325 │                                                                      │\n│ ❱ 326 │   args = parser.parse_args_into_dataclasses()                        │\n│   327 │                                                                      │\n│   328 │   return args                                                        │\n│                                                                              │\n│ /usr/local/lib/python3.10/site-packages/transformers/hf_argparser.py:341 in  │\n│ parse_args_into_dataclasses                                                  │\n│                                                                              │\n│   338 │   │   │   return (*outputs, remaining_args)                          │\n│   339 │   │   else:                                                          │\n│   340 │   │   │   if remaining_args:                                         │\n│ ❱ 341 │   │   │   │   raise ValueError(f\"Some specified arguments are not us │\n│   342 │   │   │                                                              │\n│   343 │   │   │   return (*outputs,)                                         │\n│   344                                                                        │\n╰──────────────────────────────────────────────────────────────────────────────╯\nValueError: Some specified arguments are not used by the HfArgumentParser: \n['--k_shot', '5']\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python3 run.py \\\n",
    "  --model_name_or_path roberta-base \\\n",
    "  --task_name superglue \\\n",
    "  --dataset_name rte \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --max_seq_length 128 \\\n",
    "  --per_device_train_batch_size 32 \\\n",
    "  --learning_rate 5e-3 \\\n",
    "  --num_train_epochs 1 \\\n",
    "  --output_dir checkpoints/rte-roberta/ \\\n",
    "  --overwrite_output_dir \\\n",
    "  --hidden_dropout_prob 0.1 \\\n",
    "  --seed 42 \\\n",
    "  --save_strategy no \\\n",
    "  --evaluation_strategy epoch \\\n",
    "  --prompt_prefix \\\n",
    "  --k_shot 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d814bc18",
   "metadata": {},
   "source": [
    "### Step 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d196be",
   "metadata": {},
   "source": [
    "Execution Start time: 00:43:34, End time: 00:43:34 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430fc4c4",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a933476e",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d08b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24650b59",
   "metadata": {},
   "source": [
    "### Step 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779ca9a6",
   "metadata": {},
   "source": [
    "Execution Start time: 00:43:35, End time: 00:43:36 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52da6efe",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the `--k_shot` argument is not recognized by the `HfArgumentParser`. It seems like the script does not support this argument directly. I need to check the script or documentation to understand how to specify the number of examples per label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83adaf4",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09f539e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!grep -r \"k_shot\" ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b701119",
   "metadata": {},
   "source": [
    "### Step 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbace5bc",
   "metadata": {},
   "source": [
    "Execution Start time: 00:43:38, End time: 00:43:38 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bfe029",
   "metadata": {},
   "source": [
    "**Thought**: The `grep` command did not return any results, indicating that the `k_shot` argument is not directly referenced in the codebase. This suggests that the script may not support specifying the number of examples per label directly through a command-line argument. I need to check the `run.py` script to understand how to set the number of examples per label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f92851",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3df9ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import logging\nimport os\nimport sys\nimport numpy as np\nfrom typing import Dict\n\nimport datasets\nimport transformers\nfrom transformers import set_seed, Trainer\nfrom transformers.trainer_utils import get_last_checkpoint\n\nfrom arguments import get_args\n\nfrom tasks.utils import *\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\nlogger = logging.getLogger(__name__)\n\ndef train(trainer, resume_from_checkpoint=None, last_checkpoint=None):\n    checkpoint = None\n    if resume_from_checkpoint is not None:\n        checkpoint = resume_from_checkpoint\n    elif last_checkpoint is not None:\n        checkpoint = last_checkpoint\n    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n    # trainer.save_model()，\n    \n    try:\n        metrics = train_result.metrics\n\n        trainer.log_metrics(\"train\", metrics)\n        trainer.save_metrics(\"train\", metrics)\n        trainer.save_state()\n\n        trainer.log_best_metrics()\n    except:\n        pass\n\ndef evaluate(trainer):\n    logger.info(\"*** Evaluate ***\")\n    metrics = trainer.evaluate()\n\n    trainer.log_metrics(\"eval\", metrics)\n    trainer.save_metrics(\"eval\", metrics)\n\ndef predict(trainer, predict_dataset=None):\n    if predict_dataset is None:\n        logger.info(\"No dataset is available for testing\")\n\n    elif isinstance(predict_dataset, dict):\n        \n        for dataset_name, d in predict_dataset.items():\n            logger.info(\"*** Predict: %s ***\" % dataset_name)\n            predictions, labels, metrics = trainer.predict(d, metric_key_prefix=\"predict\")\n            predictions = np.argmax(predictions, axis=2)\n\n            trainer.log_metrics(\"predict\", metrics)\n            trainer.save_metrics(\"predict\", metrics)\n\n    else:\n        logger.info(\"*** Predict ***\")\n        predictions, labels, metrics = trainer.predict(predict_dataset, metric_key_prefix=\"predict\")\n        predictions = np.argmax(predictions, axis=2)\n\n        trainer.log_metrics(\"predict\", metrics)\n        trainer.save_metrics(\"predict\", metrics)\n\nif __name__ == '__main__':\n\n    args = get_args()\n\n    _, data_args, training_args, semi_training_args, _ = args\n\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        handlers=[logging.StreamHandler(sys.stdout)],\n    )\n\n    log_level = training_args.get_process_log_level()\n    logger.setLevel(log_level)\n    datasets.utils.logging.set_verbosity(log_level)\n    transformers.utils.logging.set_verbosity(log_level)\n    transformers.utils.logging.enable_default_handler()\n    transformers.utils.logging.enable_explicit_format()\n\n    # Log on each process the small summary:\n    logger.warning(\n        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n    )\n    logger.info(f\"Training/evaluation parameters {training_args}\")\n    \n\n    if not os.path.isdir(\"checkpoints\") or not os.path.exists(\"checkpoints\"):\n        os.mkdir(\"checkpoints\")\n\n    if data_args.task_name.lower() == \"superglue\":\n        assert data_args.dataset_name.lower() in SUPERGLUE_DATASETS\n        from tasks.superglue.get_trainer import get_trainer\n\n    elif data_args.task_name.lower() == \"glue\":\n        assert data_args.dataset_name.lower() in GLUE_DATASETS\n        from tasks.glue.get_trainer import get_trainer\n\n    elif data_args.task_name.lower() == \"ner\":\n        assert data_args.dataset_name.lower() in NER_DATASETS\n        from tasks.ner.get_trainer import get_trainer\n\n    elif data_args.task_name.lower() == \"srl\":\n        assert data_args.dataset_name.lower() in SRL_DATASETS\n        from tasks.srl.get_trainer import get_trainer\n    \n    elif data_args.task_name.lower() == \"qa\":\n        assert data_args.dataset_name.lower() in QA_DATASETS\n        from tasks.qa.get_trainer import get_trainer\n    \n    elif data_args.task_name.lower() == \"other_cls\":\n        assert data_args.dataset_name.lower() in OTHER_DATASETS\n        from tasks.other_cls.get_trainer import get_trainer\n        \n    else:\n        raise NotImplementedError('Task {} is not implemented. Please choose a task from: {}'.format(data_args.task_name, \", \".join(TASKS)))\n\n    set_seed(training_args.seed)\n\n    trainer, predict_dataset = get_trainer(args)\n\n    last_checkpoint = None\n    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n            raise ValueError(\n                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n                \"Use --overwrite_output_dir to overcome.\"\n            )\n        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n            logger.info(\n                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n            )\n\n\n    if training_args.do_train:\n        train(trainer, training_args.resume_from_checkpoint, last_checkpoint)\n    \n    # if training_args.do_eval:\n    #     evaluate(trainer)\n\n    # if training_args.do_predict:\n    #     predict(trainer, predict_dataset)\n\n   "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!cat run.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95172a90",
   "metadata": {},
   "source": [
    "### Step 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811d2415",
   "metadata": {},
   "source": [
    "Execution Start time: 00:43:40, End time: 00:43:40 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f0409f",
   "metadata": {},
   "source": [
    "**Thought**: The `run.py` script imports arguments from the `arguments` module, which likely contains the logic for parsing command-line arguments. The `k_shot` parameter might be defined there. I need to check the `arguments` module to understand how to specify the number of examples per label. Let's explore the `arguments` module to find relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0293beaf",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b389eea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "from enum import Enum\nimport argparse\nimport dataclasses\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\nfrom transformers import HfArgumentParser, TrainingArguments\n\nfrom tasks.utils import *\n\n\n@dataclass\nclass DataTrainingArguments:\n    \"\"\"\n    Arguments pertaining to what data we are going to input our model for training and eval.\n\n    Using `HfArgumentParser` we can turn this class\n    into argparse arguments to be able to specify them on\n    the command line.training_args\n    \"\"\"\n    task_name: str = field(\n        metadata={\n            \"help\": \"The name of the task to train on: \" + \", \".join(TASKS),\n            \"choices\": TASKS\n        },\n    )\n    dataset_name: str = field(\n        metadata={\n            \"help\": \"The name of the dataset to use: \" + \", \".join(DATASETS),\n            \"choices\": DATASETS\n        }\n    )\n    # add by wjn\n    num_examples_per_label: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": \"Randomly sampling k-shot examples for each label \"\n        },\n    )\n    dataset_config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n    )\n    max_seq_length: int = field(\n        default=128,\n        metadata={\n            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n            \"than this will be truncated, sequences shorter will be padded.\"\n        },\n    )\n    overwrite_cache: bool = field(\n        default=False, metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"}\n    )\n    pad_to_max_length: bool = field(\n        default=True,\n        metadata={\n            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n        },\n    )\n    max_train_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n            \"value if set.\"\n        },\n    )\n    max_eval_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n            \"value if set.\"\n        },\n    )\n    max_predict_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n            \"value if set.\"\n        },\n    )\n    train_file: Optional[str] = field(\n        default=None, metadata={\"help\": \"A csv or a json file containing the training data.\"}\n    )\n    validation_file: Optional[str] = field(\n        default=None, metadata={\"help\": \"A csv or a json file containing the validation data.\"}\n    )\n    test_file: Optional[str] = field(\n        default=None, \n        metadata={\"help\": \"A csv or a json file containing the test data.\"}\n    )\n    template_id: Optional[int] = field(\n        default=0,\n        metadata={\n            \"help\": \"The specific prompt string to use\"\n        }\n    )\n    \n\n@dataclass\nclass ModelArguments:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n    \"\"\"\n    model_name_or_path: str = field(\n        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n    )\n    config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n    )\n    tokenizer_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n    )\n    cache_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n    )\n    use_fast_tokenizer: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n    )\n    model_revision: str = field(\n        default=\"main\",\n        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n    )\n    use_auth_token: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n            \"with private models).\"\n        },\n    )\n    use_pe: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Whether to use parameter-efficient settings. If true, that means fix the parameters of the backbone, and only\"\n            \"tune the new initialized modules (e.g., adapter, prefix, ptuning, etc.)\"\n        }\n    )\n    head_prefix: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Will use Head-tuning Prefix (P-tuning v2) during training\"\n        }\n    )\n    prompt_prefix: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Will use Prompt-tuning Prefix (P-tuning v2) during training\"\n        }\n    )\n    head_only: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Will use Head Fine-tuning during training (w/o. any new modules)\"\n        }\n    )\n    prompt_only: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Will use Prompt Fine-tuning during training (w/o. any new modules)\"\n        }\n    )\n    head_ptuning: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Will use Head-tuning P-tuning during training (w. parameter-efficient)\"\n        }\n    )\n    prompt_ptuning: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Will use Prompt-tuning P-tuning during training (w. parameter-efficient)\"\n        }\n    )\n    head_adapter: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Will use Head-tuning Adapter tuning during training (w. parameter-efficient)\"\n        }\n    )\n    prompt_adapter: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Will use Prompt-tuning Adapter tuning during training (w. parameter-efficient)\"\n        }\n    )\n    adapter_choice: str = field(\n        default=\"LiST\",\n        metadata={\"help\": \"The choice of adapter, list, lora, houlsby.\"},\n    )\n    adapter_dim: int = field(\n        default=128,\n        metadata={\"help\": \"The hidden size of adapter. default is 128.\"},\n    )\n    pre_seq_len: int = field(\n        default=4,\n        metadata={\n            \"help\": \"The length of prompt\"\n        }\n    )\n    prefix_projection: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Apply a two-layer MLP head over the prefix embeddings\"\n        }\n    ) \n    prefix_hidden_size: int = field(\n        default=512,\n        metadata={\n            \"help\": \"The hidden size of the MLP projection head in Prefix Encoder if prefix projection is used\"\n        }\n    )\n    hidden_dropout_prob: float = field(\n        default=0.1,\n        metadata={\n            \"help\": \"The dropout probability used in the models\"\n        }\n    )\n\n\n@dataclass\nclass SemiSupervisedArguments:\n    use_semi: bool = field(\n        default=False, metadata={\"help\": \"If true, the training process will be transformed into self-training framework.\"}\n    )\n    unlabeled_data_num: int = field(\n        default=-1,\n        metadata={\n            \"help\": \"The total number of unlabeled data. If set -1 means all the training data (expect of few-shot labeled data)\"\n        }\n    )\n    unlabeled_data_batch_size: int = field(\n        default=16,\n        metadata={\n            \"help\": \"The number of unlabeled data in one batch.\"\n        }\n    )\n    pseudo_sample_num_or_ratio: float = field(\n        default=0.1,\n        metadata={\n            \"help\": \"The number / ratio of pseudo-labeled data sampling. For example, if have 1000 unlabeled data, 0.1 / 100 means sampling 100 pseduo-labeled data.\"\n        }\n    )\n    teacher_training_epoch: int = field(\n        default=10,\n        metadata={\n            \"help\": \"The epoch number of teacher training at the beginning of self-training.\"\n        }\n    )\n    teacher_tuning_epoch: int = field(\n        default=10,\n        metadata={\n            \"help\": \"The epoch number of teacher tuning in each self-training iteration.\"\n        }\n    )\n    student_training_epoch: int = field(\n        default=16,\n        metadata={\n            \"help\": \"The epoch number of student training in each self-training iteration.\"\n        }\n    )\n    student_learning_rate: float = field(\n        default=1e-5,\n        metadata={\n            \"help\": \"The learning rate of student training in each self-training iteration.\"\n        }\n    )\n    self_training_epoch: int = field(\n        default=30,\n        metadata={\n            \"help\": \"The number of teacher-student iteration .\"\n        }\n    )\n    # self-training过程中，unlabeled data会先采样一个子集，因此student模型学习的样本数量只有不到1024个\n    # post_student_train设置后，即允许在self-training后，选择最佳的teacher模型，并在全部的unlabeled data上打标和mc dropout\n    # 此时student模型将会在更多的样本上训练（样本数量不超过20k）\n    post_student_train: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Whether to train a student model on large pseudo-labeled data after self-training iteration\"\n        }\n    )\n    student_pre_seq_len: int = field(\n        default=4,\n        metadata={\n            \"help\": \"The length of prompt\"\n        }\n    )\n\n\n\n\n\n@dataclass\nclass QuestionAnwseringArguments:\n    n_best_size: int = field(\n        default=20,\n        metadata={\"help\": \"The total number of n-best predictions to generate when looking for an answer.\"},\n    )\n    max_answer_length: int = field(\n        default=30,\n        metadata={\n            \"help\": \"The maximum length of an answer that can be generated. This is needed because the start \"\n            \"and end predictions are not conditioned on one another.\"\n        },\n    )\n    version_2_with_negative: bool = field(\n        default=False, metadata={\"help\": \"If true, some of the examples do not have an answer.\"}\n    )\n    null_score_diff_threshold: float = field(\n        default=0.0,\n        metadata={\n            \"help\": \"The threshold used to select the null answer: if the best answer has a score that is less than \"\n            \"the score of the null answer minus this threshold, the null answer is selected for this example. \"\n            \"Only useful when `version_2_with_negative=True`.\"\n        },\n    )\n\n\n\n\ndef get_args():\n    \"\"\"Parse all the args.\"\"\"\n    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments, SemiSupervisedArguments, QuestionAnwseringArguments))\n\n    args = parser.parse_args_into_dataclasses()\n\n    return args"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!cat arguments.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346c1d50",
   "metadata": {},
   "source": [
    "### Step 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47241c17",
   "metadata": {},
   "source": [
    "Execution Start time: 00:43:41, End time: 00:43:41 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ffcd47",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0252529f",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96cec8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c14ea5a",
   "metadata": {},
   "source": [
    "### Step 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603a16e0",
   "metadata": {},
   "source": [
    "Execution Start time: 00:43:43, End time: 00:43:43 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f5afca",
   "metadata": {},
   "source": [
    "**Thought**: The `arguments.py` file defines argument classes and uses `HfArgumentParser` to parse them. However, it doesn't seem to directly handle a `k_shot` parameter. This suggests that the number of examples per label might need to be set in a different way, possibly through a configuration file or by modifying the dataset loading logic. I need to check the dataset loading logic to see how to set the number of examples per label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a22212",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bea7b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "./metrics/perplexity/README.md:input_texts = datasets.load_dataset(\"wikitext\",\n./metrics/perplexity/perplexity.py:        >>> input_texts = datasets.load_dataset(\"wikitext\",\n./tasks/glue/dataset.py:from datasets.load import load_dataset, load_metric\n./tasks/glue/dataset.py:def random_sampling(raw_datasets: load_dataset, data_type: str=\"train\", num_examples_per_label: Optional[int]=16):\n./tasks/glue/dataset.py:        raw_datasets = load_dataset(\"glue\", data_args.dataset_name)\ngrep: ./tasks/glue/__pycache__/dataset.cpython-310.pyc: binary file matches\n./tasks/ner/dataset.py:from datasets.load import load_dataset, load_metric\n./tasks/ner/dataset.py:def random_sampling(raw_datasets: load_dataset, data_type: str=\"train\", num_examples_per_label: Optional[int]=16):\n./tasks/ner/dataset.py:        raw_datasets = load_dataset(f'tasks/ner/datasets/{data_args.dataset_name}.py')\n./tasks/other_cls/dataset.py:from datasets.load import load_dataset, load_metric\n./tasks/other_cls/dataset.py:def random_sampling(raw_datasets: load_dataset, data_type: str=\"train\", num_examples_per_label: Optional[int]=16, label_set: list=None):\n./tasks/other_cls/dataset.py:        raw_datasets = load_dataset(data_args.dataset_name)\n./tasks/qa/dataset.py:from datasets.load import load_metric, load_dataset\n./tasks/qa/dataset.py:def random_sampling(raw_datasets: load_dataset, data_type: str=\"train\", num_examples_per_label: Optional[int]=16):\n./tasks/qa/dataset.py:        raw_datasets = load_dataset(data_args.dataset_name)\n./tasks/srl/dataset.py:from datasets.load import load_dataset, load_metric\n./tasks/srl/dataset.py:def random_sampling(raw_datasets: load_dataset, data_type: str=\"train\", num_examples_per_label: Optional[int]=16):\n./tasks/srl/dataset.py:        raw_datasets = load_dataset(f'tasks/srl/datasets/{data_args.dataset_name}.py')\n./tasks/superglue/dataset.py:from datasets.load import load_dataset, load_metric\n./tasks/superglue/dataset.py:def random_sampling(raw_datasets: load_dataset, data_type: str=\"train\", num_examples_per_label: Optional[int]=16):\n./tasks/superglue/dataset.py:        raw_datasets = load_dataset(\"super_glue\", data_args.dataset_name)\n./tasks/superglue/dataset_record.py:from datasets.load import load_dataset, load_metric\n./tasks/superglue/dataset_record.py:        raw_datasets = load_dataset(\"super_glue\", data_args.dataset_name)\ngrep: ./tasks/superglue/__pycache__/dataset.cpython-310.pyc: binary file matches\n./tasks/utils.py:from datasets.load import load_dataset, load_metric\ngrep: ./tasks/__pycache__/utils.cpython-310.pyc: binary file matches\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!grep -r \"load_dataset\" ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2d05a7",
   "metadata": {},
   "source": [
    "### Step 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d95a2b",
   "metadata": {},
   "source": [
    "Execution Start time: 00:43:45, End time: 00:43:45 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f36ea73",
   "metadata": {},
   "source": [
    "**Thought**: The `random_sampling` function in `tasks/superglue/dataset.py` seems to handle the number of examples per label, as it includes a `num_examples_per_label` parameter. This function likely needs to be called with the appropriate number of examples per label. I need to check the `run.py` script to see if this function is being called and how I can modify it to use 5 examples per label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdc512e",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76923f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "from datasets.load import load_dataset, load_metric\nfrom transformers import (\n    AutoTokenizer,\n    DataCollatorWithPadding,\n    EvalPrediction,\n    default_data_collator,\n)\nimport numpy as np\nimport logging\nfrom collections import defaultdict\nfrom typing import Optional\n\n# add by wjn\ndef random_sampling(raw_datasets: load_dataset, data_type: str=\"train\", num_examples_per_label: Optional[int]=16):\n    assert data_type in [\"train\", \"validation\", \"test\"]\n    label_list = raw_datasets[data_type][\"label\"] # [0, 1, 0, 0, ...]\n    label_dict = dict()\n    # 记录每个label对应的样本索引\n    for ei, label in enumerate(label_list):\n        if label not in label_dict.keys():\n            label_dict[label] = list()\n        label_dict[label].append(ei)\n    # 对于每个类别，随机采样k个样本\n    few_example_ids = list()\n    for label, eid_list in label_dict.items():\n        # examples = deepcopy(eid_list)\n        # shuffle(examples)\n        idxs = np.random.choice(len(eid_list), size=num_examples_per_label, replace=False)\n        selected_eids = [eid_list[i] for i in idxs]\n        few_example_ids.extend(selected_eids)\n    # 保存没有被选中的example id\n    num_examples = len(label_list)\n    un_selected_examples_ids = [idx for idx in range(num_examples) if idx not in few_example_ids]\n    return few_example_ids, un_selected_examples_ids\n\ntask_to_test_key = {\n    \"boolq\": \"accuracy\",\n    \"cb\": \"accuracy\",\n    \"rte\": \"accuracy\",\n    \"wic\": \"accuracy\",\n    \"wsc\": \"accuracy\",\n    \"copa\": \"accuracy\",\n    \"record\": \"f1\",\n    \"multirc\": \"f1\"\n}\n\ntask_to_keys = {\n    \"boolq\": (\"question\", \"passage\"),\n    \"cb\": (\"premise\", \"hypothesis\"),\n    \"rte\": (\"premise\", \"hypothesis\"),\n    \"wic\": (\"processed_sentence1\", None),\n    \"wsc\": (\"span2_word_text\", \"span1_text\"),\n    \"copa\": (None, None),\n    \"record\": (None, None),\n    \"multirc\": (\"paragraph\", \"question_answer\")\n}\n\ntask_to_template = {\n    \"boolq\" : [{\"prefix_template\": \"Question: \", \"suffix_template\": \"\"}, {\"prefix_template\": \"Passage: \", \"suffix_template\": \" Answer: <mask> .\"}],\n    \"cb\": [None, {\"prefix_template\": \"? <mask> ,\", \"suffix_template\": \"\"}],\n    \"rte\": [None, {\"prefix_template\": \"? <mask> ,\", \"suffix_template\": \"\"}], # prefix / suffix template in each segment.\n    # \"wic\": [None, None],\n    # \"wsc\": [None, None],\n    # \"copa\": [None, None],\n    # \"record\": [None, None],\n    # \"multirc\": [None, None],\n}\n\n# add by wjn\nlabel_words_mapping = {\n    \"boolq\": {\"True\": [\"Yes\"], \"False\": [\"No\"]},\n    \"cb\": {\"contradiction\": [\"No\"], \"neutral\": [\"Maybe\"], \"entailment\": [\"Yes\"]},\n    \"rte\": {\"not_entailment\": [\"No\"], \"entailment\": [\"Yes\"]}\n}\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass SuperGlueDataset():\n    def __init__(\n        self, \n        tokenizer: AutoTokenizer, \n        data_args, \n        training_args, \n        semi_training_args=None,\n        use_prompt=False\n    ) -> None:\n        super().__init__()\n        raw_datasets = load_dataset(\"super_glue\", data_args.dataset_name)\n        self.tokenizer = tokenizer\n        self.data_args = data_args\n        \n        self.multiple_choice = data_args.dataset_name in [\"copa\"]\n\n        if data_args.dataset_name == \"record\":\n            self.num_labels = 2\n            self.label_list = [\"0\", \"1\"]\n        elif not self.multiple_choice:\n            self.label_list = raw_datasets[\"train\"].features[\"label\"].names\n            self.num_labels = len(self.label_list)\n        else:\n            self.num_labels = 1\n\n        # === generate template ===== add by wjn\n        self.use_prompt = False\n        if data_args.dataset_name in task_to_template.keys():\n            self.use_prompt = use_prompt\n        \n        if self.use_prompt:\n            if 't5' in type(tokenizer).__name__.lower():\n                self.special_token_mapping = {\n                    'cls': 3, 'mask': 32099, 'sep': tokenizer.eos_token_id,\n                    'sep+': tokenizer.eos_token_id,\n                    'pseudo_token': tokenizer.unk_token_id\n                }\n            else:\n                self.special_token_mapping = {\n                    'cls': tokenizer.cls_token_id, 'mask': tokenizer.mask_token_id, 'sep': tokenizer.sep_token_id,\n                    'sep+': tokenizer.sep_token_id,\n                    'pseudo_token': tokenizer.unk_token_id\n                }\n            self.template = task_to_template[data_args.dataset_name] # dict\n\n        \n        # === generate label word mapping ===== add by wjn\n        if self.use_prompt:\n            assert data_args.dataset_name in label_words_mapping.keys(), \"You must define label word mapping for the task {}\".format(data_args.dataset_name)\n            self.label_to_word = label_words_mapping[data_args.dataset_name] # e.g., {\"0\": [\"great\"], \"1\": [bad]}\n            self.label_to_word = {label: label_word[0] if type(label_word) == list else label_word for label, label_word in self.label_to_word.items()}\n\n            for key in self.label_to_word:\n                # For RoBERTa/BART/T5, tokenization also considers space, so we use space+word as label words.\n                if self.label_to_word[key][0] not in ['<', '[', '.', ',']:\n                    # Make sure space+word is in the vocabulary\n                    assert len(tokenizer.tokenize(' ' + self.label_to_word[key])) == 1\n                    self.label_to_word[key] = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(' ' + self.label_to_word[key])[0])\n                else:\n                    self.label_to_word[key] = tokenizer.convert_tokens_to_ids(self.label_to_word[key])\n                logger.info(\"Label {} to word {} ({})\".format(key, tokenizer._convert_id_to_token(self.label_to_word[key]), self.label_to_word[key]))\n\n            if len(self.label_list) > 1:\n                self.label_word_list = [self.label_to_word[label] for label in self.label_list]\n            else:\n                # Regression task\n                # '0' represents low polarity and '1' represents high polarity.\n                self.label_word_list = [self.label_to_word[label] for label in ['0', '1']]\n        # =============\n\n        # Preprocessing the raw_datasets\n        self.sentence1_key, self.sentence2_key = task_to_keys[data_args.dataset_name]\n\n        # Padding strategy\n        if data_args.pad_to_max_length:\n            self.padding = \"max_length\"\n        else:\n            # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n            self.padding = False\n\n        if not self.multiple_choice:\n            self.label2id = {l: i for i, l in enumerate(self.label_list)}\n            self.id2label = {id: label for label, id in self.label2id.items()}\n            print(f\"{self.label2id}\")\n            print(f\"{self.id2label}\")\n\n        if data_args.max_seq_length > tokenizer.model_max_length:\n            logger.warning(\n                f\"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the\"\n                f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n            )\n        \n        self.max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n\n        if data_args.dataset_name == \"record\":\n            raw_datasets = raw_datasets.map(\n                self.record_preprocess_function,\n                batched=True,\n                load_from_cache_file=not data_args.overwrite_cache,\n                remove_columns=raw_datasets[\"train\"].column_names,\n                desc=\"Running tokenizer on dataset\",\n            )\n        else:\n            raw_datasets = raw_datasets.map(\n                self.preprocess_function,\n                batched=True,\n                load_from_cache_file=not data_args.overwrite_cache,\n                desc=\"Running tokenizer on dataset\",\n            )\n\n        if training_args.do_train:\n            self.train_dataset = raw_datasets[\"train\"]\n            if data_args.max_train_samples is not None:\n                self.train_dataset = self.train_dataset.select(range(data_args.max_train_samples))\n\n        if training_args.do_eval:\n            self.eval_dataset = raw_datasets[\"validation\"]\n            if data_args.max_eval_samples is not None:\n                self.eval_dataset = self.eval_dataset.select(range(data_args.max_eval_samples))\n\n        if training_args.do_predict or data_args.dataset_name is not None or data_args.test_file is not None:\n            self.predict_dataset = raw_datasets[\"test\"]\n            if data_args.max_predict_samples is not None:\n                self.predict_dataset = self.predict_dataset.select(range(data_args.max_predict_samples))\n        \n\n        # add by wjn \n        self.unlabeled_dataset = None\n        if semi_training_args.use_semi is True:\n            assert data_args.num_examples_per_label is not None and data_args.num_examples_per_label != -1\n        \n        # 随机采样few-shot training / dev data（传入label_list，对每个label进行采样，最后得到索引列表）\n        if data_args.num_examples_per_label is not None and data_args.num_examples_per_label != -1:\n            train_examples_idx_list, un_selected_idx_list = random_sampling(\n                raw_datasets=raw_datasets, \n                data_type=\"train\", \n                num_examples_per_label=data_args.num_examples_per_label\n            )\n            self.all_train_dataset = self.train_dataset\n            self.train_dataset = self.all_train_dataset.select(train_examples_idx_list)\n            print(\"Randomly sampling {}-shot training examples for each label. Total examples number is {}\".format(\n                data_args.num_examples_per_label, \n                len(self.train_dataset)\n                ))\n            \n            if semi_training_args.use_semi is True:\n                self.unlabeled_dataset = self.all_train_dataset.select(un_selected_idx_list)\n                print(\"The number of unlabeled data is {}\".format(len(self.unlabeled_dataset)))\n\n        self.metric = load_metric(\"./metrics/super_glue\", data_args.dataset_name)\n\n        if data_args.pad_to_max_length:\n            self.data_collator = default_data_collator\n        elif training_args.fp16:\n            self.data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n\n        # self.test_key = \"accuracy\" if data_args.dataset_name not in [\"record\", \"multirc\"] else \"f1\"\n\n        # ==== define test key ====== add by wjn\n        self.test_key = task_to_test_key[data_args.dataset_name]\n\n    def preprocess_function(self, examples):\n\n        # print(\"examples[self.sentence1_key]=\", examples[self.sentence1_key][0])\n\n        # WSC\n        if self.data_args.dataset_name == \"wsc\":\n            examples[\"span2_word_text\"] = []\n            for text, span2_index, span2_word in zip(examples[\"text\"], examples[\"span2_index\"], examples[\"span2_text\"]):\n                if self.data_args.template_id == 0:\n                    examples[\"span2_word_text\"].append(span2_word + \": \" + text)\n                elif self.data_args.template_id == 1:\n                    words_a = text.split()\n                    words_a[span2_index] = \"*\" + words_a[span2_index] + \"*\"\n                    examples[\"span2_word_text\"].append(' '.join(words_a))\n\n        # WiC\n        if self.data_args.dataset_name == \"wic\":\n            examples[\"processed_sentence1\"] = []\n            if self.data_args.template_id == 1:\n                self.sentence2_key = \"processed_sentence2\"\n                examples[\"processed_sentence2\"] = []\n            for sentence1, sentence2, word, start1, end1, start2, end2 in zip(examples[\"sentence1\"], examples[\"sentence2\"], examples[\"word\"], examples[\"start1\"], examples[\"end1\"], examples[\"start2\"], examples[\"end2\"]):\n                if self.data_args.template_id == 0: #ROBERTA\n                    examples[\"processed_sentence1\"].append(f\"{sentence1} {sentence2} Does {word} have the same meaning in both sentences?\")\n                elif self.data_args.template_id == 1: #BERT\n                    examples[\"processed_sentence1\"].append(word + \": \" + sentence1)\n                    examples[\"processed_sentence2\"].append(word + \": \" + sentence2)\n\n        # MultiRC\n        if self.data_args.dataset_name == \"multirc\":\n            examples[\"question_answer\"] = []\n            for question, answer in zip(examples[\"question\"], examples[\"answer\"]):\n                examples[\"question_answer\"].append(f\"{question} {answer}\")\n\n        # COPA\n        if self.data_args.dataset_name == \"copa\":\n            examples[\"text_a\"] = []\n            for premise, question in zip(examples[\"premise\"], examples[\"question\"]):\n                joiner = \"because\" if question == \"cause\" else \"so\"\n                text_a = f\"{premise} {joiner}\"                    \n                examples[\"text_a\"].append(text_a)\n\n            result1 = self.tokenizer(examples[\"text_a\"], examples[\"choice1\"], padding=self.padding, max_length=self.max_seq_length, truncation=True) \n            result2 = self.tokenizer(examples[\"text_a\"], examples[\"choice2\"], padding=self.padding, max_length=self.max_seq_length, truncation=True)\n            result = {}  \n            for key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]:\n                if key in result1 and key in result2:\n                    result[key] = []\n                    for value1, value2 in zip(result1[key], result2[key]):\n                        result[key].append([value1, value2])\n            return result\n\n        # add by wjn\n        # adding prompt into each example\n        if self.use_prompt:\n            # if use prompt, insert template into example\n            examples = self.prompt_preprocess_function(examples)\n        \n\n        args = (\n            (examples[self.sentence1_key],) if self.sentence2_key is None else (examples[self.sentence1_key], examples[self.sentence2_key])\n        )\n        result = self.tokenizer(*args, padding=self.padding, max_length=self.max_seq_length, truncation=True)\n\n        # add by wjn\n        # adding mask pos feature\n        \n        # print(\"self.use_prompt=\", self.use_prompt)\n        if self.use_prompt:\n            mask_pos = []\n            for input_ids in result[\"input_ids\"]:\n                mask_pos.append(input_ids.index(self.special_token_mapping[\"mask\"]))\n            result[\"mask_pos\"] = mask_pos\n            # print(\"result=\", result)\n        # print(\"*\"*100)\n        # print(\"result.keys()=\", result.keys())\n        # print(\"result['token_type_ids']=\", result[\"token_type_ids\"][0])\n        return result\n    \n    # add by wjn\n    # process data for prompt (add template)\n    def prompt_preprocess_function(self, examples):\n        \n        def replace_mask_token(template):\n            return template.replace(\"<mask>\", self.tokenizer.convert_ids_to_tokens(self.special_token_mapping[\"mask\"]))\n        \n        sequence1_prefix_template = replace_mask_token(self.template[0][\"prefix_template\"] if self.template[0] is not None else \"\")\n        sequence1_suffix_template = replace_mask_token(self.template[0][\"suffix_template\"] if self.template[0] is not None else \"\")\n        sequence2_prefix_template = replace_mask_token(self.template[1][\"prefix_template\"] if self.template[1] is not None else \"\")\n        sequence2_suffix_template = replace_mask_token(self.template[1][\"suffix_template\"] if self.template[1] is not None else \"\")\n        example_num = len(examples[self.sentence1_key])\n        for example_id in range(example_num):\n            sequence1 = examples[self.sentence1_key][example_id]\n            if self.sentence2_key is None:\n                sequence1 = sequence1[:self.data_args.max_seq_length - len(sequence1_suffix_template) - 10]\n            examples[self.sentence1_key][example_id] = \"{}{}{}\".format(\n                sequence1_prefix_template, sequence1, sequence1_suffix_template)\n\n            if self.sentence2_key is not None:\n                sequence2 = examples[self.sentence2_key][example_id]\n                sequence2 = sequence2[:self.data_args.max_seq_length - len(sequence1) - len(sequence1_prefix_template) - len(sequence1_suffix_template) - len(sequence2_prefix_template)- 10]\n                examples[self.sentence2_key][example_id] = \"{}{}{}\".format(\n                    sequence2_prefix_template, sequence2, sequence2_suffix_template)\n        return examples\n\n\n    def compute_metrics(self, p: EvalPrediction):\n        preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n        preds = np.argmax(preds, axis=1)\n\n        if self.data_args.dataset_name == \"record\":\n            return self.reocrd_compute_metrics(p)\n\n        if self.data_args.dataset_name == \"multirc\":\n            from sklearn.metrics import f1_score\n            return {\"f1\": f1_score(preds, p.label_ids)}\n\n        if self.data_args.dataset_name is not None:\n            result = self.metric.compute(predictions=preds, references=p.label_ids)\n            if len(result) > 1:\n                result[\"combined_score\"] = np.mean(list(result.values())).item()\n            return result\n        elif self.is_regression:\n            return {\"mse\": ((preds - p.label_ids) ** 2).mean().item()}\n        else:\n            return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}\n\n    def reocrd_compute_metrics(self, p: EvalPrediction):\n        from tasks.superglue.utils import f1_score, exact_match_score, metric_max_over_ground_truths\n        probs = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n        examples = self.eval_dataset\n        qid2pred = defaultdict(list)\n        qid2ans = {}\n        for prob, example in zip(probs, examples):\n            qid = example['question_id']\n            qid2pred[qid].append((prob[1], example['entity']))\n            if qid not in qid2ans:\n                qid2ans[qid] = example['answers']\n        n_correct, n_total = 0, 0\n        f1, em = 0, 0\n        for qid in qid2pred:\n            preds = sorted(qid2pred[qid], reverse=True)\n            entity = preds[0][1]\n            n_total += 1\n            n_correct += (entity in qid2ans[qid])\n            f1 += metric_max_over_ground_truths(f1_score, entity, qid2ans[qid])\n            em += metric_max_over_ground_truths(exact_match_score, entity, qid2ans[qid])\n        acc = n_correct / n_total\n        f1 = f1 / n_total\n        em = em / n_total\n        return {'f1': f1, 'exact_match': em}\n\n    def record_preprocess_function(self, examples, split=\"train\"):\n        results = {\n            \"index\": list(),\n            \"question_id\": list(),\n            \"input_ids\": list(),\n            \"attention_mask\": list(),\n            \"token_type_ids\": list(),\n            \"label\": list(),\n            \"entity\": list(),\n            \"answers\": list()\n        }\n        for idx, passage in enumerate(examples[\"passage\"]):\n            query, entities, answers =  examples[\"query\"][idx], examples[\"entities\"][idx], examples[\"answers\"][idx]\n            index = examples[\"idx\"][idx]\n            passage = passage.replace(\"@highlight\\n\", \"- \")\n            \n            for ent_idx, ent in enumerate(entities):\n                question = query.replace(\"@placeholder\", ent)\n                result = self.tokenizer(passage, question, padding=self.padding, max_length=self.max_seq_length, truncation=True)\n                label = 1 if ent in answers else 0\n\n                results[\"input_ids\"].append(result[\"input_ids\"])\n                results[\"attention_mask\"].append(result[\"attention_mask\"])\n                if \"token_type_ids\" in result: results[\"token_type_ids\"].append(result[\"token_type_ids\"])\n                results[\"label\"].append(label)\n                results[\"index\"].append(index)\n                results[\"question_id\"].append(index[\"query\"])\n                results[\"entity\"].append(ent)\n                results[\"answers\"].append(answers)\n\n        return results\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!cat tasks/superglue/dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b169a4b",
   "metadata": {},
   "source": [
    "### Step 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade32576",
   "metadata": {},
   "source": [
    "Execution Start time: 00:43:47, End time: 00:43:47 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef087524",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a94505e",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30c92cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9a2daf6",
   "metadata": {},
   "source": [
    "### Step 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2033b6c",
   "metadata": {},
   "source": [
    "Execution Start time: 00:43:49, End time: 00:43:50 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b09b82",
   "metadata": {},
   "source": [
    "**Thought**: The `random_sampling` function in `tasks/superglue/dataset.py` is responsible for sampling a specific number of examples per label. However, I need to ensure that this function is called with the correct parameters in the `run.py` script. Since the `k_shot` parameter is not directly supported, I might need to modify the script to call `random_sampling` with 5 examples per label. I will check the `run.py` script for any calls to `random_sampling` and modify it if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f653c4",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f6cfa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "./tasks/glue/dataset.py:def random_sampling(raw_datasets: load_dataset, data_type: str=\"train\", num_examples_per_label: Optional[int]=16):\n./tasks/glue/dataset.py:            train_examples_idx_list, un_selected_idx_list = random_sampling(\ngrep: ./tasks/glue/__pycache__/dataset.cpython-310.pyc: binary file matches\n./tasks/ner/dataset.py:def random_sampling(raw_datasets: load_dataset, data_type: str=\"train\", num_examples_per_label: Optional[int]=16):\n./tasks/ner/dataset.py:            train_examples_idx_list = random_sampling(\n./tasks/other_cls/dataset.py:def random_sampling(raw_datasets: load_dataset, data_type: str=\"train\", num_examples_per_label: Optional[int]=16, label_set: list=None):\n./tasks/other_cls/dataset.py:            train_examples_idx_list, un_selected_idx_list = random_sampling(\n./tasks/qa/dataset.py:def random_sampling(raw_datasets: load_dataset, data_type: str=\"train\", num_examples_per_label: Optional[int]=16):\n./tasks/qa/dataset.py:            train_examples_idx_list = random_sampling(\n./tasks/srl/dataset.py:def random_sampling(raw_datasets: load_dataset, data_type: str=\"train\", num_examples_per_label: Optional[int]=16):\n./tasks/srl/dataset.py:            train_examples_idx_list = random_sampling(\n./tasks/superglue/dataset.py:def random_sampling(raw_datasets: load_dataset, data_type: str=\"train\", num_examples_per_label: Optional[int]=16):\n./tasks/superglue/dataset.py:            train_examples_idx_list, un_selected_idx_list = random_sampling(\ngrep: ./tasks/superglue/__pycache__/dataset.cpython-310.pyc: binary file matches\n./tasks/utils.py:# def random_sampling(sentences, labels, num):\n./training/self_trainer.py:def random_sampling(raw_datasets, num_examples_per_label: Optional[int]=16):\n./training/self_trainer.py:            recalled_examples_idx_list = random_sampling(\n./training/self_trainer_copy.py:def random_sampling(raw_datasets, num_examples_per_label: Optional[int]=16):\n./training/self_trainer_copy.py:            recalled_examples_idx_list = random_sampling(\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!grep -r \"random_sampling\" ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ba2261",
   "metadata": {},
   "source": [
    "### Step 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b57704b",
   "metadata": {},
   "source": [
    "Execution Start time: 00:43:52, End time: 00:43:52 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f541ef5",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edecd0ad",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c5f4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a10b24cc",
   "metadata": {},
   "source": [
    "### Step 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99222b9e",
   "metadata": {},
   "source": [
    "Execution Start time: 00:43:53, End time: 00:43:53 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f3c281",
   "metadata": {},
   "source": [
    "**Thought**: The `random_sampling` function is defined in `self_trainer.py` and `self_trainer_copy.py`. It is used to sample a specific number of examples per label. I need to modify the call to `random_sampling` to use 5 examples per label. I will check the `self_trainer.py` file to see how this function is called and modify it accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bd5b22",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763f00b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import os\nfrom typing import Dict, OrderedDict\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nfrom typing import Union, Optional, Callable, List, Tuple\nfrom transformers import Trainer\nimport datasets\nimport numpy as np\nfrom typing import Optional, List\nfrom datasets import Dataset, DatasetInfo, NamedSplit, DatasetDict\nfrom datasets.table import Table, list_table_cache_files\nfrom torch.utils.data import RandomSampler, DistributedSampler, DataLoader\nfrom transformers import PreTrainedModel, DataCollator, PreTrainedTokenizerBase, EvalPrediction, TrainerCallback\nfrom transformers.trainer_pt_utils import DistributedSamplerWithLoop, get_length_grouped_indices\nfrom transformers.trainer_pt_utils import DistributedLengthGroupedSampler as DistributedLengthGroupedSamplerOri\nfrom transformers.trainer_pt_utils import LengthGroupedSampler as LengthGroupedSamplerOri\n# from transformers.trainer_utils import has_length\nfrom transformers.training_args import ParallelMode\nfrom transformers.trainer_pt_utils import (\n    DistributedTensorGatherer, \n    SequentialDistributedSampler, \n    nested_concat,\n    )\nfrom transformers.utils import logging\nfrom transformers.trainer_utils import denumpify_detensorize, TrainOutput\nfrom training.sampler import sample_by_bald_class_easiness\nfrom training.trainer_base import BaseTrainer\n\n\nlogger = logging.get_logger('Self-training')\n\nWEIGHTS_NAME = \"pytorch_model.bin\"\nWEIGHTS_INDEX_NAME = \"pytorch_model.bin.index.json\"\n\nclass DatasetK(Dataset):\n    def __init__(\n            self,\n            arrow_table: Table,\n            info: Optional[DatasetInfo] = None,\n            split: Optional[NamedSplit] = None,\n            indices_table: Optional[Table] = None,\n            fingerprint: Optional[str] = None,\n    ):\n        self.custom_cache_files = None\n        super(DatasetK, self).__init__(arrow_table, info, split, indices_table, fingerprint)\n\n\n    @property\n    def cache_files(self) -> List[dict]:\n        \"\"\"The cache files containing the Apache Arrow table backing the dataset.\"\"\"\n        if self.custom_cache_files:\n            return self.custom_cache_files\n        cache_files = list_table_cache_files(self._data)\n        if self._indices is not None:\n            cache_files += list_table_cache_files(self._indices)\n        return [{\"filename\": cache_filename} for cache_filename in cache_files]\n\n    def set_cache_files(self, custom_cache_files):\n        self.custom_cache_files = custom_cache_files\n\n\n# add by wjn\ndef random_sampling(raw_datasets, num_examples_per_label: Optional[int]=16):\n    label_list = raw_datasets[\"label\"] # [0, 1, 0, 0, ...]\n    label_dict = dict()\n    # 记录每个label对应的样本索引\n    for ei, label in enumerate(label_list):\n        if label not in label_dict.keys():\n            label_dict[label] = list()\n        label_dict[label].append(ei)\n    # 对于每个类别，随机采样k个样本\n    few_example_ids = list()\n    for label, eid_list in label_dict.items():\n        # examples = deepcopy(eid_list)\n        # shuffle(examples)\n        idxs = np.random.choice(len(eid_list), size=num_examples_per_label, replace=False)\n        selected_eids = [eid_list[i] for i in idxs]\n        few_example_ids.extend(selected_eids)\n    return few_example_ids\n\nclass TeacherTrainer(BaseTrainer):\n    def __init__(\n            self,\n            model: Union[PreTrainedModel, nn.Module] = None,\n            args = None,\n            data_collator: Optional[DataCollator] = None,\n            train_dataset: Optional[Dataset] = None,\n            eval_dataset: Optional[Dataset] = None,\n            tokenizer: Optional[PreTrainedTokenizerBase] = None,\n            model_init: Callable[[], PreTrainedModel] = None,\n            compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,\n            callbacks: Optional[List[TrainerCallback]] = None,\n            optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None),\n            test_key: str = \"accuracy\"\n    ):\n        super(TeacherTrainer, self).__init__(model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers)\n        self.predict_dataset = eval_dataset\n        self.test_key = test_key\n        # if self.args.do_adv:\n        #     self.fgm = FGM(self.model)\n        # for callback in callbacks:\n        #     callback.trainer = self\n        self.best_metrics = OrderedDict({\n            \"best_epoch\": 0,\n            f\"best_eval_{self.test_key}\": 0,\n        })\n        self.global_step_ = 0\n    \n\n    def mc_evaluate(\n        self,\n        unlabeled_dataset: Optional[Dataset] = None,\n        unlabeled_data_num: int = -1,\n        description: str = \"Evaluate on Unlabeled Data via MC Dropout Uncertainty Estimation\",\n        prediction_loss_only: Optional[bool] = None,\n        ignore_keys: Optional[List[str]] = None,\n        metric_key_prefix: str = \"eval\",\n        T: int = 30,\n        num_classes: int = 0\n    ):\n        \"\"\"\n        Prediction/evaluation loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.\n\n        Works both with or without labels.\n        \"\"\"\n        args = self.args\n\n        prediction_loss_only = prediction_loss_only if prediction_loss_only is not None else args.prediction_loss_only\n        is_sample = True\n        if unlabeled_data_num == -1 or unlabeled_data_num >= len(unlabeled_dataset):\n            unlabeled_data_num = len(unlabeled_dataset)\n            is_sample = False\n        \n        if is_sample:\n            recalled_examples_idx_list = random_sampling(\n                raw_datasets=unlabeled_dataset, \n                num_examples_per_label=unlabeled_data_num // num_classes\n            )\n            unlabeled_dataset = unlabeled_dataset.select(recalled_examples_idx_list)\n            unlabeled_data_num = len(unlabeled_dataset)\n\n        unlabeled_dataloader = self.get_eval_dataloader(unlabeled_dataset)\n        model = self._wrap_model(self.model, training=True, dataloader=unlabeled_dataloader) # reset training to True\n\n        batch_size = unlabeled_dataloader.batch_size\n        # unlabeled_data_num = self.num_examples(unlabeled_dataloader)\n        logger.info(f\"***** Running {description} *****\")\n        logger.info(f\"  Num examples = {unlabeled_data_num}\")\n        logger.info(f\"  Batch size = {batch_size}\")\n\n        # world_size = max(1, args.world_size)\n        \n        # if not prediction_loss_only:\n        #     # The actual number of eval_sample can be greater than num_examples in distributed settings (when we pass\n        #     # a batch size to the sampler)\n        #     make_multiple_of = None\n        #     if hasattr(unlabeled_dataloader, \"sampler\") and isinstance(unlabeled_dataloader.sampler, SequentialDistributedSampler):\n        #         make_multiple_of = unlabeled_dataloader.sampler.batch_size\n\n        model.train() # 开启train模式，允许模型进行Dropout\n\n        if args.past_index >= 0:\n            self._past = None\n\n        self.callback_handler.eval_dataloader = unlabeled_dataloader\n\n        # y_T = np.zeros((T, unlabeled_data_num, num_classes))\n        y_T = list()\n\n        for i in tqdm(range(T)):\n            y_pred = []\n\n            for step, inputs in enumerate(unlabeled_dataloader):\n                _, logits, __ = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n                y_pred.extend(logits.detach().cpu().numpy().tolist())\n            # print(\"y_pred.shape=\", torch.Tensor(y_pred).shape) # [n, num_class]\n            predict_proba = torch.softmax(torch.Tensor(y_pred).to(logits.device), -1)\n            # print(\"predict_proba.shape=\", predict_proba.shape) # [n, num_class]\n            # y_T[i] = predict_proba.detach().cpu().numpy().tolist()\n            y_T.append(predict_proba.detach().cpu().numpy().tolist())\n        \n        y_T = np.array(y_T)\n        #compute mean\n        y_mean = np.mean(y_T, axis=0)\n        # print(\"y_mean.shape=\", y_mean.shape) # e.g., (4095, 3) [n, class_num]\n        # print(\"(unlabeled_data_num, num_classes)=\", (unlabeled_data_num, num_classes))\n        assert y_mean.shape == (unlabeled_data_num, num_classes)\n\n        #compute majority prediction\n        y_pred = np.array([np.argmax(np.bincount(row)) for row in np.transpose(np.argmax(y_T, axis=-1))])\n        assert y_pred.shape == (unlabeled_data_num,)\n\n        #compute variance\n        y_var = np.var(y_T, axis=0)\n        assert y_var.shape == (unlabeled_data_num, num_classes)\n\n        return unlabeled_dataset, y_mean, y_var, y_pred, y_T\n\n\n\nclass RobustTrainer(TeacherTrainer):\n    def __init__(\n            self,\n            model: Union[PreTrainedModel, nn.Module] = None,\n            args = None,\n            data_collator: Optional[DataCollator] = None,\n            train_dataset: Optional[Dataset] = None,\n            eval_dataset: Optional[Dataset] = None,\n            tokenizer: Optional[PreTrainedTokenizerBase] = None,\n            model_init: Callable[[], PreTrainedModel] = None,\n            compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,\n            callbacks: Optional[List[TrainerCallback]] = None,\n            optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None),\n            test_key: str = \"accuracy\"\n    ):\n        super(RobustTrainer, self).__init__(model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers)\n        self.predict_dataset = eval_dataset\n        self.test_key = test_key\n        # if self.args.do_adv:\n        #     self.fgm = FGM(self.model)\n        # for callback in callbacks:\n        #     callback.trainer = self\n        self.best_metrics = OrderedDict({\n            \"best_epoch\": 0,\n            f\"best_eval_{self.test_key}\": 0,\n        })\n        self.global_step_ = 0\n    \n    def robust_train(self):\n        pass\n\n\n\nclass SelfTrainer(object):\n    def __init__(\n        self, \n        teacher_base_model: torch.nn.Module,\n        student_base_model: torch.nn.Module,\n        training_args,\n        semi_training_args,\n        train_dataset: Optional[Dataset]=None,\n        unlabeled_dataset: Optional[Dataset]=None,\n        eval_dataset=None,\n        compute_metrics=None,\n        tokenizer=None,\n        teacher_data_collator=None,\n        student_data_collator=None,\n        test_key=None,\n        task_type=\"cls\",\n        num_classes=0,\n    ) -> None:\n\n        logger.info(\"This is a Self-trainer.\")\n        \n        self.teacher_base_model = teacher_base_model\n        self.student_base_model = student_base_model\n        self.training_args = training_args\n        self.semi_training_args = semi_training_args\n        self.train_dataset = train_dataset\n        self.unlabeled_dataset = unlabeled_dataset\n        self.eval_dataset = eval_dataset\n        self.compute_metrics = compute_metrics\n        self.tokenizer = tokenizer\n        self.teacher_data_collator = teacher_data_collator\n        self.student_data_collator = student_data_collator\n        self.test_key = test_key\n        self.task_type = task_type\n        self.num_classes = num_classes\n\n        # self.set_teacher_trainer()\n        # self.set_student_trainer()\n        self.training_args.per_device_train_batch_size = self.semi_training_args.unlabeled_data_batch_size\n        self.teacher_training_epoch = self.semi_training_args.teacher_training_epoch # 最初teacher模型在labeled data上训练的epoch数\n        self.teacher_tuning_epoch = self.semi_training_args.teacher_tuning_epoch # 每一轮Self-training时，teacher模型继续在labeled data上tune的epoch数\n        self.student_training_epoch = self.semi_training_args.student_training_epoch # 每一轮Self-training时，student模型在pseudo-labeled data上训练的epoch数\n        self.self_training_epoch = self.semi_training_args.self_training_epoch # Self-training迭代数\n        self.unlabeled_data_num = self.semi_training_args.unlabeled_data_num # self-training每轮迭代时，首先挑选一部分用于计算MC dropout uncertainty。-1表示全部计算uncertainty\n        self.pseudo_sample_num_or_ratio = self.semi_training_args.pseudo_sample_num_or_ratio # MC dropout后，从所有计算过uncertainty的unlabeled data上采样的样本比例/数量\n        self.student_learning_rate = self.semi_training_args.student_learning_rate\n        self.student_pre_seq_len = self.semi_training_args.student_pre_seq_len\n        self.output_dir = self.training_args.output_dir\n\n    def get_teacher_trainer(\n        self, \n        base_model: torch.nn.Module, \n        num_train_epochs: int,\n        output_dir: str = None,\n        ):\n        training_args = self.training_args\n        training_args.num_train_epochs = num_train_epochs\n        if output_dir is not None:\n            training_args.output_dir = output_dir\n        # 初始化Teacher训练器\n        teacher_trainer = TeacherTrainer(\n            model=base_model,\n            args=training_args,\n            train_dataset=self.train_dataset if self.training_args.do_train else None,\n            eval_dataset=self.eval_dataset if self.training_args.do_eval else None,\n            compute_metrics=self.compute_metrics,\n            tokenizer=self.tokenizer,\n            data_collator=self.teacher_data_collator,\n            test_key=self.test_key,\n        )\n        return teacher_trainer\n\n    \n    def get_student_trainer(\n        self, \n        base_model: torch.nn.Module, \n        num_train_epochs: int, \n        student_learning_rate: float,\n        pseudo_labeled_dataset: Optional[Dataset] = None, \n        output_dir: str = None,\n        ):\n        training_args = self.training_args\n        training_args.num_train_epochs = num_train_epochs\n        training_args.learning_rate = student_learning_rate\n        if output_dir is not None:\n            training_args.output_dir = output_dir\n        # 初始化Student训练器\n        student_trainer = RobustTrainer(\n            model=base_model,\n            args=training_args,\n            train_dataset=pseudo_labeled_dataset,\n            eval_dataset=self.eval_dataset,\n            compute_metrics=self.compute_metrics,\n            tokenizer=self.tokenizer,\n            data_collator=self.student_data_collator,\n            test_key=self.test_key,\n        )\n        return student_trainer\n\n    def freeze_backbone(self, model: torch.nn.Module, use_pe: bool=False):\n        try:\n            model.freeze_backbone(use_pe=use_pe)\n        except:\n            pass\n        return model\n\n\n    def train(self, resume_from_checkpoint=None):\n        if not os.path.exists(os.path.join(self.output_dir, \"iteration\")):\n            os.makedirs(os.path.join(self.output_dir, \"iteration\"))\n\n        teacher_model = self.teacher_base_model\n        teacher_model = self.freeze_backbone(teacher_model, use_pe=False)\n        teacher_trainer: TeacherTrainer = self.get_teacher_trainer(base_model=teacher_model, num_train_epochs=self.teacher_training_epoch)\n        \n        if resume_from_checkpoint is not None and (os.path.isfile(os.path.join(resume_from_checkpoint, WEIGHTS_NAME)) or os.path.isfile(\n            os.path.join(resume_from_checkpoint, WEIGHTS_INDEX_NAME))\n        ):\n            logger.info(\"*\"*80)\n            logger.info(\"* Directly loading the trained teacher model from {} *\".format(resume_from_checkpoint))\n            logger.info(\"*\"*80)\n            print(\"*\"*80)\n            logger.info(\"* Directly loading the trained teacher model from {} *\".format(resume_from_checkpoint))\n            print(\"*\"*80)\n            # 已有teacher模型，直接加载\n            teacher_trainer._load_from_checkpoint(resume_from_checkpoint)\n        else:\n\n            # 首先对Teacher模型在labeled data上进行full parameter fine-tuning\n            logger.info(\"*\"*66)\n            logger.info(\"* Training teacher model over labeled data before self-training. *\")\n            logger.info(\"*\"*66)\n            print(\"*\"*66)\n            print(\"* Training teacher model over labeled data before self-training. *\")\n            print(\"*\"*66)\n\n            teacher_trainer.train()\n            teacher_model.load_state_dict(torch.load(os.path.join(teacher_trainer.state.best_model_checkpoint, \"pytorch_model.bin\")))\n            teacher_trainer.model = teacher_model\n\n        # 原始的训练结果\n        metrics = teacher_trainer.evaluate()\n        convention_result = metrics[\"eval_{}\".format(self.test_key)]\n\n        logger.info(\"*\"*50)\n        logger.info(\"* Conventional fine-tuning metric: {}. *\".format(convention_result))\n        logger.info(\"*\"*50)\n        print(\"*\"*50)\n        print(\"* Conventional fine-tuning metric: {}. *\".format(convention_result))\n        print(\"*\"*50)\n\n        logger.info(\"*\"*30)\n        logger.info(\"* Starting Self-training ... *\")\n        logger.info(\"*\"*30)\n        print(\"*\"*30)\n        print(\"* Starting Self-training ... *\")\n        print(\"*\"*30)\n\n        best_test_metric = None\n        best_self_training_iteration = None\n        best_teacher_model = None\n\n        # 多轮Teacher-Student迭代训练\n        for iter in range(self.self_training_epoch):\n\n            logger.info(\"*\"*34)\n            logger.info(\"* Self-training {}-th iteration *\".format(iter))\n            logger.info(\"*\"*34)\n            print(\"*\"*34)\n            print(\"* Self-training {}-th iteration *\".format(iter))\n            print(\"*\"*34)\n\n\n            # 获得Teacher模型在测试集上的效果\n            if iter > 0:\n                teacher_trainer.model = teacher_model\n                metrics = teacher_trainer.evaluate()\n                # print(\"metrics=\", metrics)\n            \n            '''\n            e.g., {'eval_loss': 0.6926815509796143, 'eval_accuracy': 0.5234657039711191, 'eval_runtime': 0.7267, 'eval_samples_per_second': 381.161, 'eval_steps_per_second': 48.161, 'epoch': 1.0}\n            '''\n            logger.info(\"*\"*60)\n            logger.info(\"* The testing result of teacher model is {} result: {} *\".format(self.test_key, metrics[\"eval_{}\".format(self.test_key)]))\n            logger.info(\"*\"*60)\n            print(\"*\"*60)\n            print(\"* The testing result of teacher model is {} result: {} *\".format(self.test_key, metrics[\"eval_{}\".format(self.test_key)]))\n            print(\"*\"*60)\n\n            if best_test_metric is None or best_test_metric < metrics[\"eval_{}\".format(self.test_key)]:\n                best_test_metric = metrics[\"eval_{}\".format(self.test_key)]\n                best_self_training_iteration = iter\n                best_teacher_model = teacher_model\n                logger.info(\"The best teacher model at {}-th self-training iteration.\".format(best_self_training_iteration))\n                logger.info(\"The best teacher model testing result is {}.\".format(best_test_metric))\n                print(\"The best teacher model at {}-th self-training iteration.\".format(best_self_training_iteration))\n                print(\"The best teacher model testing result is {}.\".format(best_test_metric))\n            \n\n            if iter == self.self_training_epoch - 1:\n                break\n\n\n            # # Teacher模型在labeled data上进行parameter-efficient tuning\n            # if iter > 0:\n            #     logger.info(\"*\"*80)\n            #     logger.info(\"* Tuning the teacher model on labeled data at {}-th self-training iteration. *\".format(iter))\n            #     logger.info(\"*\"*80)\n            #     print(\"*\"*80)\n            #     print(\"* Tuning the teacher model on labeled data at {}-th self-training iteration. *\".format(iter))\n            #     print(\"*\"*80)\n\n            #     teacher_model = self.freeze_backbone(teacher_model, use_pe=True)\n            #     # teacher_trainer: TeacherTrainer = self.get_teacher_trainer(base_model=teacher_model, num_train_epochs=self.teacher_tuning_epoch)\n            #     teacher_trainer.train()\n            #     teacher_model.load_state_dict(torch.load(os.path.join(teacher_trainer.state.best_model_checkpoint, \"pytorch_model.bin\")))\n            #     teacher_trainer.model = teacher_model\n            \n            # Teacher模型在unlabeled data上获取pseudo-labeled data，并根据uncertainty estimation进行采样\n            logger.info(\"*\"*72)\n            logger.info(\"Obtaining pseudo-labeled data and uncertainty estimation via MC dropout.\")\n            logger.info(\"*\"*72)\n            print(\"*\"*72)\n            print(\"Obtaining pseudo-labeled data and uncertainty estimation via MC dropout.\")\n            print(\"*\"*72)\n\n            unlabeled_dataset, y_mean, y_var, y_pred, y_T = teacher_trainer.mc_evaluate(\n                unlabeled_dataset=self.unlabeled_dataset, \n                unlabeled_data_num=self.unlabeled_data_num,\n                T=20, \n                num_classes=self.num_classes\n                )\n            \n            logger.info(\"*\"*42)\n            logger.info(\"* Sampling reliable pseudo-labeled data. *\")\n            logger.info(\"*\"*42)\n            print(\"*\"*42)\n            print(\"* Sampling reliable pseudo-labeled data. *\")\n            print(\"*\"*42)\n            \n            X_batch, y_batch, _ = sample_by_bald_class_easiness(\n                tokenizer=self.tokenizer, \n                X=unlabeled_dataset, \n                y_mean=y_mean, \n                y_var=y_var, \n                y=y_pred, \n                num_samples=int(y_pred.shape[0] * self.pseudo_sample_num_or_ratio) if self.pseudo_sample_num_or_ratio <= 1.0 else int(self.pseudo_sample_num_or_ratio), \n                num_classes=self.num_classes, \n                y_T=y_T)\n            pseudo_labeled_examples = X_batch\n            pseudo_labeled_examples[\"label\"] = y_batch\n            \n            # 生成pseudo-labeled dataset，并与labeled data混合\n            # pseudo_labeled_dataset = DatasetDict()\n            pseudo_labeled_dataset = DatasetK.from_dict(pseudo_labeled_examples)\n            for i in range(len(self.train_dataset)):\n                pseudo_labeled_dataset = pseudo_labeled_dataset.add_item(self.train_dataset[i])\n\n            # 初始化一个新的Student模型，并让Student模型在pseudo-labeled data上进行鲁棒学习\n            logger.info(\"*\"*56)\n            logger.info(\"* Training a new student model on pseudo-labeled data. *\")\n            logger.info(\"*\"*56)\n            print(\"*\"*56)\n            print(\"* Training a new student model on pseudo-labeled data. *\")\n            print(\"*\"*56)\n            \n            student_model = self.student_base_model\n            student_model = self.freeze_backbone(student_model, use_pe=True)\n            student_trainer: RobustTrainer = self.get_student_trainer(\n                base_model=student_model, \n                num_train_epochs=self.student_training_epoch,\n                student_learning_rate=self.student_learning_rate,\n                pseudo_labeled_dataset=pseudo_labeled_dataset,\n                output_dir=os.path.join(self.output_dir, \"iteration\", \"student_iter_{}\".format(iter))\n            )\n            student_trainer.train()\n            student_model.load_state_dict(torch.load(os.path.join(student_trainer.state.best_model_checkpoint, \"pytorch_model.bin\")))\n\n            # 将Student模型参数赋给Teacher，作为下一轮训练的Teacher初始化\n            logger.info(\"*\"*64)\n            logger.info(\"* Initializing a new teacher model from trained student model. *\")\n            logger.info(\"*\"*64)\n            print(\"*\"*64)\n            print(\"* Initializing a new teacher model from trained student model. *\")\n            print(\"*\"*64)\n            teacher_model = student_model\n            # teacher_trainer = student_trainer\n            teacher_trainer: TeacherTrainer = self.get_teacher_trainer(\n                base_model=student_model, \n                num_train_epochs=self.teacher_tuning_epoch, \n                output_dir=os.path.join(self.output_dir, \"iteration\", \"teacher_iter_{}\".format(iter))\n            )\n\n\n            \n        \n        logger.info(\"********** Finishing Self-training **********\")\n        logger.info(\"The best teacher model at {}-th self-training iteration.\".format(best_self_training_iteration))\n        logger.info(\"The best teacher model testing result is {}.\".format(best_test_metric))\n        print(\"********** Finishing Self-training **********\")\n        print(\"The best teacher model at {}-th self-training iteration.\".format(best_self_training_iteration))\n        print(\"The best teacher model testing result is {}.\".format(best_test_metric))\n\n        \n        # 根据当前最好的Teacher模型，在全部的unlabeled data上打伪标签，并进行mc dropout（样本数量最多不超过50000）\n        if self.semi_training_args.post_student_train:\n            \n            logger.info(\"********** Post training **********\")\n            print(\"********** Post training **********\")\n\n            teacher_trainer: TeacherTrainer = self.get_teacher_trainer(\n                base_model=best_teacher_model, \n                num_train_epochs=self.teacher_tuning_epoch, \n                output_dir=os.path.join(self.output_dir, \"teacher_iter_post\")\n            )\n\n            unlabeled_dataset, y_mean, y_var, y_pred, y_T = teacher_trainer.mc_evaluate(\n                unlabeled_dataset=self.unlabeled_dataset, \n                unlabeled_data_num=20480,\n                T=5, \n                num_classes=self.num_classes\n                )\n            \n            post_sample_num = int(y_pred.shape[0] * 0.5)\n            \n            X_batch, y_batch, _ = sample_by_bald_class_easiness(\n                tokenizer=self.tokenizer, \n                X=unlabeled_dataset, \n                y_mean=y_mean, \n                y_var=y_var, \n                y=y_pred, \n                num_samples=post_sample_num, \n                num_classes=self.num_classes, \n                y_T=y_T)\n            pseudo_labeled_examples = X_batch\n            pseudo_labeled_examples[\"label\"] = y_batch\n            # 生成pseudo-labeled dataset\n            # pseudo_labeled_dataset = DatasetDict()\n            pseudo_labeled_dataset = DatasetK.from_dict(pseudo_labeled_examples)\n\n\n            # 初始化一个新的Student模型，并让Student模型在pseudo-labeled data上进行鲁棒学习\n            logger.info(\"*\"*56)\n            logger.info(\"* Training a new student model on pseudo-labeled data. *\")\n            logger.info(\"*\"*56)\n            print(\"*\"*56)\n            print(\"* Training a new student model on pseudo-labeled data. *\")\n            print(\"*\"*56)\n            \n            student_model = self.student_base_model\n            student_model = self.freeze_backbone(student_model, use_pe=True)\n            student_trainer: RobustTrainer = self.get_student_trainer(\n                base_model=student_model, \n                num_train_epochs=self.student_training_epoch if len(pseudo_labeled_dataset) <= 4096 else int(self.student_training_epoch / 2),\n                student_learning_rate=self.student_learning_rate,\n                pseudo_labeled_dataset=pseudo_labeled_dataset,\n                output_dir=os.path.join(self.output_dir, \"student_iter_{}\".format(iter))\n            )\n            student_trainer.train()\n            student_model.load_state_dict(torch.load(os.path.join(student_trainer.state.best_model_checkpoint, \"pytorch_model.bin\")))\n\n            metrics = student_trainer.evaluate()\n            post_metric = metrics[\"eval_{}\".format(self.test_key)]\n\n\n        print(\"*\"*68)\n        print(\"Finishing all the processes, the results are shown in the following:\")\n        print(\"Conventional fine-tuning {} metric: {}\".format(self.test_key, convention_result))\n        print(\"Best self-training {} metric: {}\".format(self.test_key, best_test_metric))\n        if self.semi_training_args.post_student_train:\n            print(\"Post training {} metric: {}\".format(self.test_key, post_metric))\n        print(\"*\"*68)\n\n        return TrainOutput(teacher_trainer.state.global_step, 0.0, metrics)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!cat training/self_trainer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f1f81b",
   "metadata": {},
   "source": [
    "### Step 19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05283b4",
   "metadata": {},
   "source": [
    "Execution Start time: 00:43:59, End time: 00:43:59 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7c9d1c",
   "metadata": {},
   "source": [
    "**Thought**: The `random_sampling` function in `self_trainer.py` is used to sample a specific number of examples per label. It is called within the `mc_evaluate` method of the `TeacherTrainer` class. To set the number of examples per label to 5, I need to modify the call to `random_sampling` in the `mc_evaluate` method. I will edit the `self_trainer.py` file to set `num_examples_per_label` to 5 when calling `random_sampling`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ad5ede",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c20c7b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        prediction_loss_only = prediction_loss_only if prediction_loss_only is not None else args.prediction_loss_only\n        is_sample = True\n        if unlabeled_data_num == -1 or unlabeled_data_num >= len(unlabeled_dataset):\n            unlabeled_data_num = len(unlabeled_dataset)\n            is_sample = False\n        \n        if is_sample:\n            recalled_examples_idx_list = random_sampling(\n                raw_datasets=unlabeled_dataset, \n                num_examples_per_label=5\n            )\n            unlabeled_dataset = unlabeled_dataset.select(recalled_examples_idx_list)\n            unlabeled_data_num = len(unlabeled_dataset)\n\n        unlabeled_dataloader = self.get_eval_dataloader(unlabeled_dataset)\n        model = self._wrap_model(self.model, training=True, dataloader=unlabeled_dataloader) # reset training to True\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training/self_trainer.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "            recalled_examples_idx_list = random_sampling(\n",
    "                raw_datasets=unlabeled_dataset, \n",
    "                num_examples_per_label=unlabeled_data_num // num_classes\n",
    "            )\n",
    "<<<AFTER_EDIT>>>\n",
    "            recalled_examples_idx_list = random_sampling(\n",
    "                raw_datasets=unlabeled_dataset, \n",
    "                num_examples_per_label=5\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee23db63",
   "metadata": {},
   "source": [
    "### Step 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a589d7",
   "metadata": {},
   "source": [
    "Execution Start time: 00:44:04, End time: 00:49:04 (duration: 300 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0371d38a",
   "metadata": {},
   "source": [
    "**Thought**: I have successfully modified the `self_trainer.py` file to set the number of examples per label to 5 when calling the `random_sampling` function. Now, I can proceed to run the `run.py` script again with the updated configuration to train the model and evaluate its accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad820921",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec88549c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n08/21/2024 00:44:10 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\ntokenizer_config.json:   0%|                         | 0.00/25.0 [00:00<?, ?B/s]\ntokenizer_config.json: 100%|██████████████████| 25.0/25.0 [00:00<00:00, 153kB/s]\n\nconfig.json:   0%|                                    | 0.00/481 [00:00<?, ?B/s]\n\n\nvocab.json:   0%|                                    | 0.00/899k [00:00<?, ?B/s]\n\n\nmerges.txt:   0%|                                    | 0.00/456k [00:00<?, ?B/s]\n\n\ntokenizer.json:   0%|                               | 0.00/1.36M [00:00<?, ?B/s]\n\n\nDownloading:   0%|                                  | 0.00/9.47k [00:00<?, ?B/s]\n\n\nDownloading:   0%|                                  | 0.00/8.23k [00:00<?, ?B/s]\n\nDownloading and preparing dataset super_glue/rte (download: 733.32 KiB, generated: 1.83 MiB, post-processed: Unknown size, total: 2.54 MiB) to /root/.cache/huggingface/datasets/super_glue/rte/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7...\n\nDownloading:   0%|                                   | 0.00/751k [00:00<?, ?B/s]\n\n\n0 examples [00:00, ? examples/s]\n                                \n\n0 examples [00:00, ? examples/s]\n                                \n\n0 examples [00:00, ? examples/s]\n                                \nDataset super_glue downloaded and prepared to /root/.cache/huggingface/datasets/super_glue/rte/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7. Subsequent calls will reuse this data.\n\n  0%|                                                     | 0/3 [00:00<?, ?it/s]\n100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 296.84it/s]\n{'entailment': 0, 'not_entailment': 1}\n{0: 'entailment', 1: 'not_entailment'}\n08/21/2024 00:44:12 - WARNING - datasets.fingerprint - Parameter 'function'=<function SuperGlueDataset.preprocess_function at 0x7fbde0b55360> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n\nRunning tokenizer on dataset:   0%|                       | 0/3 [00:00<?, ?ba/s]\nRunning tokenizer on dataset:  33%|█████          | 1/3 [00:00<00:00,  6.59ba/s]\nRunning tokenizer on dataset: 100%|███████████████| 3/3 [00:00<00:00, 11.47ba/s]\nRunning tokenizer on dataset: 100%|███████████████| 3/3 [00:00<00:00, 10.67ba/s]\n\nRunning tokenizer on dataset:   0%|                       | 0/1 [00:00<?, ?ba/s]\nRunning tokenizer on dataset: 100%|███████████████| 1/1 [00:00<00:00, 42.34ba/s]\n\nRunning tokenizer on dataset:   0%|                       | 0/3 [00:00<?, ?ba/s]\nRunning tokenizer on dataset:  67%|██████████     | 2/3 [00:00<00:00, 14.39ba/s]\nRunning tokenizer on dataset: 100%|███████████████| 3/3 [00:00<00:00, 12.11ba/s]\n\nmodel.safetensors:   0%|                             | 0.00/499M [00:00<?, ?B/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n[WARNING|modeling_utils.py:3024] 2024-08-21 00:44:19,615 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertPrefixForPromptFinetuning: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n- This IS expected if you are initializing RobertPrefixForPromptFinetuning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertPrefixForPromptFinetuning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n***** total param is 163968345 *****\n/usr/local/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\n  0%|                                                    | 0/78 [00:00<?, ?it/s]\n  1%|▌                                           | 1/78 [00:26<33:47, 26.34s/it]\n  3%|█▏                                          | 2/78 [00:46<28:37, 22.60s/it]\n  4%|█▋                                          | 3/78 [01:05<26:26, 21.15s/it]\n  5%|██▎                                         | 4/78 [01:24<25:02, 20.30s/it]\n  6%|██▊                                         | 5/78 [01:44<24:20, 20.01s/it]\n  8%|███▍                                        | 6/78 [02:04<24:13, 20.19s/it]\n  9%|███▉                                        | 7/78 [02:23<23:27, 19.82s/it]\n 10%|████▌                                       | 8/78 [02:42<22:39, 19.42s/it]\n 12%|█████                                       | 9/78 [03:00<21:54, 19.05s/it]\n 13%|█████▌                                     | 10/78 [03:18<21:20, 18.83s/it]\n 14%|██████                                     | 11/78 [03:37<21:00, 18.81s/it]\n 15%|██████▌                                    | 12/78 [03:56<20:38, 18.77s/it]\n 17%|███████▏                                   | 13/78 [04:15<20:29, 18.92s/it]\n 18%|███████▋                                   | 14/78 [04:35<20:19, 19.06s/it]\n\nYour command has already run for 300 seconds. It is still running. You can continue to wait or interrupt it with Thought: ... \nAction: interrupt\n```\n``` \nor:\nThought: ... \nAction: continue_wait\n```\n```"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python3 run.py \\\n",
    "  --model_name_or_path roberta-base \\\n",
    "  --task_name superglue \\\n",
    "  --dataset_name rte \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --max_seq_length 128 \\\n",
    "  --per_device_train_batch_size 32 \\\n",
    "  --learning_rate 5e-3 \\\n",
    "  --num_train_epochs 1 \\\n",
    "  --output_dir checkpoints/rte-roberta/ \\\n",
    "  --overwrite_output_dir \\\n",
    "  --hidden_dropout_prob 0.1 \\\n",
    "  --seed 42 \\\n",
    "  --save_strategy no \\\n",
    "  --evaluation_strategy epoch \\\n",
    "  --prompt_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f562ac8",
   "metadata": {},
   "source": [
    "### Step 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68cd511",
   "metadata": {},
   "source": [
    "Execution Start time: 00:49:05, End time: 00:54:05 (duration: 300 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce17732",
   "metadata": {},
   "source": [
    "**Thought**: The training process is taking longer than expected, but it seems to be proceeding correctly. I will continue to wait for it to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533d4142",
   "metadata": {},
   "source": [
    "**Action (continue_wait)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6336fe1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n 19%|████████▎                                  | 15/78 [04:54<20:17, 19.32s/it]\n 21%|████████▊                                  | 16/78 [05:18<21:13, 20.53s/it]\n 22%|█████████▎                                 | 17/78 [05:41<21:35, 21.23s/it]\n 23%|█████████▉                                 | 18/78 [06:06<22:21, 22.35s/it]\n 24%|██████████▍                                | 19/78 [06:30<22:34, 22.95s/it]\n 26%|███████████                                | 20/78 [06:53<22:10, 22.95s/it]\n 27%|███████████▌                               | 21/78 [07:16<21:50, 22.99s/it]\n 28%|████████████▏                              | 22/78 [07:39<21:27, 22.98s/it]\n 29%|████████████▋                              | 23/78 [08:01<20:45, 22.65s/it]\n 31%|█████████████▏                             | 24/78 [08:26<20:58, 23.31s/it]\n 32%|█████████████▊                             | 25/78 [08:48<20:21, 23.04s/it]\n 33%|██████████████▎                            | 26/78 [09:11<19:48, 22.85s/it]\n 35%|██████████████▉                            | 27/78 [09:33<19:26, 22.88s/it]\n\nYour command has already run for 300 seconds. It is still running. You can continue to wait or interrupt it with Thought: ... \nAction: interrupt\n```\n``` \nor:\nThought: ... \nAction: continue_wait\n```\n```"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51c2b230",
   "metadata": {},
   "source": [
    "### Step 22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a08833",
   "metadata": {},
   "source": [
    "Execution Start time: 00:54:07, End time: 00:59:07 (duration: 300 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16efac85",
   "metadata": {},
   "source": [
    "**Thought**: The training process is still ongoing and taking longer than expected. However, it seems to be proceeding correctly. I will continue to wait for it to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1927a0a5",
   "metadata": {},
   "source": [
    "**Action (continue_wait)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6a67f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n 36%|███████████████▍                           | 28/78 [09:55<18:43, 22.48s/it]\n 37%|███████████████▉                           | 29/78 [10:15<17:49, 21.82s/it]\n 38%|████████████████▌                          | 30/78 [10:35<16:56, 21.17s/it]\n 40%|█████████████████                          | 31/78 [10:54<16:11, 20.67s/it]\n 41%|█████████████████▋                         | 32/78 [11:14<15:35, 20.34s/it]\n 42%|██████████████████▏                        | 33/78 [11:34<15:09, 20.22s/it]\n 44%|██████████████████▋                        | 34/78 [11:55<14:56, 20.37s/it]\n 45%|███████████████████▎                       | 35/78 [12:15<14:29, 20.21s/it]\n 46%|███████████████████▊                       | 36/78 [12:33<13:50, 19.77s/it]\n 47%|████████████████████▍                      | 37/78 [12:51<13:11, 19.31s/it]\n 49%|████████████████████▉                      | 38/78 [13:10<12:46, 19.17s/it]\n 50%|█████████████████████▌                     | 39/78 [13:29<12:15, 18.87s/it]\n 51%|██████████████████████                     | 40/78 [13:52<12:45, 20.13s/it]\n 53%|██████████████████████▌                    | 41/78 [14:14<12:49, 20.79s/it]\n 54%|███████████████████████▏                   | 42/78 [14:35<12:27, 20.75s/it]\n\nYour command has already run for 300 seconds. It is still running. You can continue to wait or interrupt it with Thought: ... \nAction: interrupt\n```\n``` \nor:\nThought: ... \nAction: continue_wait\n```\n```"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "<content>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef39fa10",
   "metadata": {},
   "source": [
    "### Step 23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa00a968",
   "metadata": {},
   "source": [
    "Execution Start time: 00:59:09, End time: 01:04:09 (duration: 300 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b985bd6",
   "metadata": {},
   "source": [
    "**Thought**: The training process is still ongoing and taking longer than expected. However, it seems to be proceeding correctly. I will continue to wait for it to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8960917",
   "metadata": {},
   "source": [
    "**Action (continue_wait)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2210fd15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n 55%|███████████████████████▋                   | 43/78 [14:54<11:54, 20.42s/it]\n 56%|████████████████████████▎                  | 44/78 [15:15<11:34, 20.42s/it]\n 58%|████████████████████████▊                  | 45/78 [15:36<11:18, 20.56s/it]\n 59%|█████████████████████████▎                 | 46/78 [15:57<11:03, 20.75s/it]\n 60%|█████████████████████████▉                 | 47/78 [16:19<10:54, 21.10s/it]\n 62%|██████████████████████████▍                | 48/78 [16:40<10:32, 21.08s/it]\n 63%|███████████████████████████                | 49/78 [17:00<10:06, 20.91s/it]\n 64%|███████████████████████████▌               | 50/78 [17:23<10:01, 21.48s/it]\n 65%|████████████████████████████               | 51/78 [17:45<09:45, 21.70s/it]\n 67%|████████████████████████████▋              | 52/78 [18:07<09:28, 21.85s/it]\n 68%|█████████████████████████████▏             | 53/78 [18:31<09:20, 22.40s/it]\n 69%|█████████████████████████████▊             | 54/78 [18:54<09:01, 22.55s/it]\n 71%|██████████████████████████████▎            | 55/78 [19:17<08:42, 22.74s/it]\n 72%|██████████████████████████████▊            | 56/78 [19:40<08:20, 22.74s/it]\n\nYour command has already run for 300 seconds. It is still running. You can continue to wait or interrupt it with Thought: ... \nAction: interrupt\n```\n``` \nor:\nThought: ... \nAction: continue_wait\n```\n```"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "<content>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b11d643",
   "metadata": {},
   "source": [
    "### Step 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cdd34a",
   "metadata": {},
   "source": [
    "Execution Start time: 01:04:10, End time: 01:09:10 (duration: 300 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187e5446",
   "metadata": {},
   "source": [
    "**Thought**: The training process is still ongoing and taking longer than expected. However, it seems to be proceeding correctly. I will continue to wait for it to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfbdba7",
   "metadata": {},
   "source": [
    "**Action (continue_wait)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01acb68b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n 73%|███████████████████████████████▍           | 57/78 [20:02<07:56, 22.68s/it]\n 74%|███████████████████████████████▉           | 58/78 [20:27<07:43, 23.16s/it]\n 76%|████████████████████████████████▌          | 59/78 [20:51<07:27, 23.54s/it]\n 77%|█████████████████████████████████          | 60/78 [21:13<06:56, 23.14s/it]\n 78%|█████████████████████████████████▋         | 61/78 [21:36<06:30, 22.99s/it]\n 79%|██████████████████████████████████▏        | 62/78 [21:59<06:08, 23.03s/it]\n 81%|██████████████████████████████████▋        | 63/78 [22:21<05:40, 22.71s/it]\n 82%|███████████████████████████████████▎       | 64/78 [22:43<05:14, 22.45s/it]\n 83%|███████████████████████████████████▊       | 65/78 [23:02<04:37, 21.38s/it]\n 85%|████████████████████████████████████▍      | 66/78 [23:21<04:09, 20.80s/it]\n 86%|████████████████████████████████████▉      | 67/78 [23:41<03:44, 20.44s/it]\n 87%|█████████████████████████████████████▍     | 68/78 [24:00<03:19, 19.97s/it]\n 88%|██████████████████████████████████████     | 69/78 [24:19<02:58, 19.81s/it]\n 90%|██████████████████████████████████████▌    | 70/78 [24:38<02:34, 19.37s/it]\n\nYour command has already run for 300 seconds. It is still running. You can continue to wait or interrupt it with Thought: ... \nAction: interrupt\n```\n``` \nor:\nThought: ... \nAction: continue_wait\n```\n```"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "<content>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dece6562",
   "metadata": {},
   "source": [
    "### Step 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb5dc5d",
   "metadata": {},
   "source": [
    "Execution Start time: 01:09:12, End time: 01:12:06 (duration: 174 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8e7d7d",
   "metadata": {},
   "source": [
    "**Thought**: The training process is still ongoing and taking longer than expected. However, it seems to be proceeding correctly. I will continue to wait for it to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c89c7c",
   "metadata": {},
   "source": [
    "**Action (continue_wait)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff70b1c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n 91%|███████████████████████████████████████▏   | 71/78 [24:56<02:13, 19.11s/it]\n 92%|███████████████████████████████████████▋   | 72/78 [25:14<01:53, 18.89s/it]\n 94%|████████████████████████████████████████▏  | 73/78 [25:33<01:33, 18.70s/it]\n 95%|████████████████████████████████████████▊  | 74/78 [25:51<01:14, 18.56s/it]\n 96%|█████████████████████████████████████████▎ | 75/78 [26:09<00:55, 18.37s/it]\n 97%|█████████████████████████████████████████▉ | 76/78 [26:28<00:37, 18.57s/it]\n 99%|██████████████████████████████████████████▍| 77/78 [26:47<00:18, 18.79s/it]\n100%|███████████████████████████████████████████| 78/78 [27:02<00:00, 17.73s/it]\n\n  0%|                                                    | 0/35 [00:00<?, ?it/s]\u001b[A\n\n  6%|██▌                                         | 2/35 [00:01<00:19,  1.68it/s]\u001b[A\n\n  9%|███▊                                        | 3/35 [00:02<00:27,  1.17it/s]\u001b[A\n\n 11%|█████                                       | 4/35 [00:03<00:30,  1.02it/s]\u001b[A\n\n 14%|██████▎                                     | 5/35 [00:04<00:31,  1.06s/it]\u001b[A\n\n 17%|███████▌                                    | 6/35 [00:06<00:32,  1.11s/it]\u001b[A\n\n 20%|████████▊                                   | 7/35 [00:07<00:31,  1.14s/it]\u001b[A\n\n 23%|██████████                                  | 8/35 [00:08<00:31,  1.16s/it]\u001b[A\n\n 26%|███████████▎                                | 9/35 [00:09<00:30,  1.17s/it]\u001b[A\n\n 29%|████████████▎                              | 10/35 [00:10<00:29,  1.18s/it]\u001b[A\n\n 31%|█████████████▌                             | 11/35 [00:12<00:28,  1.18s/it]\u001b[A\n\n 34%|██████████████▋                            | 12/35 [00:13<00:27,  1.19s/it]\u001b[A\n\n 37%|███████████████▉                           | 13/35 [00:14<00:26,  1.20s/it]\u001b[A\n\n 40%|█████████████████▏                         | 14/35 [00:15<00:25,  1.20s/it]\u001b[A\n\n 43%|██████████████████▍                        | 15/35 [00:16<00:23,  1.20s/it]\u001b[A\n\n 46%|███████████████████▋                       | 16/35 [00:18<00:22,  1.20s/it]\u001b[A\n\n 49%|████████████████████▉                      | 17/35 [00:19<00:21,  1.20s/it]\u001b[A\n\n 51%|██████████████████████                     | 18/35 [00:20<00:20,  1.20s/it]\u001b[A\n\n 54%|███████████████████████▎                   | 19/35 [00:21<00:19,  1.20s/it]\u001b[A\n\n 57%|████████████████████████▌                  | 20/35 [00:22<00:18,  1.20s/it]\u001b[A\n\n 60%|█████████████████████████▊                 | 21/35 [00:24<00:16,  1.20s/it]\u001b[A\n\n 63%|███████████████████████████                | 22/35 [00:25<00:15,  1.20s/it]\u001b[A\n\n 66%|████████████████████████████▎              | 23/35 [00:26<00:14,  1.20s/it]\u001b[A\n\n 69%|█████████████████████████████▍             | 24/35 [00:27<00:13,  1.20s/it]\u001b[A\n\n 71%|██████████████████████████████▋            | 25/35 [00:28<00:11,  1.20s/it]\u001b[A\n\n 74%|███████████████████████████████▉           | 26/35 [00:30<00:10,  1.20s/it]\u001b[A\n\n 77%|█████████████████████████████████▏         | 27/35 [00:31<00:09,  1.20s/it]\u001b[A\n\n 80%|██████████████████████████████████▍        | 28/35 [00:32<00:08,  1.20s/it]\u001b[A\n\n 83%|███████████████████████████████████▋       | 29/35 [00:33<00:07,  1.20s/it]\u001b[A\n\n 86%|████████████████████████████████████▊      | 30/35 [00:34<00:06,  1.20s/it]\u001b[A\n\n 89%|██████████████████████████████████████     | 31/35 [00:36<00:04,  1.20s/it]\u001b[A\n\n 91%|███████████████████████████████████████▎   | 32/35 [00:37<00:03,  1.20s/it]\u001b[A\n\n 94%|████████████████████████████████████████▌  | 33/35 [00:38<00:02,  1.20s/it]\u001b[A\n\n 97%|█████████████████████████████████████████▊ | 34/35 [00:39<00:01,  1.20s/it]\u001b[A\n\n100%|███████████████████████████████████████████| 35/35 [00:40<00:00,  1.07s/it]\u001b[A\n                                                                                \n\n\n                                                                                \n\u001b[A{'eval_loss': 0.6941400170326233, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 41.6262, 'eval_samples_per_second': 6.654, 'eval_steps_per_second': 0.841, 'epoch': 1.0}\n\n100%|███████████████████████████████████████████| 78/78 [27:44<00:00, 17.73s/it]\n\n100%|███████████████████████████████████████████| 35/35 [00:40<00:00,  1.07s/it]\u001b[A\n\n                                                                                \u001b[Aeval_metrics= {'eval_loss': 0.6941400170326233, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 41.6262, 'eval_samples_per_second': 6.654, 'eval_steps_per_second': 0.841, 'epoch': 1.0}\n08/21/2024 01:12:05 - INFO - training.trainer_base - ***** Epoch 0: Best results *****\n08/21/2024 01:12:05 - INFO - training.trainer_base - best_epoch = 0\n08/21/2024 01:12:05 - INFO - training.trainer_base - best_eval_accuracy = 0.4729241877256318\n\n                                                                                \nOrderedDict([('best_epoch', 0), ('best_eval_accuracy', 0.4729241877256318), ('epoch', 1.0)])\n\n100%|███████████████████████████████████████████| 78/78 [27:44<00:00, 17.73s/it]\n                                                                                \n{'train_runtime': 1664.5823, 'train_samples_per_second': 1.496, 'train_steps_per_second': 0.047, 'train_loss': 1.0209563817733374, 'epoch': 1.0}\n\n100%|███████████████████████████████████████████| 78/78 [27:44<00:00, 17.73s/it]\n100%|███████████████████████████████████████████| 78/78 [27:44<00:00, 21.34s/it]\n***** train metrics *****\n  epoch                    =        1.0\n  train_loss               =      1.021\n  train_runtime            = 0:27:44.58\n  train_samples_per_second =      1.496\n  train_steps_per_second   =      0.047\n***** best metrics *****\n  best_epoch         =      0\n  best_eval_accuracy = 0.4729\n  epoch              =    1.0\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "<content>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df61cddc",
   "metadata": {},
   "source": [
    "### Step 26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd2b504",
   "metadata": {},
   "source": [
    "Execution Start time: 01:12:07, End time: 01:12:07 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8621b4",
   "metadata": {},
   "source": [
    "**Thought**: The training and evaluation process has completed successfully. The evaluation accuracy obtained is approximately 0.4729. I will now submit this result in the required JSON format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ef3caa",
   "metadata": {},
   "source": [
    "**Action (submit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb81cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'eval_accuracy': 0.4729}"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
