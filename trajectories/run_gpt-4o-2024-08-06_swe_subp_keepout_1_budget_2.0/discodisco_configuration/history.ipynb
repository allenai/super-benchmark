{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "978c9238",
   "metadata": {},
   "source": [
    "### System Prompt\n",
    "<details>\n",
    "SETTING: You are an autonomous programmer, and you're working directly in the command line with a special Jupyter notebook interface.\n",
    "\n",
    "The special Jupyter notebook interface consists of a file editor that shows you 100 lines of a file at a time.\n",
    "You can execute commands in the notebook using:\n",
    "\n",
    "1. Bash commands: Commands starting with !.\n",
    "2. Python commands: Standard Python code.\n",
    "3. Magic commands: Commands starting with %, e.g., %cd <path>.\n",
    "\n",
    "Additionally, you can also use the following commands to help you navigate and edit files.\n",
    "\n",
    "COMMANDS:\n",
    "\n",
    "open:\n",
    "  docstring: opens the file at the given path in the editor. If line_number is provided, the window will be move to include that line\n",
    "  signature: open <path> [<line_number>]\n",
    "  arguments:\n",
    "    - path (string) [required]: the path to the file to open\n",
    "    - line_number (integer) [optional]: the line number to move the window to (if not provided, the window will start at the top of the file)\n",
    "\n",
    "create:\n",
    "  docstring: creates and opens a new file with the given name\n",
    "  signature: create <filename>\n",
    "  arguments:\n",
    "    - filename (string) [required]: the name of the file to create\n",
    "\n",
    "goto:\n",
    "  docstring: moves the window to show <line_number>\n",
    "  signature: goto <line_number>\n",
    "  arguments:\n",
    "    - line_number (integer) [required]: the line number to move the window to\n",
    "\n",
    "scroll_down:\n",
    "  docstring: moves the window down {WINDOW} lines\n",
    "  signature: scroll_down\n",
    "\n",
    "scroll_up:\n",
    "  docstring: moves the window down {WINDOW} lines\n",
    "  signature: scroll_up\n",
    "\n",
    "submit:\n",
    "  docstring: submits your final answer\n",
    "  signature: submit <answer>\n",
    "  arguments:\n",
    "    - answer (string) [required]: the answer to be submitted\n",
    "\n",
    "search_dir:\n",
    "  docstring: searches for search_term in all files in dir_path. If dir_path is not provided, searches in the current directory\n",
    "  signature: search_dir <search_term> [<dir_path>]\n",
    "  arguments:\n",
    "    - search_term (string) [required]: the term to search for\n",
    "    - dir_path (string) [optional]: the directory to search in (if not provided, searches in the current directory)\n",
    "\n",
    "search_file:\n",
    "  docstring: searches for search_term in file. If file is not provided, searches in the current open file\n",
    "  signature: search_file <search_term> [<file>]\n",
    "  arguments:\n",
    "    - search_term (string) [required]: the term to search for\n",
    "    - file (string) [optional]: the file to search in (if not provided, searches in the current open file)\n",
    "\n",
    "find_file:\n",
    "  docstring: finds all files with the given name in dir_path. If dir_path is not provided, searches in the current directory\n",
    "  signature: find_file <file_name> [<dir_path>]\n",
    "  arguments:\n",
    "    - file_name (string) [required]: the name of the file to search for\n",
    "    - dir_path (string) [optional]: the directory to search in (if not provided, searches in the current directory)\n",
    "\n",
    "edit:\n",
    "  docstring: replaces lines <start_line> through <end_line> (inclusive) with the given text in the open file. The replacement text is terminated by a line with only end_of_edit on it. All of the <replacement text> will be entered, so make sure your indentation is formatted properly. Python files will be checked for syntax errors after the edit. If the system detects a syntax error, the edit will not be executed. Simply try to edit the file again, but make sure to read the error message and modify the edit command you issue accordingly. Issuing the same command a second time will just lead to the same error message again.\n",
    "  signature: edit <start_line>:<end_line>\n",
    "<replacement_text>\n",
    "end_of_edit\n",
    "  arguments:\n",
    "    - start_line (integer) [required]: the line number to start the edit at\n",
    "    - end_line (integer) [required]: the line number to end the edit at (inclusive)\n",
    "    - replacement_text (string) [required]: the text to replace the current selection with\n",
    "\n",
    "\n",
    "\n",
    "Please note that THE EDIT COMMAND REQUIRES PROPER INDENTATION. \n",
    "If you'd like to add the line '        print(x)' you must fully write that out, with all those spaces before the code! Indentation is important and code that is not indented correctly will fail and require fixing before it can be run.\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "Your shell prompt is formatted as follows:\n",
    "(Open file: <path>)\n",
    "(Current directory: <cwd>)\n",
    "In [ ]\n",
    "\n",
    "You need to format your output using two fields: discussion and command.\n",
    "Your output should always include _one_ discussion and _one_ command field EXACTLY as in the following example:\n",
    "DISCUSSION\n",
    "First I'll start by using ls to see what files are in the current directory. Then maybe we can look at some relevant files to see what they look like.\n",
    "```\n",
    "!ls -a\n",
    "```\n",
    "\n",
    "You should only include a *SINGLE* command in the command section and then wait for a response from the shell before continuing with more discussion and commands. Everything you include in the DISCUSSION section will be saved for future reference.\n",
    "If you'd like to issue two commands at once, PLEASE DO NOT DO THAT! Please instead first submit just the first command, and then after receiving a response you'll be able to issue the second command. \n",
    "You're free to use any other bash commands you want (e.g. find, grep, cat, ls, cd) in addition to the special commands listed above.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dfb1b3",
   "metadata": {},
   "source": [
    "### Query\n",
    "train and evaluate discourse segmentation on gold segmentation data for the \"deu.rst.pcc\" dataset, starting from the \"google-bert/bert-base-german-dbmdz-uncased\" model (from hf). Report the total number of tokens, the number of gold and predicted edus, and the f-score, as a json structured as follows: {\"total_tokens\": 0, \"gold_edus\": 0, \"predicted_edus\": 0, \"f_score\": 0} (replace 0 with the actual values).\n",
    "\n",
    "Additional instructions:\n",
    "1. Use only the first 10 rows of each set of the dataset\n",
    "2. Train only one epoch.\n",
    "\n",
    "Git repository: https://github.com/gucorpling/DisCoDisCo\n",
    "\n",
    "I have already executed some of the required steps. Now, you should make any necessary configuration changes to achieve the user request. Your goal is to successfully make the required changes and successfully run\n",
    "```\n",
    "%cd /content/DisCoDisCo\n",
    "!bash seg_scripts/single_corpus_train_and_test_ft.sh deu.rst.pcc\n",
    "```\n",
    "then submit the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5702d2",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ef9589",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c33429",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c93d127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cloning into 'DisCoDisCo'...\r\nremote: Enumerating objects: 1994, done.\u001b[K\r\nremote: Counting objects:   2% (1/42)\u001b[K\rremote: Counting objects:   4% (2/42)\u001b[K\rremote: Counting objects:   7% (3/42)\u001b[K\rremote: Counting objects:   9% (4/42)\u001b[K\rremote: Counting objects:  11% (5/42)\u001b[K\rremote: Counting objects:  14% (6/42)\u001b[K\rremote: Counting objects:  16% (7/42)\u001b[K\rremote: Counting objects:  19% (8/42)\u001b[K\rremote: Counting objects:  21% (9/42)\u001b[K\rremote: Counting objects:  23% (10/42)\u001b[K\rremote: Counting objects:  26% (11/42)\u001b[K\rremote: Counting objects:  28% (12/42)\u001b[K\rremote: Counting objects:  30% (13/42)\u001b[K\rremote: Counting objects:  33% (14/42)\u001b[K\rremote: Counting objects:  35% (15/42)\u001b[K\rremote: Counting objects:  38% (16/42)\u001b[K\rremote: Counting objects:  40% (17/42)\u001b[K\rremote: Counting objects:  42% (18/42)\u001b[K\rremote: Counting objects:  45% (19/42)\u001b[K\rremote: Counting objects:  47% (20/42)\u001b[K\rremote: Counting objects:  50% (21/42)\u001b[K\rremote: Counting objects:  52% (22/42)\u001b[K\rremote: Counting objects:  54% (23/42)\u001b[K\rremote: Counting objects:  57% (24/42)\u001b[K\rremote: Counting objects:  59% (25/42)\u001b[K\rremote: Counting objects:  61% (26/42)\u001b[K\rremote: Counting objects:  64% (27/42)\u001b[K\rremote: Counting objects:  66% (28/42)\u001b[K\rremote: Counting objects:  69% (29/42)\u001b[K\rremote: Counting objects:  71% (30/42)\u001b[K\rremote: Counting objects:  73% (31/42)\u001b[K\rremote: Counting objects:  76% (32/42)\u001b[K\rremote: Counting objects:  78% (33/42)\u001b[K\rremote: Counting objects:  80% (34/42)\u001b[K\rremote: Counting objects:  83% (35/42)\u001b[K\rremote: Counting objects:  85% (36/42)\u001b[K\rremote: Counting objects:  88% (37/42)\u001b[K\rremote: Counting objects:  90% (38/42)\u001b[K\rremote: Counting objects:  92% (39/42)\u001b[K\rremote: Counting objects:  95% (40/42)\u001b[K\rremote: Counting objects:  97% (41/42)\u001b[K\rremote: Counting objects: 100% (42/42)\u001b[K\rremote: Counting objects: 100% (42/42), done.\u001b[K\r\nremote: Compressing objects:   3% (1/28)\u001b[K\rremote: Compressing objects:   7% (2/28)\u001b[K\rremote: Compressing objects:  10% (3/28)\u001b[K\rremote: Compressing objects:  14% (4/28)\u001b[K\rremote: Compressing objects:  17% (5/28)\u001b[K\rremote: Compressing objects:  21% (6/28)\u001b[K\rremote: Compressing objects:  25% (7/28)\u001b[K\rremote: Compressing objects:  28% (8/28)\u001b[K\rremote: Compressing objects:  32% (9/28)\u001b[K\rremote: Compressing objects:  35% (10/28)\u001b[K\rremote: Compressing objects:  39% (11/28)\u001b[K\rremote: Compressing objects:  42% (12/28)\u001b[K\rremote: Compressing objects:  46% (13/28)\u001b[K\rremote: Compressing objects:  50% (14/28)\u001b[K\rremote: Compressing objects:  53% (15/28)\u001b[K\rremote: Compressing objects:  57% (16/28)\u001b[K\rremote: Compressing objects:  60% (17/28)\u001b[K\rremote: Compressing objects:  64% (18/28)\u001b[K\rremote: Compressing objects:  67% (19/28)\u001b[K\rremote: Compressing objects:  71% (20/28)\u001b[K\rremote: Compressing objects:  75% (21/28)\u001b[K\rremote: Compressing objects:  78% (22/28)\u001b[K\rremote: Compressing objects:  82% (23/28)\u001b[K\rremote: Compressing objects:  85% (24/28)\u001b[K\rremote: Compressing objects:  89% (25/28)\u001b[K\rremote: Compressing objects:  92% (26/28)\u001b[K\rremote: Compressing objects:  96% (27/28)\u001b[K\rremote: Compressing objects: 100% (28/28)\u001b[K\rremote: Compressing objects: 100% (28/28), done.\u001b[K\r\nReceiving objects:   0% (1/1994)\rReceiving objects:   1% (20/1994)\rReceiving objects:   2% (40/1994)\rReceiving objects:   3% (60/1994)\rReceiving objects:   4% (80/1994)\rReceiving objects:   5% (100/1994)\rReceiving objects:   6% (120/1994)\rReceiving objects:   7% (140/1994)\rReceiving objects:   8% (160/1994)\rReceiving objects:   9% (180/1994)\rReceiving objects:  10% (200/1994)\rReceiving objects:  11% (220/1994)\rReceiving objects:  12% (240/1994)\rReceiving objects:  13% (260/1994)\rReceiving objects:  14% (280/1994)\rReceiving objects:  15% (300/1994)\rReceiving objects:  16% (320/1994)\rReceiving objects:  17% (339/1994)\rReceiving objects:  18% (359/1994)\rReceiving objects:  19% (379/1994)\rReceiving objects:  20% (399/1994)\rReceiving objects:  21% (419/1994)\rReceiving objects:  22% (439/1994)\rReceiving objects:  23% (459/1994)\rReceiving objects:  24% (479/1994)\rReceiving objects:  25% (499/1994)\rReceiving objects:  26% (519/1994)\rReceiving objects:  27% (539/1994)\rReceiving objects:  28% (559/1994)\rReceiving objects:  29% (579/1994)\rReceiving objects:  30% (599/1994)\rReceiving objects:  31% (619/1994)\rReceiving objects:  32% (639/1994)\rReceiving objects:  33% (659/1994)\rReceiving objects:  34% (678/1994)\rReceiving objects:  35% (698/1994)\rReceiving objects:  36% (718/1994)\rReceiving objects:  37% (738/1994)\rReceiving objects:  38% (758/1994)\rReceiving objects:  39% (778/1994)\rReceiving objects:  40% (798/1994)\rReceiving objects:  41% (818/1994)\rReceiving objects:  42% (838/1994)\rReceiving objects:  43% (858/1994)\rReceiving objects:  44% (878/1994)\rReceiving objects:  45% (898/1994)\rReceiving objects:  46% (918/1994)\rReceiving objects:  47% (938/1994)\rReceiving objects:  48% (958/1994)\rReceiving objects:  49% (978/1994)\rReceiving objects:  50% (997/1994)\rReceiving objects:  51% (1017/1994)\rReceiving objects:  52% (1037/1994)\rReceiving objects:  53% (1057/1994)\rReceiving objects:  54% (1077/1994)\rReceiving objects:  55% (1097/1994)\rReceiving objects:  56% (1117/1994)\rReceiving objects:  57% (1137/1994)\rReceiving objects:  58% (1157/1994)\rReceiving objects:  59% (1177/1994)\rReceiving objects:  60% (1197/1994)\rReceiving objects:  61% (1217/1994)\rReceiving objects:  62% (1237/1994)\rReceiving objects:  63% (1257/1994)\rReceiving objects:  64% (1277/1994)\rReceiving objects:  65% (1297/1994)\rReceiving objects:  66% (1317/1994)\rReceiving objects:  67% (1336/1994)\rReceiving objects:  68% (1356/1994)\rReceiving objects:  69% (1376/1994)\rReceiving objects:  70% (1396/1994)\rReceiving objects:  71% (1416/1994)\rReceiving objects:  72% (1436/1994)\rReceiving objects:  73% (1456/1994)\rReceiving objects:  74% (1476/1994)\rReceiving objects:  75% (1496/1994)\rReceiving objects:  76% (1516/1994)\rReceiving objects:  77% (1536/1994)\rReceiving objects:  78% (1556/1994)\rReceiving objects:  79% (1576/1994)\rReceiving objects:  80% (1596/1994)\rReceiving objects:  81% (1616/1994)\rReceiving objects:  82% (1636/1994)\rReceiving objects:  83% (1656/1994)\rReceiving objects:  84% (1675/1994)\rReceiving objects:  85% (1695/1994)\rremote: Total 1994 (delta 19), reused 22 (delta 12), pack-reused 1952 (from 1)\u001b[K\r\nReceiving objects:  86% (1715/1994)\rReceiving objects:  87% (1735/1994)\rReceiving objects:  88% (1755/1994)\rReceiving objects:  89% (1775/1994)\rReceiving objects:  90% (1795/1994)\rReceiving objects:  91% (1815/1994)\rReceiving objects:  92% (1835/1994)\rReceiving objects:  93% (1855/1994)\rReceiving objects:  94% (1875/1994)\rReceiving objects:  95% (1895/1994)\rReceiving objects:  96% (1915/1994)\rReceiving objects:  97% (1935/1994)\rReceiving objects:  98% (1955/1994)\rReceiving objects:  99% (1975/1994)\rReceiving objects: 100% (1994/1994)\rReceiving objects: 100% (1994/1994), 6.72 MiB | 21.72 MiB/s, done.\r\nResolving deltas:   0% (0/1255)\rResolving deltas:   1% (13/1255)\rResolving deltas:   2% (26/1255)\rResolving deltas:   3% (38/1255)\rResolving deltas:   4% (51/1255)\rResolving deltas:   5% (63/1255)\rResolving deltas:   6% (76/1255)\rResolving deltas:   7% (88/1255)\rResolving deltas:   8% (101/1255)\rResolving deltas:   9% (113/1255)\rResolving deltas:  10% (126/1255)\rResolving deltas:  11% (139/1255)\rResolving deltas:  12% (151/1255)\rResolving deltas:  13% (164/1255)\rResolving deltas:  14% (176/1255)\rResolving deltas:  15% (189/1255)\rResolving deltas:  16% (201/1255)\rResolving deltas:  17% (214/1255)\rResolving deltas:  18% (226/1255)\rResolving deltas:  19% (239/1255)\rResolving deltas:  20% (251/1255)\rResolving deltas:  21% (264/1255)\rResolving deltas:  22% (277/1255)\rResolving deltas:  23% (289/1255)\rResolving deltas:  24% (302/1255)\rResolving deltas:  25% (314/1255)\rResolving deltas:  26% (327/1255)\rResolving deltas:  27% (339/1255)\rResolving deltas:  28% (352/1255)\rResolving deltas:  29% (364/1255)\rResolving deltas:  30% (377/1255)\rResolving deltas:  31% (390/1255)\rResolving deltas:  32% (402/1255)\rResolving deltas:  33% (415/1255)\rResolving deltas:  34% (427/1255)\rResolving deltas:  35% (440/1255)\rResolving deltas:  36% (452/1255)\rResolving deltas:  37% (465/1255)\rResolving deltas:  38% (477/1255)\rResolving deltas:  39% (490/1255)\rResolving deltas:  40% (502/1255)\rResolving deltas:  41% (515/1255)\rResolving deltas:  42% (528/1255)\rResolving deltas:  43% (540/1255)\rResolving deltas:  44% (553/1255)\rResolving deltas:  45% (565/1255)\rResolving deltas:  46% (578/1255)\rResolving deltas:  47% (590/1255)\rResolving deltas:  48% (603/1255)\rResolving deltas:  49% (615/1255)\rResolving deltas:  50% (628/1255)\rResolving deltas:  51% (641/1255)\rResolving deltas:  52% (653/1255)\rResolving deltas:  53% (666/1255)\rResolving deltas:  54% (678/1255)\rResolving deltas:  55% (691/1255)\rResolving deltas:  56% (703/1255)\rResolving deltas:  57% (716/1255)\rResolving deltas:  58% (728/1255)\rResolving deltas:  59% (741/1255)\rResolving deltas:  60% (753/1255)\rResolving deltas:  61% (766/1255)\rResolving deltas:  62% (779/1255)\rResolving deltas:  63% (791/1255)\rResolving deltas:  64% (804/1255)\rResolving deltas:  65% (816/1255)\rResolving deltas:  66% (829/1255)\rResolving deltas:  67% (841/1255)\rResolving deltas:  68% (854/1255)\rResolving deltas:  69% (866/1255)\rResolving deltas:  70% (879/1255)\rResolving deltas:  71% (892/1255)\rResolving deltas:  72% (904/1255)\rResolving deltas:  73% (917/1255)\rResolving deltas:  74% (929/1255)\rResolving deltas:  75% (942/1255)\rResolving deltas:  76% (954/1255)\rResolving deltas:  77% (967/1255)\rResolving deltas:  78% (979/1255)\rResolving deltas:  79% (992/1255)\rResolving deltas:  80% (1004/1255)\rResolving deltas:  81% (1017/1255)\rResolving deltas:  82% (1030/1255)\rResolving deltas:  83% (1042/1255)\rResolving deltas:  84% (1055/1255)\rResolving deltas:  85% (1067/1255)\rResolving deltas:  86% (1080/1255)\rResolving deltas:  87% (1092/1255)\rResolving deltas:  88% (1105/1255)\rResolving deltas:  89% (1117/1255)\rResolving deltas:  90% (1130/1255)\rResolving deltas:  91% (1143/1255)\rResolving deltas:  92% (1155/1255)\rResolving deltas:  93% (1168/1255)\rResolving deltas:  94% (1180/1255)\rResolving deltas:  95% (1193/1255)\rResolving deltas:  96% (1205/1255)\rResolving deltas:  97% (1218/1255)\rResolving deltas:  98% (1230/1255)\rResolving deltas:  99% (1243/1255)\rResolving deltas: 100% (1255/1255)\rResolving deltas: 100% (1255/1255), done.\r\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!git clone https://github.com/gucorpling/DisCoDisCo.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd6f4f2",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af845175",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5712571a",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d4ced2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996663fb",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff40df6",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ad3387",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ec7593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Clone data repository"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c01187a",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c240487",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f620d60",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31977786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content\nCloning into 'sharedtask2021'...\r\n/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\nremote: Enumerating objects: 487, done.\u001b[K\r\nremote: Counting objects:  16% (1/6)\u001b[K\rremote: Counting objects:  33% (2/6)\u001b[K\rremote: Counting objects:  50% (3/6)\u001b[K\rremote: Counting objects:  66% (4/6)\u001b[K\rremote: Counting objects:  83% (5/6)\u001b[K\rremote: Counting objects: 100% (6/6)\u001b[K\rremote: Counting objects: 100% (6/6), done.\u001b[K\r\nremote: Compressing objects:  16% (1/6)\u001b[K\rremote: Compressing objects:  33% (2/6)\u001b[K\rremote: Compressing objects:  50% (3/6)\u001b[K\rremote: Compressing objects:  66% (4/6)\u001b[K\rremote: Compressing objects:  83% (5/6)\u001b[K\rremote: Compressing objects: 100% (6/6)\u001b[K\rremote: Compressing objects: 100% (6/6), done.\u001b[K\r\nReceiving objects:   0% (1/487)\rReceiving objects:   1% (5/487)\rReceiving objects:   2% (10/487)\rReceiving objects:   3% (15/487)\rReceiving objects:   4% (20/487)\rReceiving objects:   5% (25/487)\rReceiving objects:   6% (30/487)\rReceiving objects:   7% (35/487)\rReceiving objects:   8% (39/487)\rReceiving objects:   9% (44/487)\rReceiving objects:  10% (49/487)\rReceiving objects:  11% (54/487)\rReceiving objects:  12% (59/487)\rReceiving objects:  13% (64/487)\rReceiving objects:  14% (69/487)\rReceiving objects:  15% (74/487)\rReceiving objects:  16% (78/487)\rReceiving objects:  17% (83/487)\rReceiving objects:  18% (88/487)\rReceiving objects:  19% (93/487)\rReceiving objects:  20% (98/487)\rReceiving objects:  21% (103/487)\rReceiving objects:  22% (108/487)\rReceiving objects:  23% (113/487)\rReceiving objects:  24% (117/487)\rReceiving objects:  25% (122/487)\rReceiving objects:  26% (127/487)\rReceiving objects:  27% (132/487)\rReceiving objects:  28% (137/487)\rReceiving objects:  29% (142/487)\rReceiving objects:  30% (147/487), 9.48 MiB | 18.94 MiB/s\rReceiving objects:  31% (151/487), 9.48 MiB | 18.94 MiB/s\rReceiving objects:  32% (156/487), 9.48 MiB | 18.94 MiB/s\rReceiving objects:  33% (161/487), 9.48 MiB | 18.94 MiB/s\rReceiving objects:  34% (166/487), 9.48 MiB | 18.94 MiB/s\rReceiving objects:  35% (171/487), 9.48 MiB | 18.94 MiB/s\rReceiving objects:  36% (176/487), 9.48 MiB | 18.94 MiB/s\rReceiving objects:  37% (181/487), 9.48 MiB | 18.94 MiB/s\rReceiving objects:  37% (183/487), 19.77 MiB | 19.77 MiB/s\rReceiving objects:  38% (186/487), 19.77 MiB | 19.77 MiB/s\rReceiving objects:  39% (190/487), 19.77 MiB | 19.77 MiB/s\rReceiving objects:  40% (195/487), 19.77 MiB | 19.77 MiB/s\rReceiving objects:  41% (200/487), 19.77 MiB | 19.77 MiB/s\rReceiving objects:  42% (205/487), 19.77 MiB | 19.77 MiB/s\rReceiving objects:  43% (210/487), 28.21 MiB | 18.80 MiB/s\rReceiving objects:  44% (215/487), 28.21 MiB | 18.80 MiB/s\rReceiving objects:  45% (220/487), 28.21 MiB | 18.80 MiB/s\rReceiving objects:  46% (225/487), 28.21 MiB | 18.80 MiB/s\rReceiving objects:  47% (229/487), 28.21 MiB | 18.80 MiB/s\rReceiving objects:  48% (234/487), 28.21 MiB | 18.80 MiB/s\rReceiving objects:  49% (239/487), 28.21 MiB | 18.80 MiB/s\rReceiving objects:  50% (244/487), 28.21 MiB | 18.80 MiB/s\rReceiving objects:  51% (249/487), 28.21 MiB | 18.80 MiB/s\rReceiving objects:  52% (254/487), 28.21 MiB | 18.80 MiB/s\rReceiving objects:  53% (259/487), 28.21 MiB | 18.80 MiB/s\rReceiving objects:  54% (263/487), 28.21 MiB | 18.80 MiB/s\rReceiving objects:  55% (268/487), 28.21 MiB | 18.80 MiB/s\rReceiving objects:  56% (273/487), 28.21 MiB | 18.80 MiB/s\rReceiving objects:  57% (278/487), 28.21 MiB | 18.80 MiB/s\rReceiving objects:  58% (283/487), 28.21 MiB | 18.80 MiB/s\rReceiving objects:  58% (284/487), 38.35 MiB | 19.17 MiB/s\rReceiving objects:  59% (288/487), 46.27 MiB | 18.50 MiB/s\rReceiving objects:  60% (293/487), 46.27 MiB | 18.50 MiB/s\rReceiving objects:  61% (298/487), 46.27 MiB | 18.50 MiB/s\rReceiving objects:  62% (302/487), 46.27 MiB | 18.50 MiB/s\rReceiving objects:  63% (307/487), 46.27 MiB | 18.50 MiB/s\rReceiving objects:  64% (312/487), 46.27 MiB | 18.50 MiB/s\rReceiving objects:  65% (317/487), 46.27 MiB | 18.50 MiB/s\rReceiving objects:  66% (322/487), 46.27 MiB | 18.50 MiB/s\rReceiving objects:  67% (327/487), 46.27 MiB | 18.50 MiB/s\rReceiving objects:  68% (332/487), 46.27 MiB | 18.50 MiB/s\rReceiving objects:  69% (337/487), 46.27 MiB | 18.50 MiB/s\rReceiving objects:  70% (341/487), 46.27 MiB | 18.50 MiB/s\rReceiving objects:  71% (346/487), 46.27 MiB | 18.50 MiB/s\rReceiving objects:  72% (351/487), 46.27 MiB | 18.50 MiB/s\rReceiving objects:  73% (356/487), 46.27 MiB | 18.50 MiB/s\rReceiving objects:  74% (361/487), 46.27 MiB | 18.50 MiB/s\rReceiving objects:  75% (366/487), 46.27 MiB | 18.50 MiB/s\rReceiving objects:  76% (371/487), 55.99 MiB | 18.66 MiB/s\rReceiving objects:  77% (375/487), 55.99 MiB | 18.66 MiB/s\rReceiving objects:  78% (380/487), 55.99 MiB | 18.66 MiB/s\rReceiving objects:  79% (385/487), 55.99 MiB | 18.66 MiB/s\rReceiving objects:  80% (390/487), 55.99 MiB | 18.66 MiB/s\rReceiving objects:  81% (395/487), 55.99 MiB | 18.66 MiB/s\rReceiving objects:  82% (400/487), 55.99 MiB | 18.66 MiB/s\rReceiving objects:  83% (405/487), 55.99 MiB | 18.66 MiB/s\rReceiving objects:  84% (410/487), 55.99 MiB | 18.66 MiB/s\rReceiving objects:  85% (414/487), 55.99 MiB | 18.66 MiB/s\rReceiving objects:  86% (419/487), 55.99 MiB | 18.66 MiB/s\rReceiving objects:  87% (424/487), 55.99 MiB | 18.66 MiB/s\rReceiving objects:  88% (429/487), 55.99 MiB | 18.66 MiB/s\rReceiving objects:  89% (434/487), 55.99 MiB | 18.66 MiB/s\rReceiving objects:  90% (439/487), 55.99 MiB | 18.66 MiB/s\rremote: Total 487 (delta 0), reused 0 (delta 0), pack-reused 481 (from 1)\u001b[K\r\nReceiving objects:  91% (444/487), 55.99 MiB | 18.66 MiB/s\rReceiving objects:  92% (449/487), 55.99 MiB | 18.66 MiB/s\rReceiving objects:  93% (453/487), 55.99 MiB | 18.66 MiB/s\rReceiving objects:  94% (458/487), 55.99 MiB | 18.66 MiB/s\rReceiving objects:  95% (463/487), 55.99 MiB | 18.66 MiB/s\rReceiving objects:  96% (468/487), 55.99 MiB | 18.66 MiB/s\rReceiving objects:  97% (473/487), 55.99 MiB | 18.66 MiB/s\rReceiving objects:  98% (478/487), 55.99 MiB | 18.66 MiB/s\rReceiving objects:  99% (483/487), 55.99 MiB | 18.66 MiB/s\rReceiving objects: 100% (487/487), 55.99 MiB | 18.66 MiB/s\rReceiving objects: 100% (487/487), 57.52 MiB | 18.76 MiB/s, done.\r\nResolving deltas:   0% (0/240)\rResolving deltas:   1% (3/240)\rResolving deltas:   2% (5/240)\rResolving deltas:   3% (8/240)\rResolving deltas:   4% (10/240)\rResolving deltas:   5% (12/240)\rResolving deltas:   6% (15/240)\rResolving deltas:   7% (17/240)\rResolving deltas:   8% (20/240)\rResolving deltas:   9% (22/240)\rResolving deltas:  10% (24/240)\rResolving deltas:  11% (27/240)\rResolving deltas:  12% (29/240)\rResolving deltas:  13% (32/240)\rResolving deltas:  14% (34/240)\rResolving deltas:  15% (36/240)\rResolving deltas:  16% (39/240)\rResolving deltas:  17% (41/240)\rResolving deltas:  18% (44/240)\rResolving deltas:  19% (46/240)\rResolving deltas:  20% (48/240)\rResolving deltas:  21% (51/240)\rResolving deltas:  22% (53/240)\rResolving deltas:  23% (56/240)\rResolving deltas:  24% (58/240)\rResolving deltas:  25% (60/240)\rResolving deltas:  26% (63/240)\rResolving deltas:  27% (65/240)\rResolving deltas:  28% (68/240)\rResolving deltas:  29% (70/240)\rResolving deltas:  30% (72/240)\rResolving deltas:  31% (75/240)\rResolving deltas:  32% (77/240)\rResolving deltas:  33% (80/240)\rResolving deltas:  34% (82/240)\rResolving deltas:  35% (84/240)\rResolving deltas:  36% (87/240)\rResolving deltas:  37% (89/240)\rResolving deltas:  38% (92/240)\rResolving deltas:  39% (94/240)\rResolving deltas:  40% (96/240)\rResolving deltas:  41% (99/240)\rResolving deltas:  42% (101/240)\rResolving deltas:  43% (104/240)\rResolving deltas:  44% (106/240)\rResolving deltas:  45% (108/240)\rResolving deltas:  46% (111/240)\rResolving deltas:  47% (113/240)\rResolving deltas:  48% (116/240)\rResolving deltas:  49% (118/240)\rResolving deltas:  50% (120/240)\rResolving deltas:  51% (123/240)\rResolving deltas:  52% (125/240)\rResolving deltas:  53% (128/240)\rResolving deltas:  54% (130/240)\rResolving deltas:  55% (132/240)\rResolving deltas:  56% (135/240)\rResolving deltas:  57% (137/240)\rResolving deltas:  58% (140/240)\rResolving deltas:  59% (142/240)\rResolving deltas:  60% (144/240)\rResolving deltas:  61% (147/240)\rResolving deltas:  62% (149/240)\rResolving deltas:  63% (152/240)\rResolving deltas:  64% (154/240)\rResolving deltas:  65% (156/240)\rResolving deltas:  66% (159/240)\rResolving deltas:  67% (161/240)\rResolving deltas:  68% (164/240)\rResolving deltas:  69% (166/240)\rResolving deltas:  70% (168/240)\rResolving deltas:  71% (171/240)\rResolving deltas:  72% (173/240)\rResolving deltas:  73% (176/240)\rResolving deltas:  74% (178/240)\rResolving deltas:  75% (180/240)\rResolving deltas:  76% (183/240)\rResolving deltas:  77% (185/240)\rResolving deltas:  78% (188/240)\rResolving deltas:  79% (190/240)\rResolving deltas:  80% (192/240)\rResolving deltas:  81% (195/240)\rResolving deltas:  82% (197/240)\rResolving deltas:  83% (200/240)\rResolving deltas:  84% (202/240)\rResolving deltas:  85% (204/240)\rResolving deltas:  86% (207/240)\rResolving deltas:  87% (209/240)\rResolving deltas:  88% (212/240)\rResolving deltas:  89% (214/240)\rResolving deltas:  90% (216/240)\rResolving deltas:  91% (219/240)\rResolving deltas:  92% (221/240)\rResolving deltas:  93% (224/240)\rResolving deltas:  94% (226/240)\rResolving deltas:  95% (228/240)\rResolving deltas:  96% (231/240)\rResolving deltas:  97% (233/240)\rResolving deltas:  98% (236/240)\rResolving deltas:  99% (238/240)\rResolving deltas: 100% (240/240)\rResolving deltas: 100% (240/240), done.\r\nUpdating files:  31% (53/167)\rUpdating files:  32% (54/167)\rUpdating files:  33% (56/167)\rUpdating files:  34% (57/167)\rUpdating files:  35% (59/167)\rUpdating files:  36% (61/167)\rUpdating files:  37% (62/167)\rUpdating files:  38% (64/167)\rUpdating files:  39% (66/167)\rUpdating files:  40% (67/167)\rUpdating files:  41% (69/167)\rUpdating files:  42% (71/167)\rUpdating files:  43% (72/167)\rUpdating files:  44% (74/167)\rUpdating files:  45% (76/167)\rUpdating files:  46% (77/167)\rUpdating files:  47% (79/167)\rUpdating files:  48% (81/167)\rUpdating files:  49% (82/167)\rUpdating files:  50% (84/167)\rUpdating files:  51% (86/167)\rUpdating files:  52% (87/167)\rUpdating files:  53% (89/167)\rUpdating files:  54% (91/167)\rUpdating files:  55% (92/167)\rUpdating files:  56% (94/167)\rUpdating files:  57% (96/167)\rUpdating files:  58% (97/167)\rUpdating files:  59% (99/167)\rUpdating files:  60% (101/167)\rUpdating files:  61% (102/167)\rUpdating files:  62% (104/167)\rUpdating files:  63% (106/167)\rUpdating files:  64% (107/167)\rUpdating files:  65% (109/167)\rUpdating files:  66% (111/167)\rUpdating files:  67% (112/167)\rUpdating files:  68% (114/167)\rUpdating files:  69% (116/167)\rUpdating files:  70% (117/167)\rUpdating files:  71% (119/167)\rUpdating files:  72% (121/167)\rUpdating files:  73% (122/167)\rUpdating files:  74% (124/167)\rUpdating files:  75% (126/167)\rUpdating files:  76% (127/167)\rUpdating files:  77% (129/167)\rUpdating files:  78% (131/167)\rUpdating files:  79% (132/167)\rUpdating files:  80% (134/167)\rUpdating files:  81% (136/167)\rUpdating files:  82% (137/167)\rUpdating files:  83% (139/167)\rUpdating files:  84% (141/167)\rUpdating files:  85% (142/167)\rUpdating files:  85% (143/167)\rUpdating files:  86% (144/167)\rUpdating files:  87% (146/167)\rUpdating files:  88% (147/167)\rUpdating files:  89% (149/167)\rUpdating files:  90% (151/167)\rUpdating files:  91% (152/167)\rUpdating files:  92% (154/167)\rUpdating files:  93% (156/167)\rUpdating files:  94% (157/167)\rUpdating files:  95% (159/167)\rUpdating files:  96% (161/167)\rUpdating files:  97% (162/167)\rUpdating files:  98% (164/167)\rUpdating files:  99% (166/167)\rUpdating files: 100% (167/167)\rUpdating files: 100% (167/167), done.\r\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/\n",
    "!git clone https://github.com/disrpt/sharedtask2021.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a14c1ef",
   "metadata": {},
   "source": [
    "### Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799f81cf",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a440a7a",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cedb22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Move `deu.rst.pcc` datasets to the corresponding folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c555a5",
   "metadata": {},
   "source": [
    "### Step 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8930e02a",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437a9983",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89e12fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!mkdir -p /content/DisCoDisCo/data/2021/\n",
    "!cp -r /content/sharedtask2021/data/deu.rst.pcc /content/DisCoDisCo/data/2021/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7238941",
   "metadata": {},
   "source": [
    "### Step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8574d828",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06200f10",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f217b61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Keep only 10 lines per dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e46b2d1",
   "metadata": {},
   "source": [
    "### Step 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4082ab8",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9018ce3",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5ec4e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/DisCoDisCo/data/2021/deu.rst.pcc\n/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/DisCoDisCo/data/2021/deu.rst.pcc\n",
    "\n",
    "n_samples_per_dataset = 10\n",
    "for dataset in [\"train\", \"dev\", \"test\"]:\n",
    "    filename = f\"deu.rst.pcc_{dataset}.conllu\"\n",
    "    lines = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        n_samples = 0\n",
    "        line = f.readline()\n",
    "        while n_samples < n_samples_per_dataset:\n",
    "            line = f.readline()\n",
    "            if line == \"\\n\":\n",
    "                n_samples += 1\n",
    "            lines.append(line)\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.writelines(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d544d37c",
   "metadata": {},
   "source": [
    "### Step 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480c8c5c",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b440ef01",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9616262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Run model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9911687a",
   "metadata": {},
   "source": [
    "### Step 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4512941",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909b9474",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29ee2f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### First attempt to run the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f066557c",
   "metadata": {},
   "source": [
    "### Step 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b84a4cc",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2179469",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6f6a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/DisCoDisCo\n\r\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\r\n# Training on deu.rst.pcc\r\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\r\n\r\ndata/2021/deu.rst.pcc/deu.rst.pcc_train.conllu\r\nseg_scripts/single_corpus_train_and_test_ft.sh: line 101: allennlp: command not found\r\n\r\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\r\n# Testing on deu.rst.pcc\r\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\r\n\r\nseg_scripts/single_corpus_train_and_test_ft.sh: line 114: allennlp: command not found\r\nRemoving model files...\r\nrm: cannot remove 'models/deu.rst.pcc_seg_bert_baseline_ft/*.th': No such file or directory\r\n\r\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\r\n# Scoring on deu.rst.pcc\r\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\r\n\r\n/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\nTraceback (most recent call last):\r\n  File \"/content/DisCoDisCo/seg_scripts/format_output.py\", line 14, in <module>\r\n    with open(sys.argv[1], encoding=\"utf8\") as f:\r\nFileNotFoundError: [Errno 2] No such file or directory: 'models/deu.rst.pcc_seg_bert_baseline_ft/output_test.jsonl'\r\ntee: models/deu.rst.pcc_seg_bert_baseline_ft/score.txt: No such file or directory\r\nTraceback (most recent call last):\r\n  File \"/content/DisCoDisCo/seg_scripts/seg_eval_2019_modified.py\", line 237, in <module>\r\n    score_dict = get_scores(opts.goldfile,opts.predfile,opts.string_input)\r\n  File \"/content/DisCoDisCo/seg_scripts/seg_eval_2019_modified.py\", line 138, in get_scores\r\n    pred_tokens, pred_labels, pred_spans = parse_data(pred_file, string_input)\r\n  File \"/content/DisCoDisCo/seg_scripts/seg_eval_2019_modified.py\", line 77, in parse_data\r\n    data = io.open(infile, encoding=\"utf8\").read().strip().replace(\"\\r\", \"\")\r\nFileNotFoundError: [Errno 2] No such file or directory: 'models/deu.rst.pcc_seg_bert_baseline_ft/output_test.conllu'\r\nseg_scripts/single_corpus_train_and_test_ft.sh: line 131: models/deu.rst.pcc_seg_bert_baseline_ft/calc_score.sh: No such file or directory\r\ncat: models/deu.rst.pcc_seg_bert_baseline_ft/score.txt: No such file or directory\r\ncat: models/deu.rst.pcc_seg_bert_baseline_ft/score.txt: No such file or directory\r\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/DisCoDisCo\n",
    "!bash seg_scripts/single_corpus_train_and_test_ft.sh deu.rst.pcc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0b72a2",
   "metadata": {},
   "source": [
    "### Step 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e9808e",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7f1c41",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42babd22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/DisCoDisCo\n/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/DisCoDisCo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a897df27",
   "metadata": {},
   "source": [
    "### Step 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d945956",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebace977",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1afb755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Installing pip dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9cbd39",
   "metadata": {},
   "source": [
    "### Step 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2621b4c",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b27387",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e64a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# requirements.txt requires `flair==0.7`, but this version cannot be installed in colab. On the other hand, the version installed using the default command `pip install flair` doesn't work with the repository, as it raises the following error:\n",
    "# \n",
    "# ```\n",
    "# File \"/content/DisCoDisCo/gucorpling_models/seg/flair_edu_segmenter.py\", line 7, in <module>\n",
    "#     from flair.embeddings import StackedEmbeddings, FlairEmbeddings, CharacterEmbeddings, BertEmbeddings, XLNetEmbeddings\n",
    "# ImportError: cannot import name 'BertEmbeddings' from 'flair.embeddings'\n",
    "# ```\n",
    "# \n",
    "# The closest version working is `flair==0.10`.\n",
    "# \n",
    "# Also, `overrides<5` is required to avoid the following error:\n",
    "# \n",
    "# ```\n",
    "# File \"/usr/local/lib/python3.10/dist-packages/overrides/signature.py\", line 221, in ensure_all_positional_args_defined_in_sub\n",
    "#     raise TypeError(f\"{method_name}: `{super_param.name}` must be present\")\n",
    "# TypeError: CombinedSequenceModel.forward: `inputs` must be present\n",
    "# ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1899bc",
   "metadata": {},
   "source": [
    "### Step 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13461b52",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9e913d",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020b17e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting allennlp\r\n  Downloading allennlp-2.10.1-py3-none-any.whl.metadata (21 kB)\r\nCollecting allennlp_models\r\n  Downloading allennlp_models-2.10.1-py3-none-any.whl.metadata (23 kB)\r\nCollecting torch<1.13.0,>=1.10.0 (from allennlp)\r\n  Downloading torch-1.12.1-cp310-cp310-manylinux1_x86_64.whl.metadata (22 kB)\r\nCollecting torchvision<0.14.0,>=0.8.1 (from allennlp)\r\n  Downloading torchvision-0.13.1-cp310-cp310-manylinux1_x86_64.whl.metadata (10 kB)\r\nCollecting cached-path<1.2.0,>=1.1.3 (from allennlp)\r\n  Downloading cached_path-1.1.6-py3-none-any.whl.metadata (6.0 kB)\r\nCollecting fairscale==0.4.6 (from allennlp)\r\n  Downloading fairscale-0.4.6.tar.gz (248 kB)\r\n\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/248.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m225.3/248.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.2/248.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25h  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\r\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\r\n\u001b[?25h  Installing backend dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n\u001b[?25hRequirement already satisfied: nltk>=3.6.5 in /usr/local/lib/python3.10/site-packages (from allennlp) (3.8.1)\r\nCollecting spacy<3.4,>=2.1.0 (from allennlp)\r\n  Downloading spacy-3.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)\r\nRequirement already satisfied: numpy>=1.21.4 in /usr/local/lib/python3.10/site-packages (from allennlp) (1.25.2)\r\nCollecting tensorboardX>=1.2 (from allennlp)\r\n  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\r\nRequirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/site-packages (from allennlp) (2.31.0)\r\nRequirement already satisfied: tqdm>=4.62 in /usr/local/lib/python3.10/site-packages (from allennlp) (4.66.4)\r\nCollecting h5py>=3.6.0 (from allennlp)\r\n  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\r\nRequirement already satisfied: scikit-learn>=1.0.1 in /usr/local/lib/python3.10/site-packages (from allennlp) (1.5.0)\r\nRequirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.10/site-packages (from allennlp) (1.11.4)\r\nCollecting pytest>=6.2.5 (from allennlp)\r\n  Downloading pytest-8.3.2-py3-none-any.whl.metadata (7.5 kB)\r\nCollecting transformers<4.21,>=4.1 (from allennlp)\r\n  Downloading transformers-4.20.1-py3-none-any.whl.metadata (77 kB)\r\n\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.3/77.3 kB\u001b[0m \u001b[31m163.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hRequirement already satisfied: sentencepiece>=0.1.96 in /usr/local/lib/python3.10/site-packages (from allennlp) (0.1.99)\r\nCollecting filelock<3.8,>=3.3 (from allennlp)\r\n  Downloading filelock-3.7.1-py3-none-any.whl.metadata (2.5 kB)\r\nCollecting lmdb>=1.2.1 (from allennlp)\r\n  Downloading lmdb-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\r\nCollecting more-itertools>=8.12.0 (from allennlp)\r\n  Downloading more_itertools-10.4.0-py3-none-any.whl.metadata (36 kB)\r\nCollecting termcolor==1.1.0 (from allennlp)\r\n  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\r\n  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\r\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\r\n\u001b[?25hCollecting wandb<0.13.0,>=0.10.0 (from allennlp)\r\n  Downloading wandb-0.12.21-py2.py3-none-any.whl.metadata (7.2 kB)\r\nRequirement already satisfied: huggingface-hub>=0.0.16 in /usr/local/lib/python3.10/site-packages (from allennlp) (0.23.3)\r\nCollecting dill>=0.3.4 (from allennlp)\r\n  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\r\nCollecting base58>=2.1.1 (from allennlp)\r\n  Downloading base58-2.1.1-py3-none-any.whl.metadata (3.1 kB)\r\nCollecting sacremoses (from allennlp)\r\n  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\r\nRequirement already satisfied: typer>=0.4.1 in /usr/local/lib/python3.10/site-packages (from allennlp) (0.6.1)\r\nCollecting protobuf<4.0.0,>=3.12.0 (from allennlp)\r\n  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\r\nRequirement already satisfied: traitlets>5.1.1 in /usr/local/lib/python3.10/site-packages (from allennlp) (5.14.3)\r\nCollecting jsonnet>=0.10.0 (from allennlp)\r\n  Downloading jsonnet-0.20.0.tar.gz (594 kB)\r\n\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/594.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.2/594.2 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25h  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\r\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n\u001b[?25hCollecting conllu==4.4.2 (from allennlp_models)\r\n  Downloading conllu-4.4.2-py2.py3-none-any.whl.metadata (19 kB)\r\nCollecting word2number>=1.1 (from allennlp_models)\r\n  Downloading word2number-1.1.zip (9.7 kB)\r\n  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\r\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\r\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n\u001b[?25hCollecting py-rouge==1.1 (from allennlp_models)\r\n  Downloading py_rouge-1.1-py3-none-any.whl.metadata (8.7 kB)\r\nCollecting ftfy (from allennlp_models)\r\n  Downloading ftfy-6.2.3-py3-none-any.whl.metadata (7.8 kB)\r\nCollecting datasets (from allennlp_models)\r\n  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\r\nRequirement already satisfied: rich<13.0,>=12.1 in /usr/local/lib/python3.10/site-packages (from cached-path<1.2.0,>=1.1.3->allennlp) (12.3.0)\r\nCollecting boto3<2.0,>=1.0 (from cached-path<1.2.0,>=1.1.3->allennlp)\r\n  Downloading boto3-1.35.3-py3-none-any.whl.metadata (6.6 kB)\r\nCollecting google-cloud-storage<3.0,>=1.32.0 (from cached-path<1.2.0,>=1.1.3->allennlp)\r\n  Downloading google_cloud_storage-2.18.2-py2.py3-none-any.whl.metadata (9.1 kB)\r\nCollecting huggingface-hub>=0.0.16 (from allennlp)\r\n  Downloading huggingface_hub-0.10.1-py3-none-any.whl.metadata (6.1 kB)\r\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.0.16->allennlp) (6.0.1)\r\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.0.16->allennlp) (4.12.2)\r\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.0.16->allennlp) (24.1)\r\nRequirement already satisfied: click in /usr/local/lib/python3.10/site-packages (from nltk>=3.6.5->allennlp) (8.1.7)\r\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/site-packages (from nltk>=3.6.5->allennlp) (1.4.2)\r\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/site-packages (from nltk>=3.6.5->allennlp) (2023.12.25)\r\nCollecting iniconfig (from pytest>=6.2.5->allennlp)\r\n  Downloading iniconfig-2.0.0-py3-none-any.whl.metadata (2.6 kB)\r\nCollecting pluggy<2,>=1.5 (from pytest>=6.2.5->allennlp)\r\n  Downloading pluggy-1.5.0-py3-none-any.whl.metadata (4.8 kB)\r\nRequirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/site-packages (from pytest>=6.2.5->allennlp) (1.2.1)\r\nRequirement already satisfied: tomli>=1 in /usr/local/lib/python3.10/site-packages (from pytest>=6.2.5->allennlp) (2.0.1)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests>=2.28->allennlp) (2.1.1)\r\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.28->allennlp) (3.7)\r\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.28->allennlp) (2.0.7)\r\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.28->allennlp) (2024.6.2)\r\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn>=1.0.1->allennlp) (3.5.0)\r\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.12)\r\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.5)\r\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.10)\r\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.8)\r\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.9)\r\nCollecting thinc<8.1.0,>=8.0.14 (from spacy<3.4,>=2.1.0->allennlp)\r\n  Downloading thinc-8.0.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\r\nRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (0.7.11)\r\nCollecting wasabi<1.1.0,>=0.9.1 (from spacy<3.4,>=2.1.0->allennlp)\r\n  Downloading wasabi-0.10.1-py3-none-any.whl.metadata (28 kB)\r\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (2.4.8)\r\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.10)\r\nCollecting typer>=0.4.1 (from allennlp)\r\n  Downloading typer-0.4.2-py3-none-any.whl.metadata (12 kB)\r\nCollecting pathy>=0.3.5 (from spacy<3.4,>=2.1.0->allennlp)\r\n  Downloading pathy-0.11.0-py3-none-any.whl.metadata (16 kB)\r\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (6.4.0)\r\nCollecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 (from spacy<3.4,>=2.1.0->allennlp)\r\n  Downloading pydantic-1.8.2-py3-none-any.whl.metadata (103 kB)\r\n\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/103.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m127.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (3.1.4)\r\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (68.1.2)\r\nCollecting typing-extensions>=3.7.4.3 (from huggingface-hub>=0.0.16->allennlp)\r\n  Downloading typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\r\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (3.4.0)\r\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/site-packages (from torchvision<0.14.0,>=0.8.1->allennlp) (10.3.0)\r\nCollecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers<4.21,>=4.1->allennlp)\r\n  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.5 kB)\r\nCollecting GitPython>=1.0.0 (from wandb<0.13.0,>=0.10.0->allennlp)\r\n  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\r\nCollecting promise<3,>=2.0 (from wandb<0.13.0,>=0.10.0->allennlp)\r\n  Downloading promise-2.3.tar.gz (19 kB)\r\n  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\r\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n\u001b[?25hCollecting shortuuid>=0.5.0 (from wandb<0.13.0,>=0.10.0->allennlp)\r\n  Downloading shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)\r\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/site-packages (from wandb<0.13.0,>=0.10.0->allennlp) (5.9.8)\r\nCollecting sentry-sdk>=1.0.0 (from wandb<0.13.0,>=0.10.0->allennlp)\r\n  Downloading sentry_sdk-2.13.0-py2.py3-none-any.whl.metadata (9.7 kB)\r\nRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.10/site-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.16.0)\r\nCollecting docker-pycreds>=0.4.0 (from wandb<0.13.0,>=0.10.0->allennlp)\r\n  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\r\nCollecting pathtools (from wandb<0.13.0,>=0.10.0->allennlp)\r\n  Downloading pathtools-0.1.2.tar.gz (11 kB)\r\n  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\r\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n\u001b[?25hCollecting setproctitle (from wandb<0.13.0,>=0.10.0->allennlp)\r\n  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\r\nCollecting pyarrow>=15.0.0 (from datasets->allennlp_models)\r\n  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\r\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from datasets->allennlp_models) (2.0.3)\r\nCollecting requests>=2.28 (from allennlp)\r\n  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\r\nCollecting xxhash (from datasets->allennlp_models)\r\n  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\r\nCollecting multiprocess (from datasets->allennlp_models)\r\n  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\r\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets->allennlp_models) (2023.6.0)\r\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/site-packages (from datasets->allennlp_models) (3.8.3)\r\nINFO: pip is looking at multiple versions of datasets to determine which version is compatible with other requirements. This could take a while.\r\nCollecting datasets (from allennlp_models)\r\n  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\r\nCollecting pyarrow-hotfix (from datasets->allennlp_models)\r\n  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\r\nCollecting datasets (from allennlp_models)\r\n  Downloading datasets-2.19.2-py3-none-any.whl.metadata (19 kB)\r\n  Downloading datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\r\n  Downloading datasets-2.19.0-py3-none-any.whl.metadata (19 kB)\r\n  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\r\n  Downloading datasets-2.17.1-py3-none-any.whl.metadata (20 kB)\r\n  Downloading datasets-2.17.0-py3-none-any.whl.metadata (20 kB)\r\nINFO: pip is still looking at multiple versions of datasets to determine which version is compatible with other requirements. This could take a while.\r\n  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\r\nCollecting dill>=0.3.4 (from allennlp)\r\n  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\r\nCollecting datasets (from allennlp_models)\r\n  Downloading datasets-2.16.0-py3-none-any.whl.metadata (20 kB)\r\n  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\r\n  Downloading datasets-2.14.7-py3-none-any.whl.metadata (19 kB)\r\n  Downloading datasets-2.14.6-py3-none-any.whl.metadata (19 kB)\r\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\r\n  Downloading datasets-2.14.5-py3-none-any.whl.metadata (19 kB)\r\n  Downloading datasets-2.14.4-py3-none-any.whl.metadata (19 kB)\r\n  Downloading datasets-2.14.3-py3-none-any.whl.metadata (19 kB)\r\n  Downloading datasets-2.14.2-py3-none-any.whl.metadata (19 kB)\r\n  Downloading datasets-2.14.1-py3-none-any.whl.metadata (19 kB)\r\n  Downloading datasets-2.14.0-py3-none-any.whl.metadata (19 kB)\r\n  Downloading datasets-2.13.2-py3-none-any.whl.metadata (20 kB)\r\nCollecting dill>=0.3.4 (from allennlp)\r\n  Downloading dill-0.3.6-py3-none-any.whl.metadata (9.8 kB)\r\nCollecting datasets (from allennlp_models)\r\n  Downloading datasets-2.13.1-py3-none-any.whl.metadata (20 kB)\r\n  Downloading datasets-2.13.0-py3-none-any.whl.metadata (20 kB)\r\n  Downloading datasets-2.12.0-py3-none-any.whl.metadata (20 kB)\r\n  Downloading datasets-2.11.0-py3-none-any.whl.metadata (20 kB)\r\n  Downloading datasets-2.10.1-py3-none-any.whl.metadata (20 kB)\r\nCollecting responses<0.19 (from datasets->allennlp_models)\r\n  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\r\nRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/site-packages (from ftfy->allennlp_models) (0.2.13)\r\nCollecting botocore<1.36.0,>=1.35.3 (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp)\r\n  Downloading botocore-1.35.3-py3-none-any.whl.metadata (5.7 kB)\r\nCollecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp)\r\n  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\r\nCollecting s3transfer<0.11.0,>=0.10.0 (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp)\r\n  Downloading s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\r\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets->allennlp_models) (23.2.0)\r\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets->allennlp_models) (6.0.5)\r\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets->allennlp_models) (4.0.3)\r\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets->allennlp_models) (1.9.4)\r\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets->allennlp_models) (1.4.1)\r\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets->allennlp_models) (1.3.1)\r\nCollecting gitdb<5,>=4.0.1 (from GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp)\r\n  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\r\nRequirement already satisfied: google-auth<3.0dev,>=2.26.1 in /usr/local/lib/python3.10/site-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.30.0)\r\nCollecting google-api-core<3.0.0dev,>=2.15.0 (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp)\r\n  Downloading google_api_core-2.19.1-py3-none-any.whl.metadata (2.7 kB)\r\nCollecting google-cloud-core<3.0dev,>=2.3.0 (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp)\r\n  Downloading google_cloud_core-2.4.1-py2.py3-none-any.whl.metadata (2.7 kB)\r\nCollecting google-resumable-media>=2.7.2 (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp)\r\n  Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl.metadata (2.2 kB)\r\nCollecting google-crc32c<2.0dev,>=1.0 (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp)\r\n  Downloading google_crc32c-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\r\nRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.4,>=2.1.0->allennlp) (1.2.0)\r\nCollecting pathlib-abc==0.1.1 (from pathy>=0.3.5->spacy<3.4,>=2.1.0->allennlp)\r\n  Downloading pathlib_abc-0.1.1-py3-none-any.whl.metadata (18 kB)\r\nRequirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/site-packages (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp) (0.9.1)\r\nRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.10/site-packages (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp) (2.18.0)\r\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->spacy<3.4,>=2.1.0->allennlp) (2.1.5)\r\nINFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\r\nCollecting multiprocess (from datasets->allennlp_models)\r\n  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\r\n  Downloading multiprocess-0.70.14-py310-none-any.whl.metadata (6.6 kB)\r\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas->datasets->allennlp_models) (2.9.0.post0)\r\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets->allennlp_models) (2024.1)\r\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets->allennlp_models) (2024.1)\r\nCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp)\r\n  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\r\nCollecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp)\r\n  Downloading googleapis_common_protos-1.63.2-py2.py3-none-any.whl.metadata (1.5 kB)\r\nCollecting proto-plus<2.0.0dev,>=1.22.3 (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp)\r\n  Downloading proto_plus-1.24.0-py3-none-any.whl.metadata (2.2 kB)\r\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (5.3.3)\r\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.4.0)\r\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (4.9)\r\nRequirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.4,>=2.1.0->allennlp) (1.2.0)\r\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.26.1->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.6.0)\r\nDownloading allennlp-2.10.1-py3-none-any.whl (730 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/730.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m730.2/730.2 kB\u001b[0m \u001b[31m154.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading allennlp_models-2.10.1-py3-none-any.whl (464 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/464.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m464.5/464.5 kB\u001b[0m \u001b[31m180.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading conllu-4.4.2-py2.py3-none-any.whl (15 kB)\r\nDownloading py_rouge-1.1-py3-none-any.whl (56 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/56.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m145.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading base58-2.1.1-py3-none-any.whl (5.6 kB)\r\nDownloading cached_path-1.1.6-py3-none-any.whl (26 kB)\r\nDownloading filelock-3.7.1-py3-none-any.whl (10 kB)\r\nDownloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/5.3 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/163.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading lmdb-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (294 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/294.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m121.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading more_itertools-10.4.0-py3-none-any.whl (60 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/60.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m159.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m151.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading pytest-8.3.2-py3-none-any.whl (341 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/341.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m155.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading spacy-3.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.3 MB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/6.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/6.3 MB\u001b[0m \u001b[31m119.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m6.1/6.3 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/101.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m185.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading torch-1.12.1-cp310-cp310-manylinux1_x86_64.whl (776.3 MB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/776.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/776.3 MB\u001b[0m \u001b[31m116.5 MB/s\u001b[0m eta \u001b[36m0:00:07\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/776.3 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:10\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/776.3 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:10\u001b[0m\r\u001b[2K   \u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/776.3 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:09\u001b[0m\r\u001b[2K   \u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/776.3 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:09\u001b[0m\r\u001b[2K   \u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/776.3 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:08\u001b[0m\r\u001b[2K   \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.2/776.3 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:09\u001b[0m\r\u001b[2K   \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/776.3 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:09\u001b[0m\r\u001b[2K   \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/776.3 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:09\u001b[0m\r\u001b[2K   \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.1/776.3 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:09\u001b[0m\r\u001b[2K   \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.6/776.3 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:09\u001b[0m\r\u001b[2K   \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.1/776.3 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:09\u001b[0m\r\u001b[2K   \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.8/776.3 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:10\u001b[0m\r\u001b[2K   \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/776.3 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:10\u001b[0m\r\u001b[2K   \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/776.3 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:10\u001b[0m\r\u001b[2K   \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.6/776.3 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:10\u001b[0m\r\u001b[2K   \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/776.3 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:10\u001b[0m\r\u001b[2K   \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/776.3 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:10\u001b[0m\r\u001b[2K   \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/776.3 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:10\u001b[0m\r\u001b[2K   \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/776.3 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:09\u001b[0m\r\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.5/776.3 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:09\u001b[0m\r\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.2/776.3 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:09\u001b[0m\r\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/776.3 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:09\u001b[0m\r\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.5/776.3 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:10\u001b[0m\r\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.3/776.3 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:10\u001b[0m\r\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/776.3 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:10\u001b[0m\r\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/776.3 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:09\u001b[0m\r\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.0/776.3 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:09\u001b[0m\r\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.5/776.3 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:10\u001b[0m\r\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.1/776.3 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:10\u001b[0m\r\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/776.3 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:10\u001b[0m\r\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.3/776.3 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:09\u001b[0m\r\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.0/776.3 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:09\u001b[0m\r\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.4/776.3 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:08\u001b[0m\r\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.1/776.3 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:08\u001b[0m\r\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/776.3 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:08\u001b[0m\r\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.1/776.3 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:08\u001b[0m\r\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.0/776.3 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:08\u001b[0m\r\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.2/776.3 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:08\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.9/776.3 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:08\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.5/776.3 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:07\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.5/776.3 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m eta \u001b[36m0:00:07\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.1/776.3 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:07\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.2/776.3 MB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:06\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.1/776.3 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:06\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.0/776.3 MB\u001b[0m \u001b[31m110.3 MB/s\u001b[0m eta \u001b[36m0:00:06\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/776.3 MB\u001b[0m \u001b[31m110.9 MB/s\u001b[0m eta \u001b[36m0:00:06\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.3/776.3 MB\u001b[0m \u001b[31m113.1 MB/s\u001b[0m eta \u001b[36m0:00:06\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.2/776.3 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:06\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.1/776.3 MB\u001b[0m \u001b[31m109.7 MB/s\u001b[0m eta \u001b[36m0:00:06\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.1/776.3 MB\u001b[0m \u001b[31m110.2 MB/s\u001b[0m eta \u001b[36m0:00:06\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.1/776.3 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:07\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.1/776.3 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:07\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.3/776.3 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:08\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.6/776.3 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:07\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.7/776.3 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:07\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.1/776.3 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:07\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.6/776.3 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:07\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.7/776.3 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:06\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.7/776.3 MB\u001b[0m \u001b[31m103.7 MB/s\u001b[0m eta \u001b[36m0:00:06\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/776.3 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:06\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.3/776.3 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:07\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/776.3 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:07\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.6/776.3 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:07\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.9/776.3 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:07\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.5/776.3 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:07\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.2/776.3 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:06\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m217.8/776.3 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:06\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.5/776.3 MB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:06\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.2/776.3 MB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:06\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.6/776.3 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:06\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.3/776.3 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:06\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m234.8/776.3 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:07\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.7/776.3 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:07\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.4/776.3 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:07\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.5/776.3 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:08\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.0/776.3 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:07\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.6/776.3 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:08\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.2/776.3 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:08\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.6/776.3 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:07\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.3/776.3 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:06\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.0/776.3 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:06\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.1/776.3 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:06\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.9/776.3 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:06\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.7/776.3 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:07\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.4/776.3 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:07\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.1/776.3 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:07\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.8/776.3 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:07\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m282.3/776.3 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:06\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.7/776.3 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:06\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.3/776.3 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.1/776.3 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.7/776.3 MB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.2/776.3 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.2/776.3 MB\u001b[0m \u001b[31m103.3 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.8/776.3 MB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/776.3 MB\u001b[0m \u001b[31m107.7 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.6/776.3 MB\u001b[0m \u001b[31m108.7 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.2/776.3 MB\u001b[0m \u001b[31m104.7 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.0/776.3 MB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.8/776.3 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.9/776.3 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m334.2/776.3 MB\u001b[0m \u001b[31m100.7 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m338.1/776.3 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/776.3 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m344.9/776.3 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.9/776.3 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.9/776.3 MB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.9/776.3 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.4/776.3 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:06\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m360.5/776.3 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:06\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.6/776.3 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:06\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.1/776.3 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m371.1/776.3 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.6/776.3 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.7/776.3 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.8/776.3 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.6/776.3 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/776.3 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.7/776.3 MB\u001b[0m \u001b[31m112.0 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.7/776.3 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.2/776.3 MB\u001b[0m \u001b[31m98.2 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m403.8/776.3 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.7/776.3 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/776.3 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.5/776.3 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.4/776.3 MB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.4/776.3 MB\u001b[0m \u001b[31m109.6 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.2/776.3 MB\u001b[0m \u001b[31m107.7 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.2/776.3 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.8/776.3 MB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.5/776.3 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.2/776.3 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m446.0/776.3 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m449.9/776.3 MB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.8/776.3 MB\u001b[0m \u001b[31m105.9 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.7/776.3 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m461.6/776.3 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.4/776.3 MB\u001b[0m \u001b[31m111.8 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.6/776.3 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m473.7/776.3 MB\u001b[0m \u001b[31m112.4 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m477.8/776.3 MB\u001b[0m \u001b[31m118.6 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.8/776.3 MB\u001b[0m \u001b[31m113.7 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.9/776.3 MB\u001b[0m \u001b[31m111.0 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m490.0/776.3 MB\u001b[0m \u001b[31m111.1 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.9/776.3 MB\u001b[0m \u001b[31m109.1 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m497.8/776.3 MB\u001b[0m \u001b[31m111.0 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m501.9/776.3 MB\u001b[0m \u001b[31m110.3 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m506.0/776.3 MB\u001b[0m \u001b[31m111.9 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m509.8/776.3 MB\u001b[0m \u001b[31m108.6 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m513.8/776.3 MB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m517.8/776.3 MB\u001b[0m \u001b[31m109.1 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m521.6/776.3 MB\u001b[0m \u001b[31m108.9 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/776.3 MB\u001b[0m \u001b[31m107.7 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m529.2/776.3 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m533.1/776.3 MB\u001b[0m \u001b[31m107.1 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m537.1/776.3 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m541.1/776.3 MB\u001b[0m \u001b[31m109.1 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m545.2/776.3 MB\u001b[0m \u001b[31m112.5 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m549.1/776.3 MB\u001b[0m \u001b[31m110.8 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m553.2/776.3 MB\u001b[0m \u001b[31m112.0 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m557.2/776.3 MB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m561.2/776.3 MB\u001b[0m \u001b[31m110.7 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m565.2/776.3 MB\u001b[0m \u001b[31m110.9 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m569.3/776.3 MB\u001b[0m \u001b[31m111.7 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m573.3/776.3 MB\u001b[0m \u001b[31m111.6 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m577.3/776.3 MB\u001b[0m \u001b[31m110.2 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m581.2/776.3 MB\u001b[0m \u001b[31m113.3 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m585.2/776.3 MB\u001b[0m \u001b[31m110.7 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m589.3/776.3 MB\u001b[0m \u001b[31m109.4 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m593.3/776.3 MB\u001b[0m \u001b[31m112.3 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m596.9/776.3 MB\u001b[0m \u001b[31m110.2 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m600.8/776.3 MB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m604.8/776.3 MB\u001b[0m \u001b[31m108.9 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m608.7/776.3 MB\u001b[0m \u001b[31m108.8 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m612.7/776.3 MB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m616.5/776.3 MB\u001b[0m \u001b[31m109.7 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m620.4/776.3 MB\u001b[0m \u001b[31m107.1 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m624.3/776.3 MB\u001b[0m \u001b[31m108.7 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m628.4/776.3 MB\u001b[0m \u001b[31m108.8 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m632.5/776.3 MB\u001b[0m \u001b[31m113.2 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m636.5/776.3 MB\u001b[0m \u001b[31m112.3 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m640.7/776.3 MB\u001b[0m \u001b[31m114.7 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m644.8/776.3 MB\u001b[0m \u001b[31m116.4 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m648.9/776.3 MB\u001b[0m \u001b[31m115.8 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m653.0/776.3 MB\u001b[0m \u001b[31m115.8 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m657.1/776.3 MB\u001b[0m \u001b[31m112.6 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m661.1/776.3 MB\u001b[0m \u001b[31m112.9 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m665.3/776.3 MB\u001b[0m \u001b[31m112.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m669.4/776.3 MB\u001b[0m \u001b[31m116.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m673.4/776.3 MB\u001b[0m \u001b[31m112.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m677.5/776.3 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m681.5/776.3 MB\u001b[0m \u001b[31m111.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m685.5/776.3 MB\u001b[0m \u001b[31m109.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m689.4/776.3 MB\u001b[0m \u001b[31m110.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m693.6/776.3 MB\u001b[0m \u001b[31m112.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m697.8/776.3 MB\u001b[0m \u001b[31m114.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m701.7/776.3 MB\u001b[0m \u001b[31m114.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m705.7/776.3 MB\u001b[0m \u001b[31m109.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m709.7/776.3 MB\u001b[0m \u001b[31m109.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m713.9/776.3 MB\u001b[0m \u001b[31m112.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m718.0/776.3 MB\u001b[0m \u001b[31m114.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m722.0/776.3 MB\u001b[0m \u001b[31m116.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m726.2/776.3 MB\u001b[0m \u001b[31m114.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m730.4/776.3 MB\u001b[0m \u001b[31m119.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m734.6/776.3 MB\u001b[0m \u001b[31m117.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m738.7/776.3 MB\u001b[0m \u001b[31m119.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m742.9/776.3 MB\u001b[0m \u001b[31m119.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m747.0/776.3 MB\u001b[0m \u001b[31m118.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m751.2/776.3 MB\u001b[0m \u001b[31m118.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m755.3/776.3 MB\u001b[0m \u001b[31m119.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m759.5/776.3 MB\u001b[0m \u001b[31m118.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m763.6/776.3 MB\u001b[0m \u001b[31m116.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m767.8/776.3 MB\u001b[0m \u001b[31m117.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m771.7/776.3 MB\u001b[0m \u001b[31m113.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m775.9/776.3 MB\u001b[0m \u001b[31m114.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m113.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading torchvision-0.13.1-cp310-cp310-manylinux1_x86_64.whl (19.1 MB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/19.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/19.1 MB\u001b[0m \u001b[31m128.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/19.1 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/19.1 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/19.1 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m16.0/19.1 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m18.9/19.1 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/4.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/4.4 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading typer-0.4.2-py3-none-any.whl (27 kB)\r\nDownloading wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m191.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading datasets-2.10.1-py3-none-any.whl (469 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/469.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m150.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading dill-0.3.6-py3-none-any.whl (110 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/110.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m196.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading ftfy-6.2.3-py3-none-any.whl (43 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.0/43.0 kB\u001b[0m \u001b[31m166.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/897.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m173.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading boto3-1.35.3-py3-none-any.whl (139 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/139.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 kB\u001b[0m \u001b[31m203.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\r\nDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/207.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m132.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading google_cloud_storage-2.18.2-py2.py3-none-any.whl (130 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/130.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m152.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading pathy-0.11.0-py3-none-any.whl (47 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/47.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m160.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading pathlib_abc-0.1.1-py3-none-any.whl (23 kB)\r\nDownloading pluggy-1.5.0-py3-none-any.whl (20 kB)\r\nDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/39.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/39.9 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/39.9 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/39.9 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/39.9 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.2/39.9 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/39.9 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.6/39.9 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.4/39.9 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.7/39.9 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m28.7/39.9 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m31.6/39.9 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m34.4/39.9 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m37.0/39.9 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading pydantic-1.8.2-py3-none-any.whl (126 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/126.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m132.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading responses-0.18.0-py3-none-any.whl (38 kB)\r\nDownloading sentry_sdk-2.13.0-py2.py3-none-any.whl (309 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/309.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.1/309.1 kB\u001b[0m \u001b[31m123.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\r\nDownloading thinc-8.0.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (659 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/659.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m659.5/659.5 kB\u001b[0m \u001b[31m161.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/6.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/6.6 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m4.7/6.6 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\r\nDownloading wasabi-0.10.1-py3-none-any.whl (26 kB)\r\nDownloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\r\nDownloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/134.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\r\nDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/194.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m108.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading botocore-1.35.3-py3-none-any.whl (12.5 MB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/12.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/12.5 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/12.5 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m8.7/12.5 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m12.0/12.5 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m180.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading google_api_core-2.19.1-py3-none-any.whl (139 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/139.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m189.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading google_cloud_core-2.4.1-py2.py3-none-any.whl (29 kB)\r\nDownloading google_crc32c-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\r\nDownloading google_resumable_media-2.7.2-py2.py3-none-any.whl (81 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/81.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m176.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\r\nDownloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/82.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m154.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading googleapis_common_protos-1.63.2-py2.py3-none-any.whl (220 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/220.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m194.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading proto_plus-1.24.0-py3-none-any.whl (50 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m134.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\r\nBuilding wheels for collected packages: fairscale, termcolor, jsonnet, word2number, promise, pathtools\r\n  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n\u001b[?25h  Created wheel for fairscale: filename=fairscale-0.4.6-py3-none-any.whl size=307221 sha256=e3e9b36c9556961a8c5e52d4fdd4afab8afd7a4e7da5cb465fc94010c87df4a0\r\n  Stored in directory: /tmp/pip-ephem-wheel-cache-_n94su25/wheels/a1/58/3d/e114952ab4a8f31eb9dae230658450afff986b211a5b1f2256\r\n  Building wheel for termcolor (pyproject.toml) ... \u001b[?25ldone\r\n\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=56376603b1ae11038db2fcb9472c5a605cc732dce38edb92b2ff55fe8c60f12c\r\n  Stored in directory: /tmp/pip-ephem-wheel-cache-_n94su25/wheels/a1/49/46/1b13a65d8da11238af9616b00fdde6d45b0f95d9291bac8452\r\n  Building wheel for jsonnet (pyproject.toml) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \bdone\r\n\u001b[?25h  Created wheel for jsonnet: filename=jsonnet-0.20.0-cp310-cp310-linux_x86_64.whl size=6394569 sha256=b3005fd2da72bf5e976000b7f6e7b1156d061839421ee4feedd33eb417ad4b6c\r\n  Stored in directory: /tmp/pip-ephem-wheel-cache-_n94su25/wheels/63/0d/6b/5467dd1db9332ba4bd5cf4153e2870c5f89bb4db473d989cc2\r\n  Building wheel for word2number (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n\u001b[?25h  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5567 sha256=589f6089dcd7f3ca16f1dca295a37726de282d1492e58d1f4d5efb9a99667786\r\n  Stored in directory: /tmp/pip-ephem-wheel-cache-_n94su25/wheels/84/ff/26/d3cfbd971e96c5aa3737ecfced81628830d7359b55fbb8ca3b\r\n  Building wheel for promise (pyproject.toml) ... \u001b[?25l-\b \b\\\b \bdone\r\n\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21481 sha256=bf49a04dcc0a29b71fbac4dfc0a03caef6e770028ed827d482bffd153e659b98\r\n  Stored in directory: /tmp/pip-ephem-wheel-cache-_n94su25/wheels/54/4e/28/3ed0e1c8a752867445bab994d2340724928aa3ab059c57c8db\r\n  Building wheel for pathtools (pyproject.toml) ... \u001b[?25l-\b \b\\\b \bdone\r\n\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=63741e4ebd5eb1cc667cf294b7f024005c202d70ad1f237cfa1dbc1da7595e08\r\n  Stored in directory: /tmp/pip-ephem-wheel-cache-_n94su25/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\r\nSuccessfully built fairscale termcolor jsonnet word2number promise pathtools\r\nInstalling collected packages: word2number, wasabi, tokenizers, termcolor, py-rouge, pathtools, lmdb, jsonnet, xxhash, typing-extensions, typer, smmap, shortuuid, setproctitle, sentry-sdk, sacremoses, pyarrow, protobuf, promise, pluggy, pathlib-abc, more-itertools, jmespath, iniconfig, h5py, google-crc32c, ftfy, filelock, docker-pycreds, dill, conllu, base58, torch, tensorboardX, responses, pytest, pydantic, proto-plus, pathy, multiprocess, huggingface-hub, googleapis-common-protos, google-resumable-media, gitdb, botocore, transformers, torchvision, thinc, s3transfer, google-api-core, GitPython, fairscale, wandb, spacy, google-cloud-core, datasets, boto3, google-cloud-storage, cached-path, allennlp, allennlp_models\r\n  Attempting uninstall: wasabi\r\n    Found existing installation: wasabi 1.1.3\r\n    Uninstalling wasabi-1.1.3:\r\n      Successfully uninstalled wasabi-1.1.3\r\n  Attempting uninstall: tokenizers\r\n    Found existing installation: tokenizers 0.19.1\r\n    Uninstalling tokenizers-0.19.1:\r\n      Successfully uninstalled tokenizers-0.19.1\r\n  Attempting uninstall: termcolor\r\n    Found existing installation: termcolor 2.4.0\r\n    Uninstalling termcolor-2.4.0:\r\n      Successfully uninstalled termcolor-2.4.0\r\n  Attempting uninstall: typing-extensions\r\n    Found existing installation: typing_extensions 4.12.2\r\n    Uninstalling typing_extensions-4.12.2:\r\n      Successfully uninstalled typing_extensions-4.12.2\r\n  Attempting uninstall: typer\r\n    Found existing installation: typer 0.6.1\r\n    Uninstalling typer-0.6.1:\r\n      Successfully uninstalled typer-0.6.1\r\n  Attempting uninstall: protobuf\r\n    Found existing installation: protobuf 5.27.1\r\n    Uninstalling protobuf-5.27.1:\r\n      Successfully uninstalled protobuf-5.27.1\r\n  Attempting uninstall: filelock\r\n    Found existing installation: filelock 3.15.1\r\n    Uninstalling filelock-3.15.1:\r\n      Successfully uninstalled filelock-3.15.1\r\n  Attempting uninstall: torch\r\n    Found existing installation: torch 2.2.1+cpu\r\n    Uninstalling torch-2.2.1+cpu:\r\n      Successfully uninstalled torch-2.2.1+cpu\r\n  Attempting uninstall: pydantic\r\n    Found existing installation: pydantic 1.10.16\r\n    Uninstalling pydantic-1.10.16:\r\n      Successfully uninstalled pydantic-1.10.16\r\n  Attempting uninstall: huggingface-hub\r\n    Found existing installation: huggingface-hub 0.23.3\r\n    Uninstalling huggingface-hub-0.23.3:\r\n      Successfully uninstalled huggingface-hub-0.23.3\r\n  Attempting uninstall: transformers\r\n    Found existing installation: transformers 4.40.2\r\n    Uninstalling transformers-4.40.2:\r\n      Successfully uninstalled transformers-4.40.2\r\n  Attempting uninstall: thinc\r\n    Found existing installation: thinc 8.2.4\r\n    Uninstalling thinc-8.2.4:\r\n      Successfully uninstalled thinc-8.2.4\r\n  Attempting uninstall: spacy\r\n    Found existing installation: spacy 3.7.4\r\n    Uninstalling spacy-3.7.4:\r\n      Successfully uninstalled spacy-3.7.4\r\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\nmodal 0.62.65 requires synchronicity~=0.6.5, which is not installed.\r\nmodal 0.62.65 requires watchfiles, which is not installed.\r\nmodal 0.62.65 requires aiostream~=0.5.2, but you have aiostream 0.4.4 which is incompatible.\r\nmodal 0.62.65 requires grpclib==0.4.7, but you have grpclib 0.4.3 which is incompatible.\r\nmodal 0.62.65 requires typer~=0.9.0, but you have typer 0.4.2 which is incompatible.\r\nmodal 0.62.65 requires typing_extensions~=4.6, but you have typing-extensions 4.5.0 which is incompatible.\r\nen-core-web-sm 3.7.1 requires spacy<3.8.0,>=3.7.2, but you have spacy 3.3.3 which is incompatible.\r\nipython 8.25.0 requires typing-extensions>=4.6; python_version < \"3.12\", but you have typing-extensions 4.5.0 which is incompatible.\r\ntypeguard 4.3.0 requires typing-extensions>=4.10.0, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\r\n\u001b[0mSuccessfully installed GitPython-3.1.43 allennlp-2.10.1 allennlp_models-2.10.1 base58-2.1.1 boto3-1.35.3 botocore-1.35.3 cached-path-1.1.6 conllu-4.4.2 datasets-2.10.1 dill-0.3.6 docker-pycreds-0.4.0 fairscale-0.4.6 filelock-3.7.1 ftfy-6.2.3 gitdb-4.0.11 google-api-core-2.19.1 google-cloud-core-2.4.1 google-cloud-storage-2.18.2 google-crc32c-1.5.0 google-resumable-media-2.7.2 googleapis-common-protos-1.63.2 h5py-3.11.0 huggingface-hub-0.10.1 iniconfig-2.0.0 jmespath-1.0.1 jsonnet-0.20.0 lmdb-1.5.1 more-itertools-10.4.0 multiprocess-0.70.14 pathlib-abc-0.1.1 pathtools-0.1.2 pathy-0.11.0 pluggy-1.5.0 promise-2.3 proto-plus-1.24.0 protobuf-3.20.3 py-rouge-1.1 pyarrow-17.0.0 pydantic-1.8.2 pytest-8.3.2 responses-0.18.0 s3transfer-0.10.2 sacremoses-0.1.1 sentry-sdk-2.13.0 setproctitle-1.3.3 shortuuid-1.0.13 smmap-5.0.1 spacy-3.3.3 tensorboardX-2.6.2.2 termcolor-1.1.0 thinc-8.0.17 tokenizers-0.12.1 torch-1.12.1 torchvision-0.13.1 transformers-4.20.1 typer-0.4.2 typing-extensions-4.5.0 wandb-0.12.21 wasabi-0.10.1 word2number-1.1 xxhash-3.5.0\r\n\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\nCollecting overrides<5\r\n  Downloading overrides-4.1.2-py3-none-any.whl.metadata (4.0 kB)\r\nCollecting typing-utils>=0.0.3 (from overrides<5)\r\n  Downloading typing_utils-0.1.0-py3-none-any.whl.metadata (5.0 kB)\r\nDownloading overrides-4.1.2-py3-none-any.whl (11 kB)\r\nDownloading typing_utils-0.1.0-py3-none-any.whl (10 kB)\r\nInstalling collected packages: typing-utils, overrides\r\n  Attempting uninstall: overrides\r\n    Found existing installation: overrides 7.7.0\r\n    Uninstalling overrides-7.7.0:\r\n      Successfully uninstalled overrides-7.7.0\r\nSuccessfully installed overrides-4.1.2 typing-utils-0.1.0\r\n\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\nCollecting flair==0.10\r\n  Downloading flair-0.10-py3-none-any.whl.metadata (12 kB)\r\nRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.10/site-packages (from flair==0.10) (2.9.0.post0)\r\nRequirement already satisfied: torch!=1.8,>=1.5.0 in /usr/local/lib/python3.10/site-packages (from flair==0.10) (1.12.1)\r\nRequirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.10/site-packages (from flair==0.10) (4.3.2)\r\nRequirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.10/site-packages (from flair==0.10) (4.66.4)\r\nCollecting segtok>=1.5.7 (from flair==0.10)\r\n  Downloading segtok-1.5.11-py3-none-any.whl.metadata (9.0 kB)\r\nRequirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.10/site-packages (from flair==0.10) (3.7.1)\r\nCollecting mpld3==0.3 (from flair==0.10)\r\n  Downloading mpld3-0.3.tar.gz (788 kB)\r\n\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/788.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/788.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m788.5/788.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25h  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\r\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/site-packages (from flair==0.10) (1.5.0)\r\nCollecting sqlitedict>=1.6.0 (from flair==0.10)\r\n  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\r\n  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\r\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n\u001b[?25hCollecting deprecated>=1.2.4 (from flair==0.10)\r\n  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\r\nRequirement already satisfied: transformers>=4.0.0 in /usr/local/lib/python3.10/site-packages (from flair==0.10) (4.20.1)\r\nCollecting bpemb>=0.3.2 (from flair==0.10)\r\n  Downloading bpemb-0.3.5-py3-none-any.whl.metadata (19 kB)\r\nRequirement already satisfied: regex in /usr/local/lib/python3.10/site-packages (from flair==0.10) (2023.12.25)\r\nCollecting tabulate (from flair==0.10)\r\n  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\r\nCollecting langdetect (from flair==0.10)\r\n  Downloading langdetect-1.0.9.tar.gz (981 kB)\r\n\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25h  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\r\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n\u001b[?25hCollecting lxml (from flair==0.10)\r\n  Downloading lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\r\nRequirement already satisfied: ftfy in /usr/local/lib/python3.10/site-packages (from flair==0.10) (6.2.3)\r\nCollecting sentencepiece==0.1.95 (from flair==0.10)\r\n  Downloading sentencepiece-0.1.95.tar.gz (508 kB)\r\n\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/508.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.7/508.7 kB\u001b[0m \u001b[31m173.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25h  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\r\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n\u001b[?25hCollecting konoha<5.0.0,>=4.0.0 (from flair==0.10)\r\n  Downloading konoha-4.6.5-py3-none-any.whl.metadata (10.0 kB)\r\nCollecting janome (from flair==0.10)\r\n  Downloading Janome-0.5.0-py2.py3-none-any.whl.metadata (2.6 kB)\r\nCollecting gdown==3.12.2 (from flair==0.10)\r\n  Downloading gdown-3.12.2.tar.gz (8.2 kB)\r\n  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\r\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n\u001b[?25hRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/site-packages (from flair==0.10) (0.10.1)\r\nRequirement already satisfied: conllu>=4.0 in /usr/local/lib/python3.10/site-packages (from flair==0.10) (4.4.2)\r\nCollecting more-itertools~=8.8.0 (from flair==0.10)\r\n  Downloading more_itertools-8.8.0-py3-none-any.whl.metadata (40 kB)\r\n\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m159.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hCollecting wikipedia-api (from flair==0.10)\r\n  Downloading Wikipedia_API-0.6.0-py3-none-any.whl.metadata (22 kB)\r\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from gdown==3.12.2->flair==0.10) (3.7.1)\r\nRequirement already satisfied: requests[socks] in /usr/local/lib/python3.10/site-packages (from gdown==3.12.2->flair==0.10) (2.31.0)\r\nRequirement already satisfied: six in /usr/local/lib/python3.10/site-packages (from gdown==3.12.2->flair==0.10) (1.16.0)\r\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from bpemb>=0.3.2->flair==0.10) (1.25.2)\r\nCollecting wrapt<2,>=1.10 (from deprecated>=1.2.4->flair==0.10)\r\n  Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\r\nRequirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/site-packages (from gensim>=3.4.0->flair==0.10) (1.11.4)\r\nRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/site-packages (from gensim>=3.4.0->flair==0.10) (6.4.0)\r\nCollecting importlib-metadata<4.0.0,>=3.7.0 (from konoha<5.0.0,>=4.0.0->flair==0.10)\r\n  Downloading importlib_metadata-3.10.1-py3-none-any.whl.metadata (3.5 kB)\r\nCollecting overrides<4.0.0,>=3.0.0 (from konoha<5.0.0,>=4.0.0->flair==0.10)\r\n  Downloading overrides-3.1.0.tar.gz (11 kB)\r\n  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\r\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\r\n\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/site-packages (from matplotlib>=2.2.3->flair==0.10) (1.2.1)\r\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/site-packages (from matplotlib>=2.2.3->flair==0.10) (0.12.1)\r\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/site-packages (from matplotlib>=2.2.3->flair==0.10) (4.53.0)\r\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/site-packages (from matplotlib>=2.2.3->flair==0.10) (1.4.5)\r\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from matplotlib>=2.2.3->flair==0.10) (24.1)\r\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/site-packages (from matplotlib>=2.2.3->flair==0.10) (10.3.0)\r\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/site-packages (from matplotlib>=2.2.3->flair==0.10) (3.1.2)\r\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn>=0.21.3->flair==0.10) (1.4.2)\r\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn>=0.21.3->flair==0.10) (3.5.0)\r\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/site-packages (from torch!=1.8,>=1.5.0->flair==0.10) (4.5.0)\r\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers>=4.0.0->flair==0.10) (6.0.1)\r\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.10/site-packages (from transformers>=4.0.0->flair==0.10) (0.12.1)\r\nRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/site-packages (from ftfy->flair==0.10) (0.2.13)\r\nRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/site-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair==0.10) (3.19.2)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown==3.12.2->flair==0.10) (2.1.1)\r\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown==3.12.2->flair==0.10) (3.7)\r\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown==3.12.2->flair==0.10) (2.0.7)\r\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown==3.12.2->flair==0.10) (2024.6.2)\r\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown==3.12.2->flair==0.10) (1.7.1)\r\nDownloading flair-0.10-py3-none-any.whl (322 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/322.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.7/322.7 kB\u001b[0m \u001b[31m199.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading bpemb-0.3.5-py3-none-any.whl (19 kB)\r\nDownloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\r\nDownloading konoha-4.6.5-py3-none-any.whl (20 kB)\r\nDownloading more_itertools-8.8.0-py3-none-any.whl (48 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/48.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.8/48.8 kB\u001b[0m \u001b[31m148.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading segtok-1.5.11-py3-none-any.whl (24 kB)\r\nDownloading Janome-0.5.0-py2.py3-none-any.whl (19.7 MB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/19.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/19.7 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/19.7 MB\u001b[0m \u001b[31m108.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/19.7 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m15.8/19.7 MB\u001b[0m \u001b[31m115.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (5.0 MB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/5.0 MB\u001b[0m \u001b[31m125.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading tabulate-0.9.0-py3-none-any.whl (35 kB)\r\nDownloading Wikipedia_API-0.6.0-py3-none-any.whl (14 kB)\r\nDownloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\r\nDownloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/80.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m203.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hBuilding wheels for collected packages: gdown, mpld3, sentencepiece, sqlitedict, langdetect, overrides\r\n  Building wheel for gdown (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n\u001b[?25h  Created wheel for gdown: filename=gdown-3.12.2-py3-none-any.whl size=9670 sha256=7f5148db6cd87ec145210d188bc22cb7922533b2fc54236065f101be13eb2977\r\n  Stored in directory: /tmp/pip-ephem-wheel-cache-r6y6q775/wheels/fd/45/ac/c7557ccd8fe79de5da94d7b39b5ca92f22f86a0f85367e2b0a\r\n  Building wheel for mpld3 (pyproject.toml) ... \u001b[?25l-\b \b\\\b \bdone\r\n\u001b[?25h  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116684 sha256=22898507d2973d21ea1277a3521cc9cef032d752ce261853fa14fc1a9bd1ef88\r\n  Stored in directory: /tmp/pip-ephem-wheel-cache-r6y6q775/wheels/9c/92/f7/45d9aac5dcfb1c2a1761a272365599cc7ba1050ce211a3fd9a\r\n  Building wheel for sentencepiece (pyproject.toml) ... \u001b[?25l-\b \b\\\b \b|\b \berror\r\n  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\r\n  \r\n  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for sentencepiece \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\r\n  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\r\n  \u001b[31m╰─>\u001b[0m \u001b[31m[60 lines of output]\u001b[0m\r\n  \u001b[31m   \u001b[0m /tmp/pip-build-env-iimn2p1o/overlay/lib/python3.10/site-packages/setuptools/_distutils/dist.py:268: UserWarning: Unknown distribution option: 'test_suite'\r\n  \u001b[31m   \u001b[0m   warnings.warn(msg)\r\n  \u001b[31m   \u001b[0m /tmp/pip-build-env-iimn2p1o/overlay/lib/python3.10/site-packages/setuptools/dist.py:450: SetuptoolsDeprecationWarning: Invalid dash-separated options\r\n  \u001b[31m   \u001b[0m !!\r\n  \u001b[31m   \u001b[0m \r\n  \u001b[31m   \u001b[0m         ********************************************************************************\r\n  \u001b[31m   \u001b[0m         Usage of dash-separated 'description-file' will not be supported in future\r\n  \u001b[31m   \u001b[0m         versions. Please use the underscore name 'description_file' instead.\r\n  \u001b[31m   \u001b[0m \r\n  \u001b[31m   \u001b[0m         By 2024-Sep-26, you need to update your project and remove deprecated calls\r\n  \u001b[31m   \u001b[0m         or your builds will no longer be supported.\r\n  \u001b[31m   \u001b[0m \r\n  \u001b[31m   \u001b[0m         See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\r\n  \u001b[31m   \u001b[0m         ********************************************************************************\r\n  \u001b[31m   \u001b[0m \r\n  \u001b[31m   \u001b[0m !!\r\n  \u001b[31m   \u001b[0m   opt = self.warn_dash_deprecation(opt, section)\r\n  \u001b[31m   \u001b[0m running bdist_wheel\r\n  \u001b[31m   \u001b[0m running build\r\n  \u001b[31m   \u001b[0m running build_py\r\n  \u001b[31m   \u001b[0m creating build\r\n  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310\r\n  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/sentencepiece\r\n  \u001b[31m   \u001b[0m copying src/sentencepiece/__init__.py -> build/lib.linux-x86_64-cpython-310/sentencepiece\r\n  \u001b[31m   \u001b[0m copying src/sentencepiece/sentencepiece_model_pb2.py -> build/lib.linux-x86_64-cpython-310/sentencepiece\r\n  \u001b[31m   \u001b[0m copying src/sentencepiece/sentencepiece_pb2.py -> build/lib.linux-x86_64-cpython-310/sentencepiece\r\n  \u001b[31m   \u001b[0m running build_ext\r\n  \u001b[31m   \u001b[0m Package sentencepiece was not found in the pkg-config search path.\r\n  \u001b[31m   \u001b[0m Perhaps you should add the directory containing `sentencepiece.pc'\r\n  \u001b[31m   \u001b[0m to the PKG_CONFIG_PATH environment variable\r\n  \u001b[31m   \u001b[0m No package 'sentencepiece' found\r\n  \u001b[31m   \u001b[0m Cloning into 'sentencepiece'...\r\n  \u001b[31m   \u001b[0m Note: switching to '0e6dfbf86e2fa6d86a3d9a8a08a628da71c073e0'.\r\n  \u001b[31m   \u001b[0m \r\n  \u001b[31m   \u001b[0m You are in 'detached HEAD' state. You can look around, make experimental\r\n  \u001b[31m   \u001b[0m changes and commit them, and you can discard any commits you make in this\r\n  \u001b[31m   \u001b[0m state without impacting any branches by switching back to a branch.\r\n  \u001b[31m   \u001b[0m \r\n  \u001b[31m   \u001b[0m If you want to create a new branch to retain commits you create, you may\r\n  \u001b[31m   \u001b[0m do so (now or later) by using -c with the switch command. Example:\r\n  \u001b[31m   \u001b[0m \r\n  \u001b[31m   \u001b[0m   git switch -c <new-branch-name>\r\n  \u001b[31m   \u001b[0m \r\n  \u001b[31m   \u001b[0m Or undo this operation with:\r\n  \u001b[31m   \u001b[0m \r\n  \u001b[31m   \u001b[0m   git switch -\r\n  \u001b[31m   \u001b[0m \r\n  \u001b[31m   \u001b[0m Turn off this advice by setting config variable advice.detachedHead to false\r\n  \u001b[31m   \u001b[0m \r\n  \u001b[31m   \u001b[0m Traceback (most recent call last):\r\n  \u001b[31m   \u001b[0m   File \"/usr/local/bin/cmake\", line 5, in <module>\r\n  \u001b[31m   \u001b[0m     from cmake import cmake\r\n  \u001b[31m   \u001b[0m ModuleNotFoundError: No module named 'cmake'\r\n  \u001b[31m   \u001b[0m make: *** No targets specified and no makefile found.  Stop.\r\n  \u001b[31m   \u001b[0m make: *** No rule to make target 'install'.  Stop.\r\n  \u001b[31m   \u001b[0m Package sentencepiece was not found in the pkg-config search path.\r\n  \u001b[31m   \u001b[0m Perhaps you should add the directory containing `sentencepiece.pc'\r\n  \u001b[31m   \u001b[0m to the PKG_CONFIG_PATH environment variable\r\n  \u001b[31m   \u001b[0m No package 'sentencepiece' found\r\n  \u001b[31m   \u001b[0m Failed to find sentencepiece pkg-config\r\n  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\r\n  \r\n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\r\n\u001b[31m  ERROR: Failed building wheel for sentencepiece\u001b[0m\u001b[31m\r\n\u001b[0m\u001b[?25h  Building wheel for sqlitedict (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n\u001b[?25h  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16863 sha256=b4505e21ed066104a1869bba2144bff8c6b414bffcab41d1737b470f26166710\r\n  Stored in directory: /tmp/pip-ephem-wheel-cache-r6y6q775/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\r\n  Building wheel for langdetect (pyproject.toml) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\r\n\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=a21cdee4e3388ba6a982226ce3ee6f10a9b22558fd3c55f76076e52eb9d01b31\r\n  Stored in directory: /tmp/pip-ephem-wheel-cache-r6y6q775/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\r\n  Building wheel for overrides (pyproject.toml) ... \u001b[?25ldone\r\n\u001b[?25h  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10171 sha256=c0b1389e87f9329b1d138a2d005545de82aba7f6aed273326d7eadc25060f4d4\r\n  Stored in directory: /tmp/pip-ephem-wheel-cache-r6y6q775/wheels/bd/23/63/4d5849844f8f9d32be09e1b9b278e80de2d8314fbf1e28068b\r\nSuccessfully built gdown mpld3 sqlitedict langdetect overrides\r\nFailed to build sentencepiece\r\n\u001b[31mERROR: Could not build wheels for sentencepiece, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\r\n\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\nCollecting scipy==1.10.1\r\n  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\r\n\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/58.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hRequirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/site-packages (from scipy==1.10.1) (1.25.2)\r\nDownloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/34.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/34.4 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r\u001b[2K   \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/34.4 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/34.4 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/34.4 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.3/34.4 MB\u001b[0m \u001b[31m115.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/34.4 MB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.3/34.4 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m26.2/34.4 MB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m30.1/34.4 MB\u001b[0m \u001b[31m110.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m34.3/34.4 MB\u001b[0m \u001b[31m114.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m111.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hInstalling collected packages: scipy\r\n  Attempting uninstall: scipy\r\n    Found existing installation: scipy 1.11.4\r\n    Uninstalling scipy-1.11.4:\r\n      Successfully uninstalled scipy-1.11.4\r\nSuccessfully installed scipy-1.10.1\r\n\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install allennlp allennlp_models\n",
    "!pip install \"overrides<5\"\n",
    "!pip install \"flair==0.10\"\n",
    "!pip install \"scipy==1.10.1\"\n",
    "#!pip install --upgrade pytorch-pretrained-bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c429e698",
   "metadata": {},
   "source": [
    "### Step 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0cd486",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb996211",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f399738c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Second attempt to run the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1766e4bf",
   "metadata": {},
   "source": [
    "### Step 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a35d4b",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462a6da0",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0235da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/DisCoDisCo\n/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/DisCoDisCo\n",
    "# !bash seg_scripts/single_corpus_train_and_test_ft.sh deu.rst.pcc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d58c96b",
   "metadata": {},
   "source": [
    "### Step 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf136a85",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63de049",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ef6406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Patch files\n",
    "# \n",
    "# Changes:\n",
    "# \n",
    "# *   Select required model `google-bert/bert-base-german-dbmdz-uncased`\n",
    "# *   Remove flat `--cuda-device` to allow CPU running\n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838b498a",
   "metadata": {},
   "source": [
    "### Step 19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee08511a",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93e149c",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee0d5ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Second attempt to run the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b43b24b",
   "metadata": {},
   "source": [
    "### Step 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3948c2e5",
   "metadata": {},
   "source": [
    "Execution Start time: 05:01:11, End time: 05:01:11 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149b30f1",
   "metadata": {},
   "source": [
    "**Thought**: Let's open the script `seg_scripts/single_corpus_train_and_test_ft.sh` to make the necessary changes for selecting the required model and adjusting any other configurations needed for the task. This will help us ensure that the script runs correctly with the specified parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b5382d",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67653c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/DisCoDisCo/seg_scripts/single_corpus_train_and_test_ft.sh (136 lines total)]\n1:#!/bin/bash\n2:#set -o errexit\n3:if [ $# -eq 0 ]; then\n4:  echo \"Supply the name of a corpus\"\n5:  exit 1\n6:fi\n7:CORPUS=\"$1\"\n8:CORPUS_DIR=\"data/2021/${1}\"\n9:MODEL_DIR=${2:-models}/${CORPUS}_seg_bert_baseline_ft\n10:if [[ ! -d $CORPUS_DIR ]]; then\n11:  echo \"Corpus \\\"$CORPUS_DIR\\\" not found\"\n12:  exit 1\n13:fi\n14:if [[ -d $MODEL_DIR ]]; then\n15:  echo \"\\\"$MODEL_DIR\\\" already exists. Removing it now...\"\n16:  rm -rf \"$MODEL_DIR\"\n17:fi\n18:\n19:# use language-specific berts if we can\n20:export EMBEDDING_DIMS=768\n21:if [[ \"$CORPUS\" == \"eng\"* ]]; then \n22:  export EMBEDDING_DIMS=1024\n23:  #export EMBEDDING_MODEL_NAME=\"roberta-large\"\n24:  export EMBEDDING_MODEL_NAME=\"google/electra-large-discriminator\"\n25:elif [[ \"$CORPUS\" == \"fas\"* ]]; then\n26:  export EMBEDDING_MODEL_NAME=\"HooshvareLab/bert-fa-base-uncased\"\n27:#elif [[ \"$CORPUS\" == \"deu\"* ]]; then\n28:#  #export EMBEDDING_DIMS=1024\n29:#  #export EMBEDDING_MODEL_NAME=\"deepset/gelectra-large\"\n30:#  export EMBEDDING_DIMS=1024\n31:#  export EMBEDDING_MODEL_NAME=\"deepset/gbert-large\"\n32:#elif [[ \"$CORPUS\" == \"fra\"* ]]; then\n33:#  export EMBEDDING_DIMS=1024\n34:#  export EMBEDDING_MODEL_NAME=\"camembert/camembert-large\"\n35:elif [[ \"$CORPUS\" == \"zho\"* ]]; then\n36:  #export EMBEDDING_DIMS=1024\n37:  #export EMBEDDING_MODEL_NAME=\"hfl/chinese-electra-180g-large-discriminator\"\n38:  export EMBEDDING_MODEL_NAME=\"bert-base-chinese\"\n39:elif [[ \"$CORPUS\" == \"nld\"* ]]; then\n40:  #export EMBEDDING_MODEL_NAME=\"GroNLP/bert-base-dutch-cased\"\n41:  export EMBEDDING_MODEL_NAME=\"pdelobelle/robbert-v2-dutch-base\"\n42:elif [[ \"$CORPUS\" == \"eus\"* ]]; then\n43:  export EMBEDDING_MODEL_NAME=\"ixa-ehu/berteus-base-cased\"\n44:elif [[ \"$CORPUS\" == \"spa\"* ]]; then\n45:  #export EMBEDDING_MODEL_NAME=\"mrm8488/electricidad-base-discriminator\"\n46:  export EMBEDDING_MODEL_NAME=\"dccuchile/bert-base-spanish-wwm-cased\"\n47:elif [[ \"$CORPUS\" == \"por\"* ]]; then\n48:  export EMBEDDING_MODEL_NAME=\"neuralmind/bert-base-portuguese-cased\"\n49:elif [[ \"$CORPUS\" == \"tur\"* ]]; then\n50:  #export EMBEDDING_MODEL_NAME=\"dbmdz/electra-base-turkish-cased-discriminator\"\n51:  export EMBEDDING_MODEL_NAME=\"dbmdz/bert-base-turkish-cased\"\n52:elif [[ \"$CORPUS\" == \"rus\"* ]]; then\n53:  export EMBEDDING_MODEL_NAME=\"DeepPavlov/rubert-base-cased\"\n54:else\n55:  export EMBEDDING_DIMS=1024\n56:  export EMBEDDING_MODEL_NAME=\"xlm-roberta-large\"\n57:fi\n58:\n59:# do not use CRF on RST datasets\n60:export USE_CRF=0\n61:if [[ \"$CORPUS\" == *\".pdtb.\"* ]]; then\n62:  export USE_CRF=1\n63:fi\n64:\n65:# use fastText embeddings\n66:if [[ \"$CORPUS\" == \"eng\"* ]]; then\n67:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\"\n68:elif [[ \"$CORPUS\" == \"deu\"* ]]; then\n69:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.de.300.vec.gz\"\n70:elif [[ \"$CORPUS\" == \"eus\"* ]]; then\n71:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.eu.300.vec.gz\"\n72:elif [[ \"$CORPUS\" == \"fas\"* ]]; then\n73:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.vec.gz\"\n74:elif [[ \"$CORPUS\" == \"fra\"* ]]; then\n75:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.vec.gz\"\n76:elif [[ \"$CORPUS\" == \"nld\"* ]]; then\n77:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.nl.300.vec.gz\"\n78:elif [[ \"$CORPUS\" == \"por\"* ]]; then\n79:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.pt.300.vec.gz\"\n80:elif [[ \"$CORPUS\" == \"rus\"* ]]; then\n81:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ru.300.vec.gz\"\n82:elif [[ \"$CORPUS\" == \"spa\"* ]]; then\n83:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.es.300.vec.gz\"\n84:elif [[ \"$CORPUS\" == \"tur\"* ]]; then\n85:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.tr.300.vec.gz\"\n86:elif [[ \"$CORPUS\" == \"zho\"* ]]; then\n87:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.zh.300.vec.gz\"\n88:else\n89:  echo \"Couldn't find a fasttext embedding for \\\"$CORPUS\\\"\" >&2\n90:  exit 1\n91:fi\n92:\n93:echo \"\"\n94:echo \"#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\"\n95:echo \"# Training on $CORPUS\"\n96:echo \"#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\"\n97:echo \"\"\n98:export TRAIN_DATA_PATH=\"${CORPUS_DIR}/${CORPUS}_train.conllu\"\n99:export VALIDATION_DATA_PATH=\"${CORPUS_DIR}/${CORPUS}_dev.conllu\"\n100:echo $TRAIN_DATA_PATH\n(36 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open seg_scripts/single_corpus_train_and_test_ft.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6324a336",
   "metadata": {},
   "source": [
    "### Step 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeca855",
   "metadata": {},
   "source": [
    "Execution Start time: 05:01:13, End time: 05:01:13 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4200167c",
   "metadata": {},
   "source": [
    "**Thought**: We need to modify the script to use the \"google-bert/bert-base-german-dbmdz-uncased\" model for the \"deu.rst.pcc\" dataset. Let's update the relevant section in the script to set the correct model name.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcff5c49",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f42ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/DisCoDisCo/seg_scripts/single_corpus_train_and_test_ft.sh (133 lines total)]\n1:#!/bin/bash\n2:#set -o errexit\n3:if [ $# -eq 0 ]; then\n4:  echo \"Supply the name of a corpus\"\n5:  exit 1\n6:fi\n7:CORPUS=\"$1\"\n8:CORPUS_DIR=\"data/2021/${1}\"\n9:MODEL_DIR=${2:-models}/${CORPUS}_seg_bert_baseline_ft\n10:if [[ ! -d $CORPUS_DIR ]]; then\n11:  echo \"Corpus \\\"$CORPUS_DIR\\\" not found\"\n12:  exit 1\n13:fi\n14:if [[ -d $MODEL_DIR ]]; then\n15:  echo \"\\\"$MODEL_DIR\\\" already exists. Removing it now...\"\n16:  rm -rf \"$MODEL_DIR\"\n17:fi\n18:\n19:# use language-specific berts if we can\n20:export EMBEDDING_DIMS=768\n21:if [[ \"$CORPUS\" == \"eng\"* ]]; then \n22:  export EMBEDDING_DIMS=1024\n23:  #export EMBEDDING_MODEL_NAME=\"roberta-large\"\n24:  export EMBEDDING_MODEL_NAME=\"google/electra-large-discriminator\"\n25:elif [[ \"$CORPUS\" == \"fas\"* ]]; then\n26:  export EMBEDDING_MODEL_NAME=\"HooshvareLab/bert-fa-base-uncased\"\n27:elif [[ \"$CORPUS\" == \"deu\"* ]]; then\n28:  export EMBEDDING_MODEL_NAME=\"google-bert/bert-base-german-dbmdz-uncased\"\n29:#elif [[ \"$CORPUS\" == \"fra\"* ]]; then\n30:#  export EMBEDDING_DIMS=1024\n31:#  export EMBEDDING_MODEL_NAME=\"camembert/camembert-large\"\n32:elif [[ \"$CORPUS\" == \"zho\"* ]]; then\n33:  #export EMBEDDING_DIMS=1024\n34:  #export EMBEDDING_MODEL_NAME=\"hfl/chinese-electra-180g-large-discriminator\"\n35:  export EMBEDDING_MODEL_NAME=\"bert-base-chinese\"\n36:elif [[ \"$CORPUS\" == \"nld\"* ]]; then\n37:  #export EMBEDDING_MODEL_NAME=\"GroNLP/bert-base-dutch-cased\"\n38:  export EMBEDDING_MODEL_NAME=\"pdelobelle/robbert-v2-dutch-base\"\n39:elif [[ \"$CORPUS\" == \"eus\"* ]]; then\n40:  export EMBEDDING_MODEL_NAME=\"ixa-ehu/berteus-base-cased\"\n41:elif [[ \"$CORPUS\" == \"spa\"* ]]; then\n42:  #export EMBEDDING_MODEL_NAME=\"mrm8488/electricidad-base-discriminator\"\n43:  export EMBEDDING_MODEL_NAME=\"dccuchile/bert-base-spanish-wwm-cased\"\n44:elif [[ \"$CORPUS\" == \"por\"* ]]; then\n45:  export EMBEDDING_MODEL_NAME=\"neuralmind/bert-base-portuguese-cased\"\n46:elif [[ \"$CORPUS\" == \"tur\"* ]]; then\n47:  #export EMBEDDING_MODEL_NAME=\"dbmdz/electra-base-turkish-cased-discriminator\"\n48:  export EMBEDDING_MODEL_NAME=\"dbmdz/bert-base-turkish-cased\"\n49:elif [[ \"$CORPUS\" == \"rus\"* ]]; then\n50:  export EMBEDDING_MODEL_NAME=\"DeepPavlov/rubert-base-cased\"\n51:else\n52:  export EMBEDDING_DIMS=1024\n53:  export EMBEDDING_MODEL_NAME=\"xlm-roberta-large\"\n54:fi\n55:\n56:# do not use CRF on RST datasets\n57:export USE_CRF=0\n58:if [[ \"$CORPUS\" == *\".pdtb.\"* ]]; then\n59:  export USE_CRF=1\n60:fi\n61:\n62:# use fastText embeddings\n63:if [[ \"$CORPUS\" == \"eng\"* ]]; then\n64:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\"\n65:elif [[ \"$CORPUS\" == \"deu\"* ]]; then\n66:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.de.300.vec.gz\"\n67:elif [[ \"$CORPUS\" == \"eus\"* ]]; then\n68:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.eu.300.vec.gz\"\n69:elif [[ \"$CORPUS\" == \"fas\"* ]]; then\n70:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.vec.gz\"\n71:elif [[ \"$CORPUS\" == \"fra\"* ]]; then\n72:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.vec.gz\"\n73:elif [[ \"$CORPUS\" == \"nld\"* ]]; then\n74:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.nl.300.vec.gz\"\n75:elif [[ \"$CORPUS\" == \"por\"* ]]; then\n76:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.pt.300.vec.gz\"\n77:elif [[ \"$CORPUS\" == \"rus\"* ]]; then\n78:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ru.300.vec.gz\"\n79:elif [[ \"$CORPUS\" == \"spa\"* ]]; then\n80:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.es.300.vec.gz\"\n81:elif [[ \"$CORPUS\" == \"tur\"* ]]; then\n82:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.tr.300.vec.gz\"\n83:elif [[ \"$CORPUS\" == \"zho\"* ]]; then\n84:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.zh.300.vec.gz\"\n85:else\n86:  echo \"Couldn't find a fasttext embedding for \\\"$CORPUS\\\"\" >&2\n87:  exit 1\n88:fi\n89:\n90:echo \"\"\n91:echo \"#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\"\n92:echo \"# Training on $CORPUS\"\n93:echo \"#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\"\n94:echo \"\"\n95:export TRAIN_DATA_PATH=\"${CORPUS_DIR}/${CORPUS}_train.conllu\"\n96:export VALIDATION_DATA_PATH=\"${CORPUS_DIR}/${CORPUS}_dev.conllu\"\n97:echo $TRAIN_DATA_PATH\n98:allennlp train \\\n99:  configs/seg/baseline/bert_baseline_ft.jsonnet \\\n100:  -s \"$MODEL_DIR\"\n(33 more lines below)\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 27:31\n",
    "elif [[ \"$CORPUS\" == \"deu\"* ]]; then\n",
    "  export EMBEDDING_MODEL_NAME=\"google-bert/bert-base-german-dbmdz-uncased\"\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a53824b",
   "metadata": {},
   "source": [
    "### Step 22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61202ec6",
   "metadata": {},
   "source": [
    "Execution Start time: 05:01:15, End time: 05:06:21 (duration: 305 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552da4a0",
   "metadata": {},
   "source": [
    "**Thought**: The script has been updated to use the \"google-bert/bert-base-german-dbmdz-uncased\" model for the \"deu.rst.pcc\" dataset. Now, let's run the script again to train and evaluate the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcbe907",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55574ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n# Training on deu.rst.pcc\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n\ndata/2021/deu.rst.pcc/deu.rst.pcc_train.conllu\n[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n[nltk_data] Downloading package wordnet to /root/nltk_data...\n2024-08-22 05:01:24,106 - INFO - allennlp.common.plugins - Plugin allennlp_models available\n2024-08-22 05:01:25,083 - ERROR - allennlp.common.plugins - Plugin gucorpling_models could not be loaded: No module named 'flair'\n2024-08-22 05:01:25,192 - INFO - allennlp.common.checks - Pytorch version: 1.12.1+cu102\n\nDownloading:   0%|                                   | 0.00/48.0 [00:00<?, ?B/s]\nDownloading: 100%|████████████████████████████| 48.0/48.0 [00:00<00:00, 204kB/s]\n\nDownloading:   0%|                                    | 0.00/433 [00:00<?, ?B/s]\n\n\nDownloading:   0%|                                   | 0.00/242k [00:00<?, ?B/s]\n\n\nDownloading:   0%|                                   | 0.00/475k [00:00<?, ?B/s]\n\n\nloading instances: 0it [00:00, ?it/s]2024-08-22 05:01:26,523 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'case'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n2024-08-22 05:01:26,523 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'xpos'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n2024-08-22 05:01:26,523 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'depchunk'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n2024-08-22 05:01:26,524 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'deprel'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n2024-08-22 05:01:26,527 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'genre'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n2024-08-22 05:01:26,527 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'parent_clauses'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n2024-08-22 05:01:26,527 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'upos'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n2024-08-22 05:01:26,527 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 's_type'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n\nloading instances: 10it [00:00, 683.16it/s]\n\nloading instances: 0it [00:00, ?it/s]\nloading instances: 10it [00:00, 1205.68it/s]\n2024-08-22 05:01:26,541 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.\n\nbuilding vocab: 0it [00:00, ?it/s]\nbuilding vocab: 20it [00:00, 3640.10it/s]\n2024-08-22 05:01:26,550 - INFO - allennlp.modules.token_embedders.embedding - Reading pretrained embeddings from file\n2024-08-22 05:01:26,844 - INFO - cached_path - https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.de.300.vec.gz not found in cache, downloading to /root/.allennlp/cache/d78765195f65290f8caf5173f8306d3ec7999e1957d3bd92067dfc6563de0d08.4bd353225952066f38b58e55b15e9706a715a158f382dbc065b78af3ad8983dc\n\u001b[?25l\n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n                                                                            GB  \n\u001b[?25h2024-08-22 05:02:30,958 - INFO - allennlp.modules.token_embedders.embedding - Recognized a header line in the embedding file with number of tokens: 2000000\n\n  0%|          | 0/2000000 [00:00<?, ?it/s]\n  0%|          | 3576/2000000 [00:00<00:55, 35728.35it/s]\n  0%|          | 7328/2000000 [00:00<00:54, 36766.90it/s]\n  1%|          | 11144/2000000 [00:00<00:53, 37396.30it/s]\n  1%|          | 14986/2000000 [00:00<00:52, 37789.65it/s]\n  1%|          | 18798/2000000 [00:00<00:52, 37896.43it/s]\n  1%|1         | 22690/2000000 [00:00<00:51, 38234.56it/s]\n  1%|1         | 26530/2000000 [00:00<00:51, 38278.31it/s]\n  2%|1         | 30374/2000000 [00:00<00:51, 38325.29it/s]\n  2%|1         | 34254/2000000 [00:00<00:51, 38466.68it/s]\n  2%|1         | 38101/2000000 [00:01<00:51, 38432.09it/s]\n  2%|2         | 41967/2000000 [00:01<00:50, 38489.27it/s]\n  2%|2         | 45816/2000000 [00:01<00:50, 38467.90it/s]\n  2%|2         | 49679/2000000 [00:01<00:50, 38507.97it/s]\n  3%|2         | 53551/2000000 [00:01<00:50, 38560.45it/s]\n  3%|2         | 57408/2000000 [00:01<00:50, 38330.07it/s]\n  3%|3         | 61254/2000000 [00:01<00:50, 38357.72it/s]\n  3%|3         | 65097/2000000 [00:01<00:50, 38374.64it/s]\n  3%|3         | 68945/2000000 [00:01<00:50, 38397.37it/s]\n  4%|3         | 72863/2000000 [00:01<00:49, 38621.87it/s]\n  4%|3         | 76771/2000000 [00:02<00:49, 38757.55it/s]\n  4%|4         | 80660/2000000 [00:02<00:49, 38793.98it/s]\n  4%|4         | 84541/2000000 [00:02<00:49, 38796.10it/s]\n  4%|4         | 88430/2000000 [00:02<00:49, 38814.36it/s]\n  5%|4         | 92315/2000000 [00:02<00:49, 38814.49it/s]\n  5%|4         | 96197/2000000 [00:02<00:49, 38117.48it/s]\n  5%|5         | 100080/2000000 [00:02<00:49, 38321.38it/s]\n  5%|5         | 103915/2000000 [00:02<00:49, 38315.53it/s]\n  5%|5         | 107760/2000000 [00:02<00:49, 38349.48it/s]\n  6%|5         | 111597/2000000 [00:02<00:49, 38175.73it/s]\n  6%|5         | 115416/2000000 [00:03<00:49, 38152.35it/s]\n  6%|5         | 119234/2000000 [00:03<00:49, 38152.22it/s]\n  6%|6         | 123050/2000000 [00:03<00:49, 37911.38it/s]\n  6%|6         | 126910/2000000 [00:03<00:49, 38114.15it/s]\n  7%|6         | 130722/2000000 [00:03<00:49, 37933.85it/s]\n  7%|6         | 134542/2000000 [00:03<00:49, 38008.84it/s]\n  7%|6         | 138344/2000000 [00:03<00:49, 37954.42it/s]\n  7%|7         | 142140/2000000 [00:03<00:49, 37880.71it/s]\n  7%|7         | 145929/2000000 [00:03<00:49, 37584.74it/s]\n  7%|7         | 149726/2000000 [00:03<00:49, 37691.76it/s]\n  8%|7         | 153517/2000000 [00:04<00:48, 37750.39it/s]\n  8%|7         | 157333/2000000 [00:04<00:48, 37869.31it/s]\n  8%|8         | 161121/2000000 [00:04<00:48, 37844.94it/s]\n  8%|8         | 164906/2000000 [00:04<00:48, 37830.25it/s]\n  8%|8         | 168791/2000000 [00:04<00:48, 38129.90it/s]\n  9%|8         | 172610/2000000 [00:04<00:47, 38143.30it/s]\n  9%|8         | 176425/2000000 [00:04<00:47, 38085.68it/s]\n  9%|9         | 180292/2000000 [00:04<00:47, 38254.20it/s]\n  9%|9         | 184165/2000000 [00:04<00:47, 38393.20it/s]\n  9%|9         | 188032/2000000 [00:04<00:47, 38473.24it/s]\n 10%|9         | 191917/2000000 [00:05<00:46, 38545.67it/s]\n 10%|9         | 195784/2000000 [00:05<00:46, 38580.26it/s]\n 10%|9         | 199661/2000000 [00:05<00:46, 38632.03it/s]\n 10%|#         | 203525/2000000 [00:05<00:46, 38629.02it/s]\n 10%|#         | 207393/2000000 [00:05<00:46, 38641.66it/s]\n 11%|#         | 211258/2000000 [00:05<00:46, 38641.82it/s]\n 11%|#         | 215144/2000000 [00:05<00:46, 38701.87it/s]\n 11%|#         | 219015/2000000 [00:05<00:46, 38565.48it/s]\n 11%|#1        | 222872/2000000 [00:05<00:46, 38134.33it/s]\n 11%|#1        | 226687/2000000 [00:05<00:46, 37853.56it/s]\n 12%|#1        | 230530/2000000 [00:06<00:46, 38015.68it/s]\n 12%|#1        | 234396/2000000 [00:06<00:46, 38198.94it/s]\n 12%|#1        | 238239/2000000 [00:06<00:46, 38262.83it/s]\n 12%|#2        | 242091/2000000 [00:06<00:45, 38335.36it/s]\n 12%|#2        | 245939/2000000 [00:06<00:45, 38377.59it/s]\n 12%|#2        | 249813/2000000 [00:06<00:45, 38476.52it/s]\n 13%|#2        | 253668/2000000 [00:06<00:45, 38488.40it/s]\n 13%|#2        | 257518/2000000 [00:06<00:45, 38275.18it/s]\n 13%|#3        | 261346/2000000 [00:06<00:45, 37978.03it/s]\n 13%|#3        | 265145/2000000 [00:06<00:45, 37733.24it/s]\n 13%|#3        | 268919/2000000 [00:07<00:46, 37451.47it/s]\n 14%|#3        | 272665/2000000 [00:07<00:46, 37042.16it/s]\n 14%|#3        | 276507/2000000 [00:07<00:46, 37441.21it/s]\n 14%|#4        | 280359/2000000 [00:07<00:45, 37757.09it/s]\n 14%|#4        | 284239/2000000 [00:07<00:45, 38058.59it/s]\n 14%|#4        | 288109/2000000 [00:07<00:44, 38243.19it/s]\n 15%|#4        | 291935/2000000 [00:07<00:46, 36943.24it/s]\n 15%|#4        | 295640/2000000 [00:07<00:47, 35567.48it/s]\n 15%|#4        | 299381/2000000 [00:07<00:47, 36093.23it/s]\n 15%|#5        | 303244/2000000 [00:07<00:46, 36820.00it/s]\n 15%|#5        | 307008/2000000 [00:08<00:45, 37059.35it/s]\n 16%|#5        | 310738/2000000 [00:08<00:45, 37124.58it/s]\n 16%|#5        | 314591/2000000 [00:08<00:44, 37535.69it/s]\n 16%|#5        | 318350/2000000 [00:08<00:45, 37175.50it/s]\n 16%|#6        | 322080/2000000 [00:08<00:45, 36828.89it/s]\n 16%|#6        | 325766/2000000 [00:08<00:46, 35900.73it/s]\n 16%|#6        | 329617/2000000 [00:08<00:45, 36650.19it/s]\n 17%|#6        | 333428/2000000 [00:08<00:44, 37077.93it/s]\n 17%|#6        | 337142/2000000 [00:08<00:44, 37080.21it/s]\n 17%|#7        | 340881/2000000 [00:08<00:44, 37161.73it/s]\n 17%|#7        | 344600/2000000 [00:09<00:44, 36907.10it/s]\n 17%|#7        | 348370/2000000 [00:09<00:44, 37135.56it/s]\n 18%|#7        | 352164/2000000 [00:09<00:44, 37366.39it/s]\n 18%|#7        | 355903/2000000 [00:09<00:44, 37237.04it/s]\n 18%|#7        | 359628/2000000 [00:09<00:44, 36967.89it/s]\n 18%|#8        | 363493/2000000 [00:09<00:43, 37461.11it/s]\n 18%|#8        | 367241/2000000 [00:09<00:43, 37321.43it/s]\n 19%|#8        | 370974/2000000 [00:09<00:43, 37271.37it/s]\n 19%|#8        | 374702/2000000 [00:09<00:43, 37112.01it/s]\n 19%|#8        | 378414/2000000 [00:09<00:44, 36668.33it/s]\n 19%|#9        | 382091/2000000 [00:10<00:44, 36690.56it/s]\n 19%|#9        | 385761/2000000 [00:10<00:44, 35964.19it/s]\n 19%|#9        | 389453/2000000 [00:10<00:44, 36235.20it/s]\n 20%|#9        | 393088/2000000 [00:10<00:44, 36268.17it/s]\n 20%|#9        | 396899/2000000 [00:10<00:43, 36810.01it/s]\n 20%|##        | 400724/2000000 [00:10<00:42, 37238.13it/s]\n 20%|##        | 404450/2000000 [00:10<00:42, 37218.63it/s]\n 20%|##        | 408174/2000000 [00:10<00:43, 36693.44it/s]\n 21%|##        | 411985/2000000 [00:10<00:42, 37106.03it/s]\n 21%|##        | 415717/2000000 [00:11<00:42, 37161.14it/s]\n 21%|##        | 419442/2000000 [00:11<00:42, 37182.03it/s]\n 21%|##1       | 423162/2000000 [00:11<00:42, 36681.95it/s]\n 21%|##1       | 426833/2000000 [00:11<00:45, 34739.33it/s]\n 22%|##1       | 430328/2000000 [00:11<00:45, 34614.80it/s]\n 22%|##1       | 433804/2000000 [00:11<00:45, 34520.06it/s]\n 22%|##1       | 437505/2000000 [00:11<00:44, 35242.05it/s]\n 22%|##2       | 441298/2000000 [00:11<00:43, 36024.07it/s]\n 22%|##2       | 445061/2000000 [00:11<00:42, 36492.88it/s]\n 22%|##2       | 448829/2000000 [00:11<00:42, 36837.66it/s]\n 23%|##2       | 452550/2000000 [00:12<00:41, 36947.58it/s]\n 23%|##2       | 456267/2000000 [00:12<00:41, 37006.07it/s]\n 23%|##3       | 460006/2000000 [00:12<00:41, 37117.83it/s]\n 23%|##3       | 463730/2000000 [00:12<00:41, 37152.99it/s]\n 23%|##3       | 467617/2000000 [00:12<00:40, 37657.89it/s]\n 24%|##3       | 471414/2000000 [00:12<00:40, 37748.67it/s]\n 24%|##3       | 475237/2000000 [00:12<00:40, 37881.39it/s]\n 24%|##3       | 479070/2000000 [00:12<00:40, 38008.52it/s]\n 24%|##4       | 482926/2000000 [00:12<00:39, 38170.21it/s]\n 24%|##4       | 486799/2000000 [00:12<00:39, 38334.90it/s]\n 25%|##4       | 490672/2000000 [00:13<00:39, 38450.05it/s]\n 25%|##4       | 494545/2000000 [00:13<00:39, 38529.63it/s]\n 25%|##4       | 498399/2000000 [00:13<00:39, 38375.29it/s]\n 25%|##5       | 502237/2000000 [00:13<00:39, 38344.80it/s]\n 25%|##5       | 506072/2000000 [00:13<00:39, 38261.36it/s]\n 25%|##5       | 509908/2000000 [00:13<00:38, 38289.63it/s]\n 26%|##5       | 513738/2000000 [00:13<00:39, 38034.96it/s]\n 26%|##5       | 517542/2000000 [00:13<00:39, 37998.00it/s]\n 26%|##6       | 521413/2000000 [00:13<00:38, 38200.33it/s]\n 26%|##6       | 525234/2000000 [00:13<00:38, 38102.67it/s]\n 26%|##6       | 529045/2000000 [00:14<00:38, 38065.73it/s]\n 27%|##6       | 532856/2000000 [00:14<00:38, 38068.07it/s]\n 27%|##6       | 536718/2000000 [00:14<00:38, 38232.22it/s]\n 27%|##7       | 540542/2000000 [00:14<00:38, 37955.70it/s]\n 27%|##7       | 544339/2000000 [00:14<00:38, 37837.46it/s]\n 27%|##7       | 548160/2000000 [00:14<00:38, 37946.42it/s]\n 28%|##7       | 551961/2000000 [00:14<00:38, 37963.81it/s]\n 28%|##7       | 555801/2000000 [00:14<00:37, 38086.06it/s]\n 28%|##7       | 559656/2000000 [00:14<00:37, 38220.04it/s]\n 28%|##8       | 563525/2000000 [00:14<00:37, 38355.58it/s]\n 28%|##8       | 567413/2000000 [00:15<00:37, 38502.21it/s]\n 29%|##8       | 571282/2000000 [00:15<00:37, 38557.83it/s]\n 29%|##8       | 575163/2000000 [00:15<00:36, 38631.36it/s]\n 29%|##8       | 579027/2000000 [00:15<00:36, 38523.46it/s]\n 29%|##9       | 582913/2000000 [00:15<00:36, 38619.69it/s]\n 29%|##9       | 586786/2000000 [00:15<00:36, 38648.60it/s]\n 30%|##9       | 590659/2000000 [00:15<00:36, 38667.80it/s]\n 30%|##9       | 594532/2000000 [00:15<00:36, 38682.34it/s]\n 30%|##9       | 598401/2000000 [00:15<00:36, 38519.80it/s]\n 30%|###       | 602254/2000000 [00:15<00:36, 38322.24it/s]\n 30%|###       | 606100/2000000 [00:16<00:36, 38355.01it/s]\n 30%|###       | 609984/2000000 [00:16<00:36, 38492.75it/s]\n 31%|###       | 613861/2000000 [00:16<00:35, 38569.64it/s]\n 31%|###       | 617749/2000000 [00:16<00:35, 38660.99it/s]\n 31%|###1      | 621616/2000000 [00:16<00:35, 38547.74it/s]\n 31%|###1      | 625473/2000000 [00:16<00:35, 38548.86it/s]\n 31%|###1      | 629328/2000000 [00:16<00:35, 38415.14it/s]\n 32%|###1      | 633170/2000000 [00:16<00:36, 37936.98it/s]\n 32%|###1      | 637064/2000000 [00:16<00:35, 38227.09it/s]\n 32%|###2      | 640961/2000000 [00:16<00:35, 38438.94it/s]\n 32%|###2      | 644860/2000000 [00:17<00:35, 38602.29it/s]\n 32%|###2      | 648750/2000000 [00:17<00:34, 38687.13it/s]\n 33%|###2      | 652631/2000000 [00:17<00:34, 38721.74it/s]\n 33%|###2      | 656524/2000000 [00:17<00:34, 38774.46it/s]\n 33%|###3      | 660415/2000000 [00:17<00:34, 38812.99it/s]\n 33%|###3      | 664311/2000000 [00:17<00:34, 38847.97it/s]\n 33%|###3      | 668196/2000000 [00:17<00:34, 38834.47it/s]\n 34%|###3      | 672080/2000000 [00:17<00:34, 38782.29it/s]\n 34%|###3      | 675959/2000000 [00:17<00:34, 38606.30it/s]\n 34%|###3      | 679820/2000000 [00:17<00:34, 38245.56it/s]\n 34%|###4      | 683691/2000000 [00:18<00:34, 38378.67it/s]\n 34%|###4      | 687567/2000000 [00:18<00:34, 38488.11it/s]\n 35%|###4      | 691417/2000000 [00:18<00:34, 38134.15it/s]\n 35%|###4      | 695283/2000000 [00:18<00:34, 38285.85it/s]\n 35%|###4      | 699178/2000000 [00:18<00:33, 38478.67it/s]\n 35%|###5      | 703069/2000000 [00:18<00:33, 38603.34it/s]\n 35%|###5      | 706942/2000000 [00:18<00:33, 38632.87it/s]\n 36%|###5      | 710848/2000000 [00:18<00:33, 38753.74it/s]\n 36%|###5      | 714742/2000000 [00:18<00:33, 38801.04it/s]\n 36%|###5      | 718648/2000000 [00:18<00:32, 38875.37it/s]\n 36%|###6      | 722550/2000000 [00:19<00:32, 38911.58it/s]\n 36%|###6      | 726449/2000000 [00:19<00:32, 38923.47it/s]\n 37%|###6      | 730351/2000000 [00:19<00:32, 38946.87it/s]\n 37%|###6      | 734271/2000000 [00:19<00:32, 39016.58it/s]\n 37%|###6      | 738177/2000000 [00:19<00:32, 39021.38it/s]\n 37%|###7      | 742080/2000000 [00:19<00:32, 38693.00it/s]\n 37%|###7      | 745962/2000000 [00:19<00:32, 38720.99it/s]\n 37%|###7      | 749856/2000000 [00:19<00:32, 38776.99it/s]\n 38%|###7      | 753735/2000000 [00:19<00:32, 38777.76it/s]\n 38%|###7      | 757616/2000000 [00:19<00:32, 38781.83it/s]\n 38%|###8      | 761495/2000000 [00:20<00:31, 38773.71it/s]\n 38%|###8      | 765373/2000000 [00:20<00:31, 38728.25it/s]\n 38%|###8      | 769247/2000000 [00:20<00:31, 38730.18it/s]\n 39%|###8      | 773121/2000000 [00:20<00:31, 38679.39it/s]\n 39%|###8      | 777012/2000000 [00:20<00:31, 38738.78it/s]\n 39%|###9      | 780886/2000000 [00:20<00:31, 38616.13it/s]\n 39%|###9      | 784773/2000000 [00:20<00:31, 38685.10it/s]\n 39%|###9      | 788654/2000000 [00:20<00:31, 38712.27it/s]\n 40%|###9      | 792526/2000000 [00:20<00:31, 38684.05it/s]\n 40%|###9      | 796395/2000000 [00:20<00:31, 38654.60it/s]\n 40%|####      | 800290/2000000 [00:21<00:30, 38730.90it/s]\n 40%|####      | 804177/2000000 [00:21<00:30, 38767.10it/s]\n 40%|####      | 808087/2000000 [00:21<00:30, 38855.54it/s]\n 41%|####      | 811973/2000000 [00:21<00:30, 38824.59it/s]\n 41%|####      | 815862/2000000 [00:21<00:30, 38834.55it/s]\n 41%|####      | 819746/2000000 [00:21<00:30, 38829.67it/s]\n 41%|####1     | 823646/2000000 [00:21<00:30, 38867.46it/s]\n 41%|####1     | 827545/2000000 [00:21<00:30, 38900.59it/s]\n 42%|####1     | 831451/2000000 [00:21<00:30, 38939.76it/s]\n 42%|####1     | 835379/2000000 [00:21<00:29, 39038.04it/s]\n 42%|####1     | 839283/2000000 [00:22<00:29, 38941.07it/s]\n 42%|####2     | 843178/2000000 [00:22<00:29, 38886.55it/s]\n 42%|####2     | 847067/2000000 [00:22<00:29, 38508.50it/s]\n 43%|####2     | 850935/2000000 [00:22<00:29, 38553.39it/s]\n 43%|####2     | 854834/2000000 [00:22<00:29, 38675.55it/s]\n 43%|####2     | 858719/2000000 [00:22<00:29, 38717.77it/s]\n 43%|####3     | 862613/2000000 [00:22<00:29, 38780.61it/s]\n 43%|####3     | 866504/2000000 [00:22<00:29, 38814.77it/s]\n 44%|####3     | 870402/2000000 [00:22<00:29, 38853.86it/s]\n 44%|####3     | 874305/2000000 [00:22<00:28, 38897.81it/s]\n 44%|####3     | 878195/2000000 [00:23<00:28, 38859.16it/s]\n 44%|####4     | 882087/2000000 [00:23<00:28, 38869.57it/s]\n 44%|####4     | 885982/2000000 [00:23<00:28, 38883.38it/s]\n 44%|####4     | 889871/2000000 [00:23<00:28, 38866.43it/s]\n 45%|####4     | 893772/2000000 [00:23<00:28, 38900.63it/s]\n 45%|####4     | 897663/2000000 [00:23<00:28, 38881.72it/s]\n 45%|####5     | 901552/2000000 [00:23<00:28, 38857.48it/s]\n 45%|####5     | 905438/2000000 [00:23<00:28, 38806.43it/s]\n 45%|####5     | 909319/2000000 [00:23<00:28, 38745.79it/s]\n 46%|####5     | 913194/2000000 [00:23<00:28, 38513.41it/s]\n 46%|####5     | 917095/2000000 [00:24<00:28, 38652.64it/s]\n 46%|####6     | 920975/2000000 [00:24<00:27, 38691.63it/s]\n 46%|####6     | 924884/2000000 [00:24<00:27, 38806.80it/s]\n 46%|####6     | 928804/2000000 [00:24<00:27, 38923.45it/s]\n 47%|####6     | 932697/2000000 [00:24<00:27, 38897.81it/s]\n 47%|####6     | 936605/2000000 [00:24<00:27, 38949.03it/s]\n 47%|####7     | 940500/2000000 [00:24<00:27, 38937.81it/s]\n 47%|####7     | 944394/2000000 [00:24<00:27, 38610.49it/s]\n 47%|####7     | 948256/2000000 [00:24<00:27, 38583.51it/s]\n 48%|####7     | 952181/2000000 [00:24<00:27, 38773.54it/s]\n 48%|####7     | 956069/2000000 [00:25<00:26, 38803.68it/s]\n 48%|####7     | 959950/2000000 [00:25<00:26, 38725.25it/s]\n 48%|####8     | 963926/2000000 [00:25<00:26, 39033.43it/s]\n 48%|####8     | 967830/2000000 [00:25<00:26, 38977.44it/s]\n 49%|####8     | 971730/2000000 [00:25<00:26, 38976.84it/s]\n 49%|####8     | 975649/2000000 [00:25<00:26, 39030.72it/s]\n 49%|####8     | 979553/2000000 [00:25<00:26, 39008.22it/s]\n 49%|####9     | 983480/2000000 [00:25<00:26, 39082.34it/s]\n 49%|####9     | 987396/2000000 [00:25<00:25, 39098.47it/s]\n 50%|####9     | 991306/2000000 [00:25<00:25, 39077.13it/s]\n 50%|####9     | 995218/2000000 [00:26<00:25, 39086.30it/s]\n 50%|####9     | 999138/2000000 [00:26<00:25, 39115.14it/s]\n 50%|#####     | 1003051/2000000 [00:26<00:25, 39110.24it/s]\n 50%|#####     | 1006967/2000000 [00:26<00:25, 39114.32it/s]\n 51%|#####     | 1010887/2000000 [00:26<00:25, 39137.84it/s]\n 51%|#####     | 1014820/2000000 [00:26<00:25, 39187.60it/s]\n 51%|#####     | 1018739/2000000 [00:26<00:25, 39169.48it/s]\n 51%|#####1    | 1022656/2000000 [00:26<00:24, 39096.27it/s]\n 51%|#####1    | 1026581/2000000 [00:26<00:24, 39136.67it/s]\n 52%|#####1    | 1030504/2000000 [00:26<00:24, 39163.97it/s]\n 52%|#####1    | 1034421/2000000 [00:27<00:24, 39064.24it/s]\n 52%|#####1    | 1038346/2000000 [00:27<00:24, 39112.39it/s]\n 52%|#####2    | 1042268/2000000 [00:27<00:24, 39143.87it/s]\n 52%|#####2    | 1046200/2000000 [00:27<00:24, 39193.23it/s]\n 53%|#####2    | 1050120/2000000 [00:27<00:24, 39133.87it/s]\n 53%|#####2    | 1054039/2000000 [00:27<00:24, 39140.88it/s]\n 53%|#####2    | 1057970/2000000 [00:27<00:24, 39186.10it/s]\n 53%|#####3    | 1061889/2000000 [00:27<00:23, 39186.30it/s]\n 53%|#####3    | 1065808/2000000 [00:27<00:23, 39091.59it/s]\n 53%|#####3    | 1069722/2000000 [00:27<00:23, 39098.39it/s]\n 54%|#####3    | 1073653/2000000 [00:28<00:23, 39155.29it/s]\n 54%|#####3    | 1077580/2000000 [00:28<00:23, 39186.62it/s]\n 54%|#####4    | 1081499/2000000 [00:28<00:23, 39077.82it/s]\n 54%|#####4    | 1085415/2000000 [00:28<00:23, 39100.30it/s]\n 54%|#####4    | 1089349/2000000 [00:28<00:23, 39170.35it/s]\n 55%|#####4    | 1093276/2000000 [00:28<00:23, 39195.99it/s]\n 55%|#####4    | 1097196/2000000 [00:28<00:23, 39130.77it/s]\n 55%|#####5    | 1101126/2000000 [00:28<00:22, 39171.11it/s]\n 55%|#####5    | 1105044/2000000 [00:28<00:22, 39167.80it/s]\n 55%|#####5    | 1108970/2000000 [00:28<00:22, 39188.46it/s]\n 56%|#####5    | 1112889/2000000 [00:29<00:22, 38988.76it/s]\n 56%|#####5    | 1116789/2000000 [00:29<00:22, 38972.59it/s]\n 56%|#####6    | 1120715/2000000 [00:29<00:22, 39050.49it/s]\n 56%|#####6    | 1124631/2000000 [00:29<00:22, 39080.18it/s]\n 56%|#####6    | 1128551/2000000 [00:29<00:22, 39107.36it/s]\n 57%|#####6    | 1132462/2000000 [00:29<00:22, 39009.33it/s]\n 57%|#####6    | 1136397/2000000 [00:29<00:22, 39102.53it/s]\n 57%|#####7    | 1140363/2000000 [00:29<00:21, 39268.57it/s]\n 57%|#####7    | 1144304/2000000 [00:29<00:21, 39301.09it/s]\n 57%|#####7    | 1148235/2000000 [00:30<00:21, 39265.29it/s]\n 58%|#####7    | 1152162/2000000 [00:30<00:21, 39205.67it/s]\n 58%|#####7    | 1156091/2000000 [00:30<00:21, 39224.40it/s]\n 58%|#####8    | 1160021/2000000 [00:30<00:21, 39241.45it/s]\n 58%|#####8    | 1163946/2000000 [00:30<00:21, 39213.83it/s]\n 58%|#####8    | 1167868/2000000 [00:30<00:21, 39139.15it/s]\n 59%|#####8    | 1171782/2000000 [00:30<00:21, 39107.27it/s]\n 59%|#####8    | 1175708/2000000 [00:30<00:21, 39152.41it/s]\n 59%|#####8    | 1179624/2000000 [00:30<00:20, 39142.84it/s]\n 59%|#####9    | 1183539/2000000 [00:30<00:21, 38667.02it/s]\n 59%|#####9    | 1187407/2000000 [00:31<00:21, 38349.72it/s]\n 60%|#####9    | 1191244/2000000 [00:31<00:21, 38157.85it/s]\n 60%|#####9    | 1195061/2000000 [00:31<00:21, 38036.79it/s]\n 60%|#####9    | 1198866/2000000 [00:31<00:21, 37965.76it/s]\n 60%|######    | 1202663/2000000 [00:31<00:21, 37898.80it/s]\n 60%|######    | 1206454/2000000 [00:31<00:20, 37855.58it/s]\n 61%|######    | 1210240/2000000 [00:31<00:20, 37758.49it/s]\n 61%|######    | 1214016/2000000 [00:31<00:20, 37713.48it/s]\n 61%|######    | 1217794/2000000 [00:31<00:20, 37732.11it/s]\n 61%|######1   | 1221707/2000000 [00:31<00:20, 38149.14it/s]\n 61%|######1   | 1225523/2000000 [00:32<00:20, 38035.91it/s]\n 61%|######1   | 1229327/2000000 [00:32<00:20, 37896.34it/s]\n 62%|######1   | 1233117/2000000 [00:32<00:20, 37874.49it/s]\n 62%|######1   | 1237037/2000000 [00:32<00:19, 38268.37it/s]\n 62%|######2   | 1240949/2000000 [00:32<00:19, 38520.45it/s]\n 62%|######2   | 1244859/2000000 [00:32<00:19, 38690.83it/s]\n 62%|######2   | 1248760/2000000 [00:32<00:19, 38777.61it/s]\n 63%|######2   | 1252665/2000000 [00:32<00:19, 38852.52it/s]\n 63%|######2   | 1256581/2000000 [00:32<00:19, 38935.62it/s]\n 63%|######3   | 1260475/2000000 [00:32<00:19, 38833.43it/s]\n 63%|######3   | 1264359/2000000 [00:33<00:19, 38617.73it/s]\n 63%|######3   | 1268245/2000000 [00:33<00:18, 38684.26it/s]\n 64%|######3   | 1272139/2000000 [00:33<00:18, 38759.36it/s]\n 64%|######3   | 1276044/2000000 [00:33<00:18, 38842.96it/s]\n 64%|######3   | 1279960/2000000 [00:33<00:18, 38931.58it/s]\n 64%|######4   | 1283854/2000000 [00:33<00:18, 38885.68it/s]\n 64%|######4   | 1287743/2000000 [00:33<00:18, 38688.06it/s]\n 65%|######4   | 1291636/2000000 [00:33<00:18, 38750.94it/s]\n 65%|######4   | 1295526/2000000 [00:33<00:18, 38787.16it/s]\n 65%|######4   | 1299406/2000000 [00:33<00:18, 38790.29it/s]\n 65%|######5   | 1303314/2000000 [00:34<00:17, 38876.16it/s]\n 65%|######5   | 1307205/2000000 [00:34<00:17, 38877.15it/s]\n 66%|######5   | 1311110/2000000 [00:34<00:17, 38927.76it/s]\n 66%|######5   | 1315003/2000000 [00:34<00:17, 38881.90it/s]\n 66%|######5   | 1318892/2000000 [00:34<00:17, 38825.77it/s]\n 66%|######6   | 1322796/2000000 [00:34<00:17, 38883.62it/s]\n 66%|######6   | 1326685/2000000 [00:34<00:17, 38524.68it/s]\n 67%|######6   | 1330567/2000000 [00:34<00:17, 38611.22it/s]\n 67%|######6   | 1334429/2000000 [00:34<00:17, 38335.58it/s]\n 67%|######6   | 1338329/2000000 [00:34<00:17, 38522.38it/s]\n 67%|######7   | 1342212/2000000 [00:35<00:17, 38610.08it/s]\n 67%|######7   | 1346095/2000000 [00:35<00:16, 38666.41it/s]\n 68%|######7   | 1350015/2000000 [00:35<00:16, 38814.45it/s]\n 68%|######7   | 1353927/2000000 [00:35<00:16, 38904.92it/s]\n 68%|######7   | 1357836/2000000 [00:35<00:16, 38959.19it/s]\n 68%|######8   | 1361748/2000000 [00:35<00:16, 39005.88it/s]\n 68%|######8   | 1365649/2000000 [00:35<00:16, 39004.17it/s]\n 68%|######8   | 1369550/2000000 [00:35<00:16, 38854.85it/s]\n 69%|######8   | 1373436/2000000 [00:35<00:16, 38826.26it/s]\n 69%|######8   | 1377349/2000000 [00:35<00:16, 38912.50it/s]\n 69%|######9   | 1381272/2000000 [00:36<00:15, 39003.31it/s]\n 69%|######9   | 1385180/2000000 [00:36<00:15, 39024.98it/s]\n 69%|######9   | 1389085/2000000 [00:36<00:15, 39025.99it/s]\n 70%|######9   | 1392988/2000000 [00:36<00:15, 39004.29it/s]\n 70%|######9   | 1396889/2000000 [00:36<00:15, 38924.88it/s]\n 70%|#######   | 1400782/2000000 [00:36<00:15, 38885.88it/s]\n 70%|#######   | 1404681/2000000 [00:36<00:15, 38912.28it/s]\n 70%|#######   | 1408600/2000000 [00:36<00:15, 38989.32it/s]\n 71%|#######   | 1412513/2000000 [00:36<00:15, 39022.53it/s]\n 71%|#######   | 1416416/2000000 [00:36<00:14, 38965.65it/s]\n 71%|#######1  | 1420313/2000000 [00:37<00:14, 38877.83it/s]\n 71%|#######1  | 1424201/2000000 [00:37<00:14, 38863.14it/s]\n 71%|#######1  | 1428088/2000000 [00:37<00:14, 38777.67it/s]\n 72%|#######1  | 1431966/2000000 [00:37<00:14, 38736.51it/s]\n 72%|#######1  | 1435840/2000000 [00:37<00:14, 38679.23it/s]\n 72%|#######1  | 1439717/2000000 [00:37<00:14, 38697.31it/s]\n 72%|#######2  | 1443604/2000000 [00:37<00:14, 38746.07it/s]\n 72%|#######2  | 1447513/2000000 [00:37<00:14, 38846.01it/s]\n 73%|#######2  | 1451398/2000000 [00:37<00:14, 38829.91it/s]\n 73%|#######2  | 1455286/2000000 [00:37<00:14, 38844.29it/s]\n 73%|#######2  | 1459171/2000000 [00:38<00:13, 38819.88it/s]\n 73%|#######3  | 1463064/2000000 [00:38<00:13, 38850.25it/s]\n 73%|#######3  | 1466954/2000000 [00:38<00:13, 38861.48it/s]\n 74%|#######3  | 1470859/2000000 [00:38<00:13, 38917.46it/s]\n 74%|#######3  | 1474774/2000000 [00:38<00:13, 38986.25it/s]\n 74%|#######3  | 1478697/2000000 [00:38<00:13, 39051.77it/s]\n 74%|#######4  | 1482606/2000000 [00:38<00:13, 39061.79it/s]\n 74%|#######4  | 1486513/2000000 [00:38<00:13, 38993.97it/s]\n 75%|#######4  | 1490413/2000000 [00:38<00:13, 38863.89it/s]\n 75%|#######4  | 1494302/2000000 [00:38<00:13, 38871.12it/s]\n 75%|#######4  | 1498215/2000000 [00:39<00:12, 38944.03it/s]\n 75%|#######5  | 1502126/2000000 [00:39<00:12, 38993.19it/s]\n 75%|#######5  | 1506033/2000000 [00:39<00:12, 39012.67it/s]\n 75%|#######5  | 1509941/2000000 [00:39<00:12, 39030.74it/s]\n 76%|#######5  | 1513853/2000000 [00:39<00:12, 39051.96it/s]\n 76%|#######5  | 1517759/2000000 [00:39<00:12, 39051.90it/s]\n 76%|#######6  | 1521665/2000000 [00:39<00:12, 39010.19it/s]\n 76%|#######6  | 1525567/2000000 [00:39<00:12, 38923.37it/s]\n 76%|#######6  | 1529466/2000000 [00:39<00:12, 38941.46it/s]\n 77%|#######6  | 1533366/2000000 [00:39<00:11, 38956.70it/s]\n 77%|#######6  | 1537282/2000000 [00:40<00:11, 39012.52it/s]\n 77%|#######7  | 1541185/2000000 [00:40<00:11, 39007.04it/s]\n 77%|#######7  | 1545086/2000000 [00:40<00:11, 38995.76it/s]\n 77%|#######7  | 1548986/2000000 [00:40<00:11, 38995.34it/s]\n 78%|#######7  | 1552888/2000000 [00:40<00:11, 39001.52it/s]\n 78%|#######7  | 1556789/2000000 [00:40<00:11, 38985.65it/s]\n 78%|#######8  | 1560688/2000000 [00:40<00:11, 38950.35it/s]\n 78%|#######8  | 1564588/2000000 [00:40<00:11, 38964.27it/s]\n 78%|#######8  | 1568511/2000000 [00:40<00:11, 39041.90it/s]\n 79%|#######8  | 1572430/2000000 [00:40<00:10, 39084.67it/s]\n 79%|#######8  | 1576349/2000000 [00:41<00:10, 39105.89it/s]\n 79%|#######9  | 1580297/2000000 [00:41<00:10, 39214.27it/s]\n 79%|#######9  | 1584227/2000000 [00:41<00:10, 39237.91it/s]\n 79%|#######9  | 1588151/2000000 [00:41<00:10, 39183.75it/s]\n 80%|#######9  | 1592073/2000000 [00:41<00:10, 39187.79it/s]\n 80%|#######9  | 1595992/2000000 [00:41<00:10, 39069.98it/s]\n 80%|#######9  | 1599900/2000000 [00:41<00:10, 39010.72it/s]\n 80%|########  | 1603814/2000000 [00:41<00:10, 39041.57it/s]\n 80%|########  | 1607719/2000000 [00:41<00:10, 39013.95it/s]\n 81%|########  | 1611625/2000000 [00:41<00:09, 39020.70it/s]\n 81%|########  | 1615562/2000000 [00:42<00:09, 39114.63it/s]\n 81%|########  | 1619496/2000000 [00:42<00:09, 39179.62it/s]\n 81%|########1 | 1623423/2000000 [00:42<00:09, 39198.34it/s]\n 81%|########1 | 1627343/2000000 [00:42<00:09, 39177.82it/s]\n 82%|########1 | 1631261/2000000 [00:42<00:09, 39132.77it/s]\n 82%|########1 | 1635175/2000000 [00:42<00:09, 39133.02it/s]\n 82%|########1 | 1639089/2000000 [00:42<00:09, 39077.10it/s]\n 82%|########2 | 1642997/2000000 [00:42<00:09, 39029.05it/s]\n 82%|########2 | 1646900/2000000 [00:42<00:09, 39023.02it/s]\n 83%|########2 | 1650811/2000000 [00:42<00:08, 39043.54it/s]\n 83%|########2 | 1654730/2000000 [00:43<00:08, 39084.72it/s]\n 83%|########2 | 1658660/2000000 [00:43<00:08, 39148.22it/s]\n 83%|########3 | 1662575/2000000 [00:43<00:08, 38696.04it/s]\n 83%|########3 | 1666495/2000000 [00:43<00:08, 38844.59it/s]\n 84%|########3 | 1670394/2000000 [00:43<00:08, 38887.49it/s]\n 84%|########3 | 1674284/2000000 [00:43<00:08, 38847.47it/s]\n 84%|########3 | 1678170/2000000 [00:43<00:08, 38838.69it/s]\n 84%|########4 | 1682059/2000000 [00:43<00:08, 38850.14it/s]\n 84%|########4 | 1685967/2000000 [00:43<00:08, 38918.16it/s]\n 84%|########4 | 1689891/2000000 [00:43<00:07, 39010.04it/s]\n 85%|########4 | 1693793/2000000 [00:44<00:07, 39006.71it/s]\n 85%|########4 | 1697714/2000000 [00:44<00:07, 39058.44it/s]\n 85%|########5 | 1701659/2000000 [00:44<00:07, 39166.68it/s]\n 85%|########5 | 1705586/2000000 [00:44<00:07, 39187.44it/s]\n 85%|########5 | 1709505/2000000 [00:44<00:07, 39136.77it/s]\n 86%|########5 | 1713419/2000000 [00:44<00:07, 38837.88it/s]\n 86%|########5 | 1717304/2000000 [00:44<00:07, 38184.94it/s]\n 86%|########6 | 1721125/2000000 [00:44<00:07, 37982.14it/s]\n 86%|########6 | 1724925/2000000 [00:44<00:07, 37801.64it/s]\n 86%|########6 | 1728707/2000000 [00:44<00:07, 37721.78it/s]\n 87%|########6 | 1732480/2000000 [00:45<00:07, 37706.13it/s]\n 87%|########6 | 1736305/2000000 [00:45<00:06, 37862.23it/s]\n 87%|########7 | 1740147/2000000 [00:45<00:06, 38021.48it/s]\n 87%|########7 | 1744108/2000000 [00:45<00:06, 38495.37it/s]\n 87%|########7 | 1748051/2000000 [00:45<00:06, 38772.51it/s]\n 88%|########7 | 1751974/2000000 [00:45<00:06, 38903.00it/s]\n 88%|########7 | 1755898/2000000 [00:45<00:06, 39003.33it/s]\n 88%|########7 | 1759861/2000000 [00:45<00:06, 39190.67it/s]\n 88%|########8 | 1763815/2000000 [00:45<00:06, 39286.83it/s]\n 88%|########8 | 1767744/2000000 [00:45<00:05, 39267.89it/s]\n 89%|########8 | 1771671/2000000 [00:46<00:05, 39209.78it/s]\n 89%|########8 | 1775610/2000000 [00:46<00:05, 39263.09it/s]\n 89%|########8 | 1779566/2000000 [00:46<00:05, 39351.54it/s]\n 89%|########9 | 1783502/2000000 [00:46<00:05, 39316.63it/s]\n 89%|########9 | 1787434/2000000 [00:46<00:05, 39296.85it/s]\n 90%|########9 | 1791364/2000000 [00:46<00:05, 39268.29it/s]\n 90%|########9 | 1795291/2000000 [00:46<00:05, 39265.09it/s]\n 90%|########9 | 1799218/2000000 [00:46<00:05, 39236.91it/s]\n 90%|######### | 1803142/2000000 [00:46<00:05, 39211.71it/s]\n 90%|######### | 1807064/2000000 [00:46<00:04, 39188.93it/s]\n 91%|######### | 1810983/2000000 [00:47<00:04, 39124.71it/s]\n 91%|######### | 1814896/2000000 [00:47<00:04, 39063.21it/s]\n 91%|######### | 1818803/2000000 [00:47<00:04, 38999.07it/s]\n 91%|#########1| 1822703/2000000 [00:47<00:04, 38897.37it/s]\n 91%|#########1| 1826593/2000000 [00:47<00:04, 38872.02it/s]\n 92%|#########1| 1830492/2000000 [00:47<00:04, 38897.36it/s]\n 92%|#########1| 1834389/2000000 [00:47<00:04, 38911.50it/s]\n 92%|#########1| 1838307/2000000 [00:47<00:04, 38980.95it/s]\n 92%|#########2| 1842237/2000000 [00:47<00:04, 39069.11it/s]\n 92%|#########2| 1846170/2000000 [00:47<00:03, 39139.94it/s]\n 93%|#########2| 1850111/2000000 [00:48<00:03, 39217.70it/s]\n 93%|#########2| 1854051/2000000 [00:48<00:03, 39268.01it/s]\n 93%|#########2| 1857978/2000000 [00:48<00:03, 39185.62it/s]\n 93%|#########3| 1861922/2000000 [00:48<00:03, 39251.29it/s]\n 93%|#########3| 1865848/2000000 [00:48<00:03, 39206.20it/s]\n 93%|#########3| 1869769/2000000 [00:48<00:03, 39040.93it/s]\n 94%|#########3| 1873674/2000000 [00:48<00:03, 38920.67it/s]\n 94%|#########3| 1877569/2000000 [00:48<00:03, 38918.85it/s]\n 94%|#########4| 1881461/2000000 [00:48<00:03, 38915.27it/s]\n 94%|#########4| 1885353/2000000 [00:48<00:02, 38874.24it/s]\n 94%|#########4| 1889284/2000000 [00:49<00:02, 38993.59it/s]\n 95%|#########4| 1893207/2000000 [00:49<00:02, 39055.06it/s]\n 95%|#########4| 1897126/2000000 [00:49<00:02, 39086.03it/s]\n 95%|#########5| 1901052/2000000 [00:49<00:02, 39128.18it/s]\n 95%|#########5| 1904985/2000000 [00:49<00:02, 39179.02it/s]\n 95%|#########5| 1908915/2000000 [00:49<00:02, 39204.98it/s]\n 96%|#########5| 1912836/2000000 [00:49<00:02, 39111.27it/s]\n 96%|#########5| 1916748/2000000 [00:49<00:02, 38987.72it/s]\n 96%|#########6| 1920664/2000000 [00:49<00:02, 39033.15it/s]\n 96%|#########6| 1924575/2000000 [00:49<00:01, 39053.06it/s]\n 96%|#########6| 1928487/2000000 [00:50<00:01, 39070.52it/s]\n 97%|#########6| 1932395/2000000 [00:50<00:01, 38975.19it/s]\n 97%|#########6| 1936293/2000000 [00:50<00:01, 38887.91it/s]\n 97%|#########7| 1940206/2000000 [00:50<00:01, 38953.74it/s]\n 97%|#########7| 1944115/2000000 [00:50<00:01, 38991.21it/s]\n 97%|#########7| 1948034/2000000 [00:50<00:01, 39044.75it/s]\n 98%|#########7| 1951966/2000000 [00:50<00:01, 39115.12it/s]\n 98%|#########7| 1955904/2000000 [00:50<00:01, 39189.97it/s]\n 98%|#########7| 1959845/2000000 [00:50<00:01, 39249.36it/s]\n 98%|#########8| 1963790/2000000 [00:50<00:00, 39306.06it/s]\n 98%|#########8| 1967734/2000000 [00:51<00:00, 39340.83it/s]\n 99%|#########8| 1971671/2000000 [00:51<00:00, 39343.54it/s]\n 99%|#########8| 1975609/2000000 [00:51<00:00, 39345.32it/s]\n 99%|#########8| 1979544/2000000 [00:51<00:00, 39333.71it/s]\n 99%|#########9| 1983483/2000000 [00:51<00:00, 39344.89it/s]\n 99%|#########9| 1987418/2000000 [00:51<00:00, 39235.08it/s]\n100%|#########9| 1991342/2000000 [00:51<00:00, 39172.89it/s]\n100%|#########9| 1995260/2000000 [00:51<00:00, 39047.69it/s]\n100%|#########9| 1999165/2000000 [00:51<00:00, 38943.24it/s]\n100%|##########| 2000000/2000000 [00:51<00:00, 38532.80it/s]\n2024-08-22 05:03:22,870 - INFO - allennlp.modules.token_embedders.embedding - Initializing pre-trained embedding layer\n2024-08-22 05:03:22,883 - INFO - allennlp.modules.token_embedders.embedding - Pretrained embeddings were found for 186 out of 192 tokens\n/usr/local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n  warnings.warn(\"dropout option adds dropout after all but last \"\n\nDownloading:   0%|                                   | 0.00/422M [00:00<?, ?B/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nVocabulary with namespaces:\n \tNon Padded Namespaces: {'*labels', '*tags'}\n \tNamespace: fasttext, Size: 192 \n \tNamespace: token_characters, Size: 62 \n \tNamespace: case, Size: 6 \n \tNamespace: xpos, Size: 38 \n \tNamespace: depchunk, Size: 20 \n \tNamespace: deprel, Size: 29 \n \tNamespace: genre, Size: 3 \n \tNamespace: parent_clauses, Size: 17 \n \tNamespace: upos, Size: 16 \n \tNamespace: s_type, Size: 3 \n \tNamespace: labels, Size: 2 \n\ncase\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: 'l', 3: 't', 4: 'o', 5: 'd'}\nxpos\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: 'NN', 3: 'ART', 4: 'ADV', 5: '$.', 6: 'APPR', 7: 'ADJA', 8: 'VAFIN', 9: 'ADJD', 10: '$,', 11: 'VVFIN', 12: 'VVPP', 13: 'VVINF', 14: 'NE', 15: 'PPER', 16: 'KON', 17: 'VMFIN', 18: 'PIS', 19: 'PROAV', 20: 'KOUS', 21: 'PIAT', 22: 'PRELS', 23: 'KOKOM', 24: 'PTKZU', 25: 'VAINF', 26: 'PWS', 27: 'PTKVZ', 28: 'APPRART', 29: 'VMINF', 30: 'CARD', 31: 'TRUNC', 32: 'PWAV', 33: 'PTKA', 34: 'PRF', 35: 'PDS', 36: 'PTKNEG', 37: 'PPOSAT'}\ndepchunk\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: '_', 3: 'Iacl', 4: 'Iadvcl', 5: 'Iccomp', 6: 'Icsubj', 7: 'Ixcomp', 8: 'Bacl', 9: 'Eacl', 10: 'Bxcomp', 11: 'Excomp', 12: 'Badvcl', 13: 'Bccomp', 14: 'Eccomp', 15: 'Bappos', 16: 'Eadvcl', 17: 'Bcsubj', 18: 'Ecsubj', 19: 'Eappos'}\ndeprel\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: 'advmod', 3: 'punct', 4: 'det', 5: 'nsubj', 6: 'case', 7: 'root', 8: 'obl', 9: 'nmod', 10: 'aux', 11: 'obj', 12: 'amod', 13: 'cc', 14: 'mark', 15: 'conj', 16: 'aux:pass', 17: 'nsubj:pass', 18: 'acl', 19: 'cop', 20: 'expl', 21: 'flat', 22: 'xcomp', 23: 'advcl', 24: 'ccomp', 25: 'compound:prt', 26: 'appos', 27: 'csubj:pass', 28: 'det:poss'}\ngenre\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: '_'}\nparent_clauses\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: '', 3: 'ccomp', 4: 'csubj:pass', 5: 'acl', 6: 'advcl|conj|acl', 7: 'advcl', 8: 'acl|xcomp', 9: 'xcomp|conj', 10: 'acl|ccomp', 11: 'conj', 12: 'conj|acl', 13: 'xcomp', 14: 'conj|advcl', 15: 'conj|acl|xcomp', 16: 'appos'}\nupos\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: 'NOUN', 3: 'VERB', 4: 'ADV', 5: 'PUNCT', 6: 'DET', 7: 'ADP', 8: 'PRON', 9: 'ADJ', 10: 'AUX', 11: 'PROPN', 12: 'CCONJ', 13: 'SCONJ', 14: 'PART', 15: 'NUM'}\ns_type\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: 'NoQ_NoSubj_NoConj_NoCop_NoAdvcl_NoAcl'}\nlabels\n  {0: 'O', 1: 'B'}\nDisrpt2021Baseline(\n  (feature_modules): ModuleDict(\n    (case_tags): Embedding(6, 3, padding_idx=0)\n    (cpos_tags): Embedding(38, 7, padding_idx=0)\n    (dep_chunk_tags): Embedding(20, 5, padding_idx=0)\n    (deprel_tags): Embedding(29, 6, padding_idx=0)\n    (document_depth): Identity()\n    (genre_tags): Embedding(3, 2, padding_idx=0)\n    (head_distances): Identity()\n    (parent_clauses): Embedding(17, 5, padding_idx=0)\n    (pos_tags): Embedding(16, 4, padding_idx=0)\n    (s_type): Embedding(3, 2, padding_idx=0)\n    (sentence_length): Identity()\n    (token_lengths): Identity()\n  )\n  (embedder): BasicTextFieldEmbedder(\n    (token_embedder_fasttext): Embedding()\n    (token_embedder_token_characters): TokenCharactersEncoder(\n      (_embedding): TimeDistributed(\n        (_module): Embedding()\n      )\n      (_encoder): TimeDistributed(\n        (_module): LstmSeq2VecEncoder(\n          (_module): LSTM(32, 64, batch_first=True, dropout=0.2, bidirectional=True)\n        )\n      )\n    )\n    (token_embedder_tokens): PretrainedTransformerMismatchedEmbedder(\n      (_matched_embedder): PretrainedTransformerEmbedder(\n        (transformer_model): BertModel(\n          (embeddings): BertEmbeddings(\n            (word_embeddings): Embedding(31102, 768, padding_idx=0)\n            (position_embeddings): Embedding(512, 768)\n            (token_type_embeddings): Embedding(2, 768)\n          )\n          (encoder): BertEncoder(\n            (layer): ModuleList(\n              (0): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (1): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (2): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (3): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (4): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (5): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (6): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (7): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (8): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (9): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (10): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (11): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n            )\n          )\n          (pooler): BertPooler(\n            (activation): Tanh()\n          )\n        )\n      )\n    )\n  )\n  (encoder): PytorchSeq2SeqWrapper(\n    (_module): StackedBidirectionalLstm(\n      (forward_layer_0): AugmentedLstm(\n        (cell): AugmentedLSTMCell(\n        )\n      )\n      (backward_layer_0): AugmentedLstm(\n        (cell): AugmentedLSTMCell(\n        )\n      )\n    )\n  )\n  (prev_sentence_encoder): LstmSeq2VecEncoder(\n    (_module): LSTM(1196, 100, batch_first=True, dropout=0.2, bidirectional=True)\n  )\n  (next_sentence_encoder): LstmSeq2VecEncoder(\n    (_module): LSTM(1196, 100, batch_first=True, dropout=0.2, bidirectional=True)\n  )\n  (label_projection_layer): TimeDistributed(\n  )\n)\n2024-08-22 05:03:42,593 - INFO - allennlp.training.optimizers - Done constructing parameter groups.\n2024-08-22 05:03:42,594 - INFO - allennlp.training.optimizers - Group 1: ['encoder._module.backward_layer_0.cell.input_linearity.weight', 'embedder.token_embedder_token_characters._encoder._module._module.weight_hh_l0_reverse', 'next_sentence_encoder._module.bias_ih_l0', 'prev_sentence_encoder._module.bias_hh_l0', 'feature_modules.case_tags.weight', 'encoder._module.forward_layer_0.cell.input_linearity.weight', 'prev_sentence_encoder._module.weight_ih_l0_reverse', 'embedder.token_embedder_token_characters._encoder._module._module.weight_ih_l0', 'next_sentence_encoder._module.weight_hh_l0_reverse', 'label_projection_layer._module.bias', 'embedder.token_embedder_token_characters._encoder._module._module.weight_hh_l0', 'prev_sentence_encoder._module.weight_hh_l0', 'prev_sentence_encoder._module.weight_ih_l0', 'embedder.token_embedder_token_characters._encoder._module._module.bias_ih_l0', 'feature_modules.genre_tags.weight', 'next_sentence_encoder._module.bias_hh_l0_reverse', 'feature_modules.cpos_tags.weight', 'embedder.token_embedder_token_characters._encoder._module._module.bias_hh_l0', 'feature_modules.deprel_tags.weight', 'next_sentence_encoder._module.weight_ih_l0', 'label_projection_layer._module.weight', 'feature_modules.pos_tags.weight', 'next_sentence_encoder._module.weight_ih_l0_reverse', 'prev_sentence_encoder._module.bias_ih_l0_reverse', 'next_sentence_encoder._module.bias_ih_l0_reverse', 'feature_modules.parent_clauses.weight', 'embedder.token_embedder_token_characters._embedding._module.weight', 'prev_sentence_encoder._module.bias_ih_l0', 'encoder._module.backward_layer_0.cell.state_linearity.weight', 'embedder.token_embedder_token_characters._encoder._module._module.bias_ih_l0_reverse', 'encoder._module.forward_layer_0.cell.state_linearity.weight', 'encoder._module.forward_layer_0.cell.state_linearity.bias', 'embedder.token_embedder_token_characters._encoder._module._module.bias_hh_l0_reverse', 'next_sentence_encoder._module.weight_hh_l0', 'prev_sentence_encoder._module.weight_hh_l0_reverse', 'encoder._module.backward_layer_0.cell.state_linearity.bias', 'prev_sentence_encoder._module.bias_hh_l0_reverse', 'feature_modules.dep_chunk_tags.weight', 'embedder.token_embedder_token_characters._encoder._module._module.weight_ih_l0_reverse', 'next_sentence_encoder._module.bias_hh_l0', 'feature_modules.s_type.weight'], {}\n2024-08-22 05:03:42,594 - INFO - allennlp.training.optimizers - Number of trainable parameters: 124725265\n/usr/local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n2024-08-22 05:03:42,620 - INFO - allennlp.training.gradient_descent_trainer - Beginning training.\n2024-08-22 05:03:42,620 - INFO - allennlp.training.gradient_descent_trainer - Epoch 0/59\n2024-08-22 05:03:42,620 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 2.8G\n2024-08-22 05:03:42,621 - INFO - allennlp.training.gradient_descent_trainer - Training\n\n  0%|          | 0/3 [00:00<?, ?it/s]/usr/local/lib/python3.10/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n2024-08-22 05:03:48,817 - INFO - allennlp.training.callbacks.console_logger - Batch inputs\n2024-08-22 05:03:48,817 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/fasttext/tokens (Shape: 4 x 25)\ntensor([[ 91,  10,  92,  ..., 104, 105,   2],\n        [ 20,  66,  67,  ...,   2,   0,   0],\n        [ 54,   3,  55,  ...,   0,   0,   0],\n        [ 36,  37,  38,  ...,   0,   0,   0]])\n2024-08-22 05:03:48,819 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/token_characters/token_characters (Shape: 4 x 25 x 21)\ntensor([[[31, 15,  0,  ...,  0,  0,  0],\n         [ 5,  4,  8,  ...,  0,  0,  0],\n         [ 2,  8, 17,  ...,  0,  0,  0],\n         ...,\n         [24, 13,  4,  ...,  0,  0,  0],\n         [ 3, 13,  8,  ...,  0,  0,  0],\n         [20,  0,  0,  ...,  0,  0,  0]],\n\n        [[30,  4,  0,  ...,  0,  0,  0],\n         [10, 26,  6,  ...,  0,  0,  0],\n         [10,  2,  6,  ...,  0,  0,  0],\n         ...,\n         [20,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[57, 19,  2,  ...,  0,  0,  0],\n         [23,  0,  0,  ...,  0,  0,  0],\n         [17,  2,  5,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[28,  7, 11,  ...,  0,  0,  0],\n         [47,  5,  2,  ...,  0,  0,  0],\n         [ 4,  5,  8,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]]])\n2024-08-22 05:03:48,822 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/token_ids (Shape: 4 x 31)\ntensor([[  102,   224,   207,  ...,  3682,   552,   103],\n        [  102,   233,  9647,  ...,     0,     0,     0],\n        [  102, 24026, 12979,  ...,     0,     0,     0],\n        [  102, 19590,   906,  ...,     0,     0,     0]])\n2024-08-22 05:03:48,823 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/mask (Shape: 4 x 25)\ntensor([[ True,  True,  True,  ...,  True,  True,  True],\n        [ True,  True,  True,  ...,  True, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False]])\n2024-08-22 05:03:48,824 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/type_ids (Shape: 4 x 31)\ntensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]])\n2024-08-22 05:03:48,824 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/wordpiece_mask (Shape: 4 x 31)\ntensor([[ True,  True,  True,  ...,  True,  True,  True],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False]])\n2024-08-22 05:03:48,825 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/offsets (Shape: 4 x 25 x 2)\ntensor([[[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [26, 27],\n         [28, 28],\n         [29, 29]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [26, 26],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  4],\n         [ 5,  5],\n         [ 6,  6],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  2],\n         [ 3,  4],\n         [ 5,  5],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]]])\n2024-08-22 05:03:48,826 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/fasttext/tokens (Shape: 4 x 31)\ntensor([[86, 32, 33,  ...,  0,  0,  0],\n        [19, 62,  4,  ...,  0,  0,  0],\n        [12, 40,  4,  ...,  8, 53,  2],\n        [12, 27, 28,  ...,  0,  0,  0]])\n2024-08-22 05:03:48,827 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/token_characters/token_characters (Shape: 4 x 31 x 16)\ntensor([[[36,  7,  8,  ...,  0,  0,  0],\n         [11,  5, 19,  ...,  0,  0,  0],\n         [ 2,  4,  0,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[28,  2,  6,  ...,  0,  0,  0],\n         [29, 26, 14,  ...,  0,  0,  0],\n         [10,  2,  6,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[24, 13, 21,  ...,  0,  0,  0],\n         [40,  6, 13,  ...,  0,  0,  0],\n         [10,  2,  6,  ...,  0,  0,  0],\n         ...,\n         [18, 13,  0,  ...,  0,  0,  0],\n         [ 2,  3,  8,  ...,  0,  0,  0],\n         [20,  0,  0,  ...,  0,  0,  0]],\n\n        [[24, 13, 21,  ...,  0,  0,  0],\n         [30,  5,  4,  ...,  0,  0,  0],\n         [11,  2, 12,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]]])\n2024-08-22 05:03:48,830 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/token_ids (Shape: 4 x 41)\ntensor([[  102,  2472,   175,  ...,     0,     0,     0],\n        [  102,   127, 22026,  ...,     0,     0,     0],\n        [  102,   193,   854,  ...,  3432,   552,   103],\n        [  102,   193,  3029,  ...,     0,     0,     0]])\n2024-08-22 05:03:48,831 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/mask (Shape: 4 x 31)\ntensor([[ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ...,  True,  True,  True],\n        [ True,  True,  True,  ..., False, False, False]])\n2024-08-22 05:03:48,831 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/type_ids (Shape: 4 x 41)\ntensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]])\n2024-08-22 05:03:48,832 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/wordpiece_mask (Shape: 4 x 41)\ntensor([[ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ...,  True,  True,  True],\n        [ True,  True,  True,  ..., False, False, False]])\n2024-08-22 05:03:48,833 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/offsets (Shape: 4 x 31 x 2)\ntensor([[[ 1,  3],\n         [ 4,  4],\n         [ 5,  5],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  3],\n         [ 4,  4],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [37, 37],\n         [38, 38],\n         [39, 39]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]]])\n2024-08-22 05:03:48,834 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/fasttext/tokens (Shape: 4 x 31)\ntensor([[169,  32,  33,  ...,   0,   0,   0],\n        [ 31,  80,  22,  ...,   0,   0,   0],\n        [ 19,  62,   4,  ...,   0,   0,   0],\n        [ 12,  40,   4,  ...,   8,  53,   2]])\n2024-08-22 05:03:48,835 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/token_characters/token_characters (Shape: 4 x 31 x 16)\ntensor([[[28,  2,  3,  ...,  0,  0,  0],\n         [11,  5, 19,  ...,  0,  0,  0],\n         [ 2,  4,  0,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[29,  2,  5,  ...,  0,  0,  0],\n         [35,  5,  3,  ..., 11,  2,  3],\n         [17,  2,  6,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[28,  2,  6,  ...,  0,  0,  0],\n         [29, 26, 14,  ...,  0,  0,  0],\n         [10,  2,  6,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[24, 13, 21,  ...,  0,  0,  0],\n         [40,  6, 13,  ...,  0,  0,  0],\n         [10,  2,  6,  ...,  0,  0,  0],\n         ...,\n         [18, 13,  0,  ...,  0,  0,  0],\n         [ 2,  3,  8,  ...,  0,  0,  0],\n         [20,  0,  0,  ...,  0,  0,  0]]])\n2024-08-22 05:03:48,838 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/token_ids (Shape: 4 x 41)\ntensor([[  102,  4041,   773,  ...,     0,     0,     0],\n        [  102, 14216,  2672,  ...,     0,     0,     0],\n        [  102,   127, 22026,  ...,     0,     0,     0],\n        [  102,   193,   854,  ...,  3432,   552,   103]])\n2024-08-22 05:03:48,838 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/mask (Shape: 4 x 31)\ntensor([[ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ...,  True,  True,  True]])\n2024-08-22 05:03:48,839 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/type_ids (Shape: 4 x 41)\ntensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]])\n2024-08-22 05:03:48,840 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/wordpiece_mask (Shape: 4 x 41)\ntensor([[ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ...,  True,  True,  True]])\n2024-08-22 05:03:48,841 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/offsets (Shape: 4 x 31 x 2)\ntensor([[[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  3],\n         [ 4,  4],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  3],\n         [ 4,  4],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [37, 37],\n         [38, 38],\n         [39, 39]]])\n2024-08-22 05:03:48,842 - INFO - allennlp.training.callbacks.console_logger - Field : \"batch_input/sentence_tokens\" : (Length 4 of type \"<class 'list'>\")\n2024-08-22 05:03:48,842 - INFO - allennlp.training.callbacks.console_logger - batch_input/case_tags (Shape: 4 x 25)\ntensor([[3, 2, 2,  ..., 3, 2, 4],\n        [3, 2, 2,  ..., 4, 0, 0],\n        [3, 4, 2,  ..., 0, 0, 0],\n        [3, 3, 2,  ..., 0, 0, 0]])\n2024-08-22 05:03:48,843 - INFO - allennlp.training.callbacks.console_logger - batch_input/cpos_tags (Shape: 4 x 25)\ntensor([[ 4,  8,  4,  ...,  2, 11,  5],\n        [15, 17,  4,  ...,  5,  0,  0],\n        [ 9, 10, 20,  ...,  0,  0,  0],\n        [14, 14, 11,  ...,  0,  0,  0]])\n2024-08-22 05:03:48,843 - INFO - allennlp.training.callbacks.console_logger - batch_input/dep_chunk_tags (Shape: 4 x 25)\ntensor([[ 2,  2,  2,  ...,  4,  9,  2],\n        [ 2,  2,  2,  ...,  2,  0,  0],\n        [ 2, 12,  4,  ...,  0,  0,  0],\n        [ 2,  2,  2,  ...,  0,  0,  0]])\n2024-08-22 05:03:48,844 - INFO - allennlp.training.callbacks.console_logger - batch_input/deprel_tags (Shape: 4 x 25)\ntensor([[ 2, 19,  2,  ...,  9, 23,  3],\n        [20, 10,  2,  ...,  3,  0,  0],\n        [ 7,  3, 14,  ...,  0,  0,  0],\n        [ 5, 21,  7,  ...,  0,  0,  0]])\n2024-08-22 05:03:48,845 - INFO - allennlp.training.callbacks.console_logger - batch_input/document_depth (Shape: 4 x 25)\ntensor([[0.9000, 0.9000, 0.9000,  ..., 0.9000, 0.9000, 0.9000],\n        [0.8571, 0.8571, 0.8571,  ..., 0.8571, 0.0000, 0.0000],\n        [0.8000, 0.8000, 0.8000,  ..., 0.0000, 0.0000, 0.0000],\n        [0.6667, 0.6667, 0.6667,  ..., 0.0000, 0.0000, 0.0000]])\n2024-08-22 05:03:48,848 - INFO - allennlp.training.callbacks.console_logger - batch_input/genre_tags (Shape: 4 x 25)\ntensor([[2, 2, 2,  ..., 2, 2, 2],\n        [2, 2, 2,  ..., 2, 0, 0],\n        [2, 2, 2,  ..., 0, 0, 0],\n        [2, 2, 2,  ..., 0, 0, 0]])\n2024-08-22 05:03:48,848 - INFO - allennlp.training.callbacks.console_logger - batch_input/head_distances (Shape: 4 x 25)\ntensor([[1.0986, 0.6931, 0.0000,  ..., 1.0986, 2.3979, 3.0445],\n        [1.6094, 1.3863, 1.0986,  ..., 2.8332, 0.0000, 0.0000],\n        [0.0000, 2.3026, 2.1972,  ..., 0.0000, 0.0000, 0.0000],\n        [0.6931, 0.0000, 1.0986,  ..., 0.0000, 0.0000, 0.0000]])\n2024-08-22 05:03:48,849 - INFO - allennlp.training.callbacks.console_logger - batch_input/parent_clauses (Shape: 4 x 25)\ntensor([[ 2,  2,  2,  ...,  6, 12,  2],\n        [ 2,  2,  2,  ...,  2,  0,  0],\n        [ 2,  7,  7,  ...,  0,  0,  0],\n        [ 2,  2,  2,  ...,  0,  0,  0]])\n2024-08-22 05:03:48,850 - INFO - allennlp.training.callbacks.console_logger - batch_input/pos_tags (Shape: 4 x 25)\ntensor([[ 4,  3,  4,  ...,  2,  3,  5],\n        [ 8, 10,  4,  ...,  5,  0,  0],\n        [ 9,  5, 13,  ...,  0,  0,  0],\n        [11, 11,  3,  ...,  0,  0,  0]])\n2024-08-22 05:03:48,851 - INFO - allennlp.training.callbacks.console_logger - batch_input/s_type (Shape: 4 x 25)\ntensor([[2, 2, 2,  ..., 2, 2, 2],\n        [2, 2, 2,  ..., 2, 0, 0],\n        [2, 2, 2,  ..., 0, 0, 0],\n        [2, 2, 2,  ..., 0, 0, 0]])\n2024-08-22 05:03:48,851 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence_length (Shape: 4 x 25)\ntensor([[3.2189, 3.2189, 3.2189,  ..., 3.2189, 3.2189, 3.2189],\n        [3.1355, 3.1355, 3.1355,  ..., 3.1355, 0.0000, 0.0000],\n        [2.6391, 2.6391, 2.6391,  ..., 0.0000, 0.0000, 0.0000],\n        [1.9459, 1.9459, 1.9459,  ..., 0.0000, 0.0000, 0.0000]])\n2024-08-22 05:03:48,852 - INFO - allennlp.training.callbacks.console_logger - batch_input/token_lengths (Shape: 4 x 25)\ntensor([[0.6931, 1.0986, 1.3863,  ..., 2.1972, 1.7918, 0.0000],\n        [0.6931, 1.7918, 1.9459,  ..., 0.0000, 0.0000, 0.0000],\n        [2.4849, 0.0000, 1.3863,  ..., 0.0000, 0.0000, 0.0000],\n        [1.7918, 1.9459, 1.6094,  ..., 0.0000, 0.0000, 0.0000]])\n2024-08-22 05:03:48,853 - INFO - allennlp.training.callbacks.console_logger - batch_input/labels (Shape: 4 x 25)\ntensor([[1, 0, 0,  ..., 0, 0, 0],\n        [1, 0, 0,  ..., 0, 0, 0],\n        [1, 0, 1,  ..., 0, 0, 0],\n        [1, 0, 0,  ..., 0, 0, 0]])\n\ntag_accuracy: 0.4203, span_precision: 0.1489, span_recall: 1.0000, span_f1: 0.2593, batch_loss: 0.7650, loss: 0.7650 ||:  33%|###3      | 1/3 [00:06<00:12,  6.23s/it]\ntag_accuracy: 0.6400, span_precision: 0.1667, span_recall: 0.6154, span_f1: 0.2623, batch_loss: 0.3780, loss: 0.5715 ||:  67%|######6   | 2/3 [00:11<00:05,  5.70s/it]\ntag_accuracy: 0.6713, span_precision: 0.1667, span_recall: 0.5333, span_f1: 0.2540, batch_loss: 0.3870, loss: 0.5100 ||: 100%|##########| 3/3 [00:14<00:00,  4.48s/it]\ntag_accuracy: 0.6713, span_precision: 0.1667, span_recall: 0.5333, span_f1: 0.2540, batch_loss: 0.3870, loss: 0.5100 ||: 100%|##########| 3/3 [00:14<00:00,  4.87s/it]\n2024-08-22 05:03:57,227 - INFO - allennlp.training.gradient_descent_trainer - Validating\n\n  0%|          | 0/3 [00:00<?, ?it/s]2024-08-22 05:03:58,061 - INFO - allennlp.training.callbacks.console_logger - Batch inputs\n2024-08-22 05:03:58,061 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/fasttext/tokens (Shape: 4 x 24)\ntensor([[176, 177, 178,  ..., 190, 191,   2],\n        [ 19, 146, 147,  ...,   0,   0,   0],\n        [ 12, 156, 157,  ...,   0,   0,   0],\n        [166, 167, 168,  ...,   0,   0,   0]])\n2024-08-22 05:03:58,062 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/token_characters/token_characters (Shape: 4 x 24 x 14)\ntensor([[[34,  3, 10,  ...,  0,  0,  0],\n         [ 4, 15,  0,  ...,  0,  0,  0],\n         [16, 13,  4,  ...,  0,  0,  0],\n         ...,\n         [41, 15, 12,  ...,  0,  0,  0],\n         [ 7,  3, 11,  ...,  0,  0,  0],\n         [20,  0,  0,  ...,  0,  0,  0]],\n\n        [[28,  2,  6,  ...,  0,  0,  0],\n         [18, 13,  4,  ...,  0,  0,  0],\n         [31,  8,  7,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[24, 13, 21,  ...,  0,  0,  0],\n         [10,  2, 16,  ...,  0,  0,  0],\n         [33,  7,  9,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[28,  7,  4,  ...,  0,  0,  0],\n         [ 6,  2,  5,  ...,  0,  0,  0],\n         [ 3,  5, 14,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]]])\n2024-08-22 05:03:58,066 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/token_ids (Shape: 4 x 29)\ntensor([[  102,   143,   224,  ..., 14762,   552,   103],\n        [  102,   127,  6218,  ...,     0,     0,     0],\n        [  102,   193,   232,  ...,     0,     0,     0],\n        [  102,   173,  5094,  ...,     0,     0,     0]])\n2024-08-22 05:03:58,067 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/mask (Shape: 4 x 24)\ntensor([[ True,  True,  True,  ...,  True,  True,  True],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False]])\n2024-08-22 05:03:58,068 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/type_ids (Shape: 4 x 29)\ntensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]])\n2024-08-22 05:03:58,069 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/wordpiece_mask (Shape: 4 x 29)\ntensor([[ True,  True,  True,  ...,  True,  True,  True],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False]])\n2024-08-22 05:03:58,070 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/offsets (Shape: 4 x 24 x 2)\ntensor([[[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [25, 25],\n         [26, 26],\n         [27, 27]],\n\n        [[ 1,  1],\n         [ 2,  3],\n         [ 4,  4],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]]])\n2024-08-22 05:03:58,071 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/fasttext/tokens (Shape: 4 x 18)\ntensor([[166, 167, 168,  ...,   0,   0,   0],\n        [137, 138, 139,  ...,   0,   0,   0],\n        [ 19, 146, 147,  ...,  11, 155,   2],\n        [ 12, 156, 157,  ...,   0,   0,   0]])\n2024-08-22 05:03:58,072 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/token_characters/token_characters (Shape: 4 x 18 x 16)\ntensor([[[28,  7,  4,  ...,  0,  0,  0],\n         [ 6,  2,  5,  ...,  0,  0,  0],\n         [ 3,  5, 14,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[48,  2,  8,  ...,  0,  0,  0],\n         [ 9,  7, 19,  ...,  0,  0,  0],\n         [ 4,  5, 14,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[28,  2,  6,  ...,  0,  0,  0],\n         [18, 13,  4,  ...,  0,  0,  0],\n         [31,  8,  7,  ...,  0,  0,  0],\n         ...,\n         [16,  5,  8,  ...,  0,  0,  0],\n         [47, 26, 11,  ...,  0,  0,  0],\n         [20,  0,  0,  ...,  0,  0,  0]],\n\n        [[24, 13, 21,  ...,  0,  0,  0],\n         [10,  2, 16,  ...,  0,  0,  0],\n         [33,  7,  9,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]]])\n2024-08-22 05:03:58,076 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/token_ids (Shape: 4 x 25)\ntensor([[  102,   173,  5094,  ...,     0,     0,     0],\n        [  102,   951,   423,  ...,     0,     0,     0],\n        [  102,   127,  6218,  ..., 30938,   552,   103],\n        [  102,   193,   232,  ...,     0,     0,     0]])\n2024-08-22 05:03:58,077 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/mask (Shape: 4 x 18)\ntensor([[ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ...,  True,  True,  True],\n        [ True,  True,  True,  ..., False, False, False]])\n2024-08-22 05:03:58,078 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/type_ids (Shape: 4 x 25)\ntensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]])\n2024-08-22 05:03:58,079 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/wordpiece_mask (Shape: 4 x 25)\ntensor([[ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ...,  True,  True,  True],\n        [ True,  True,  True,  ..., False, False, False]])\n2024-08-22 05:03:58,080 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/offsets (Shape: 4 x 18 x 2)\ntensor([[[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  3],\n         [ 4,  4],\n         ...,\n         [20, 20],\n         [21, 22],\n         [23, 23]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]]])\n2024-08-22 05:03:58,081 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/fasttext/tokens (Shape: 4 x 24)\ntensor([[ 35,   0,   0,  ...,   0,   0,   0],\n        [ 12, 156, 157,  ...,   0,   0,   0],\n        [166, 167, 168,  ...,   0,   0,   0],\n        [176, 177, 178,  ..., 190, 191,   2]])\n2024-08-22 05:03:58,082 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/token_characters/token_characters (Shape: 4 x 24 x 21)\ntensor([[[32, 32, 28,  ..., 54, 32, 32],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[24, 13, 21,  ...,  0,  0,  0],\n         [10,  2, 16,  ...,  0,  0,  0],\n         [33,  7,  9,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[28,  7,  4,  ...,  0,  0,  0],\n         [ 6,  2,  5,  ...,  0,  0,  0],\n         [ 3,  5, 14,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[34,  3, 10,  ...,  0,  0,  0],\n         [ 4, 15,  0,  ...,  0,  0,  0],\n         [16, 13,  4,  ...,  0,  0,  0],\n         ...,\n         [41, 15, 12,  ...,  0,  0,  0],\n         [ 7,  3, 11,  ...,  0,  0,  0],\n         [20,  0,  0,  ...,  0,  0,  0]]])\n2024-08-22 05:03:58,086 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/token_ids (Shape: 4 x 29)\ntensor([[  102, 16249, 16249,  ...,     0,     0,     0],\n        [  102,   193,   232,  ...,     0,     0,     0],\n        [  102,   173,  5094,  ...,     0,     0,     0],\n        [  102,   143,   224,  ..., 14762,   552,   103]])\n2024-08-22 05:03:58,087 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/mask (Shape: 4 x 24)\ntensor([[ True, False, False,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ...,  True,  True,  True]])\n2024-08-22 05:03:58,088 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/type_ids (Shape: 4 x 29)\ntensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]])\n2024-08-22 05:03:58,089 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/wordpiece_mask (Shape: 4 x 29)\ntensor([[ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ...,  True,  True,  True]])\n2024-08-22 05:03:58,090 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/offsets (Shape: 4 x 24 x 2)\ntensor([[[ 1,  9],\n         [ 0,  0],\n         [ 0,  0],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [25, 25],\n         [26, 26],\n         [27, 27]]])\n2024-08-22 05:03:58,091 - INFO - allennlp.training.callbacks.console_logger - Field : \"batch_input/sentence_tokens\" : (Length 4 of type \"<class 'list'>\")\n2024-08-22 05:03:58,091 - INFO - allennlp.training.callbacks.console_logger - batch_input/case_tags (Shape: 4 x 24)\ntensor([[3, 2, 2,  ..., 3, 2, 4],\n        [3, 2, 3,  ..., 0, 0, 0],\n        [3, 2, 3,  ..., 0, 0, 0],\n        [3, 2, 2,  ..., 0, 0, 0]])\n2024-08-22 05:03:58,092 - INFO - allennlp.training.callbacks.console_logger - batch_input/cpos_tags (Shape: 4 x 24)\ntensor([[16,  4, 17,  ...,  2, 11,  5],\n        [ 3,  7,  2,  ...,  0,  0,  0],\n        [ 6,  3,  2,  ...,  0,  0,  0],\n        [35, 11, 36,  ...,  0,  0,  0]])\n2024-08-22 05:03:58,093 - INFO - allennlp.training.callbacks.console_logger - batch_input/dep_chunk_tags (Shape: 4 x 24)\ntensor([[ 2,  2,  2,  ...,  3, 11,  2],\n        [ 2,  2,  2,  ...,  0,  0,  0],\n        [ 2,  2,  2,  ...,  0,  0,  0],\n        [ 2,  2,  2,  ...,  0,  0,  0]])\n2024-08-22 05:03:58,094 - INFO - allennlp.training.callbacks.console_logger - batch_input/deprel_tags (Shape: 4 x 24)\ntensor([[13,  2, 10,  ..., 15, 18,  3],\n        [ 4, 12,  5,  ...,  0,  0,  0],\n        [ 6,  4,  8,  ...,  0,  0,  0],\n        [ 5,  7,  2,  ...,  0,  0,  0]])\n2024-08-22 05:03:58,095 - INFO - allennlp.training.callbacks.console_logger - batch_input/document_depth (Shape: 4 x 24)\ntensor([[0.9091, 0.9091, 0.9091,  ..., 0.9091, 0.9091, 0.9091],\n        [0.8750, 0.8750, 0.8750,  ..., 0.0000, 0.0000, 0.0000],\n        [0.8889, 0.8889, 0.8889,  ..., 0.0000, 0.0000, 0.0000],\n        [0.9000, 0.9000, 0.9000,  ..., 0.0000, 0.0000, 0.0000]])\n2024-08-22 05:03:58,096 - INFO - allennlp.training.callbacks.console_logger - batch_input/genre_tags (Shape: 4 x 24)\ntensor([[2, 2, 2,  ..., 2, 2, 2],\n        [2, 2, 2,  ..., 0, 0, 0],\n        [2, 2, 2,  ..., 0, 0, 0],\n        [2, 2, 2,  ..., 0, 0, 0]])\n2024-08-22 05:03:58,097 - INFO - allennlp.training.callbacks.console_logger - batch_input/head_distances (Shape: 4 x 24)\ntensor([[2.3979, 2.3026, 2.1972,  ..., 1.0986, 2.5649, 2.4849],\n        [0.6931, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n        [0.6931, 0.0000, 1.9459,  ..., 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.6931, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])\n2024-08-22 05:03:58,098 - INFO - allennlp.training.callbacks.console_logger - batch_input/parent_clauses (Shape: 4 x 24)\ntensor([[ 2,  2,  2,  ...,  8, 13,  2],\n        [ 2,  2,  2,  ...,  0,  0,  0],\n        [ 2,  2,  2,  ...,  0,  0,  0],\n        [ 2,  2,  2,  ...,  0,  0,  0]])\n2024-08-22 05:03:58,099 - INFO - allennlp.training.callbacks.console_logger - batch_input/pos_tags (Shape: 4 x 24)\ntensor([[12,  4, 10,  ...,  2,  3,  5],\n        [ 6,  9,  2,  ...,  0,  0,  0],\n        [ 7,  6,  2,  ...,  0,  0,  0],\n        [ 8,  3,  4,  ...,  0,  0,  0]])\n2024-08-22 05:03:58,100 - INFO - allennlp.training.callbacks.console_logger - batch_input/s_type (Shape: 4 x 24)\ntensor([[2, 2, 2,  ..., 2, 2, 2],\n        [2, 2, 2,  ..., 0, 0, 0],\n        [2, 2, 2,  ..., 0, 0, 0],\n        [2, 2, 2,  ..., 0, 0, 0]])\n2024-08-22 05:03:58,101 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence_length (Shape: 4 x 24)\ntensor([[3.1781, 3.1781, 3.1781,  ..., 3.1781, 3.1781, 3.1781],\n        [2.8904, 2.8904, 2.8904,  ..., 0.0000, 0.0000, 0.0000],\n        [2.6391, 2.6391, 2.6391,  ..., 0.0000, 0.0000, 0.0000],\n        [1.7918, 1.7918, 1.7918,  ..., 0.0000, 0.0000, 0.0000]])\n2024-08-22 05:03:58,102 - INFO - allennlp.training.callbacks.console_logger - batch_input/token_lengths (Shape: 4 x 24)\ntensor([[1.0986, 0.6931, 1.3863,  ..., 1.7918, 1.7918, 0.0000],\n        [1.0986, 2.3026, 1.3863,  ..., 0.0000, 0.0000, 0.0000],\n        [1.0986, 1.0986, 1.9459,  ..., 0.0000, 0.0000, 0.0000],\n        [1.0986, 1.7918, 1.6094,  ..., 0.0000, 0.0000, 0.0000]])\n2024-08-22 05:03:58,104 - INFO - allennlp.training.callbacks.console_logger - batch_input/labels (Shape: 4 x 24)\ntensor([[1, 0, 0,  ..., 0, 0, 0],\n        [1, 0, 0,  ..., 0, 0, 0],\n        [1, 0, 0,  ..., 0, 0, 0],\n        [1, 0, 0,  ..., 0, 0, 0]])\n\ntag_accuracy: 0.9032, span_precision: 0.0000, span_recall: 0.0000, span_f1: 0.0000, batch_loss: 0.4510, loss: 0.4510 ||:  33%|###3      | 1/3 [00:00<00:01,  1.14it/s]\ntag_accuracy: 0.9027, span_precision: 0.0000, span_recall: 0.0000, span_f1: 0.0000, batch_loss: 0.4173, loss: 0.4342 ||:  67%|######6   | 2/3 [00:01<00:00,  1.11it/s]\ntag_accuracy: 0.8943, span_precision: 0.0000, span_recall: 0.0000, span_f1: 0.0000, batch_loss: 0.6441, loss: 0.5041 ||: 100%|##########| 3/3 [00:02<00:00,  1.51it/s]\ntag_accuracy: 0.8943, span_precision: 0.0000, span_recall: 0.0000, span_f1: 0.0000, batch_loss: 0.6441, loss: 0.5041 ||: 100%|##########| 3/3 [00:02<00:00,  1.38it/s]\n2024-08-22 05:03:59,403 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n2024-08-22 05:03:59,403 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.510  |     0.504\n2024-08-22 05:03:59,403 - INFO - allennlp.training.callbacks.console_logger - span_f1            |     0.254  |     0.000\n2024-08-22 05:03:59,403 - INFO - allennlp.training.callbacks.console_logger - span_precision     |     0.167  |     0.000\n2024-08-22 05:03:59,403 - INFO - allennlp.training.callbacks.console_logger - span_recall        |     0.533  |     0.000\n2024-08-22 05:03:59,404 - INFO - allennlp.training.callbacks.console_logger - tag_accuracy       |     0.671  |     0.894\n2024-08-22 05:03:59,404 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  2827.902  |       N/A\n2024-08-22 05:04:01,097 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:18.477282\n2024-08-22 05:04:01,098 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:16:30\n2024-08-22 05:04:01,098 - INFO - allennlp.training.gradient_descent_trainer - Epoch 1/59\n2024-08-22 05:04:01,098 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 5.3G\n2024-08-22 05:04:01,099 - INFO - allennlp.training.gradient_descent_trainer - Training\n\n  0%|          | 0/3 [00:00<?, ?it/s]\ntag_accuracy: 0.8750, span_precision: 0.0000, span_recall: 0.0000, span_f1: 0.0000, batch_loss: 0.6221, loss: 0.6221 ||:  33%|###3      | 1/3 [00:04<00:09,  4.77s/it]\ntag_accuracy: 0.8984, span_precision: 0.0000, span_recall: 0.0000, span_f1: 0.0000, batch_loss: 0.3253, loss: 0.4737 ||:  67%|######6   | 2/3 [00:08<00:04,  4.37s/it]\ntag_accuracy: 0.8951, span_precision: 0.0000, span_recall: 0.0000, span_f1: 0.0000, batch_loss: 0.2834, loss: 0.4103 ||: 100%|##########| 3/3 [00:11<00:00,  3.58s/it]\ntag_accuracy: 0.8951, span_precision: 0.0000, span_recall: 0.0000, span_f1: 0.0000, batch_loss: 0.2834, loss: 0.4103 ||: 100%|##########| 3/3 [00:11<00:00,  3.84s/it]\n2024-08-22 05:04:12,610 - INFO - allennlp.training.gradient_descent_trainer - Validating\n\n  0%|          | 0/3 [00:00<?, ?it/s]\ntag_accuracy: 0.8837, span_precision: 0.0000, span_recall: 0.0000, span_f1: 0.0000, batch_loss: 0.3602, loss: 0.3602 ||:  33%|###3      | 1/3 [00:00<00:01,  1.26it/s]\ntag_accuracy: 0.8972, span_precision: 0.0000, span_recall: 0.0000, span_f1: 0.0000, batch_loss: 0.2561, loss: 0.3082 ||:  67%|######6   | 2/3 [00:01<00:00,  1.28it/s]\ntag_accuracy: 0.8943, span_precision: 0.0000, span_recall: 0.0000, span_f1: 0.0000, batch_loss: 1.1012, loss: 0.5725 ||: 100%|##########| 3/3 [00:02<00:00,  1.53it/s]\ntag_accuracy: 0.8943, span_precision: 0.0000, span_recall: 0.0000, span_f1: 0.0000, batch_loss: 1.1012, loss: 0.5725 ||: 100%|##########| 3/3 [00:02<00:00,  1.45it/s]\n2024-08-22 05:04:14,679 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n2024-08-22 05:04:14,680 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.410  |     0.573\n2024-08-22 05:04:14,680 - INFO - allennlp.training.callbacks.console_logger - span_f1            |     0.000  |     0.000\n2024-08-22 05:04:14,680 - INFO - allennlp.training.callbacks.console_logger - span_precision     |     0.000  |     0.000\n2024-08-22 05:04:14,680 - INFO - allennlp.training.callbacks.console_logger - span_recall        |     0.000  |     0.000\n2024-08-22 05:04:14,680 - INFO - allennlp.training.callbacks.console_logger - tag_accuracy       |     0.895  |     0.894\n2024-08-22 05:04:14,680 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  5416.535  |       N/A\n2024-08-22 05:04:16,311 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:15.213081\n2024-08-22 05:04:16,311 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:15:29\n2024-08-22 05:04:16,312 - INFO - allennlp.training.gradient_descent_trainer - Epoch 2/59\n2024-08-22 05:04:16,312 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 5.3G\n2024-08-22 05:04:16,313 - INFO - allennlp.training.gradient_descent_trainer - Training\n\n  0%|          | 0/3 [00:00<?, ?it/s]\ntag_accuracy: 0.8936, span_precision: 0.0000, span_recall: 0.0000, span_f1: 0.0000, batch_loss: 0.4922, loss: 0.4922 ||:  33%|###3      | 1/3 [00:04<00:09,  4.51s/it]\ntag_accuracy: 0.9118, span_precision: 1.0000, span_recall: 0.1818, span_f1: 0.3077, batch_loss: 0.1880, loss: 0.3401 ||:  67%|######6   | 2/3 [00:08<00:04,  4.30s/it]\ntag_accuracy: 0.9231, span_precision: 1.0000, span_recall: 0.2667, span_f1: 0.4211, batch_loss: 0.1500, loss: 0.2767 ||: 100%|##########| 3/3 [00:12<00:00,  3.95s/it]\ntag_accuracy: 0.9231, span_precision: 1.0000, span_recall: 0.2667, span_f1: 0.4211, batch_loss: 0.1500, loss: 0.2767 ||: 100%|##########| 3/3 [00:12<00:00,  4.07s/it]\n2024-08-22 05:04:28,514 - INFO - allennlp.training.gradient_descent_trainer - Validating\n\n  0%|          | 0/3 [00:00<?, ?it/s]\ntag_accuracy: 0.9038, span_precision: 0.5000, span_recall: 0.2000, span_f1: 0.2857, batch_loss: 0.2048, loss: 0.2048 ||:  33%|###3      | 1/3 [00:00<00:01,  1.17it/s]\ntag_accuracy: 0.9010, span_precision: 0.6667, span_recall: 0.1818, span_f1: 0.2857, batch_loss: 0.4765, loss: 0.3406 ||:  67%|######6   | 2/3 [00:01<00:00,  1.24it/s]\ntag_accuracy: 0.9187, span_precision: 0.8000, span_recall: 0.3077, span_f1: 0.4444, batch_loss: 0.1661, loss: 0.2825 ||: 100%|##########| 3/3 [00:02<00:00,  1.57it/s]\ntag_accuracy: 0.9187, span_precision: 0.8000, span_recall: 0.3077, span_f1: 0.4444, batch_loss: 0.1661, loss: 0.2825 ||: 100%|##########| 3/3 [00:02<00:00,  1.45it/s]\n2024-08-22 05:04:30,580 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n2024-08-22 05:04:30,580 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.277  |     0.282\n2024-08-22 05:04:30,580 - INFO - allennlp.training.callbacks.console_logger - span_f1            |     0.421  |     0.444\n2024-08-22 05:04:30,580 - INFO - allennlp.training.callbacks.console_logger - span_precision     |     1.000  |     0.800\n2024-08-22 05:04:30,580 - INFO - allennlp.training.callbacks.console_logger - span_recall        |     0.267  |     0.308\n2024-08-22 05:04:30,580 - INFO - allennlp.training.callbacks.console_logger - tag_accuracy       |     0.923  |     0.919\n2024-08-22 05:04:30,581 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  5441.223  |       N/A\n2024-08-22 05:04:32,268 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:15.956480\n2024-08-22 05:04:32,268 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:15:11\n2024-08-22 05:04:32,268 - INFO - allennlp.training.gradient_descent_trainer - Epoch 3/59\n2024-08-22 05:04:32,269 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 5.3G\n2024-08-22 05:04:32,270 - INFO - allennlp.training.gradient_descent_trainer - Training\n\n  0%|          | 0/3 [00:00<?, ?it/s]\ntag_accuracy: 0.9531, span_precision: 1.0000, span_recall: 0.5000, span_f1: 0.6667, batch_loss: 0.1307, loss: 0.1307 ||:  33%|###3      | 1/3 [00:04<00:09,  4.54s/it]\ntag_accuracy: 0.9727, span_precision: 1.0000, span_recall: 0.7273, span_f1: 0.8421, batch_loss: 0.1285, loss: 0.1296 ||:  67%|######6   | 2/3 [00:08<00:04,  4.24s/it]\ntag_accuracy: 0.9650, span_precision: 1.0000, span_recall: 0.6667, span_f1: 0.8000, batch_loss: 0.1931, loss: 0.1507 ||: 100%|##########| 3/3 [00:11<00:00,  3.76s/it]\ntag_accuracy: 0.9650, span_precision: 1.0000, span_recall: 0.6667, span_f1: 0.8000, batch_loss: 0.1931, loss: 0.1507 ||: 100%|##########| 3/3 [00:11<00:00,  3.92s/it]\n2024-08-22 05:04:44,027 - INFO - allennlp.training.gradient_descent_trainer - Validating\n\n  0%|          | 0/3 [00:00<?, ?it/s]\ntag_accuracy: 0.9138, span_precision: 0.6667, span_recall: 0.3333, span_f1: 0.4444, batch_loss: 0.3633, loss: 0.3633 ||:  33%|###3      | 1/3 [00:00<00:01,  1.31it/s]\ntag_accuracy: 0.9266, span_precision: 0.8000, span_recall: 0.3636, span_f1: 0.5000, batch_loss: 0.1923, loss: 0.2778 ||:  67%|######6   | 2/3 [00:01<00:00,  1.22it/s]\ntag_accuracy: 0.9268, span_precision: 0.8333, span_recall: 0.3846, span_f1: 0.5263, batch_loss: 0.1722, loss: 0.2426 ||: 100%|##########| 3/3 [00:02<00:00,  1.56it/s]\ntag_accuracy: 0.9268, span_precision: 0.8333, span_recall: 0.3846, span_f1: 0.5263, batch_loss: 0.1722, loss: 0.2426 ||: 100%|##########| 3/3 [00:02<00:00,  1.46it/s]\n2024-08-22 05:04:46,084 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n2024-08-22 05:04:46,084 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.151  |     0.243\n2024-08-22 05:04:46,084 - INFO - allennlp.training.callbacks.console_logger - span_f1            |     0.800  |     0.526\n2024-08-22 05:04:46,084 - INFO - allennlp.training.callbacks.console_logger - span_precision     |     1.000  |     0.833\n2024-08-22 05:04:46,084 - INFO - allennlp.training.callbacks.console_logger - span_recall        |     0.667  |     0.385\n2024-08-22 05:04:46,084 - INFO - allennlp.training.callbacks.console_logger - tag_accuracy       |     0.965  |     0.927\n2024-08-22 05:04:46,084 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  5441.223  |       N/A\n2024-08-22 05:04:47,822 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:15.553228\n2024-08-22 05:04:47,822 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:14:48\n2024-08-22 05:04:47,822 - INFO - allennlp.training.gradient_descent_trainer - Epoch 4/59\n2024-08-22 05:04:47,822 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 5.3G\n2024-08-22 05:04:47,824 - INFO - allennlp.training.gradient_descent_trainer - Training\n\n  0%|          | 0/3 [00:00<?, ?it/s]\ntag_accuracy: 0.9437, span_precision: 1.0000, span_recall: 0.5000, span_f1: 0.6667, batch_loss: 0.1154, loss: 0.1154 ||:  33%|###3      | 1/3 [00:04<00:09,  4.62s/it]\ntag_accuracy: 0.9545, span_precision: 1.0000, span_recall: 0.6154, span_f1: 0.7619, batch_loss: 0.1666, loss: 0.1410 ||:  67%|######6   | 2/3 [00:08<00:04,  4.20s/it]\ntag_accuracy: 0.9580, span_precision: 1.0000, span_recall: 0.6000, span_f1: 0.7500, batch_loss: 0.0413, loss: 0.1078 ||: 100%|##########| 3/3 [00:11<00:00,  3.73s/it]\ntag_accuracy: 0.9580, span_precision: 1.0000, span_recall: 0.6000, span_f1: 0.7500, batch_loss: 0.0413, loss: 0.1078 ||: 100%|##########| 3/3 [00:11<00:00,  3.90s/it]\n2024-08-22 05:04:59,532 - INFO - allennlp.training.gradient_descent_trainer - Validating\n\n  0%|          | 0/3 [00:00<?, ?it/s]\ntag_accuracy: 0.9492, span_precision: 1.0000, span_recall: 0.4000, span_f1: 0.5714, batch_loss: 0.1407, loss: 0.1407 ||:  33%|###3      | 1/3 [00:00<00:01,  1.16it/s]\ntag_accuracy: 0.9174, span_precision: 0.7500, span_recall: 0.2727, span_f1: 0.4000, batch_loss: 0.3266, loss: 0.2336 ||:  67%|######6   | 2/3 [00:01<00:00,  1.26it/s]\ntag_accuracy: 0.9187, span_precision: 0.8000, span_recall: 0.3077, span_f1: 0.4444, batch_loss: 0.1529, loss: 0.2067 ||: 100%|##########| 3/3 [00:02<00:00,  1.58it/s]\ntag_accuracy: 0.9187, span_precision: 0.8000, span_recall: 0.3077, span_f1: 0.4444, batch_loss: 0.1529, loss: 0.2067 ||: 100%|##########| 3/3 [00:02<00:00,  1.46it/s]\n2024-08-22 05:05:01,583 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n2024-08-22 05:05:01,583 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.108  |     0.207\n2024-08-22 05:05:01,583 - INFO - allennlp.training.callbacks.console_logger - span_f1            |     0.750  |     0.444\n2024-08-22 05:05:01,583 - INFO - allennlp.training.callbacks.console_logger - span_precision     |     1.000  |     0.800\n2024-08-22 05:05:01,583 - INFO - allennlp.training.callbacks.console_logger - span_recall        |     0.600  |     0.308\n2024-08-22 05:05:01,583 - INFO - allennlp.training.callbacks.console_logger - tag_accuracy       |     0.958  |     0.919\n2024-08-22 05:05:01,584 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  5441.223  |       N/A\n2024-08-22 05:05:03,301 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:15.479036\n2024-08-22 05:05:03,302 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:14:28\n2024-08-22 05:05:03,302 - INFO - allennlp.training.gradient_descent_trainer - Epoch 5/59\n2024-08-22 05:05:03,302 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 5.3G\n2024-08-22 05:05:03,303 - INFO - allennlp.training.gradient_descent_trainer - Training\n\n  0%|          | 0/3 [00:00<?, ?it/s]\ntag_accuracy: 0.9605, span_precision: 1.0000, span_recall: 0.5714, span_f1: 0.7273, batch_loss: 0.0582, loss: 0.0582 ||:  33%|###3      | 1/3 [00:04<00:08,  4.24s/it]\ntag_accuracy: 0.9524, span_precision: 0.8889, span_recall: 0.6154, span_f1: 0.7273, batch_loss: 0.1807, loss: 0.1195 ||:  67%|######6   | 2/3 [00:08<00:04,  4.38s/it]\ntag_accuracy: 0.9580, span_precision: 0.9091, span_recall: 0.6667, span_f1: 0.7692, batch_loss: 0.0195, loss: 0.0862 ||: 100%|##########| 3/3 [00:11<00:00,  3.64s/it]\ntag_accuracy: 0.9580, span_precision: 0.9091, span_recall: 0.6667, span_f1: 0.7692, batch_loss: 0.0195, loss: 0.0862 ||: 100%|##########| 3/3 [00:11<00:00,  3.83s/it]\n2024-08-22 05:05:14,796 - INFO - allennlp.training.gradient_descent_trainer - Validating\n\n  0%|          | 0/3 [00:00<?, ?it/s]\ntag_accuracy: 0.9423, span_precision: 1.0000, span_recall: 0.4000, span_f1: 0.5714, batch_loss: 0.1637, loss: 0.1637 ||:  33%|###3      | 1/3 [00:00<00:01,  1.25it/s]\ntag_accuracy: 0.9320, span_precision: 1.0000, span_recall: 0.3000, span_f1: 0.4615, batch_loss: 0.1458, loss: 0.1548 ||:  67%|######6   | 2/3 [00:01<00:00,  1.25it/s]\ntag_accuracy: 0.9187, span_precision: 1.0000, span_recall: 0.2308, span_f1: 0.3750, batch_loss: 0.5128, loss: 0.2741 ||: 100%|##########| 3/3 [00:02<00:00,  1.52it/s]\ntag_accuracy: 0.9187, span_precision: 1.0000, span_recall: 0.2308, span_f1: 0.3750, batch_loss: 0.5128, loss: 0.2741 ||: 100%|##########| 3/3 [00:02<00:00,  1.44it/s]\n2024-08-22 05:05:16,885 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n2024-08-22 05:05:16,885 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.086  |     0.274\n2024-08-22 05:05:16,886 - INFO - allennlp.training.callbacks.console_logger - span_f1            |     0.769  |     0.375\n2024-08-22 05:05:16,886 - INFO - allennlp.training.callbacks.console_logger - span_precision     |     0.909  |     1.000\n2024-08-22 05:05:16,886 - INFO - allennlp.training.callbacks.console_logger - span_recall        |     0.667  |     0.231\n2024-08-22 05:05:16,886 - INFO - allennlp.training.callbacks.console_logger - tag_accuracy       |     0.958  |     0.919\n2024-08-22 05:05:16,886 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  5441.223  |       N/A\n2024-08-22 05:05:18,586 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:15.284152\n2024-08-22 05:05:18,586 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:14:08\n2024-08-22 05:05:18,587 - INFO - allennlp.training.gradient_descent_trainer - Epoch 6/59\n2024-08-22 05:05:18,587 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 5.3G\n2024-08-22 05:05:18,588 - INFO - allennlp.training.gradient_descent_trainer - Training\n\n  0%|          | 0/3 [00:00<?, ?it/s]\ntag_accuracy: 0.9701, span_precision: 1.0000, span_recall: 0.7500, span_f1: 0.8571, batch_loss: 0.0948, loss: 0.0948 ||:  33%|###3      | 1/3 [00:04<00:08,  4.46s/it]\ntag_accuracy: 0.9811, span_precision: 1.0000, span_recall: 0.8333, span_f1: 0.9091, batch_loss: 0.0215, loss: 0.0582 ||:  67%|######6   | 2/3 [00:08<00:04,  4.15s/it]\ntag_accuracy: 0.9860, span_precision: 1.0000, span_recall: 0.8667, span_f1: 0.9286, batch_loss: 0.0321, loss: 0.0495 ||: 100%|##########| 3/3 [00:11<00:00,  3.71s/it]\ntag_accuracy: 0.9860, span_precision: 1.0000, span_recall: 0.8667, span_f1: 0.9286, batch_loss: 0.0321, loss: 0.0495 ||: 100%|##########| 3/3 [00:11<00:00,  3.86s/it]\n2024-08-22 05:05:30,176 - INFO - allennlp.training.gradient_descent_trainer - Validating\n\n  0%|          | 0/3 [00:00<?, ?it/s]\ntag_accuracy: 0.9344, span_precision: 1.0000, span_recall: 0.3333, span_f1: 0.5000, batch_loss: 0.1711, loss: 0.1711 ||:  33%|###3      | 1/3 [00:00<00:01,  1.42it/s]\ntag_accuracy: 0.9121, span_precision: 1.0000, span_recall: 0.2000, span_f1: 0.3333, batch_loss: 0.4608, loss: 0.3159 ||:  67%|######6   | 2/3 [00:01<00:00,  1.38it/s]\ntag_accuracy: 0.9187, span_precision: 1.0000, span_recall: 0.2308, span_f1: 0.3750, batch_loss: 0.1467, loss: 0.2595 ||: 100%|##########| 3/3 [00:01<00:00,  1.60it/s]\ntag_accuracy: 0.9187, span_precision: 1.0000, span_recall: 0.2308, span_f1: 0.3750, batch_loss: 0.1467, loss: 0.2595 ||: 100%|##########| 3/3 [00:01<00:00,  1.54it/s]\n2024-08-22 05:05:32,126 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n2024-08-22 05:05:32,127 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.049  |     0.260\n2024-08-22 05:05:32,127 - INFO - allennlp.training.callbacks.console_logger - span_f1            |     0.929  |     0.375\n2024-08-22 05:05:32,127 - INFO - allennlp.training.callbacks.console_logger - span_precision     |     1.000  |     1.000\n2024-08-22 05:05:32,127 - INFO - allennlp.training.callbacks.console_logger - span_recall        |     0.867  |     0.231\n2024-08-22 05:05:32,127 - INFO - allennlp.training.callbacks.console_logger - tag_accuracy       |     0.986  |     0.919\n2024-08-22 05:05:32,127 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  5441.223  |       N/A\n2024-08-22 05:05:33,926 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:15.339587\n2024-08-22 05:05:33,927 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:13:49\n2024-08-22 05:05:33,927 - INFO - allennlp.training.gradient_descent_trainer - Epoch 7/59\n2024-08-22 05:05:33,927 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 5.3G\n2024-08-22 05:05:33,929 - INFO - allennlp.training.gradient_descent_trainer - Training\n\n  0%|          | 0/3 [00:00<?, ?it/s]\ntag_accuracy: 1.0000, span_precision: 1.0000, span_recall: 1.0000, span_f1: 1.0000, batch_loss: 0.0305, loss: 0.0305 ||:  33%|###3      | 1/3 [00:04<00:08,  4.17s/it]\ntag_accuracy: 1.0000, span_precision: 1.0000, span_recall: 1.0000, span_f1: 1.0000, batch_loss: 0.0338, loss: 0.0321 ||:  67%|######6   | 2/3 [00:07<00:03,  3.88s/it]\ntag_accuracy: 0.9930, span_precision: 1.0000, span_recall: 0.9333, span_f1: 0.9655, batch_loss: 0.0372, loss: 0.0338 ||: 100%|##########| 3/3 [00:11<00:00,  3.70s/it]\ntag_accuracy: 0.9930, span_precision: 1.0000, span_recall: 0.9333, span_f1: 0.9655, batch_loss: 0.0372, loss: 0.0338 ||: 100%|##########| 3/3 [00:11<00:00,  3.78s/it]\n2024-08-22 05:05:45,274 - INFO - allennlp.training.gradient_descent_trainer - Validating\n\n  0%|          | 0/3 [00:00<?, ?it/s]\ntag_accuracy: 0.9412, span_precision: 1.0000, span_recall: 0.4000, span_f1: 0.5714, batch_loss: 0.1634, loss: 0.1634 ||:  33%|###3      | 1/3 [00:00<00:01,  1.25it/s]\ntag_accuracy: 0.9121, span_precision: 1.0000, span_recall: 0.2000, span_f1: 0.3333, batch_loss: 0.5139, loss: 0.3386 ||:  67%|######6   | 2/3 [00:01<00:00,  1.28it/s]\ntag_accuracy: 0.9187, span_precision: 1.0000, span_recall: 0.2308, span_f1: 0.3750, batch_loss: 0.1236, loss: 0.2670 ||: 100%|##########| 3/3 [00:02<00:00,  1.58it/s]\ntag_accuracy: 0.9187, span_precision: 1.0000, span_recall: 0.2308, span_f1: 0.3750, batch_loss: 0.1236, loss: 0.2670 ||: 100%|##########| 3/3 [00:02<00:00,  1.48it/s]\n2024-08-22 05:05:47,306 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n2024-08-22 05:05:47,307 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.034  |     0.267\n2024-08-22 05:05:47,307 - INFO - allennlp.training.callbacks.console_logger - span_f1            |     0.966  |     0.375\n2024-08-22 05:05:47,307 - INFO - allennlp.training.callbacks.console_logger - span_precision     |     1.000  |     1.000\n2024-08-22 05:05:47,307 - INFO - allennlp.training.callbacks.console_logger - span_recall        |     0.933  |     0.231\n2024-08-22 05:05:47,307 - INFO - allennlp.training.callbacks.console_logger - tag_accuracy       |     0.993  |     0.919\n2024-08-22 05:05:47,307 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  5448.684  |       N/A\n2024-08-22 05:05:49,003 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:15.075721\n2024-08-22 05:05:49,003 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:13:30\n2024-08-22 05:05:49,003 - INFO - allennlp.training.gradient_descent_trainer - Epoch 8/59\n2024-08-22 05:05:49,004 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 5.3G\n2024-08-22 05:05:49,005 - INFO - allennlp.training.gradient_descent_trainer - Training\n\n  0%|          | 0/3 [00:00<?, ?it/s]\ntag_accuracy: 1.0000, span_precision: 1.0000, span_recall: 1.0000, span_f1: 1.0000, batch_loss: 0.0117, loss: 0.0117 ||:  33%|###3      | 1/3 [00:04<00:09,  4.57s/it]\ntag_accuracy: 1.0000, span_precision: 1.0000, span_recall: 1.0000, span_f1: 1.0000, batch_loss: 0.0211, loss: 0.0164 ||:  67%|######6   | 2/3 [00:09<00:04,  4.58s/it]\ntag_accuracy: 1.0000, span_precision: 1.0000, span_recall: 1.0000, span_f1: 1.0000, batch_loss: 0.0348, loss: 0.0225 ||: 100%|##########| 3/3 [00:12<00:00,  3.81s/it]\ntag_accuracy: 1.0000, span_precision: 1.0000, span_recall: 1.0000, span_f1: 1.0000, batch_loss: 0.0348, loss: 0.0225 ||: 100%|##########| 3/3 [00:12<00:00,  4.02s/it]\n2024-08-22 05:06:01,057 - INFO - allennlp.training.gradient_descent_trainer - Validating\n\n  0%|          | 0/3 [00:00<?, ?it/s]\ntag_accuracy: 0.9091, span_precision: 1.0000, span_recall: 0.2000, span_f1: 0.3333, batch_loss: 0.1437, loss: 0.1437 ||:  33%|###3      | 1/3 [00:00<00:01,  1.26it/s]\ntag_accuracy: 0.9346, span_precision: 1.0000, span_recall: 0.3636, span_f1: 0.5333, batch_loss: 0.1363, loss: 0.1400 ||:  67%|######6   | 2/3 [00:01<00:00,  1.29it/s]\ntag_accuracy: 0.9350, span_precision: 1.0000, span_recall: 0.3846, span_f1: 0.5556, batch_loss: 0.8716, loss: 0.3839 ||: 100%|##########| 3/3 [00:01<00:00,  1.61it/s]\ntag_accuracy: 0.9350, span_precision: 1.0000, span_recall: 0.3846, span_f1: 0.5556, batch_loss: 0.8716, loss: 0.3839 ||: 100%|##########| 3/3 [00:01<00:00,  1.51it/s]\n2024-08-22 05:06:03,052 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n2024-08-22 05:06:03,053 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.023  |     0.384\n2024-08-22 05:06:03,053 - INFO - allennlp.training.callbacks.console_logger - span_f1            |     1.000  |     0.556\n2024-08-22 05:06:03,053 - INFO - allennlp.training.callbacks.console_logger - span_precision     |     1.000  |     1.000\n2024-08-22 05:06:03,053 - INFO - allennlp.training.callbacks.console_logger - span_recall        |     1.000  |     0.385\n2024-08-22 05:06:03,053 - INFO - allennlp.training.callbacks.console_logger - tag_accuracy       |     1.000  |     0.935\n2024-08-22 05:06:03,053 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  5448.684  |       N/A\n2024-08-22 05:06:04,746 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:15.743013\n2024-08-22 05:06:04,747 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:13:15\n2024-08-22 05:06:04,747 - INFO - allennlp.training.gradient_descent_trainer - Epoch 9/59\n2024-08-22 05:06:04,747 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 5.4G\n2024-08-22 05:06:04,748 - INFO - allennlp.training.gradient_descent_trainer - Training\n\n  0%|          | 0/3 [00:00<?, ?it/s]\ntag_accuracy: 1.0000, span_precision: 1.0000, span_recall: 1.0000, span_f1: 1.0000, batch_loss: 0.0202, loss: 0.0202 ||:  33%|###3      | 1/3 [00:04<00:08,  4.15s/it]\ntag_accuracy: 1.0000, span_precision: 1.0000, span_recall: 1.0000, span_f1: 1.0000, batch_loss: 0.0079, loss: 0.0140 ||:  67%|######6   | 2/3 [00:08<00:04,  4.22s/it]\ntag_accuracy: 1.0000, span_precision: 1.0000, span_recall: 1.0000, span_f1: 1.0000, batch_loss: 0.0023, loss: 0.0101 ||: 100%|##########| 3/3 [00:11<00:00,  3.51s/it]\ntag_accuracy: 1.0000, span_precision: 1.0000, span_recall: 1.0000, span_f1: 1.0000, batch_loss: 0.0023, loss: 0.0101 ||: 100%|##########| 3/3 [00:11<00:00,  3.69s/it]\n2024-08-22 05:06:15,837 - INFO - allennlp.training.gradient_descent_trainer - Validating\n\n  0%|          | 0/3 [00:00<?, ?it/s]\n\nYour command has already run for 300 seconds. It is still running. You can continue to wait or interrupt it with \"action\": {\"type\": \"interrupt\"} or \"action\": {\"type\": \"continue_wait\"}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!bash seg_scripts/single_corpus_train_and_test_ft.sh deu.rst.pcc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eb245c",
   "metadata": {},
   "source": [
    "### Step 23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63f18b8",
   "metadata": {},
   "source": [
    "Execution Start time: 05:06:27, End time: 05:06:27 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685b160c",
   "metadata": {},
   "source": [
    "**Thought**: The script execution was taking too long, likely due to running multiple epochs instead of just one as required. I will modify the script to ensure it only runs for one epoch and then rerun it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f2c65c",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2699af1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/DisCoDisCo/seg_scripts/single_corpus_train_and_test_ft.sh (133 lines total)]\n1:#!/bin/bash\n2:#set -o errexit\n3:if [ $# -eq 0 ]; then\n4:  echo \"Supply the name of a corpus\"\n5:  exit 1\n6:fi\n7:CORPUS=\"$1\"\n8:CORPUS_DIR=\"data/2021/${1}\"\n9:MODEL_DIR=${2:-models}/${CORPUS}_seg_bert_baseline_ft\n10:if [[ ! -d $CORPUS_DIR ]]; then\n11:  echo \"Corpus \\\"$CORPUS_DIR\\\" not found\"\n12:  exit 1\n13:fi\n14:if [[ -d $MODEL_DIR ]]; then\n15:  echo \"\\\"$MODEL_DIR\\\" already exists. Removing it now...\"\n16:  rm -rf \"$MODEL_DIR\"\n17:fi\n18:\n19:# use language-specific berts if we can\n20:export EMBEDDING_DIMS=768\n21:if [[ \"$CORPUS\" == \"eng\"* ]]; then \n22:  export EMBEDDING_DIMS=1024\n23:  #export EMBEDDING_MODEL_NAME=\"roberta-large\"\n24:  export EMBEDDING_MODEL_NAME=\"google/electra-large-discriminator\"\n25:elif [[ \"$CORPUS\" == \"fas\"* ]]; then\n26:  export EMBEDDING_MODEL_NAME=\"HooshvareLab/bert-fa-base-uncased\"\n27:elif [[ \"$CORPUS\" == \"deu\"* ]]; then\n28:  export EMBEDDING_MODEL_NAME=\"google-bert/bert-base-german-dbmdz-uncased\"\n29:#elif [[ \"$CORPUS\" == \"fra\"* ]]; then\n30:#  export EMBEDDING_DIMS=1024\n31:#  export EMBEDDING_MODEL_NAME=\"camembert/camembert-large\"\n32:elif [[ \"$CORPUS\" == \"zho\"* ]]; then\n33:  #export EMBEDDING_DIMS=1024\n34:  #export EMBEDDING_MODEL_NAME=\"hfl/chinese-electra-180g-large-discriminator\"\n35:  export EMBEDDING_MODEL_NAME=\"bert-base-chinese\"\n36:elif [[ \"$CORPUS\" == \"nld\"* ]]; then\n37:  #export EMBEDDING_MODEL_NAME=\"GroNLP/bert-base-dutch-cased\"\n38:  export EMBEDDING_MODEL_NAME=\"pdelobelle/robbert-v2-dutch-base\"\n39:elif [[ \"$CORPUS\" == \"eus\"* ]]; then\n40:  export EMBEDDING_MODEL_NAME=\"ixa-ehu/berteus-base-cased\"\n41:elif [[ \"$CORPUS\" == \"spa\"* ]]; then\n42:  #export EMBEDDING_MODEL_NAME=\"mrm8488/electricidad-base-discriminator\"\n43:  export EMBEDDING_MODEL_NAME=\"dccuchile/bert-base-spanish-wwm-cased\"\n44:elif [[ \"$CORPUS\" == \"por\"* ]]; then\n45:  export EMBEDDING_MODEL_NAME=\"neuralmind/bert-base-portuguese-cased\"\n46:elif [[ \"$CORPUS\" == \"tur\"* ]]; then\n47:  #export EMBEDDING_MODEL_NAME=\"dbmdz/electra-base-turkish-cased-discriminator\"\n48:  export EMBEDDING_MODEL_NAME=\"dbmdz/bert-base-turkish-cased\"\n49:elif [[ \"$CORPUS\" == \"rus\"* ]]; then\n50:  export EMBEDDING_MODEL_NAME=\"DeepPavlov/rubert-base-cased\"\n51:else\n52:  export EMBEDDING_DIMS=1024\n53:  export EMBEDDING_MODEL_NAME=\"xlm-roberta-large\"\n54:fi\n55:\n56:# do not use CRF on RST datasets\n57:export USE_CRF=0\n58:if [[ \"$CORPUS\" == *\".pdtb.\"* ]]; then\n59:  export USE_CRF=1\n60:fi\n61:\n62:# use fastText embeddings\n63:if [[ \"$CORPUS\" == \"eng\"* ]]; then\n64:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\"\n65:elif [[ \"$CORPUS\" == \"deu\"* ]]; then\n66:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.de.300.vec.gz\"\n67:elif [[ \"$CORPUS\" == \"eus\"* ]]; then\n68:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.eu.300.vec.gz\"\n69:elif [[ \"$CORPUS\" == \"fas\"* ]]; then\n70:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.vec.gz\"\n71:elif [[ \"$CORPUS\" == \"fra\"* ]]; then\n72:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.vec.gz\"\n73:elif [[ \"$CORPUS\" == \"nld\"* ]]; then\n74:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.nl.300.vec.gz\"\n75:elif [[ \"$CORPUS\" == \"por\"* ]]; then\n76:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.pt.300.vec.gz\"\n77:elif [[ \"$CORPUS\" == \"rus\"* ]]; then\n78:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ru.300.vec.gz\"\n79:elif [[ \"$CORPUS\" == \"spa\"* ]]; then\n80:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.es.300.vec.gz\"\n81:elif [[ \"$CORPUS\" == \"tur\"* ]]; then\n82:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.tr.300.vec.gz\"\n83:elif [[ \"$CORPUS\" == \"zho\"* ]]; then\n84:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.zh.300.vec.gz\"\n85:else\n86:  echo \"Couldn't find a fasttext embedding for \\\"$CORPUS\\\"\" >&2\n87:  exit 1\n88:fi\n89:\n90:echo \"\"\n91:echo \"#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\"\n92:echo \"# Training on $CORPUS\"\n93:echo \"#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\"\n94:echo \"\"\n95:export TRAIN_DATA_PATH=\"${CORPUS_DIR}/${CORPUS}_train.conllu\"\n96:export VALIDATION_DATA_PATH=\"${CORPUS_DIR}/${CORPUS}_dev.conllu\"\n97:echo $TRAIN_DATA_PATH\n98:allennlp train \\\n99:  configs/seg/baseline/bert_baseline_ft.jsonnet \\\n100:  -s \"$MODEL_DIR\"\n(33 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open seg_scripts/single_corpus_train_and_test_ft.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa263cd",
   "metadata": {},
   "source": [
    "### Step 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49572fc8",
   "metadata": {},
   "source": [
    "Execution Start time: 05:06:29, End time: 05:06:29 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebef2f84",
   "metadata": {},
   "source": [
    "**Thought**: Let's modify the script to ensure it only runs for one epoch. We need to locate the configuration file `configs/seg/baseline/bert_baseline_ft.jsonnet` and set the number of epochs to 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becc72d7",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbde2e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/DisCoDisCo/configs/seg/baseline/bert_baseline_ft.jsonnet (103 lines total)]\n1:local transformer_model_name = std.extVar(\"EMBEDDING_MODEL_NAME\");\n2:local embedding_dim = std.parseInt(std.extVar(\"EMBEDDING_DIMS\")) + 64 * 2 + 300;\n3:local context_hidden_size = 400;\n4:local encoder_hidden_dim = 512;\n5:\n6:local context_encoder = {\n7:    \"type\": \"lstm\",\n8:    \"input_size\": embedding_dim,\n9:    \"hidden_size\": context_hidden_size / 4, // 4 <= 2 bilstms applied to 2 sentences\n10:    \"num_layers\": 1,\n11:    \"bidirectional\": true,\n12:    \"dropout\": 0.2\n13:};\n14:\n15:local token_features = {\n16:    \"pos_tags\": {\"source_key\": \"pos\", \"label_namespace\": \"upos\"},\n17:    \"cpos_tags\": {\"source_key\": \"cpos\", \"label_namespace\": \"xpos\"},\n18:    \"deprel_tags\": {\"source_key\": \"deprel\", \"label_namespace\": \"deprel\"},\n19:    \"dep_chunk_tags\": {\"source_key\": \"depchunk\", \"label_namespace\": \"depchunk\"},\n20:    \"parent_clauses\": {\"source_key\": \"parentclauses\", \"label_namespace\": \"parent_clauses\"},\n21:    \"s_type\": {\"source_key\": \"s_type\", \"label_namespace\": \"s_type\"},\n22:    \"case_tags\": {\"source_key\": \"case\", \"label_namespace\": \"case\"},\n23:    \"genre_tags\": {\"source_key\": \"genre\", \"label_namespace\": \"genre\"},\n24:    \"head_distances\": {\"source_key\": \"head_dist\", \"xform_fn\": \"abs_natural_log\"},\n25:    \"document_depth\": {\"source_key\": \"sent_doc_percentile\"},\n26:    \"sentence_length\": {\"source_key\": \"s_len\", \"xform_fn\": \"natural_log\"},\n27:    \"token_lengths\": {\"source_key\": \"tok_len\", \"xform_fn\": \"natural_log\"}\n28:};\n29:\n30:// For more info on config files generally, see https://guide.allennlp.org/using-config-files\n31:{\n32:    \"dataset_reader\" : {\n33:        \"type\": \"disrpt_2021_seg\",\n34:        \"token_indexers\": {\n35:            \"tokens\": {\n36:                \"type\": \"pretrained_transformer_mismatched\",\n37:                \"model_name\": transformer_model_name\n38:            },\n39:            \"fasttext\": {\n40:                \"type\": \"single_id\",\n41:                \"namespace\": \"fasttext\",\n42:            },\n43:            \"token_characters\": import \"../../components/char_indexer.libsonnet\"\n44:        },\n45:        \"tokenizer\": {\n46:            \"type\": \"whitespace\"\n47:        },\n48:        \"token_features\": token_features\n49:    },\n50:    \"model\": {\n51:        \"type\": \"disrpt_2021_seg_baseline\",\n52:        \"embedder\": {\n53:            \"token_embedders\": {\n54:                \"tokens\": {\n55:                    \"type\": \"pretrained_transformer_mismatched\",\n56:                    \"model_name\": transformer_model_name,\n57:                    \"train_parameters\": true,\n58:                    \"last_layer_only\": true\n59:                },\n60:                \"fasttext\": {\n61:                    \"type\": \"embedding\",\n62:                    \"vocab_namespace\": \"fasttext\",\n63:                    \"embedding_dim\": 300,\n64:                    \"trainable\": false,\n65:                    \"pretrained_file\": std.extVar(\"FASTTEXT_EMBEDDING_FILE\")\n66:                },\n67:                \"token_characters\": import \"../../components/char_embedder.libsonnet\"\n68:            }\n69:        },\n70:        // seq2vec encoders for neighbor sentences\n71:        \"prev_sentence_encoder\": context_encoder,\n72:        \"next_sentence_encoder\": context_encoder,\n73:        // our encoder isn't fully configurable here because its input size needs to be determined\n74:        // at the start of the program. so, it'll always be a bilstm, but you can use the two items\n75:        // below to configure the most important hyperparameters it has\n76:        \"encoder_hidden_dim\": encoder_hidden_dim,\n77:        \"encoder_recurrent_dropout\": 0.1,\n78:        // end encoder hyperparams\n79:        \"dropout\": 0.4,\n80:        \"feature_dropout\": 0.0,\n81:        \"token_features\": token_features,\n82:        \"use_crf\": if std.extVar(\"USE_CRF\") == \"1\" then true else false\n83:    },\n84:    \"train_data_path\": std.extVar(\"TRAIN_DATA_PATH\"),\n85:    \"validation_data_path\": std.extVar(\"VALIDATION_DATA_PATH\"),\n86:    \"data_loader\": {\n87:        \"batch_size\": 4,\n88:        \"shuffle\": true\n89:    },\n90:    \"trainer\": {\n91:        \"optimizer\": {\n92:            \"type\": \"huggingface_adamw\",\n93:            \"lr\": 5e-4,\n94:            \"parameter_groups\": [\n95:                [[\".*transformer.*\"], {\"lr\": 1e-5}]\n96:            ]\n97:        },\n98:        \"patience\": 10,\n99:        \"num_epochs\": 60,\n100:        // probably best to just use loss\n(3 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open configs/seg/baseline/bert_baseline_ft.jsonnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630f2ea3",
   "metadata": {},
   "source": [
    "### Step 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807b1998",
   "metadata": {},
   "source": [
    "Execution Start time: 05:06:31, End time: 05:06:31 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44cccf7",
   "metadata": {},
   "source": [
    "**Thought**: Let's edit the configuration file to set the number of epochs to 1. This will ensure that the training process runs only for a single epoch as required.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a21def",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3d19fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Usage: edit <start_line>:<end_line>\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 99\n",
    "        \"num_epochs\": 1,\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1af876",
   "metadata": {},
   "source": [
    "### Step 26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962f9692",
   "metadata": {},
   "source": [
    "Execution Start time: 05:06:33, End time: 05:06:34 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fba87b",
   "metadata": {},
   "source": [
    "**Thought**: I made a mistake in the edit command syntax. Let's correct it by specifying the line range properly to update the number of epochs to 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4894871d",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492396f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/DisCoDisCo/configs/seg/baseline/bert_baseline_ft.jsonnet (103 lines total)]\n(3 more lines above)\n4:local encoder_hidden_dim = 512;\n5:\n6:local context_encoder = {\n7:    \"type\": \"lstm\",\n8:    \"input_size\": embedding_dim,\n9:    \"hidden_size\": context_hidden_size / 4, // 4 <= 2 bilstms applied to 2 sentences\n10:    \"num_layers\": 1,\n11:    \"bidirectional\": true,\n12:    \"dropout\": 0.2\n13:};\n14:\n15:local token_features = {\n16:    \"pos_tags\": {\"source_key\": \"pos\", \"label_namespace\": \"upos\"},\n17:    \"cpos_tags\": {\"source_key\": \"cpos\", \"label_namespace\": \"xpos\"},\n18:    \"deprel_tags\": {\"source_key\": \"deprel\", \"label_namespace\": \"deprel\"},\n19:    \"dep_chunk_tags\": {\"source_key\": \"depchunk\", \"label_namespace\": \"depchunk\"},\n20:    \"parent_clauses\": {\"source_key\": \"parentclauses\", \"label_namespace\": \"parent_clauses\"},\n21:    \"s_type\": {\"source_key\": \"s_type\", \"label_namespace\": \"s_type\"},\n22:    \"case_tags\": {\"source_key\": \"case\", \"label_namespace\": \"case\"},\n23:    \"genre_tags\": {\"source_key\": \"genre\", \"label_namespace\": \"genre\"},\n24:    \"head_distances\": {\"source_key\": \"head_dist\", \"xform_fn\": \"abs_natural_log\"},\n25:    \"document_depth\": {\"source_key\": \"sent_doc_percentile\"},\n26:    \"sentence_length\": {\"source_key\": \"s_len\", \"xform_fn\": \"natural_log\"},\n27:    \"token_lengths\": {\"source_key\": \"tok_len\", \"xform_fn\": \"natural_log\"}\n28:};\n29:\n30:// For more info on config files generally, see https://guide.allennlp.org/using-config-files\n31:{\n32:    \"dataset_reader\" : {\n33:        \"type\": \"disrpt_2021_seg\",\n34:        \"token_indexers\": {\n35:            \"tokens\": {\n36:                \"type\": \"pretrained_transformer_mismatched\",\n37:                \"model_name\": transformer_model_name\n38:            },\n39:            \"fasttext\": {\n40:                \"type\": \"single_id\",\n41:                \"namespace\": \"fasttext\",\n42:            },\n43:            \"token_characters\": import \"../../components/char_indexer.libsonnet\"\n44:        },\n45:        \"tokenizer\": {\n46:            \"type\": \"whitespace\"\n47:        },\n48:        \"token_features\": token_features\n49:    },\n50:    \"model\": {\n51:        \"type\": \"disrpt_2021_seg_baseline\",\n52:        \"embedder\": {\n53:            \"token_embedders\": {\n54:                \"tokens\": {\n55:                    \"type\": \"pretrained_transformer_mismatched\",\n56:                    \"model_name\": transformer_model_name,\n57:                    \"train_parameters\": true,\n58:                    \"last_layer_only\": true\n59:                },\n60:                \"fasttext\": {\n61:                    \"type\": \"embedding\",\n62:                    \"vocab_namespace\": \"fasttext\",\n63:                    \"embedding_dim\": 300,\n64:                    \"trainable\": false,\n65:                    \"pretrained_file\": std.extVar(\"FASTTEXT_EMBEDDING_FILE\")\n66:                },\n67:                \"token_characters\": import \"../../components/char_embedder.libsonnet\"\n68:            }\n69:        },\n70:        // seq2vec encoders for neighbor sentences\n71:        \"prev_sentence_encoder\": context_encoder,\n72:        \"next_sentence_encoder\": context_encoder,\n73:        // our encoder isn't fully configurable here because its input size needs to be determined\n74:        // at the start of the program. so, it'll always be a bilstm, but you can use the two items\n75:        // below to configure the most important hyperparameters it has\n76:        \"encoder_hidden_dim\": encoder_hidden_dim,\n77:        \"encoder_recurrent_dropout\": 0.1,\n78:        // end encoder hyperparams\n79:        \"dropout\": 0.4,\n80:        \"feature_dropout\": 0.0,\n81:        \"token_features\": token_features,\n82:        \"use_crf\": if std.extVar(\"USE_CRF\") == \"1\" then true else false\n83:    },\n84:    \"train_data_path\": std.extVar(\"TRAIN_DATA_PATH\"),\n85:    \"validation_data_path\": std.extVar(\"VALIDATION_DATA_PATH\"),\n86:    \"data_loader\": {\n87:        \"batch_size\": 4,\n88:        \"shuffle\": true\n89:    },\n90:    \"trainer\": {\n91:        \"optimizer\": {\n92:            \"type\": \"huggingface_adamw\",\n93:            \"lr\": 5e-4,\n94:            \"parameter_groups\": [\n95:                [[\".*transformer.*\"], {\"lr\": 1e-5}]\n96:            ]\n97:        },\n98:        \"patience\": 10,\n99:        \"num_epochs\": 1,\n100:        // probably best to just use loss\n101:        \"validation_metric\": \"+span_f1\"\n102:    }\n103:}\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 99:99\n",
    "        \"num_epochs\": 1,\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300cd62f",
   "metadata": {},
   "source": [
    "### Step 27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c29b3b0",
   "metadata": {},
   "source": [
    "Execution Start time: 05:06:35, End time: 05:08:55 (duration: 140 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05f602f",
   "metadata": {},
   "source": [
    "**Thought**: The configuration file has been updated to run for only one epoch. Now, let's run the script again to train and evaluate the model with the correct settings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1016ae3c",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22805d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"models/deu.rst.pcc_seg_bert_baseline_ft\" already exists. Removing it now...\n\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n# Training on deu.rst.pcc\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n\ndata/2021/deu.rst.pcc/deu.rst.pcc_train.conllu\n2024-08-22 05:06:43,497 - INFO - allennlp.common.plugins - Plugin allennlp_models available\n2024-08-22 05:06:44,265 - ERROR - allennlp.common.plugins - Plugin gucorpling_models could not be loaded: No module named 'flair'\n2024-08-22 05:06:44,362 - INFO - allennlp.common.checks - Pytorch version: 1.12.1+cu102\n\nloading instances: 0it [00:00, ?it/s]2024-08-22 05:06:45,308 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'case'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n2024-08-22 05:06:45,308 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'xpos'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n2024-08-22 05:06:45,308 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'depchunk'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n2024-08-22 05:06:45,309 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'deprel'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n2024-08-22 05:06:45,309 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'genre'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n2024-08-22 05:06:45,309 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'parent_clauses'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n2024-08-22 05:06:45,309 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'upos'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n2024-08-22 05:06:45,309 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 's_type'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n\nloading instances: 10it [00:00, 866.14it/s]\n\nloading instances: 0it [00:00, ?it/s]\nloading instances: 10it [00:00, 1262.01it/s]\n2024-08-22 05:06:45,322 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.\n\nbuilding vocab: 0it [00:00, ?it/s]\nbuilding vocab: 20it [00:00, 3738.74it/s]\n2024-08-22 05:06:45,331 - INFO - allennlp.modules.token_embedders.embedding - Reading pretrained embeddings from file\n2024-08-22 05:06:45,389 - INFO - cached_path - cache of https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.de.300.vec.gz is up-to-date\n2024-08-22 05:06:45,392 - INFO - allennlp.modules.token_embedders.embedding - Recognized a header line in the embedding file with number of tokens: 2000000\n\n  0%|          | 0/2000000 [00:00<?, ?it/s]\n  0%|          | 3576/2000000 [00:00<00:55, 35737.03it/s]\n  0%|          | 7455/2000000 [00:00<00:53, 37528.07it/s]\n  1%|          | 11213/2000000 [00:00<00:52, 37544.40it/s]\n  1%|          | 15087/2000000 [00:00<00:52, 38008.37it/s]\n  1%|          | 18929/2000000 [00:00<00:51, 38155.86it/s]\n  1%|1         | 22817/2000000 [00:00<00:51, 38395.12it/s]\n  1%|1         | 26693/2000000 [00:00<00:51, 38505.27it/s]\n  2%|1         | 30588/2000000 [00:00<00:50, 38636.32it/s]\n  2%|1         | 34479/2000000 [00:00<00:50, 38717.41it/s]\n  2%|1         | 38351/2000000 [00:01<00:50, 38657.95it/s]\n  2%|2         | 42231/2000000 [00:01<00:50, 38699.32it/s]\n  2%|2         | 46101/2000000 [00:01<00:50, 38693.34it/s]\n  2%|2         | 49988/2000000 [00:01<00:50, 38736.06it/s]\n  3%|2         | 53866/2000000 [00:01<00:50, 38738.66it/s]\n  3%|2         | 57740/2000000 [00:01<00:50, 38642.19it/s]\n  3%|3         | 61620/2000000 [00:01<00:50, 38683.37it/s]\n  3%|3         | 65504/2000000 [00:01<00:49, 38729.62it/s]\n  3%|3         | 69378/2000000 [00:01<00:49, 38731.25it/s]\n  4%|3         | 73262/2000000 [00:01<00:49, 38754.77it/s]\n  4%|3         | 77138/2000000 [00:02<00:49, 38717.80it/s]\n  4%|4         | 81018/2000000 [00:02<00:49, 38736.78it/s]\n  4%|4         | 84896/2000000 [00:02<00:49, 38744.13it/s]\n  4%|4         | 88771/2000000 [00:02<00:49, 38674.37it/s]\n  5%|4         | 92639/2000000 [00:02<00:49, 38620.22it/s]\n  5%|4         | 96560/2000000 [00:02<00:49, 38785.35it/s]\n  5%|5         | 100439/2000000 [00:02<00:49, 38736.11it/s]\n  5%|5         | 104318/2000000 [00:02<00:48, 38751.48it/s]\n  5%|5         | 108194/2000000 [00:02<00:48, 38740.27it/s]\n  6%|5         | 112069/2000000 [00:02<00:48, 38683.04it/s]\n  6%|5         | 115947/2000000 [00:03<00:48, 38704.45it/s]\n  6%|5         | 119825/2000000 [00:03<00:48, 38724.72it/s]\n  6%|6         | 123710/2000000 [00:03<00:48, 38760.75it/s]\n  6%|6         | 127587/2000000 [00:03<00:48, 38760.62it/s]\n  7%|6         | 131464/2000000 [00:03<00:48, 38755.11it/s]\n  7%|6         | 135343/2000000 [00:03<00:48, 38759.98it/s]\n  7%|6         | 139220/2000000 [00:03<00:48, 38751.86it/s]\n  7%|7         | 143097/2000000 [00:03<00:47, 38754.82it/s]\n  7%|7         | 146973/2000000 [00:03<00:47, 38700.16it/s]\n  8%|7         | 150889/2000000 [00:03<00:47, 38834.33it/s]\n  8%|7         | 154804/2000000 [00:04<00:47, 38919.52it/s]\n  8%|7         | 158696/2000000 [00:04<00:47, 38913.64it/s]\n  8%|8         | 162609/2000000 [00:04<00:47, 38972.42it/s]\n  8%|8         | 166515/2000000 [00:04<00:47, 38991.75it/s]\n  9%|8         | 170415/2000000 [00:04<00:47, 38915.84it/s]\n  9%|8         | 174307/2000000 [00:04<00:46, 38871.94it/s]\n  9%|8         | 178195/2000000 [00:04<00:46, 38791.82it/s]\n  9%|9         | 182078/2000000 [00:04<00:46, 38799.27it/s]\n  9%|9         | 185958/2000000 [00:04<00:46, 38769.97it/s]\n  9%|9         | 189837/2000000 [00:04<00:46, 38770.53it/s]\n 10%|9         | 193729/2000000 [00:05<00:46, 38811.99it/s]\n 10%|9         | 197629/2000000 [00:05<00:46, 38865.28it/s]\n 10%|#         | 201516/2000000 [00:05<00:46, 38794.15it/s]\n 10%|#         | 205396/2000000 [00:05<00:46, 38753.86it/s]\n 10%|#         | 209277/2000000 [00:05<00:46, 38759.34it/s]\n 11%|#         | 213162/2000000 [00:05<00:46, 38779.44it/s]\n 11%|#         | 217047/2000000 [00:05<00:45, 38792.42it/s]\n 11%|#1        | 220927/2000000 [00:05<00:45, 38724.75it/s]\n 11%|#1        | 224801/2000000 [00:05<00:45, 38728.44it/s]\n 11%|#1        | 228674/2000000 [00:05<00:45, 38686.29it/s]\n 12%|#1        | 232545/2000000 [00:06<00:45, 38687.74it/s]\n 12%|#1        | 236414/2000000 [00:06<00:45, 38671.80it/s]\n 12%|#2        | 240282/2000000 [00:06<00:45, 38590.46it/s]\n 12%|#2        | 244159/2000000 [00:06<00:45, 38640.43it/s]\n 12%|#2        | 248048/2000000 [00:06<00:45, 38711.89it/s]\n 13%|#2        | 251940/2000000 [00:06<00:45, 38773.20it/s]\n 13%|#2        | 255828/2000000 [00:06<00:44, 38800.73it/s]\n 13%|#2        | 259709/2000000 [00:06<00:44, 38767.53it/s]\n 13%|#3        | 263586/2000000 [00:06<00:44, 38765.40it/s]\n 13%|#3        | 267463/2000000 [00:06<00:44, 38752.97it/s]\n 14%|#3        | 271350/2000000 [00:07<00:44, 38781.49it/s]\n 14%|#3        | 275239/2000000 [00:07<00:44, 38810.70it/s]\n 14%|#3        | 279121/2000000 [00:07<00:44, 38735.17it/s]\n 14%|#4        | 283018/2000000 [00:07<00:44, 38797.02it/s]\n 14%|#4        | 286898/2000000 [00:07<00:44, 38784.74it/s]\n 15%|#4        | 290777/2000000 [00:07<00:44, 38741.71it/s]\n 15%|#4        | 294652/2000000 [00:07<00:44, 38631.33it/s]\n 15%|#4        | 298516/2000000 [00:07<00:44, 38600.80it/s]\n 15%|#5        | 302377/2000000 [00:07<00:44, 38460.56it/s]\n 15%|#5        | 306224/2000000 [00:07<00:44, 38453.65it/s]\n 16%|#5        | 310071/2000000 [00:08<00:43, 38447.23it/s]\n 16%|#5        | 313934/2000000 [00:08<00:43, 38491.58it/s]\n 16%|#5        | 317808/2000000 [00:08<00:43, 38562.41it/s]\n 16%|#6        | 321665/2000000 [00:08<00:43, 38523.21it/s]\n 16%|#6        | 325541/2000000 [00:08<00:43, 38586.83it/s]\n 16%|#6        | 329414/2000000 [00:08<00:43, 38624.66it/s]\n 17%|#6        | 333277/2000000 [00:08<00:43, 38596.29it/s]\n 17%|#6        | 337171/2000000 [00:08<00:42, 38692.87it/s]\n 17%|#7        | 341044/2000000 [00:08<00:42, 38698.87it/s]\n 17%|#7        | 344917/2000000 [00:08<00:42, 38700.96it/s]\n 17%|#7        | 348788/2000000 [00:09<00:42, 38617.07it/s]\n 18%|#7        | 352650/2000000 [00:09<00:42, 38592.09it/s]\n 18%|#7        | 356536/2000000 [00:09<00:42, 38667.99it/s]\n 18%|#8        | 360403/2000000 [00:09<00:42, 38654.25it/s]\n 18%|#8        | 364269/2000000 [00:09<00:42, 38583.05it/s]\n 18%|#8        | 368128/2000000 [00:09<00:42, 38478.62it/s]\n 19%|#8        | 371976/2000000 [00:09<00:42, 38409.48it/s]\n 19%|#8        | 375818/2000000 [00:09<00:42, 38410.09it/s]\n 19%|#8        | 379660/2000000 [00:09<00:42, 38364.84it/s]\n 19%|#9        | 383522/2000000 [00:09<00:42, 38435.38it/s]\n 19%|#9        | 387366/2000000 [00:10<00:41, 38417.71it/s]\n 20%|#9        | 391221/2000000 [00:10<00:41, 38449.74it/s]\n 20%|#9        | 395066/2000000 [00:10<00:41, 38440.99it/s]\n 20%|#9        | 398942/2000000 [00:10<00:41, 38530.87it/s]\n 20%|##        | 402796/2000000 [00:10<00:41, 38507.23it/s]\n 20%|##        | 406657/2000000 [00:10<00:41, 38533.55it/s]\n 21%|##        | 410537/2000000 [00:10<00:41, 38610.31it/s]\n 21%|##        | 414399/2000000 [00:10<00:41, 37961.86it/s]\n 21%|##        | 418198/2000000 [00:10<00:41, 37878.21it/s]\n 21%|##1       | 421988/2000000 [00:10<00:42, 37532.98it/s]\n 21%|##1       | 425743/2000000 [00:11<00:42, 37062.05it/s]\n 21%|##1       | 429452/2000000 [00:11<00:42, 36947.45it/s]\n 22%|##1       | 433301/2000000 [00:11<00:41, 37391.90it/s]\n 22%|##1       | 437139/2000000 [00:11<00:41, 37676.79it/s]\n 22%|##2       | 441023/2000000 [00:11<00:41, 38019.24it/s]\n 22%|##2       | 444876/2000000 [00:11<00:40, 38169.67it/s]\n 22%|##2       | 448756/2000000 [00:11<00:40, 38357.21it/s]\n 23%|##2       | 452626/2000000 [00:11<00:40, 38450.53it/s]\n 23%|##2       | 456477/2000000 [00:11<00:40, 38461.37it/s]\n 23%|##3       | 460328/2000000 [00:11<00:40, 38474.00it/s]\n 23%|##3       | 464186/2000000 [00:12<00:39, 38500.13it/s]\n 23%|##3       | 468037/2000000 [00:12<00:40, 38225.19it/s]\n 24%|##3       | 471918/2000000 [00:12<00:39, 38388.40it/s]\n 24%|##3       | 475758/2000000 [00:12<00:39, 38361.33it/s]\n 24%|##3       | 479621/2000000 [00:12<00:39, 38434.32it/s]\n 24%|##4       | 483476/2000000 [00:12<00:39, 38461.42it/s]\n 24%|##4       | 487335/2000000 [00:12<00:39, 38492.79it/s]\n 25%|##4       | 491197/2000000 [00:12<00:39, 38528.73it/s]\n 25%|##4       | 495050/2000000 [00:12<00:39, 38481.61it/s]\n 25%|##4       | 498900/2000000 [00:12<00:39, 38474.05it/s]\n 25%|##5       | 502751/2000000 [00:13<00:38, 38483.90it/s]\n 25%|##5       | 506614/2000000 [00:13<00:38, 38519.19it/s]\n 26%|##5       | 510491/2000000 [00:13<00:38, 38589.69it/s]\n 26%|##5       | 514383/2000000 [00:13<00:38, 38678.50it/s]\n 26%|##5       | 518251/2000000 [00:13<00:38, 38573.33it/s]\n 26%|##6       | 522109/2000000 [00:13<00:38, 38397.68it/s]\n 26%|##6       | 525949/2000000 [00:13<00:38, 38346.83it/s]\n 26%|##6       | 529828/2000000 [00:13<00:38, 38470.53it/s]\n 27%|##6       | 533682/2000000 [00:13<00:38, 38488.79it/s]\n 27%|##6       | 537533/2000000 [00:13<00:37, 38489.85it/s]\n 27%|##7       | 541383/2000000 [00:14<00:38, 38219.07it/s]\n 27%|##7       | 545206/2000000 [00:14<00:38, 37703.40it/s]\n 27%|##7       | 548986/2000000 [00:14<00:38, 37711.64it/s]\n 28%|##7       | 552820/2000000 [00:14<00:38, 37894.33it/s]\n 28%|##7       | 556631/2000000 [00:14<00:38, 37949.04it/s]\n 28%|##8       | 560435/2000000 [00:14<00:37, 37972.82it/s]\n 28%|##8       | 564297/2000000 [00:14<00:37, 38163.92it/s]\n 28%|##8       | 568130/2000000 [00:14<00:37, 38202.64it/s]\n 29%|##8       | 571963/2000000 [00:14<00:37, 38234.92it/s]\n 29%|##8       | 575787/2000000 [00:14<00:37, 38027.72it/s]\n 29%|##8       | 579591/2000000 [00:15<00:37, 37808.02it/s]\n 29%|##9       | 583373/2000000 [00:15<00:37, 37731.24it/s]\n 29%|##9       | 587147/2000000 [00:15<00:37, 37645.66it/s]\n 30%|##9       | 590912/2000000 [00:15<00:37, 37555.85it/s]\n 30%|##9       | 594668/2000000 [00:15<00:37, 37531.54it/s]\n 30%|##9       | 598449/2000000 [00:15<00:37, 37609.84it/s]\n 30%|###       | 602211/2000000 [00:15<00:37, 37578.74it/s]\n 30%|###       | 605977/2000000 [00:15<00:37, 37601.12it/s]\n 30%|###       | 609807/2000000 [00:15<00:36, 37804.78it/s]\n 31%|###       | 613588/2000000 [00:15<00:36, 37488.88it/s]\n 31%|###       | 617370/2000000 [00:16<00:36, 37586.70it/s]\n 31%|###1      | 621219/2000000 [00:16<00:36, 37847.37it/s]\n 31%|###1      | 625005/2000000 [00:16<00:36, 37668.05it/s]\n 31%|###1      | 628773/2000000 [00:16<00:36, 37544.29it/s]\n 32%|###1      | 632626/2000000 [00:16<00:36, 37831.18it/s]\n 32%|###1      | 636492/2000000 [00:16<00:35, 38075.01it/s]\n 32%|###2      | 640356/2000000 [00:16<00:35, 38242.17it/s]\n 32%|###2      | 644215/2000000 [00:16<00:35, 38343.14it/s]\n 32%|###2      | 648055/2000000 [00:16<00:35, 38359.08it/s]\n 33%|###2      | 651913/2000000 [00:16<00:35, 38420.95it/s]\n 33%|###2      | 655797/2000000 [00:17<00:34, 38539.09it/s]\n 33%|###2      | 659680/2000000 [00:17<00:34, 38618.99it/s]\n 33%|###3      | 663579/2000000 [00:17<00:34, 38722.78it/s]\n 33%|###3      | 667456/2000000 [00:17<00:34, 38734.07it/s]\n 34%|###3      | 671330/2000000 [00:17<00:34, 38674.78it/s]\n 34%|###3      | 675198/2000000 [00:17<00:34, 38577.88it/s]\n 34%|###3      | 679064/2000000 [00:17<00:34, 38595.08it/s]\n 34%|###4      | 682955/2000000 [00:17<00:34, 38678.77it/s]\n 34%|###4      | 686853/2000000 [00:17<00:33, 38758.79it/s]\n 35%|###4      | 690729/2000000 [00:17<00:33, 38702.68it/s]\n 35%|###4      | 694609/2000000 [00:18<00:33, 38725.20it/s]\n 35%|###4      | 698482/2000000 [00:18<00:33, 38501.36it/s]\n 35%|###5      | 702333/2000000 [00:18<00:33, 38363.93it/s]\n 35%|###5      | 706189/2000000 [00:18<00:33, 38417.62it/s]\n 36%|###5      | 710051/2000000 [00:18<00:33, 38471.78it/s]\n 36%|###5      | 713935/2000000 [00:18<00:33, 38576.98it/s]\n 36%|###5      | 717825/2000000 [00:18<00:33, 38672.93it/s]\n 36%|###6      | 721731/2000000 [00:18<00:32, 38784.19it/s]\n 36%|###6      | 725619/2000000 [00:18<00:32, 38809.64it/s]\n 36%|###6      | 729503/2000000 [00:18<00:32, 38807.13it/s]\n 37%|###6      | 733384/2000000 [00:19<00:32, 38741.17it/s]\n 37%|###6      | 737260/2000000 [00:19<00:32, 38742.81it/s]\n 37%|###7      | 741148/2000000 [00:19<00:32, 38782.18it/s]\n 37%|###7      | 745027/2000000 [00:19<00:32, 38764.17it/s]\n 37%|###7      | 748925/2000000 [00:19<00:32, 38826.20it/s]\n 38%|###7      | 752808/2000000 [00:19<00:32, 38764.35it/s]\n 38%|###7      | 756692/2000000 [00:19<00:32, 38784.14it/s]\n 38%|###8      | 760576/2000000 [00:19<00:31, 38797.59it/s]\n 38%|###8      | 764476/2000000 [00:19<00:31, 38849.05it/s]\n 38%|###8      | 768363/2000000 [00:19<00:31, 38846.21it/s]\n 39%|###8      | 772248/2000000 [00:20<00:31, 38792.54it/s]\n 39%|###8      | 776143/2000000 [00:20<00:31, 38830.51it/s]\n 39%|###9      | 780027/2000000 [00:20<00:31, 38818.67it/s]\n 39%|###9      | 783929/2000000 [00:20<00:31, 38870.01it/s]\n 39%|###9      | 787817/2000000 [00:20<00:31, 38791.19it/s]\n 40%|###9      | 791707/2000000 [00:20<00:31, 38815.41it/s]\n 40%|###9      | 795589/2000000 [00:20<00:31, 38331.76it/s]\n 40%|###9      | 799471/2000000 [00:20<00:31, 38474.07it/s]\n 40%|####      | 803347/2000000 [00:20<00:31, 38554.03it/s]\n 40%|####      | 807253/2000000 [00:20<00:30, 38694.11it/s]\n 41%|####      | 811124/2000000 [00:21<00:30, 38683.32it/s]\n 41%|####      | 815004/2000000 [00:21<00:30, 38713.40it/s]\n 41%|####      | 818876/2000000 [00:21<00:30, 38639.82it/s]\n 41%|####1     | 822752/2000000 [00:21<00:30, 38664.78it/s]\n 41%|####1     | 826646/2000000 [00:21<00:30, 38741.29it/s]\n 42%|####1     | 830521/2000000 [00:21<00:30, 38638.70it/s]\n 42%|####1     | 834412/2000000 [00:21<00:30, 38712.76it/s]\n 42%|####1     | 838284/2000000 [00:21<00:30, 38659.84it/s]\n 42%|####2     | 842151/2000000 [00:21<00:30, 38451.53it/s]\n 42%|####2     | 846013/2000000 [00:21<00:29, 38498.38it/s]\n 42%|####2     | 849867/2000000 [00:22<00:29, 38506.35it/s]\n 43%|####2     | 853758/2000000 [00:22<00:29, 38617.05it/s]\n 43%|####2     | 857639/2000000 [00:22<00:29, 38672.28it/s]\n 43%|####3     | 861519/2000000 [00:22<00:29, 38708.59it/s]\n 43%|####3     | 865414/2000000 [00:22<00:29, 38771.24it/s]\n 43%|####3     | 869300/2000000 [00:22<00:29, 38792.14it/s]\n 44%|####3     | 873196/2000000 [00:22<00:29, 38837.73it/s]\n 44%|####3     | 877084/2000000 [00:22<00:28, 38841.00it/s]\n 44%|####4     | 880969/2000000 [00:22<00:28, 38776.91it/s]\n 44%|####4     | 884870/2000000 [00:22<00:28, 38836.94it/s]\n 44%|####4     | 888772/2000000 [00:23<00:28, 38889.24it/s]\n 45%|####4     | 892681/2000000 [00:23<00:28, 38947.31it/s]\n 45%|####4     | 896601/2000000 [00:23<00:28, 39019.61it/s]\n 45%|####5     | 900503/2000000 [00:23<00:28, 38985.74it/s]\n 45%|####5     | 904415/2000000 [00:23<00:28, 39017.07it/s]\n 45%|####5     | 908317/2000000 [00:23<00:28, 38953.32it/s]\n 46%|####5     | 912213/2000000 [00:23<00:27, 38935.18it/s]\n 46%|####5     | 916131/2000000 [00:23<00:27, 39002.70it/s]\n 46%|####6     | 920032/2000000 [00:23<00:27, 38948.91it/s]\n 46%|####6     | 923943/2000000 [00:23<00:27, 38988.98it/s]\n 46%|####6     | 927848/2000000 [00:24<00:27, 39004.01it/s]\n 47%|####6     | 931749/2000000 [00:24<00:27, 38989.16it/s]\n 47%|####6     | 935648/2000000 [00:24<00:27, 38942.79it/s]\n 47%|####6     | 939550/2000000 [00:24<00:27, 38957.86it/s]\n 47%|####7     | 943446/2000000 [00:24<00:27, 38846.81it/s]\n 47%|####7     | 947349/2000000 [00:24<00:27, 38891.47it/s]\n 48%|####7     | 951239/2000000 [00:24<00:27, 38827.58it/s]\n 48%|####7     | 955122/2000000 [00:24<00:26, 38797.66it/s]\n 48%|####7     | 959013/2000000 [00:24<00:26, 38823.86it/s]\n 48%|####8     | 962916/2000000 [00:24<00:26, 38883.10it/s]\n 48%|####8     | 966805/2000000 [00:25<00:26, 38824.03it/s]\n 49%|####8     | 970727/2000000 [00:25<00:26, 38941.89it/s]\n 49%|####8     | 974639/2000000 [00:25<00:26, 38990.67it/s]\n 49%|####8     | 978539/2000000 [00:25<00:26, 38964.69it/s]\n 49%|####9     | 982441/2000000 [00:25<00:26, 38977.93it/s]\n 49%|####9     | 986352/2000000 [00:25<00:25, 39006.38it/s]\n 50%|####9     | 990276/2000000 [00:25<00:25, 39064.49it/s]\n 50%|####9     | 994189/2000000 [00:25<00:25, 39082.41it/s]\n 50%|####9     | 998102/2000000 [00:25<00:25, 39095.71it/s]\n 50%|#####     | 1002012/2000000 [00:25<00:25, 39087.95it/s]\n 50%|#####     | 1005921/2000000 [00:26<00:25, 38938.45it/s]\n 50%|#####     | 1009833/2000000 [00:26<00:25, 38985.98it/s]\n 51%|#####     | 1013732/2000000 [00:26<00:25, 38940.42it/s]\n 51%|#####     | 1017639/2000000 [00:26<00:25, 38972.68it/s]\n 51%|#####1    | 1021537/2000000 [00:26<00:25, 38917.70it/s]\n 51%|#####1    | 1025429/2000000 [00:26<00:25, 38882.96it/s]\n 51%|#####1    | 1029318/2000000 [00:26<00:25, 38819.23it/s]\n 52%|#####1    | 1033200/2000000 [00:26<00:24, 38683.96it/s]\n 52%|#####1    | 1037069/2000000 [00:26<00:25, 37593.40it/s]\n 52%|#####2    | 1040835/2000000 [00:27<00:26, 36173.03it/s]\n 52%|#####2    | 1044466/2000000 [00:27<00:26, 36179.08it/s]\n 52%|#####2    | 1048287/2000000 [00:27<00:25, 36763.70it/s]\n 53%|#####2    | 1052119/2000000 [00:27<00:25, 37210.80it/s]\n 53%|#####2    | 1056007/2000000 [00:27<00:25, 37701.79it/s]\n 53%|#####2    | 1059897/2000000 [00:27<00:24, 38051.99it/s]\n 53%|#####3    | 1063755/2000000 [00:27<00:24, 38208.18it/s]\n 53%|#####3    | 1067650/2000000 [00:27<00:24, 38423.10it/s]\n 54%|#####3    | 1071544/2000000 [00:27<00:24, 38569.59it/s]\n 54%|#####3    | 1075457/2000000 [00:27<00:23, 38736.37it/s]\n 54%|#####3    | 1079334/2000000 [00:28<00:23, 38743.52it/s]\n 54%|#####4    | 1083210/2000000 [00:28<00:23, 38728.88it/s]\n 54%|#####4    | 1087103/2000000 [00:28<00:23, 38785.02it/s]\n 55%|#####4    | 1090997/2000000 [00:28<00:23, 38829.92it/s]\n 55%|#####4    | 1094881/2000000 [00:28<00:23, 38614.24it/s]\n 55%|#####4    | 1098775/2000000 [00:28<00:23, 38702.36it/s]\n 55%|#####5    | 1102709/2000000 [00:28<00:23, 38886.82it/s]\n 55%|#####5    | 1106611/2000000 [00:28<00:22, 38915.69it/s]\n 56%|#####5    | 1110521/2000000 [00:28<00:22, 38960.32it/s]\n 56%|#####5    | 1114418/2000000 [00:28<00:22, 38911.32it/s]\n 56%|#####5    | 1118320/2000000 [00:29<00:22, 38940.90it/s]\n 56%|#####6    | 1122215/2000000 [00:29<00:22, 38858.60it/s]\n 56%|#####6    | 1126131/2000000 [00:29<00:22, 38942.55it/s]\n 57%|#####6    | 1130026/2000000 [00:29<00:22, 38819.50it/s]\n 57%|#####6    | 1133909/2000000 [00:29<00:22, 38796.08it/s]\n 57%|#####6    | 1137789/2000000 [00:29<00:22, 38772.43it/s]\n 57%|#####7    | 1141667/2000000 [00:29<00:22, 38739.99it/s]\n 57%|#####7    | 1145575/2000000 [00:29<00:22, 38831.95it/s]\n 57%|#####7    | 1149459/2000000 [00:29<00:21, 38824.37it/s]\n 58%|#####7    | 1153378/2000000 [00:29<00:21, 38930.96it/s]\n 58%|#####7    | 1157272/2000000 [00:30<00:21, 38914.80it/s]\n 58%|#####8    | 1161164/2000000 [00:30<00:21, 38903.53it/s]\n 58%|#####8    | 1165055/2000000 [00:30<00:21, 38872.26it/s]\n 58%|#####8    | 1168943/2000000 [00:30<00:21, 38796.71it/s]\n 59%|#####8    | 1172842/2000000 [00:30<00:21, 38848.15it/s]\n 59%|#####8    | 1176727/2000000 [00:30<00:21, 38828.17it/s]\n 59%|#####9    | 1180615/2000000 [00:30<00:21, 38837.44it/s]\n 59%|#####9    | 1184522/2000000 [00:30<00:20, 38902.24it/s]\n 59%|#####9    | 1188413/2000000 [00:30<00:20, 38832.40it/s]\n 60%|#####9    | 1192314/2000000 [00:30<00:20, 38881.92it/s]\n 60%|#####9    | 1196241/2000000 [00:31<00:20, 38988.92it/s]\n 60%|######    | 1200154/2000000 [00:31<00:20, 39022.90it/s]\n 60%|######    | 1204057/2000000 [00:31<00:20, 38957.17it/s]\n 60%|######    | 1207953/2000000 [00:31<00:20, 38946.13it/s]\n 61%|######    | 1211848/2000000 [00:31<00:20, 38691.81it/s]\n 61%|######    | 1215744/2000000 [00:31<00:20, 38766.34it/s]\n 61%|######    | 1219656/2000000 [00:31<00:20, 38868.86it/s]\n 61%|######1   | 1223546/2000000 [00:31<00:19, 38873.17it/s]\n 61%|######1   | 1227466/2000000 [00:31<00:19, 38966.01it/s]\n 62%|######1   | 1231368/2000000 [00:31<00:19, 38980.89it/s]\n 62%|######1   | 1235280/2000000 [00:32<00:19, 39021.02it/s]\n 62%|######1   | 1239207/2000000 [00:32<00:19, 39091.92it/s]\n 62%|######2   | 1243117/2000000 [00:32<00:19, 38916.60it/s]\n 62%|######2   | 1247009/2000000 [00:32<00:19, 38875.46it/s]\n 63%|######2   | 1250897/2000000 [00:32<00:19, 38851.87it/s]\n 63%|######2   | 1254817/2000000 [00:32<00:19, 38953.90it/s]\n 63%|######2   | 1258713/2000000 [00:32<00:19, 38891.71it/s]\n 63%|######3   | 1262623/2000000 [00:32<00:18, 38951.52it/s]\n 63%|######3   | 1266519/2000000 [00:32<00:18, 38944.56it/s]\n 64%|######3   | 1270414/2000000 [00:32<00:18, 38789.04it/s]\n 64%|######3   | 1274301/2000000 [00:33<00:18, 38808.52it/s]\n 64%|######3   | 1278221/2000000 [00:33<00:18, 38917.60it/s]\n 64%|######4   | 1282134/2000000 [00:33<00:18, 38977.13it/s]\n 64%|######4   | 1286039/2000000 [00:33<00:18, 38996.38it/s]\n 64%|######4   | 1289939/2000000 [00:33<00:18, 38912.77it/s]\n 65%|######4   | 1293831/2000000 [00:33<00:18, 38891.02it/s]\n 65%|######4   | 1297743/2000000 [00:33<00:18, 38952.88it/s]\n 65%|######5   | 1301712/2000000 [00:33<00:17, 39171.45it/s]\n 65%|######5   | 1305676/2000000 [00:33<00:17, 39308.13it/s]\n 65%|######5   | 1309607/2000000 [00:33<00:17, 39184.12it/s]\n 66%|######5   | 1313526/2000000 [00:34<00:17, 39029.94it/s]\n 66%|######5   | 1317430/2000000 [00:34<00:17, 38919.48it/s]\n 66%|######6   | 1321329/2000000 [00:34<00:17, 38937.12it/s]\n 66%|######6   | 1325234/2000000 [00:34<00:17, 38961.90it/s]\n 66%|######6   | 1329139/2000000 [00:34<00:17, 38981.76it/s]\n 67%|######6   | 1333038/2000000 [00:34<00:17, 38727.12it/s]\n 67%|######6   | 1336938/2000000 [00:34<00:17, 38798.59it/s]\n 67%|######7   | 1340819/2000000 [00:34<00:17, 38684.83it/s]\n 67%|######7   | 1344688/2000000 [00:34<00:16, 38630.92it/s]\n 67%|######7   | 1348552/2000000 [00:34<00:17, 38312.77it/s]\n 68%|######7   | 1352420/2000000 [00:35<00:16, 38414.38it/s]\n 68%|######7   | 1356282/2000000 [00:35<00:16, 38472.26it/s]\n 68%|######8   | 1360130/2000000 [00:35<00:16, 38434.14it/s]\n 68%|######8   | 1364045/2000000 [00:35<00:16, 38647.34it/s]\n 68%|######8   | 1367939/2000000 [00:35<00:16, 38730.12it/s]\n 69%|######8   | 1371815/2000000 [00:35<00:16, 38734.46it/s]\n 69%|######8   | 1375715/2000000 [00:35<00:16, 38811.77it/s]\n 69%|######8   | 1379609/2000000 [00:35<00:15, 38848.65it/s]\n 69%|######9   | 1383536/2000000 [00:35<00:15, 38972.24it/s]\n 69%|######9   | 1387452/2000000 [00:35<00:15, 39023.94it/s]\n 70%|######9   | 1391375/2000000 [00:36<00:15, 39075.99it/s]\n 70%|######9   | 1395308/2000000 [00:36<00:15, 39141.95it/s]\n 70%|######9   | 1399223/2000000 [00:36<00:15, 39038.66it/s]\n 70%|#######   | 1403127/2000000 [00:36<00:15, 39035.93it/s]\n 70%|#######   | 1407050/2000000 [00:36<00:15, 39084.96it/s]\n 71%|#######   | 1410984/2000000 [00:36<00:15, 39152.63it/s]\n 71%|#######   | 1414903/2000000 [00:36<00:14, 39162.19it/s]\n 71%|#######   | 1418820/2000000 [00:36<00:14, 39163.39it/s]\n 71%|#######1  | 1422737/2000000 [00:36<00:14, 39029.15it/s]\n 71%|#######1  | 1426641/2000000 [00:36<00:14, 38985.67it/s]\n 72%|#######1  | 1430540/2000000 [00:37<00:14, 38956.88it/s]\n 72%|#######1  | 1434458/2000000 [00:37<00:14, 39021.71it/s]\n 72%|#######1  | 1438388/2000000 [00:37<00:14, 39101.76it/s]\n 72%|#######2  | 1442299/2000000 [00:37<00:14, 39066.77it/s]\n 72%|#######2  | 1446224/2000000 [00:37<00:14, 39119.01it/s]\n 73%|#######2  | 1450150/2000000 [00:37<00:14, 39153.83it/s]\n 73%|#######2  | 1454066/2000000 [00:37<00:13, 39112.95it/s]\n 73%|#######2  | 1457978/2000000 [00:37<00:13, 38989.37it/s]\n 73%|#######3  | 1461878/2000000 [00:37<00:13, 38988.11it/s]\n 73%|#######3  | 1465777/2000000 [00:37<00:13, 38985.46it/s]\n 73%|#######3  | 1469685/2000000 [00:38<00:13, 39011.86it/s]\n 74%|#######3  | 1473611/2000000 [00:38<00:13, 39082.34it/s]\n 74%|#######3  | 1477529/2000000 [00:38<00:13, 39110.59it/s]\n 74%|#######4  | 1481450/2000000 [00:38<00:13, 39127.85it/s]\n 74%|#######4  | 1485363/2000000 [00:38<00:13, 39076.41it/s]\n 74%|#######4  | 1489271/2000000 [00:38<00:13, 38907.59it/s]\n 75%|#######4  | 1493162/2000000 [00:38<00:13, 38890.32it/s]\n 75%|#######4  | 1497089/2000000 [00:38<00:12, 38993.11it/s]\n 75%|#######5  | 1501016/2000000 [00:38<00:12, 39069.52it/s]\n 75%|#######5  | 1504939/2000000 [00:38<00:12, 39114.69it/s]\n 75%|#######5  | 1508851/2000000 [00:39<00:12, 39063.86it/s]\n 76%|#######5  | 1512781/2000000 [00:39<00:12, 39128.71it/s]\n 76%|#######5  | 1516707/2000000 [00:39<00:12, 39164.34it/s]\n 76%|#######6  | 1520624/2000000 [00:39<00:12, 39151.50it/s]\n 76%|#######6  | 1524540/2000000 [00:39<00:12, 39100.33it/s]\n 76%|#######6  | 1528451/2000000 [00:39<00:12, 39019.79it/s]\n 77%|#######6  | 1532354/2000000 [00:39<00:11, 39004.07it/s]\n 77%|#######6  | 1536286/2000000 [00:39<00:11, 39094.83it/s]\n 77%|#######7  | 1540214/2000000 [00:39<00:11, 39139.29it/s]\n 77%|#######7  | 1544130/2000000 [00:39<00:11, 39141.76it/s]\n 77%|#######7  | 1548060/2000000 [00:40<00:11, 39187.46it/s]\n 78%|#######7  | 1551979/2000000 [00:40<00:11, 39132.71it/s]\n 78%|#######7  | 1555893/2000000 [00:40<00:11, 39092.14it/s]\n 78%|#######7  | 1559803/2000000 [00:40<00:11, 39020.73it/s]\n 78%|#######8  | 1563712/2000000 [00:40<00:11, 39031.56it/s]\n 78%|#######8  | 1567642/2000000 [00:40<00:11, 39100.11it/s]\n 79%|#######8  | 1571561/2000000 [00:40<00:10, 39117.82it/s]\n 79%|#######8  | 1575476/2000000 [00:40<00:10, 39118.62it/s]\n 79%|#######8  | 1579398/2000000 [00:40<00:10, 39142.49it/s]\n 79%|#######9  | 1583325/2000000 [00:40<00:10, 39178.62it/s]\n 79%|#######9  | 1587243/2000000 [00:41<00:10, 39172.80it/s]\n 80%|#######9  | 1591168/2000000 [00:41<00:10, 39185.04it/s]\n 80%|#######9  | 1595087/2000000 [00:41<00:10, 39038.22it/s]\n 80%|#######9  | 1598991/2000000 [00:41<00:10, 38969.51it/s]\n 80%|########  | 1602898/2000000 [00:41<00:10, 38998.79it/s]\n 80%|########  | 1606829/2000000 [00:41<00:10, 39090.83it/s]\n 81%|########  | 1610755/2000000 [00:41<00:09, 39134.66it/s]\n 81%|########  | 1614678/2000000 [00:41<00:09, 39158.65it/s]\n 81%|########  | 1618594/2000000 [00:41<00:09, 39152.10it/s]\n 81%|########1 | 1622524/2000000 [00:41<00:09, 39186.61it/s]\n 81%|########1 | 1626443/2000000 [00:42<00:09, 39177.08it/s]\n 82%|########1 | 1630361/2000000 [00:42<00:09, 39117.24it/s]\n 82%|########1 | 1634273/2000000 [00:42<00:09, 39026.86it/s]\n 82%|########1 | 1638176/2000000 [00:42<00:09, 38983.79it/s]\n 82%|########2 | 1642075/2000000 [00:42<00:09, 38895.77it/s]\n 82%|########2 | 1645988/2000000 [00:42<00:09, 38960.00it/s]\n 82%|########2 | 1649902/2000000 [00:42<00:08, 39012.84it/s]\n 83%|########2 | 1653828/2000000 [00:42<00:08, 39081.09it/s]\n 83%|########2 | 1657752/2000000 [00:42<00:08, 39128.04it/s]\n 83%|########3 | 1661665/2000000 [00:42<00:08, 38647.51it/s]\n 83%|########3 | 1665575/2000000 [00:43<00:08, 38772.44it/s]\n 83%|########3 | 1669493/2000000 [00:43<00:08, 38893.13it/s]\n 84%|########3 | 1673387/2000000 [00:43<00:08, 38898.88it/s]\n 84%|########3 | 1677281/2000000 [00:43<00:08, 38902.87it/s]\n 84%|########4 | 1681178/2000000 [00:43<00:08, 38922.19it/s]\n 84%|########4 | 1685071/2000000 [00:43<00:08, 38802.06it/s]\n 84%|########4 | 1688989/2000000 [00:43<00:07, 38909.88it/s]\n 85%|########4 | 1692888/2000000 [00:43<00:07, 38928.56it/s]\n 85%|########4 | 1696804/2000000 [00:43<00:07, 38994.59it/s]\n 85%|########5 | 1700739/2000000 [00:43<00:07, 39089.21it/s]\n 85%|########5 | 1704654/2000000 [00:44<00:07, 39100.51it/s]\n 85%|########5 | 1708565/2000000 [00:44<00:07, 39060.97it/s]\n 86%|########5 | 1712486/2000000 [00:44<00:07, 39095.88it/s]\n 86%|########5 | 1716406/2000000 [00:44<00:07, 39123.12it/s]\n 86%|########6 | 1720319/2000000 [00:44<00:07, 39106.58it/s]\n 86%|########6 | 1724230/2000000 [00:44<00:07, 38977.02it/s]\n 86%|########6 | 1728133/2000000 [00:44<00:06, 38991.96it/s]\n 87%|########6 | 1732044/2000000 [00:44<00:06, 39020.51it/s]\n 87%|########6 | 1735968/2000000 [00:44<00:06, 39076.69it/s]\n 87%|########6 | 1739912/2000000 [00:44<00:06, 39178.03it/s]\n 87%|########7 | 1743853/2000000 [00:45<00:06, 39244.56it/s]\n 87%|########7 | 1747783/2000000 [00:45<00:06, 39251.29it/s]\n 88%|########7 | 1751709/2000000 [00:45<00:06, 39196.79it/s]\n 88%|########7 | 1755640/2000000 [00:45<00:06, 39225.50it/s]\n 88%|########7 | 1759570/2000000 [00:45<00:06, 39241.44it/s]\n 88%|########8 | 1763495/2000000 [00:45<00:06, 39131.77it/s]\n 88%|########8 | 1767409/2000000 [00:45<00:05, 39117.25it/s]\n 89%|########8 | 1771321/2000000 [00:45<00:05, 39038.23it/s]\n 89%|########8 | 1775225/2000000 [00:45<00:05, 38955.82it/s]\n 89%|########8 | 1779150/2000000 [00:45<00:05, 39034.70it/s]\n 89%|########9 | 1783083/2000000 [00:46<00:05, 39117.79it/s]\n 89%|########9 | 1787012/2000000 [00:46<00:05, 39165.20it/s]\n 90%|########9 | 1790935/2000000 [00:46<00:05, 39182.86it/s]\n 90%|########9 | 1794868/2000000 [00:46<00:05, 39218.03it/s]\n 90%|########9 | 1798819/2000000 [00:46<00:05, 39296.17it/s]\n 90%|######### | 1802764/2000000 [00:46<00:05, 39339.34it/s]\n 90%|######### | 1806698/2000000 [00:46<00:04, 39314.77it/s]\n 91%|######### | 1810630/2000000 [00:46<00:04, 39274.55it/s]\n 91%|######### | 1814558/2000000 [00:46<00:04, 39183.50it/s]\n 91%|######### | 1818477/2000000 [00:46<00:04, 39065.53it/s]\n 91%|#########1| 1822384/2000000 [00:47<00:04, 38963.53it/s]\n 91%|#########1| 1826281/2000000 [00:47<00:04, 38908.72it/s]\n 92%|#########1| 1830187/2000000 [00:47<00:04, 38940.03it/s]\n 92%|#########1| 1834113/2000000 [00:47<00:04, 39026.90it/s]\n 92%|#########1| 1838025/2000000 [00:47<00:04, 39045.27it/s]\n 92%|#########2| 1841930/2000000 [00:47<00:04, 39032.79it/s]\n 92%|#########2| 1845837/2000000 [00:47<00:03, 39033.11it/s]\n 92%|#########2| 1849741/2000000 [00:47<00:03, 39022.30it/s]\n 93%|#########2| 1853671/2000000 [00:47<00:03, 39099.80it/s]\n 93%|#########2| 1857581/2000000 [00:47<00:03, 39073.68it/s]\n 93%|#########3| 1861502/2000000 [00:48<00:03, 39084.69it/s]\n 93%|#########3| 1865424/2000000 [00:48<00:03, 39117.17it/s]\n 93%|#########3| 1869336/2000000 [00:48<00:03, 39078.27it/s]\n 94%|#########3| 1873244/2000000 [00:48<00:03, 39004.95it/s]\n 94%|#########3| 1877145/2000000 [00:48<00:03, 38934.12it/s]\n 94%|#########4| 1881039/2000000 [00:48<00:03, 38913.83it/s]\n 94%|#########4| 1884931/2000000 [00:48<00:02, 38809.33it/s]\n 94%|#########4| 1888856/2000000 [00:48<00:02, 38931.96it/s]\n 95%|#########4| 1892791/2000000 [00:48<00:02, 39055.21it/s]\n 95%|#########4| 1896720/2000000 [00:48<00:02, 39115.33it/s]\n 95%|#########5| 1900657/2000000 [00:49<00:02, 39182.89it/s]\n 95%|#########5| 1904583/2000000 [00:49<00:02, 39202.20it/s]\n 95%|#########5| 1908504/2000000 [00:49<00:02, 39140.92it/s]\n 96%|#########5| 1912424/2000000 [00:49<00:02, 39150.49it/s]\n 96%|#########5| 1916350/2000000 [00:49<00:02, 39171.67it/s]\n 96%|#########6| 1920273/2000000 [00:49<00:02, 39183.11it/s]\n 96%|#########6| 1924199/2000000 [00:49<00:01, 39196.53it/s]\n 96%|#########6| 1928119/2000000 [00:49<00:01, 39159.80it/s]\n 97%|#########6| 1932035/2000000 [00:49<00:01, 39009.89it/s]\n 97%|#########6| 1935937/2000000 [00:49<00:01, 38906.28it/s]\n 97%|#########6| 1939851/2000000 [00:50<00:01, 38969.63it/s]\n 97%|#########7| 1943749/2000000 [00:50<00:01, 38956.63it/s]\n 97%|#########7| 1947645/2000000 [00:50<00:01, 38943.32it/s]\n 98%|#########7| 1951561/2000000 [00:50<00:01, 38998.71it/s]\n 98%|#########7| 1955491/2000000 [00:50<00:01, 39079.19it/s]\n 98%|#########7| 1959425/2000000 [00:50<00:01, 39147.17it/s]\n 98%|#########8| 1963391/2000000 [00:50<00:00, 39297.72it/s]\n 98%|#########8| 1967346/2000000 [00:50<00:00, 39362.16it/s]\n 99%|#########8| 1971287/2000000 [00:50<00:00, 39374.85it/s]\n 99%|#########8| 1975225/2000000 [00:50<00:00, 39286.82it/s]\n 99%|#########8| 1979154/2000000 [00:51<00:00, 39250.71it/s]\n 99%|#########9| 1983080/2000000 [00:51<00:00, 39201.01it/s]\n 99%|#########9| 1987001/2000000 [00:51<00:00, 39129.86it/s]\n100%|#########9| 1990915/2000000 [00:51<00:00, 39109.67it/s]\n100%|#########9| 1994826/2000000 [00:51<00:00, 38992.24it/s]\n100%|#########9| 1998726/2000000 [00:51<00:00, 38919.23it/s]\n100%|##########| 2000000/2000000 [00:51<00:00, 38743.59it/s]\n2024-08-22 05:07:37,015 - INFO - allennlp.modules.token_embedders.embedding - Initializing pre-trained embedding layer\n2024-08-22 05:07:37,018 - INFO - allennlp.modules.token_embedders.embedding - Pretrained embeddings were found for 186 out of 192 tokens\n/usr/local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n  warnings.warn(\"dropout option adds dropout after all but last \"\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nVocabulary with namespaces:\n \tNon Padded Namespaces: {'*labels', '*tags'}\n \tNamespace: fasttext, Size: 192 \n \tNamespace: token_characters, Size: 62 \n \tNamespace: case, Size: 6 \n \tNamespace: xpos, Size: 38 \n \tNamespace: depchunk, Size: 20 \n \tNamespace: deprel, Size: 29 \n \tNamespace: genre, Size: 3 \n \tNamespace: parent_clauses, Size: 17 \n \tNamespace: upos, Size: 16 \n \tNamespace: s_type, Size: 3 \n \tNamespace: labels, Size: 2 \n\ncase\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: 'l', 3: 't', 4: 'o', 5: 'd'}\nxpos\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: 'NN', 3: 'ART', 4: 'ADV', 5: '$.', 6: 'APPR', 7: 'ADJA', 8: 'VAFIN', 9: 'ADJD', 10: '$,', 11: 'VVFIN', 12: 'VVPP', 13: 'VVINF', 14: 'NE', 15: 'PPER', 16: 'KON', 17: 'VMFIN', 18: 'PIS', 19: 'PROAV', 20: 'KOUS', 21: 'PIAT', 22: 'PRELS', 23: 'KOKOM', 24: 'PTKZU', 25: 'VAINF', 26: 'PWS', 27: 'PTKVZ', 28: 'APPRART', 29: 'VMINF', 30: 'CARD', 31: 'TRUNC', 32: 'PWAV', 33: 'PTKA', 34: 'PRF', 35: 'PDS', 36: 'PTKNEG', 37: 'PPOSAT'}\ndepchunk\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: '_', 3: 'Iacl', 4: 'Iadvcl', 5: 'Iccomp', 6: 'Icsubj', 7: 'Ixcomp', 8: 'Bacl', 9: 'Eacl', 10: 'Bxcomp', 11: 'Excomp', 12: 'Badvcl', 13: 'Bccomp', 14: 'Eccomp', 15: 'Bappos', 16: 'Eadvcl', 17: 'Bcsubj', 18: 'Ecsubj', 19: 'Eappos'}\ndeprel\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: 'advmod', 3: 'punct', 4: 'det', 5: 'nsubj', 6: 'case', 7: 'root', 8: 'obl', 9: 'nmod', 10: 'aux', 11: 'obj', 12: 'amod', 13: 'cc', 14: 'mark', 15: 'conj', 16: 'aux:pass', 17: 'nsubj:pass', 18: 'acl', 19: 'cop', 20: 'expl', 21: 'flat', 22: 'xcomp', 23: 'advcl', 24: 'ccomp', 25: 'compound:prt', 26: 'appos', 27: 'csubj:pass', 28: 'det:poss'}\ngenre\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: '_'}\nparent_clauses\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: '', 3: 'ccomp', 4: 'csubj:pass', 5: 'acl', 6: 'advcl|conj|acl', 7: 'advcl', 8: 'acl|xcomp', 9: 'xcomp|conj', 10: 'acl|ccomp', 11: 'conj', 12: 'conj|acl', 13: 'xcomp', 14: 'conj|advcl', 15: 'conj|acl|xcomp', 16: 'appos'}\nupos\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: 'NOUN', 3: 'VERB', 4: 'ADV', 5: 'PUNCT', 6: 'DET', 7: 'ADP', 8: 'PRON', 9: 'ADJ', 10: 'AUX', 11: 'PROPN', 12: 'CCONJ', 13: 'SCONJ', 14: 'PART', 15: 'NUM'}\ns_type\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: 'NoQ_NoSubj_NoConj_NoCop_NoAdvcl_NoAcl'}\nlabels\n  {0: 'O', 1: 'B'}\nDisrpt2021Baseline(\n  (feature_modules): ModuleDict(\n    (case_tags): Embedding(6, 3, padding_idx=0)\n    (cpos_tags): Embedding(38, 7, padding_idx=0)\n    (dep_chunk_tags): Embedding(20, 5, padding_idx=0)\n    (deprel_tags): Embedding(29, 6, padding_idx=0)\n    (document_depth): Identity()\n    (genre_tags): Embedding(3, 2, padding_idx=0)\n    (head_distances): Identity()\n    (parent_clauses): Embedding(17, 5, padding_idx=0)\n    (pos_tags): Embedding(16, 4, padding_idx=0)\n    (s_type): Embedding(3, 2, padding_idx=0)\n    (sentence_length): Identity()\n    (token_lengths): Identity()\n  )\n  (embedder): BasicTextFieldEmbedder(\n    (token_embedder_fasttext): Embedding()\n    (token_embedder_token_characters): TokenCharactersEncoder(\n      (_embedding): TimeDistributed(\n        (_module): Embedding()\n      )\n      (_encoder): TimeDistributed(\n        (_module): LstmSeq2VecEncoder(\n          (_module): LSTM(32, 64, batch_first=True, dropout=0.2, bidirectional=True)\n        )\n      )\n    )\n    (token_embedder_tokens): PretrainedTransformerMismatchedEmbedder(\n      (_matched_embedder): PretrainedTransformerEmbedder(\n        (transformer_model): BertModel(\n          (embeddings): BertEmbeddings(\n            (word_embeddings): Embedding(31102, 768, padding_idx=0)\n            (position_embeddings): Embedding(512, 768)\n            (token_type_embeddings): Embedding(2, 768)\n          )\n          (encoder): BertEncoder(\n            (layer): ModuleList(\n              (0): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (1): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (2): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (3): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (4): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (5): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (6): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (7): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (8): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (9): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (10): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (11): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n            )\n          )\n          (pooler): BertPooler(\n            (activation): Tanh()\n          )\n        )\n      )\n    )\n  )\n  (encoder): PytorchSeq2SeqWrapper(\n    (_module): StackedBidirectionalLstm(\n      (forward_layer_0): AugmentedLstm(\n        (cell): AugmentedLSTMCell(\n        )\n      )\n      (backward_layer_0): AugmentedLstm(\n        (cell): AugmentedLSTMCell(\n        )\n      )\n    )\n  )\n  (prev_sentence_encoder): LstmSeq2VecEncoder(\n    (_module): LSTM(1196, 100, batch_first=True, dropout=0.2, bidirectional=True)\n  )\n  (next_sentence_encoder): LstmSeq2VecEncoder(\n    (_module): LSTM(1196, 100, batch_first=True, dropout=0.2, bidirectional=True)\n  )\n  (label_projection_layer): TimeDistributed(\n  )\n)\n2024-08-22 05:07:39,911 - INFO - allennlp.training.optimizers - Done constructing parameter groups.\n2024-08-22 05:07:39,912 - INFO - allennlp.training.optimizers - Group 1: ['encoder._module.backward_layer_0.cell.input_linearity.weight', 'embedder.token_embedder_token_characters._encoder._module._module.weight_hh_l0_reverse', 'next_sentence_encoder._module.bias_ih_l0', 'prev_sentence_encoder._module.bias_hh_l0', 'feature_modules.case_tags.weight', 'encoder._module.forward_layer_0.cell.input_linearity.weight', 'prev_sentence_encoder._module.weight_ih_l0_reverse', 'embedder.token_embedder_token_characters._encoder._module._module.weight_ih_l0', 'next_sentence_encoder._module.weight_hh_l0_reverse', 'label_projection_layer._module.bias', 'embedder.token_embedder_token_characters._encoder._module._module.weight_hh_l0', 'prev_sentence_encoder._module.weight_hh_l0', 'prev_sentence_encoder._module.weight_ih_l0', 'embedder.token_embedder_token_characters._encoder._module._module.bias_ih_l0', 'feature_modules.genre_tags.weight', 'next_sentence_encoder._module.bias_hh_l0_reverse', 'feature_modules.cpos_tags.weight', 'embedder.token_embedder_token_characters._encoder._module._module.bias_hh_l0', 'feature_modules.deprel_tags.weight', 'next_sentence_encoder._module.weight_ih_l0', 'label_projection_layer._module.weight', 'feature_modules.pos_tags.weight', 'next_sentence_encoder._module.weight_ih_l0_reverse', 'prev_sentence_encoder._module.bias_ih_l0_reverse', 'next_sentence_encoder._module.bias_ih_l0_reverse', 'feature_modules.parent_clauses.weight', 'embedder.token_embedder_token_characters._embedding._module.weight', 'prev_sentence_encoder._module.bias_ih_l0', 'encoder._module.backward_layer_0.cell.state_linearity.weight', 'embedder.token_embedder_token_characters._encoder._module._module.bias_ih_l0_reverse', 'encoder._module.forward_layer_0.cell.state_linearity.weight', 'encoder._module.forward_layer_0.cell.state_linearity.bias', 'embedder.token_embedder_token_characters._encoder._module._module.bias_hh_l0_reverse', 'next_sentence_encoder._module.weight_hh_l0', 'prev_sentence_encoder._module.weight_hh_l0_reverse', 'encoder._module.backward_layer_0.cell.state_linearity.bias', 'prev_sentence_encoder._module.bias_hh_l0_reverse', 'feature_modules.dep_chunk_tags.weight', 'embedder.token_embedder_token_characters._encoder._module._module.weight_ih_l0_reverse', 'next_sentence_encoder._module.bias_hh_l0', 'feature_modules.s_type.weight'], {}\n2024-08-22 05:07:39,912 - INFO - allennlp.training.optimizers - Number of trainable parameters: 124725265\n/usr/local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n2024-08-22 05:07:39,933 - INFO - allennlp.training.gradient_descent_trainer - Beginning training.\n2024-08-22 05:07:39,934 - INFO - allennlp.training.gradient_descent_trainer - Epoch 0/0\n2024-08-22 05:07:39,934 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 2.8G\n2024-08-22 05:07:39,935 - INFO - allennlp.training.gradient_descent_trainer - Training\n\n  0%|          | 0/3 [00:00<?, ?it/s]/usr/local/lib/python3.10/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n2024-08-22 05:07:45,313 - INFO - allennlp.training.callbacks.console_logger - Batch inputs\n2024-08-22 05:07:45,313 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/fasttext/tokens (Shape: 4 x 25)\ntensor([[ 91,  10,  92,  ..., 104, 105,   2],\n        [ 20,  66,  67,  ...,   2,   0,   0],\n        [ 54,   3,  55,  ...,   0,   0,   0],\n        [ 36,  37,  38,  ...,   0,   0,   0]])\n2024-08-22 05:07:45,314 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/token_characters/token_characters (Shape: 4 x 25 x 21)\ntensor([[[31, 15,  0,  ...,  0,  0,  0],\n         [ 5,  4,  8,  ...,  0,  0,  0],\n         [ 2,  8, 17,  ...,  0,  0,  0],\n         ...,\n         [24, 13,  4,  ...,  0,  0,  0],\n         [ 3, 13,  8,  ...,  0,  0,  0],\n         [20,  0,  0,  ...,  0,  0,  0]],\n\n        [[30,  4,  0,  ...,  0,  0,  0],\n         [10, 26,  6,  ...,  0,  0,  0],\n         [10,  2,  6,  ...,  0,  0,  0],\n         ...,\n         [20,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[57, 19,  2,  ...,  0,  0,  0],\n         [23,  0,  0,  ...,  0,  0,  0],\n         [17,  2,  5,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[28,  7, 11,  ...,  0,  0,  0],\n         [47,  5,  2,  ...,  0,  0,  0],\n         [ 4,  5,  8,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]]])\n2024-08-22 05:07:45,317 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/token_ids (Shape: 4 x 31)\ntensor([[  102,   224,   207,  ...,  3682,   552,   103],\n        [  102,   233,  9647,  ...,     0,     0,     0],\n        [  102, 24026, 12979,  ...,     0,     0,     0],\n        [  102, 19590,   906,  ...,     0,     0,     0]])\n2024-08-22 05:07:45,318 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/mask (Shape: 4 x 25)\ntensor([[ True,  True,  True,  ...,  True,  True,  True],\n        [ True,  True,  True,  ...,  True, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False]])\n2024-08-22 05:07:45,319 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/type_ids (Shape: 4 x 31)\ntensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]])\n2024-08-22 05:07:45,320 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/wordpiece_mask (Shape: 4 x 31)\ntensor([[ True,  True,  True,  ...,  True,  True,  True],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False]])\n2024-08-22 05:07:45,320 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/offsets (Shape: 4 x 25 x 2)\ntensor([[[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [26, 27],\n         [28, 28],\n         [29, 29]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [26, 26],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  4],\n         [ 5,  5],\n         [ 6,  6],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  2],\n         [ 3,  4],\n         [ 5,  5],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]]])\n2024-08-22 05:07:45,322 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/fasttext/tokens (Shape: 4 x 31)\ntensor([[86, 32, 33,  ...,  0,  0,  0],\n        [19, 62,  4,  ...,  0,  0,  0],\n        [12, 40,  4,  ...,  8, 53,  2],\n        [12, 27, 28,  ...,  0,  0,  0]])\n2024-08-22 05:07:45,322 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/token_characters/token_characters (Shape: 4 x 31 x 16)\ntensor([[[36,  7,  8,  ...,  0,  0,  0],\n         [11,  5, 19,  ...,  0,  0,  0],\n         [ 2,  4,  0,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[28,  2,  6,  ...,  0,  0,  0],\n         [29, 26, 14,  ...,  0,  0,  0],\n         [10,  2,  6,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[24, 13, 21,  ...,  0,  0,  0],\n         [40,  6, 13,  ...,  0,  0,  0],\n         [10,  2,  6,  ...,  0,  0,  0],\n         ...,\n         [18, 13,  0,  ...,  0,  0,  0],\n         [ 2,  3,  8,  ...,  0,  0,  0],\n         [20,  0,  0,  ...,  0,  0,  0]],\n\n        [[24, 13, 21,  ...,  0,  0,  0],\n         [30,  5,  4,  ...,  0,  0,  0],\n         [11,  2, 12,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]]])\n2024-08-22 05:07:45,325 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/token_ids (Shape: 4 x 41)\ntensor([[  102,  2472,   175,  ...,     0,     0,     0],\n        [  102,   127, 22026,  ...,     0,     0,     0],\n        [  102,   193,   854,  ...,  3432,   552,   103],\n        [  102,   193,  3029,  ...,     0,     0,     0]])\n2024-08-22 05:07:45,326 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/mask (Shape: 4 x 31)\ntensor([[ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ...,  True,  True,  True],\n        [ True,  True,  True,  ..., False, False, False]])\n2024-08-22 05:07:45,327 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/type_ids (Shape: 4 x 41)\ntensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]])\n2024-08-22 05:07:45,327 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/wordpiece_mask (Shape: 4 x 41)\ntensor([[ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ...,  True,  True,  True],\n        [ True,  True,  True,  ..., False, False, False]])\n2024-08-22 05:07:45,328 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/offsets (Shape: 4 x 31 x 2)\ntensor([[[ 1,  3],\n         [ 4,  4],\n         [ 5,  5],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  3],\n         [ 4,  4],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [37, 37],\n         [38, 38],\n         [39, 39]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]]])\n2024-08-22 05:07:45,329 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/fasttext/tokens (Shape: 4 x 31)\ntensor([[169,  32,  33,  ...,   0,   0,   0],\n        [ 31,  80,  22,  ...,   0,   0,   0],\n        [ 19,  62,   4,  ...,   0,   0,   0],\n        [ 12,  40,   4,  ...,   8,  53,   2]])\n2024-08-22 05:07:45,330 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/token_characters/token_characters (Shape: 4 x 31 x 16)\ntensor([[[28,  2,  3,  ...,  0,  0,  0],\n         [11,  5, 19,  ...,  0,  0,  0],\n         [ 2,  4,  0,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[29,  2,  5,  ...,  0,  0,  0],\n         [35,  5,  3,  ..., 11,  2,  3],\n         [17,  2,  6,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[28,  2,  6,  ...,  0,  0,  0],\n         [29, 26, 14,  ...,  0,  0,  0],\n         [10,  2,  6,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[24, 13, 21,  ...,  0,  0,  0],\n         [40,  6, 13,  ...,  0,  0,  0],\n         [10,  2,  6,  ...,  0,  0,  0],\n         ...,\n         [18, 13,  0,  ...,  0,  0,  0],\n         [ 2,  3,  8,  ...,  0,  0,  0],\n         [20,  0,  0,  ...,  0,  0,  0]]])\n2024-08-22 05:07:45,333 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/token_ids (Shape: 4 x 41)\ntensor([[  102,  4041,   773,  ...,     0,     0,     0],\n        [  102, 14216,  2672,  ...,     0,     0,     0],\n        [  102,   127, 22026,  ...,     0,     0,     0],\n        [  102,   193,   854,  ...,  3432,   552,   103]])\n2024-08-22 05:07:45,333 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/mask (Shape: 4 x 31)\ntensor([[ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ...,  True,  True,  True]])\n2024-08-22 05:07:45,334 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/type_ids (Shape: 4 x 41)\ntensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]])\n2024-08-22 05:07:45,335 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/wordpiece_mask (Shape: 4 x 41)\ntensor([[ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ...,  True,  True,  True]])\n2024-08-22 05:07:45,336 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/offsets (Shape: 4 x 31 x 2)\ntensor([[[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  3],\n         [ 4,  4],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  3],\n         [ 4,  4],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [37, 37],\n         [38, 38],\n         [39, 39]]])\n2024-08-22 05:07:45,337 - INFO - allennlp.training.callbacks.console_logger - Field : \"batch_input/sentence_tokens\" : (Length 4 of type \"<class 'list'>\")\n2024-08-22 05:07:45,337 - INFO - allennlp.training.callbacks.console_logger - batch_input/case_tags (Shape: 4 x 25)\ntensor([[3, 2, 2,  ..., 3, 2, 4],\n        [3, 2, 2,  ..., 4, 0, 0],\n        [3, 4, 2,  ..., 0, 0, 0],\n        [3, 3, 2,  ..., 0, 0, 0]])\n2024-08-22 05:07:45,338 - INFO - allennlp.training.callbacks.console_logger - batch_input/cpos_tags (Shape: 4 x 25)\ntensor([[ 4,  8,  4,  ...,  2, 11,  5],\n        [15, 17,  4,  ...,  5,  0,  0],\n        [ 9, 10, 20,  ...,  0,  0,  0],\n        [14, 14, 11,  ...,  0,  0,  0]])\n2024-08-22 05:07:45,338 - INFO - allennlp.training.callbacks.console_logger - batch_input/dep_chunk_tags (Shape: 4 x 25)\ntensor([[ 2,  2,  2,  ...,  4,  9,  2],\n        [ 2,  2,  2,  ...,  2,  0,  0],\n        [ 2, 12,  4,  ...,  0,  0,  0],\n        [ 2,  2,  2,  ...,  0,  0,  0]])\n2024-08-22 05:07:45,339 - INFO - allennlp.training.callbacks.console_logger - batch_input/deprel_tags (Shape: 4 x 25)\ntensor([[ 2, 19,  2,  ...,  9, 23,  3],\n        [20, 10,  2,  ...,  3,  0,  0],\n        [ 7,  3, 14,  ...,  0,  0,  0],\n        [ 5, 21,  7,  ...,  0,  0,  0]])\n2024-08-22 05:07:45,340 - INFO - allennlp.training.callbacks.console_logger - batch_input/document_depth (Shape: 4 x 25)\ntensor([[0.9000, 0.9000, 0.9000,  ..., 0.9000, 0.9000, 0.9000],\n        [0.8571, 0.8571, 0.8571,  ..., 0.8571, 0.0000, 0.0000],\n        [0.8000, 0.8000, 0.8000,  ..., 0.0000, 0.0000, 0.0000],\n        [0.6667, 0.6667, 0.6667,  ..., 0.0000, 0.0000, 0.0000]])\n2024-08-22 05:07:45,341 - INFO - allennlp.training.callbacks.console_logger - batch_input/genre_tags (Shape: 4 x 25)\ntensor([[2, 2, 2,  ..., 2, 2, 2],\n        [2, 2, 2,  ..., 2, 0, 0],\n        [2, 2, 2,  ..., 0, 0, 0],\n        [2, 2, 2,  ..., 0, 0, 0]])\n2024-08-22 05:07:45,342 - INFO - allennlp.training.callbacks.console_logger - batch_input/head_distances (Shape: 4 x 25)\ntensor([[1.0986, 0.6931, 0.0000,  ..., 1.0986, 2.3979, 3.0445],\n        [1.6094, 1.3863, 1.0986,  ..., 2.8332, 0.0000, 0.0000],\n        [0.0000, 2.3026, 2.1972,  ..., 0.0000, 0.0000, 0.0000],\n        [0.6931, 0.0000, 1.0986,  ..., 0.0000, 0.0000, 0.0000]])\n2024-08-22 05:07:45,343 - INFO - allennlp.training.callbacks.console_logger - batch_input/parent_clauses (Shape: 4 x 25)\ntensor([[ 2,  2,  2,  ...,  6, 12,  2],\n        [ 2,  2,  2,  ...,  2,  0,  0],\n        [ 2,  7,  7,  ...,  0,  0,  0],\n        [ 2,  2,  2,  ...,  0,  0,  0]])\n2024-08-22 05:07:45,344 - INFO - allennlp.training.callbacks.console_logger - batch_input/pos_tags (Shape: 4 x 25)\ntensor([[ 4,  3,  4,  ...,  2,  3,  5],\n        [ 8, 10,  4,  ...,  5,  0,  0],\n        [ 9,  5, 13,  ...,  0,  0,  0],\n        [11, 11,  3,  ...,  0,  0,  0]])\n2024-08-22 05:07:45,344 - INFO - allennlp.training.callbacks.console_logger - batch_input/s_type (Shape: 4 x 25)\ntensor([[2, 2, 2,  ..., 2, 2, 2],\n        [2, 2, 2,  ..., 2, 0, 0],\n        [2, 2, 2,  ..., 0, 0, 0],\n        [2, 2, 2,  ..., 0, 0, 0]])\n2024-08-22 05:07:45,345 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence_length (Shape: 4 x 25)\ntensor([[3.2189, 3.2189, 3.2189,  ..., 3.2189, 3.2189, 3.2189],\n        [3.1355, 3.1355, 3.1355,  ..., 3.1355, 0.0000, 0.0000],\n        [2.6391, 2.6391, 2.6391,  ..., 0.0000, 0.0000, 0.0000],\n        [1.9459, 1.9459, 1.9459,  ..., 0.0000, 0.0000, 0.0000]])\n2024-08-22 05:07:45,346 - INFO - allennlp.training.callbacks.console_logger - batch_input/token_lengths (Shape: 4 x 25)\ntensor([[0.6931, 1.0986, 1.3863,  ..., 2.1972, 1.7918, 0.0000],\n        [0.6931, 1.7918, 1.9459,  ..., 0.0000, 0.0000, 0.0000],\n        [2.4849, 0.0000, 1.3863,  ..., 0.0000, 0.0000, 0.0000],\n        [1.7918, 1.9459, 1.6094,  ..., 0.0000, 0.0000, 0.0000]])\n2024-08-22 05:07:45,347 - INFO - allennlp.training.callbacks.console_logger - batch_input/labels (Shape: 4 x 25)\ntensor([[1, 0, 0,  ..., 0, 0, 0],\n        [1, 0, 0,  ..., 0, 0, 0],\n        [1, 0, 1,  ..., 0, 0, 0],\n        [1, 0, 0,  ..., 0, 0, 0]])\n\ntag_accuracy: 0.4203, span_precision: 0.1489, span_recall: 1.0000, span_f1: 0.2593, batch_loss: 0.7650, loss: 0.7650 ||:  33%|###3      | 1/3 [00:05<00:10,  5.41s/it]\ntag_accuracy: 0.6400, span_precision: 0.1667, span_recall: 0.6154, span_f1: 0.2623, batch_loss: 0.3780, loss: 0.5715 ||:  67%|######6   | 2/3 [00:10<00:05,  5.12s/it]\ntag_accuracy: 0.6713, span_precision: 0.1667, span_recall: 0.5333, span_f1: 0.2540, batch_loss: 0.3870, loss: 0.5100 ||: 100%|##########| 3/3 [00:12<00:00,  3.94s/it]\ntag_accuracy: 0.6713, span_precision: 0.1667, span_recall: 0.5333, span_f1: 0.2540, batch_loss: 0.3870, loss: 0.5100 ||: 100%|##########| 3/3 [00:12<00:00,  4.29s/it]\n2024-08-22 05:07:52,804 - INFO - allennlp.training.gradient_descent_trainer - Validating\n\n  0%|          | 0/3 [00:00<?, ?it/s]2024-08-22 05:07:53,610 - INFO - allennlp.training.callbacks.console_logger - Batch inputs\n2024-08-22 05:07:53,610 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/fasttext/tokens (Shape: 4 x 24)\ntensor([[176, 177, 178,  ..., 190, 191,   2],\n        [ 19, 146, 147,  ...,   0,   0,   0],\n        [ 12, 156, 157,  ...,   0,   0,   0],\n        [166, 167, 168,  ...,   0,   0,   0]])\n2024-08-22 05:07:53,611 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/token_characters/token_characters (Shape: 4 x 24 x 14)\ntensor([[[34,  3, 10,  ...,  0,  0,  0],\n         [ 4, 15,  0,  ...,  0,  0,  0],\n         [16, 13,  4,  ...,  0,  0,  0],\n         ...,\n         [41, 15, 12,  ...,  0,  0,  0],\n         [ 7,  3, 11,  ...,  0,  0,  0],\n         [20,  0,  0,  ...,  0,  0,  0]],\n\n        [[28,  2,  6,  ...,  0,  0,  0],\n         [18, 13,  4,  ...,  0,  0,  0],\n         [31,  8,  7,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[24, 13, 21,  ...,  0,  0,  0],\n         [10,  2, 16,  ...,  0,  0,  0],\n         [33,  7,  9,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[28,  7,  4,  ...,  0,  0,  0],\n         [ 6,  2,  5,  ...,  0,  0,  0],\n         [ 3,  5, 14,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]]])\n2024-08-22 05:07:53,614 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/token_ids (Shape: 4 x 29)\ntensor([[  102,   143,   224,  ..., 14762,   552,   103],\n        [  102,   127,  6218,  ...,     0,     0,     0],\n        [  102,   193,   232,  ...,     0,     0,     0],\n        [  102,   173,  5094,  ...,     0,     0,     0]])\n2024-08-22 05:07:53,615 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/mask (Shape: 4 x 24)\ntensor([[ True,  True,  True,  ...,  True,  True,  True],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False]])\n2024-08-22 05:07:53,615 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/type_ids (Shape: 4 x 29)\ntensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]])\n2024-08-22 05:07:53,616 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/wordpiece_mask (Shape: 4 x 29)\ntensor([[ True,  True,  True,  ...,  True,  True,  True],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False]])\n2024-08-22 05:07:53,617 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/offsets (Shape: 4 x 24 x 2)\ntensor([[[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [25, 25],\n         [26, 26],\n         [27, 27]],\n\n        [[ 1,  1],\n         [ 2,  3],\n         [ 4,  4],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]]])\n2024-08-22 05:07:53,618 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/fasttext/tokens (Shape: 4 x 18)\ntensor([[166, 167, 168,  ...,   0,   0,   0],\n        [137, 138, 139,  ...,   0,   0,   0],\n        [ 19, 146, 147,  ...,  11, 155,   2],\n        [ 12, 156, 157,  ...,   0,   0,   0]])\n2024-08-22 05:07:53,619 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/token_characters/token_characters (Shape: 4 x 18 x 16)\ntensor([[[28,  7,  4,  ...,  0,  0,  0],\n         [ 6,  2,  5,  ...,  0,  0,  0],\n         [ 3,  5, 14,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[48,  2,  8,  ...,  0,  0,  0],\n         [ 9,  7, 19,  ...,  0,  0,  0],\n         [ 4,  5, 14,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[28,  2,  6,  ...,  0,  0,  0],\n         [18, 13,  4,  ...,  0,  0,  0],\n         [31,  8,  7,  ...,  0,  0,  0],\n         ...,\n         [16,  5,  8,  ...,  0,  0,  0],\n         [47, 26, 11,  ...,  0,  0,  0],\n         [20,  0,  0,  ...,  0,  0,  0]],\n\n        [[24, 13, 21,  ...,  0,  0,  0],\n         [10,  2, 16,  ...,  0,  0,  0],\n         [33,  7,  9,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]]])\n2024-08-22 05:07:53,621 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/token_ids (Shape: 4 x 25)\ntensor([[  102,   173,  5094,  ...,     0,     0,     0],\n        [  102,   951,   423,  ...,     0,     0,     0],\n        [  102,   127,  6218,  ..., 30938,   552,   103],\n        [  102,   193,   232,  ...,     0,     0,     0]])\n2024-08-22 05:07:53,622 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/mask (Shape: 4 x 18)\ntensor([[ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ...,  True,  True,  True],\n        [ True,  True,  True,  ..., False, False, False]])\n2024-08-22 05:07:53,623 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/type_ids (Shape: 4 x 25)\ntensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]])\n2024-08-22 05:07:53,623 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/wordpiece_mask (Shape: 4 x 25)\ntensor([[ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ...,  True,  True,  True],\n        [ True,  True,  True,  ..., False, False, False]])\n2024-08-22 05:07:53,624 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/offsets (Shape: 4 x 18 x 2)\ntensor([[[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  3],\n         [ 4,  4],\n         ...,\n         [20, 20],\n         [21, 22],\n         [23, 23]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]]])\n2024-08-22 05:07:53,625 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/fasttext/tokens (Shape: 4 x 24)\ntensor([[ 35,   0,   0,  ...,   0,   0,   0],\n        [ 12, 156, 157,  ...,   0,   0,   0],\n        [166, 167, 168,  ...,   0,   0,   0],\n        [176, 177, 178,  ..., 190, 191,   2]])\n2024-08-22 05:07:53,626 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/token_characters/token_characters (Shape: 4 x 24 x 21)\ntensor([[[32, 32, 28,  ..., 54, 32, 32],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[24, 13, 21,  ...,  0,  0,  0],\n         [10,  2, 16,  ...,  0,  0,  0],\n         [33,  7,  9,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[28,  7,  4,  ...,  0,  0,  0],\n         [ 6,  2,  5,  ...,  0,  0,  0],\n         [ 3,  5, 14,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[34,  3, 10,  ...,  0,  0,  0],\n         [ 4, 15,  0,  ...,  0,  0,  0],\n         [16, 13,  4,  ...,  0,  0,  0],\n         ...,\n         [41, 15, 12,  ...,  0,  0,  0],\n         [ 7,  3, 11,  ...,  0,  0,  0],\n         [20,  0,  0,  ...,  0,  0,  0]]])\n2024-08-22 05:07:53,628 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/token_ids (Shape: 4 x 29)\ntensor([[  102, 16249, 16249,  ...,     0,     0,     0],\n        [  102,   193,   232,  ...,     0,     0,     0],\n        [  102,   173,  5094,  ...,     0,     0,     0],\n        [  102,   143,   224,  ..., 14762,   552,   103]])\n2024-08-22 05:07:53,629 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/mask (Shape: 4 x 24)\ntensor([[ True, False, False,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ...,  True,  True,  True]])\n2024-08-22 05:07:53,630 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/type_ids (Shape: 4 x 29)\ntensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]])\n2024-08-22 05:07:53,631 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/wordpiece_mask (Shape: 4 x 29)\ntensor([[ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ...,  True,  True,  True]])\n2024-08-22 05:07:53,631 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/offsets (Shape: 4 x 24 x 2)\ntensor([[[ 1,  9],\n         [ 0,  0],\n         [ 0,  0],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [25, 25],\n         [26, 26],\n         [27, 27]]])\n2024-08-22 05:07:53,633 - INFO - allennlp.training.callbacks.console_logger - Field : \"batch_input/sentence_tokens\" : (Length 4 of type \"<class 'list'>\")\n2024-08-22 05:07:53,633 - INFO - allennlp.training.callbacks.console_logger - batch_input/case_tags (Shape: 4 x 24)\ntensor([[3, 2, 2,  ..., 3, 2, 4],\n        [3, 2, 3,  ..., 0, 0, 0],\n        [3, 2, 3,  ..., 0, 0, 0],\n        [3, 2, 2,  ..., 0, 0, 0]])\n2024-08-22 05:07:53,633 - INFO - allennlp.training.callbacks.console_logger - batch_input/cpos_tags (Shape: 4 x 24)\ntensor([[16,  4, 17,  ...,  2, 11,  5],\n        [ 3,  7,  2,  ...,  0,  0,  0],\n        [ 6,  3,  2,  ...,  0,  0,  0],\n        [35, 11, 36,  ...,  0,  0,  0]])\n2024-08-22 05:07:53,634 - INFO - allennlp.training.callbacks.console_logger - batch_input/dep_chunk_tags (Shape: 4 x 24)\ntensor([[ 2,  2,  2,  ...,  3, 11,  2],\n        [ 2,  2,  2,  ...,  0,  0,  0],\n        [ 2,  2,  2,  ...,  0,  0,  0],\n        [ 2,  2,  2,  ...,  0,  0,  0]])\n2024-08-22 05:07:53,635 - INFO - allennlp.training.callbacks.console_logger - batch_input/deprel_tags (Shape: 4 x 24)\ntensor([[13,  2, 10,  ..., 15, 18,  3],\n        [ 4, 12,  5,  ...,  0,  0,  0],\n        [ 6,  4,  8,  ...,  0,  0,  0],\n        [ 5,  7,  2,  ...,  0,  0,  0]])\n2024-08-22 05:07:53,635 - INFO - allennlp.training.callbacks.console_logger - batch_input/document_depth (Shape: 4 x 24)\ntensor([[0.9091, 0.9091, 0.9091,  ..., 0.9091, 0.9091, 0.9091],\n        [0.8750, 0.8750, 0.8750,  ..., 0.0000, 0.0000, 0.0000],\n        [0.8889, 0.8889, 0.8889,  ..., 0.0000, 0.0000, 0.0000],\n        [0.9000, 0.9000, 0.9000,  ..., 0.0000, 0.0000, 0.0000]])\n2024-08-22 05:07:53,637 - INFO - allennlp.training.callbacks.console_logger - batch_input/genre_tags (Shape: 4 x 24)\ntensor([[2, 2, 2,  ..., 2, 2, 2],\n        [2, 2, 2,  ..., 0, 0, 0],\n        [2, 2, 2,  ..., 0, 0, 0],\n        [2, 2, 2,  ..., 0, 0, 0]])\n2024-08-22 05:07:53,637 - INFO - allennlp.training.callbacks.console_logger - batch_input/head_distances (Shape: 4 x 24)\ntensor([[2.3979, 2.3026, 2.1972,  ..., 1.0986, 2.5649, 2.4849],\n        [0.6931, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n        [0.6931, 0.0000, 1.9459,  ..., 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.6931, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])\n2024-08-22 05:07:53,638 - INFO - allennlp.training.callbacks.console_logger - batch_input/parent_clauses (Shape: 4 x 24)\ntensor([[ 2,  2,  2,  ...,  8, 13,  2],\n        [ 2,  2,  2,  ...,  0,  0,  0],\n        [ 2,  2,  2,  ...,  0,  0,  0],\n        [ 2,  2,  2,  ...,  0,  0,  0]])\n2024-08-22 05:07:53,639 - INFO - allennlp.training.callbacks.console_logger - batch_input/pos_tags (Shape: 4 x 24)\ntensor([[12,  4, 10,  ...,  2,  3,  5],\n        [ 6,  9,  2,  ...,  0,  0,  0],\n        [ 7,  6,  2,  ...,  0,  0,  0],\n        [ 8,  3,  4,  ...,  0,  0,  0]])\n2024-08-22 05:07:53,640 - INFO - allennlp.training.callbacks.console_logger - batch_input/s_type (Shape: 4 x 24)\ntensor([[2, 2, 2,  ..., 2, 2, 2],\n        [2, 2, 2,  ..., 0, 0, 0],\n        [2, 2, 2,  ..., 0, 0, 0],\n        [2, 2, 2,  ..., 0, 0, 0]])\n2024-08-22 05:07:53,640 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence_length (Shape: 4 x 24)\ntensor([[3.1781, 3.1781, 3.1781,  ..., 3.1781, 3.1781, 3.1781],\n        [2.8904, 2.8904, 2.8904,  ..., 0.0000, 0.0000, 0.0000],\n        [2.6391, 2.6391, 2.6391,  ..., 0.0000, 0.0000, 0.0000],\n        [1.7918, 1.7918, 1.7918,  ..., 0.0000, 0.0000, 0.0000]])\n2024-08-22 05:07:53,641 - INFO - allennlp.training.callbacks.console_logger - batch_input/token_lengths (Shape: 4 x 24)\ntensor([[1.0986, 0.6931, 1.3863,  ..., 1.7918, 1.7918, 0.0000],\n        [1.0986, 2.3026, 1.3863,  ..., 0.0000, 0.0000, 0.0000],\n        [1.0986, 1.0986, 1.9459,  ..., 0.0000, 0.0000, 0.0000],\n        [1.0986, 1.7918, 1.6094,  ..., 0.0000, 0.0000, 0.0000]])\n2024-08-22 05:07:53,642 - INFO - allennlp.training.callbacks.console_logger - batch_input/labels (Shape: 4 x 24)\ntensor([[1, 0, 0,  ..., 0, 0, 0],\n        [1, 0, 0,  ..., 0, 0, 0],\n        [1, 0, 0,  ..., 0, 0, 0],\n        [1, 0, 0,  ..., 0, 0, 0]])\n\ntag_accuracy: 0.9032, span_precision: 0.0000, span_recall: 0.0000, span_f1: 0.0000, batch_loss: 0.4510, loss: 0.4510 ||:  33%|###3      | 1/3 [00:00<00:01,  1.19it/s]\ntag_accuracy: 0.9027, span_precision: 0.0000, span_recall: 0.0000, span_f1: 0.0000, batch_loss: 0.4173, loss: 0.4342 ||:  67%|######6   | 2/3 [00:01<00:00,  1.18it/s]\ntag_accuracy: 0.8943, span_precision: 0.0000, span_recall: 0.0000, span_f1: 0.0000, batch_loss: 0.6441, loss: 0.5041 ||: 100%|##########| 3/3 [00:02<00:00,  1.63it/s]\ntag_accuracy: 0.8943, span_precision: 0.0000, span_recall: 0.0000, span_f1: 0.0000, batch_loss: 0.6441, loss: 0.5041 ||: 100%|##########| 3/3 [00:02<00:00,  1.48it/s]\n2024-08-22 05:07:54,829 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n2024-08-22 05:07:54,829 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.510  |     0.504\n2024-08-22 05:07:54,829 - INFO - allennlp.training.callbacks.console_logger - span_f1            |     0.254  |     0.000\n2024-08-22 05:07:54,829 - INFO - allennlp.training.callbacks.console_logger - span_precision     |     0.167  |     0.000\n2024-08-22 05:07:54,829 - INFO - allennlp.training.callbacks.console_logger - span_recall        |     0.533  |     0.000\n2024-08-22 05:07:54,829 - INFO - allennlp.training.callbacks.console_logger - tag_accuracy       |     0.671  |     0.894\n2024-08-22 05:07:54,829 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  2819.910  |       N/A\n2024-08-22 05:07:56,588 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:16.654169\n  \"best_epoch\": 0,\n  \"peak_worker_0_memory_MB\": 2819.91015625,\n  \"training_duration\": \"0:00:14.894487\",\n  \"epoch\": 0,\n  \"training_tag_accuracy\": 0.6713286713286714,\n  \"training_span_precision\": 0.16666666666666632,\n  \"training_span_recall\": 0.5333333333333298,\n  \"training_span_f1\": 0.2539682539682169,\n  \"training_loss\": 0.5099876622358958,\n  \"training_worker_0_memory_MB\": 2819.91015625,\n  \"validation_tag_accuracy\": 0.8943089430894309,\n  \"validation_span_precision\": 0.0,\n  \"validation_span_recall\": 0.0,\n  \"validation_span_f1\": 0.0,\n  \"validation_loss\": 0.5041356384754181,\n  \"best_validation_tag_accuracy\": 0.8943089430894309,\n  \"best_validation_span_precision\": 0.0,\n  \"best_validation_span_recall\": 0.0,\n  \"best_validation_span_f1\": 0.0,\n  \"best_validation_loss\": 0.5041356384754181\n}\n2024-08-22 05:07:56,589 - INFO - allennlp.models.archival - archiving weights and vocabulary to models/deu.rst.pcc_seg_bert_baseline_ft/model.tar.gz\n\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n# Testing on deu.rst.pcc\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n\n2024-08-22 05:08:51,677 - INFO - allennlp.common.plugins - Plugin allennlp_models available\n2024-08-22 05:08:52,352 - ERROR - allennlp.common.plugins - Plugin gucorpling_models could not be loaded: No module named 'flair'\nTraceback (most recent call last):\n  File \"/usr/local/bin/allennlp\", line 8, in <module>\n    sys.exit(run())\n  File \"/usr/local/lib/python3.10/site-packages/allennlp/__main__.py\", line 39, in run\n    main(prog=\"allennlp\")\n  File \"/usr/local/lib/python3.10/site-packages/allennlp/commands/__init__.py\", line 120, in main\n    args.func(args)\n  File \"/usr/local/lib/python3.10/site-packages/allennlp/commands/predict.py\", line 256, in _predict\n    predictor = _get_predictor(args)\n  File \"/usr/local/lib/python3.10/site-packages/allennlp/commands/predict.py\", line 120, in _get_predictor\n    check_for_gpu(args.cuda_device)\n  File \"/usr/local/lib/python3.10/site-packages/allennlp/common/checks.py\", line 117, in check_for_gpu\n    raise ConfigurationError(\nallennlp.common.checks.ConfigurationError: Experiment specified a GPU but none is available; if you want to run on CPU use the override 'trainer.cuda_device=-1' in the json config file.\nRemoving model files...\n\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n# Scoring on deu.rst.pcc\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n\nTraceback (most recent call last):\n  File \"/content/DisCoDisCo/seg_scripts/format_output.py\", line 14, in <module>\n    with open(sys.argv[1], encoding=\"utf8\") as f:\nFileNotFoundError: [Errno 2] No such file or directory: 'models/deu.rst.pcc_seg_bert_baseline_ft/output_test.jsonl'\nTraceback (most recent call last):\n  File \"/content/DisCoDisCo/seg_scripts/seg_eval_2019_modified.py\", line 237, in <module>\n    score_dict = get_scores(opts.goldfile,opts.predfile,opts.string_input)\n  File \"/content/DisCoDisCo/seg_scripts/seg_eval_2019_modified.py\", line 138, in get_scores\n    pred_tokens, pred_labels, pred_spans = parse_data(pred_file, string_input)\n  File \"/content/DisCoDisCo/seg_scripts/seg_eval_2019_modified.py\", line 77, in parse_data\n    data = io.open(infile, encoding=\"utf8\").read().strip().replace(\"\\r\", \"\")\nFileNotFoundError: [Errno 2] No such file or directory: 'models/deu.rst.pcc_seg_bert_baseline_ft/output_test.conllu'\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!bash seg_scripts/single_corpus_train_and_test_ft.sh deu.rst.pcc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef92abe",
   "metadata": {},
   "source": [
    "### Step 28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f738c68f",
   "metadata": {},
   "source": [
    "Execution Start time: 05:08:58, End time: 05:08:59 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437ba747",
   "metadata": {},
   "source": [
    "**Thought**: The script ran into an error because it attempted to use a GPU, which is not available. We need to modify the configuration to ensure it runs on the CPU. Let's update the configuration file to set `trainer.cuda_device` to `-1`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46c3ba3",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672b7585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/DisCoDisCo/configs/seg/baseline/bert_baseline_ft.jsonnet (103 lines total)]\n(3 more lines above)\n4:local encoder_hidden_dim = 512;\n5:\n6:local context_encoder = {\n7:    \"type\": \"lstm\",\n8:    \"input_size\": embedding_dim,\n9:    \"hidden_size\": context_hidden_size / 4, // 4 <= 2 bilstms applied to 2 sentences\n10:    \"num_layers\": 1,\n11:    \"bidirectional\": true,\n12:    \"dropout\": 0.2\n13:};\n14:\n15:local token_features = {\n16:    \"pos_tags\": {\"source_key\": \"pos\", \"label_namespace\": \"upos\"},\n17:    \"cpos_tags\": {\"source_key\": \"cpos\", \"label_namespace\": \"xpos\"},\n18:    \"deprel_tags\": {\"source_key\": \"deprel\", \"label_namespace\": \"deprel\"},\n19:    \"dep_chunk_tags\": {\"source_key\": \"depchunk\", \"label_namespace\": \"depchunk\"},\n20:    \"parent_clauses\": {\"source_key\": \"parentclauses\", \"label_namespace\": \"parent_clauses\"},\n21:    \"s_type\": {\"source_key\": \"s_type\", \"label_namespace\": \"s_type\"},\n22:    \"case_tags\": {\"source_key\": \"case\", \"label_namespace\": \"case\"},\n23:    \"genre_tags\": {\"source_key\": \"genre\", \"label_namespace\": \"genre\"},\n24:    \"head_distances\": {\"source_key\": \"head_dist\", \"xform_fn\": \"abs_natural_log\"},\n25:    \"document_depth\": {\"source_key\": \"sent_doc_percentile\"},\n26:    \"sentence_length\": {\"source_key\": \"s_len\", \"xform_fn\": \"natural_log\"},\n27:    \"token_lengths\": {\"source_key\": \"tok_len\", \"xform_fn\": \"natural_log\"}\n28:};\n29:\n30:// For more info on config files generally, see https://guide.allennlp.org/using-config-files\n31:{\n32:    \"dataset_reader\" : {\n33:        \"type\": \"disrpt_2021_seg\",\n34:        \"token_indexers\": {\n35:            \"tokens\": {\n36:                \"type\": \"pretrained_transformer_mismatched\",\n37:                \"model_name\": transformer_model_name\n38:            },\n39:            \"fasttext\": {\n40:                \"type\": \"single_id\",\n41:                \"namespace\": \"fasttext\",\n42:            },\n43:            \"token_characters\": import \"../../components/char_indexer.libsonnet\"\n44:        },\n45:        \"tokenizer\": {\n46:            \"type\": \"whitespace\"\n47:        },\n48:        \"token_features\": token_features\n49:    },\n50:    \"model\": {\n51:        \"type\": \"disrpt_2021_seg_baseline\",\n52:        \"embedder\": {\n53:            \"token_embedders\": {\n54:                \"tokens\": {\n55:                    \"type\": \"pretrained_transformer_mismatched\",\n56:                    \"model_name\": transformer_model_name,\n57:                    \"train_parameters\": true,\n58:                    \"last_layer_only\": true\n59:                },\n60:                \"fasttext\": {\n61:                    \"type\": \"embedding\",\n62:                    \"vocab_namespace\": \"fasttext\",\n63:                    \"embedding_dim\": 300,\n64:                    \"trainable\": false,\n65:                    \"pretrained_file\": std.extVar(\"FASTTEXT_EMBEDDING_FILE\")\n66:                },\n67:                \"token_characters\": import \"../../components/char_embedder.libsonnet\"\n68:            }\n69:        },\n70:        // seq2vec encoders for neighbor sentences\n71:        \"prev_sentence_encoder\": context_encoder,\n72:        \"next_sentence_encoder\": context_encoder,\n73:        // our encoder isn't fully configurable here because its input size needs to be determined\n74:        // at the start of the program. so, it'll always be a bilstm, but you can use the two items\n75:        // below to configure the most important hyperparameters it has\n76:        \"encoder_hidden_dim\": encoder_hidden_dim,\n77:        \"encoder_recurrent_dropout\": 0.1,\n78:        // end encoder hyperparams\n79:        \"dropout\": 0.4,\n80:        \"feature_dropout\": 0.0,\n81:        \"token_features\": token_features,\n82:        \"use_crf\": if std.extVar(\"USE_CRF\") == \"1\" then true else false\n83:    },\n84:    \"train_data_path\": std.extVar(\"TRAIN_DATA_PATH\"),\n85:    \"validation_data_path\": std.extVar(\"VALIDATION_DATA_PATH\"),\n86:    \"data_loader\": {\n87:        \"batch_size\": 4,\n88:        \"shuffle\": true\n89:    },\n90:    \"trainer\": {\n91:        \"optimizer\": {\n92:            \"type\": \"huggingface_adamw\",\n93:            \"lr\": 5e-4,\n94:            \"parameter_groups\": [\n95:                [[\".*transformer.*\"], {\"lr\": 1e-5}]\n96:            ]\n97:        },\n98:        \"patience\": 10,\n99:        \"num_epochs\": 1,\n100:        // probably best to just use loss\n101:        \"cuda_device\": -1,\n102:    }\n103:}\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 101:101\n",
    "        \"cuda_device\": -1,\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f92cd9",
   "metadata": {},
   "source": [
    "### Step 29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3acc886",
   "metadata": {},
   "source": [
    "Execution Start time: 05:09:01, End time: 05:11:19 (duration: 138 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1302e46",
   "metadata": {},
   "source": [
    "**Thought**: The configuration file has been updated to run on the CPU by setting `cuda_device` to `-1`. Now, let's run the script again to train and evaluate the model with the correct settings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e064a4",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06e7f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"models/deu.rst.pcc_seg_bert_baseline_ft\" already exists. Removing it now...\n\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n# Training on deu.rst.pcc\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n\ndata/2021/deu.rst.pcc/deu.rst.pcc_train.conllu\n2024-08-22 05:09:07,684 - INFO - allennlp.common.plugins - Plugin allennlp_models available\n2024-08-22 05:09:08,343 - ERROR - allennlp.common.plugins - Plugin gucorpling_models could not be loaded: No module named 'flair'\n2024-08-22 05:09:08,446 - INFO - allennlp.common.checks - Pytorch version: 1.12.1+cu102\n\nloading instances: 0it [00:00, ?it/s]2024-08-22 05:09:09,479 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'case'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n2024-08-22 05:09:09,479 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'xpos'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n2024-08-22 05:09:09,479 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'depchunk'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n2024-08-22 05:09:09,480 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'deprel'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n2024-08-22 05:09:09,480 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'genre'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n2024-08-22 05:09:09,480 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'parent_clauses'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n2024-08-22 05:09:09,481 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'upos'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n2024-08-22 05:09:09,481 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 's_type'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n\nloading instances: 10it [00:00, 879.12it/s]\n\nloading instances: 0it [00:00, ?it/s]\nloading instances: 10it [00:00, 1277.12it/s]\n2024-08-22 05:09:09,493 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.\n\nbuilding vocab: 0it [00:00, ?it/s]\nbuilding vocab: 20it [00:00, 3700.48it/s]\n2024-08-22 05:09:09,502 - INFO - allennlp.modules.token_embedders.embedding - Reading pretrained embeddings from file\n2024-08-22 05:09:09,572 - INFO - cached_path - cache of https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.de.300.vec.gz is up-to-date\n2024-08-22 05:09:09,575 - INFO - allennlp.modules.token_embedders.embedding - Recognized a header line in the embedding file with number of tokens: 2000000\n\n  0%|          | 0/2000000 [00:00<?, ?it/s]\n  0%|          | 3536/2000000 [00:00<00:56, 35343.85it/s]\n  0%|          | 7353/2000000 [00:00<00:53, 37002.84it/s]\n  1%|          | 11203/2000000 [00:00<00:52, 37685.44it/s]\n  1%|          | 15080/2000000 [00:00<00:52, 38105.44it/s]\n  1%|          | 18922/2000000 [00:00<00:51, 38196.24it/s]\n  1%|1         | 22742/2000000 [00:00<00:52, 37880.25it/s]\n  1%|1         | 26599/2000000 [00:00<00:51, 38100.49it/s]\n  2%|1         | 30454/2000000 [00:00<00:51, 38242.14it/s]\n  2%|1         | 34326/2000000 [00:00<00:51, 38390.54it/s]\n  2%|1         | 38188/2000000 [00:01<00:51, 38460.63it/s]\n  2%|2         | 42068/2000000 [00:01<00:50, 38562.98it/s]\n  2%|2         | 45937/2000000 [00:01<00:50, 38589.99it/s]\n  2%|2         | 49814/2000000 [00:01<00:50, 38633.61it/s]\n  3%|2         | 53685/2000000 [00:01<00:50, 38649.17it/s]\n  3%|2         | 57550/2000000 [00:01<00:50, 38579.53it/s]\n  3%|3         | 61414/2000000 [00:01<00:50, 38588.89it/s]\n  3%|3         | 65303/2000000 [00:01<00:50, 38674.22it/s]\n  3%|3         | 69184/2000000 [00:01<00:49, 38705.55it/s]\n  4%|3         | 73062/2000000 [00:01<00:49, 38723.05it/s]\n  4%|3         | 76935/2000000 [00:02<00:49, 38699.10it/s]\n  4%|4         | 80819/2000000 [00:02<00:49, 38738.40it/s]\n  4%|4         | 84710/2000000 [00:02<00:49, 38789.32it/s]\n  4%|4         | 88596/2000000 [00:02<00:49, 38799.32it/s]\n  5%|4         | 92496/2000000 [00:02<00:49, 38849.76it/s]\n  5%|4         | 96381/2000000 [00:02<00:49, 38101.12it/s]\n  5%|5         | 100257/2000000 [00:02<00:49, 38289.25it/s]\n  5%|5         | 104147/2000000 [00:02<00:49, 38470.19it/s]\n  5%|5         | 108043/2000000 [00:02<00:48, 38611.46it/s]\n  6%|5         | 111910/2000000 [00:02<00:48, 38627.02it/s]\n  6%|5         | 115795/2000000 [00:03<00:48, 38692.50it/s]\n  6%|5         | 119674/2000000 [00:03<00:48, 38721.11it/s]\n  6%|6         | 123550/2000000 [00:03<00:48, 38726.91it/s]\n  6%|6         | 127439/2000000 [00:03<00:48, 38770.05it/s]\n  7%|6         | 131328/2000000 [00:03<00:48, 38804.84it/s]\n  7%|6         | 135227/2000000 [00:03<00:47, 38850.00it/s]\n  7%|6         | 139115/2000000 [00:03<00:47, 38854.76it/s]\n  7%|7         | 143010/2000000 [00:03<00:47, 38880.57it/s]\n  7%|7         | 146917/2000000 [00:03<00:47, 38927.92it/s]\n  8%|7         | 150817/2000000 [00:03<00:47, 38940.42it/s]\n  8%|7         | 154712/2000000 [00:04<00:47, 38926.74it/s]\n  8%|7         | 158605/2000000 [00:04<00:47, 38676.13it/s]\n  8%|8         | 162496/2000000 [00:04<00:47, 38743.77it/s]\n  8%|8         | 166399/2000000 [00:04<00:47, 38823.43it/s]\n  9%|8         | 170305/2000000 [00:04<00:47, 38886.15it/s]\n  9%|8         | 174219/2000000 [00:04<00:46, 38960.78it/s]\n  9%|8         | 178125/2000000 [00:04<00:46, 38983.67it/s]\n  9%|9         | 182042/2000000 [00:04<00:46, 39035.39it/s]\n  9%|9         | 185963/2000000 [00:04<00:46, 39078.47it/s]\n  9%|9         | 189881/2000000 [00:04<00:46, 39100.48it/s]\n 10%|9         | 193792/2000000 [00:05<00:46, 39096.47it/s]\n 10%|9         | 197702/2000000 [00:05<00:46, 39090.36it/s]\n 10%|#         | 201612/2000000 [00:05<00:46, 39031.29it/s]\n 10%|#         | 205516/2000000 [00:05<00:45, 39027.36it/s]\n 10%|#         | 209419/2000000 [00:05<00:45, 38989.19it/s]\n 11%|#         | 213318/2000000 [00:05<00:46, 38831.95it/s]\n 11%|#         | 217232/2000000 [00:05<00:45, 38920.02it/s]\n 11%|#1        | 221134/2000000 [00:05<00:45, 38945.67it/s]\n 11%|#1        | 225029/2000000 [00:05<00:45, 38775.62it/s]\n 11%|#1        | 228914/2000000 [00:05<00:45, 38789.97it/s]\n 12%|#1        | 232831/2000000 [00:06<00:45, 38892.69it/s]\n 12%|#1        | 236735/2000000 [00:06<00:45, 38933.70it/s]\n 12%|#2        | 240630/2000000 [00:06<00:45, 38935.29it/s]\n 12%|#2        | 244540/2000000 [00:06<00:45, 38974.32it/s]\n 12%|#2        | 248448/2000000 [00:06<00:44, 39005.11it/s]\n 13%|#2        | 252349/2000000 [00:06<00:44, 38964.78it/s]\n 13%|#2        | 256246/2000000 [00:06<00:45, 38686.86it/s]\n 13%|#3        | 260116/2000000 [00:06<00:44, 38679.90it/s]\n 13%|#3        | 263985/2000000 [00:06<00:44, 38650.26it/s]\n 13%|#3        | 267851/2000000 [00:06<00:44, 38636.60it/s]\n 14%|#3        | 271715/2000000 [00:07<00:44, 38632.56it/s]\n 14%|#3        | 275579/2000000 [00:07<00:44, 38588.42it/s]\n 14%|#3        | 279438/2000000 [00:07<00:44, 38578.58it/s]\n 14%|#4        | 283308/2000000 [00:07<00:44, 38607.18it/s]\n 14%|#4        | 287174/2000000 [00:07<00:44, 38616.17it/s]\n 15%|#4        | 291036/2000000 [00:07<00:44, 38604.23it/s]\n 15%|#4        | 294897/2000000 [00:07<00:44, 38604.04it/s]\n 15%|#4        | 298758/2000000 [00:07<00:44, 38586.95it/s]\n 15%|#5        | 302617/2000000 [00:07<00:43, 38580.32it/s]\n 15%|#5        | 306487/2000000 [00:07<00:43, 38610.47it/s]\n 16%|#5        | 310349/2000000 [00:08<00:43, 38606.46it/s]\n 16%|#5        | 314225/2000000 [00:08<00:43, 38640.63it/s]\n 16%|#5        | 318109/2000000 [00:08<00:43, 38692.34it/s]\n 16%|#6        | 321979/2000000 [00:08<00:43, 38655.69it/s]\n 16%|#6        | 325856/2000000 [00:08<00:43, 38681.50it/s]\n 16%|#6        | 329725/2000000 [00:08<00:43, 38655.47it/s]\n 17%|#6        | 333591/2000000 [00:08<00:43, 38634.81it/s]\n 17%|#6        | 337472/2000000 [00:08<00:42, 38678.00it/s]\n 17%|#7        | 341340/2000000 [00:08<00:43, 38350.39it/s]\n 17%|#7        | 345176/2000000 [00:08<00:43, 38195.84it/s]\n 17%|#7        | 348997/2000000 [00:09<00:43, 38010.35it/s]\n 18%|#7        | 352799/2000000 [00:09<00:43, 37959.25it/s]\n 18%|#7        | 356596/2000000 [00:09<00:43, 37870.43it/s]\n 18%|#8        | 360384/2000000 [00:09<00:43, 37867.02it/s]\n 18%|#8        | 364171/2000000 [00:09<00:43, 37815.84it/s]\n 18%|#8        | 367964/2000000 [00:09<00:43, 37849.22it/s]\n 19%|#8        | 371749/2000000 [00:09<00:43, 37847.98it/s]\n 19%|#8        | 375537/2000000 [00:09<00:42, 37849.50it/s]\n 19%|#8        | 379322/2000000 [00:09<00:42, 37827.65it/s]\n 19%|#9        | 383105/2000000 [00:09<00:42, 37771.69it/s]\n 19%|#9        | 386891/2000000 [00:10<00:42, 37788.32it/s]\n 20%|#9        | 390670/2000000 [00:10<00:42, 37756.37it/s]\n 20%|#9        | 394446/2000000 [00:10<00:42, 37755.66it/s]\n 20%|#9        | 398222/2000000 [00:10<00:42, 37749.29it/s]\n 20%|##        | 402012/2000000 [00:10<00:42, 37790.17it/s]\n 20%|##        | 405792/2000000 [00:10<00:42, 37780.09it/s]\n 20%|##        | 409571/2000000 [00:10<00:42, 37771.57it/s]\n 21%|##        | 413349/2000000 [00:10<00:42, 37725.01it/s]\n 21%|##        | 417122/2000000 [00:10<00:42, 37613.07it/s]\n 21%|##1       | 420891/2000000 [00:10<00:41, 37633.08it/s]\n 21%|##1       | 424656/2000000 [00:11<00:41, 37631.74it/s]\n 21%|##1       | 428424/2000000 [00:11<00:41, 37644.00it/s]\n 22%|##1       | 432207/2000000 [00:11<00:41, 37692.53it/s]\n 22%|##1       | 435983/2000000 [00:11<00:41, 37707.42it/s]\n 22%|##1       | 439754/2000000 [00:11<00:41, 37628.87it/s]\n 22%|##2       | 443529/2000000 [00:11<00:41, 37661.46it/s]\n 22%|##2       | 447302/2000000 [00:11<00:41, 37681.41it/s]\n 23%|##2       | 451071/2000000 [00:11<00:41, 37669.36it/s]\n 23%|##2       | 454843/2000000 [00:11<00:41, 37675.10it/s]\n 23%|##2       | 458636/2000000 [00:11<00:40, 37746.22it/s]\n 23%|##3       | 462411/2000000 [00:12<00:40, 37730.00it/s]\n 23%|##3       | 466189/2000000 [00:12<00:40, 37743.54it/s]\n 23%|##3       | 469965/2000000 [00:12<00:40, 37744.49it/s]\n 24%|##3       | 473744/2000000 [00:12<00:40, 37753.16it/s]\n 24%|##3       | 477520/2000000 [00:12<00:40, 37698.89it/s]\n 24%|##4       | 481291/2000000 [00:12<00:40, 37692.59it/s]\n 24%|##4       | 485072/2000000 [00:12<00:40, 37727.22it/s]\n 24%|##4       | 488846/2000000 [00:12<00:40, 37721.46it/s]\n 25%|##4       | 492619/2000000 [00:12<00:39, 37709.94it/s]\n 25%|##4       | 496406/2000000 [00:12<00:39, 37757.42it/s]\n 25%|##5       | 500182/2000000 [00:13<00:39, 37740.67it/s]\n 25%|##5       | 503962/2000000 [00:13<00:39, 37747.32it/s]\n 25%|##5       | 507748/2000000 [00:13<00:39, 37773.50it/s]\n 26%|##5       | 511526/2000000 [00:13<00:39, 37729.31it/s]\n 26%|##5       | 515303/2000000 [00:13<00:39, 37740.79it/s]\n 26%|##5       | 519079/2000000 [00:13<00:39, 37735.64it/s]\n 26%|##6       | 522853/2000000 [00:13<00:39, 37703.22it/s]\n 26%|##6       | 526624/2000000 [00:13<00:39, 37676.95it/s]\n 27%|##6       | 530392/2000000 [00:13<00:39, 37674.05it/s]\n 27%|##6       | 534182/2000000 [00:13<00:38, 37731.96it/s]\n 27%|##6       | 538088/2000000 [00:14<00:38, 38121.80it/s]\n 27%|##7       | 542004/2000000 [00:14<00:37, 38432.25it/s]\n 27%|##7       | 545918/2000000 [00:14<00:37, 38640.49it/s]\n 27%|##7       | 549822/2000000 [00:14<00:37, 38759.75it/s]\n 28%|##7       | 553737/2000000 [00:14<00:37, 38870.37it/s]\n 28%|##7       | 557659/2000000 [00:14<00:37, 38965.99it/s]\n 28%|##8       | 561562/2000000 [00:14<00:36, 38984.08it/s]\n 28%|##8       | 565461/2000000 [00:14<00:36, 38975.78it/s]\n 28%|##8       | 569359/2000000 [00:14<00:36, 38845.99it/s]\n 29%|##8       | 573278/2000000 [00:14<00:36, 38939.10it/s]\n 29%|##8       | 577206/2000000 [00:15<00:36, 39032.29it/s]\n 29%|##9       | 581134/2000000 [00:15<00:36, 39100.46it/s]\n 29%|##9       | 585057/2000000 [00:15<00:36, 39136.38it/s]\n 29%|##9       | 588974/2000000 [00:15<00:36, 39143.39it/s]\n 30%|##9       | 592905/2000000 [00:15<00:35, 39183.69it/s]\n 30%|##9       | 596824/2000000 [00:15<00:35, 39155.92it/s]\n 30%|###       | 600740/2000000 [00:15<00:35, 39096.31it/s]\n 30%|###       | 604662/2000000 [00:15<00:35, 39131.35it/s]\n 30%|###       | 608582/2000000 [00:15<00:35, 39147.60it/s]\n 31%|###       | 612502/2000000 [00:15<00:35, 39153.72it/s]\n 31%|###       | 616418/2000000 [00:16<00:35, 39138.49it/s]\n 31%|###1      | 620332/2000000 [00:16<00:35, 39126.74it/s]\n 31%|###1      | 624256/2000000 [00:16<00:35, 39153.93it/s]\n 31%|###1      | 628179/2000000 [00:16<00:35, 39170.78it/s]\n 32%|###1      | 632104/2000000 [00:16<00:34, 39188.16it/s]\n 32%|###1      | 636032/2000000 [00:16<00:34, 39212.20it/s]\n 32%|###1      | 639954/2000000 [00:16<00:34, 39204.55it/s]\n 32%|###2      | 643875/2000000 [00:16<00:34, 39195.57it/s]\n 32%|###2      | 647795/2000000 [00:16<00:34, 39187.55it/s]\n 33%|###2      | 651714/2000000 [00:16<00:34, 39152.22it/s]\n 33%|###2      | 655645/2000000 [00:17<00:34, 39195.46it/s]\n 33%|###2      | 659565/2000000 [00:17<00:34, 39169.70it/s]\n 33%|###3      | 663492/2000000 [00:17<00:34, 39192.52it/s]\n 33%|###3      | 667412/2000000 [00:17<00:34, 39168.99it/s]\n 34%|###3      | 671336/2000000 [00:17<00:33, 39183.42it/s]\n 34%|###3      | 675260/2000000 [00:17<00:33, 39189.67it/s]\n 34%|###3      | 679180/2000000 [00:17<00:33, 39185.33it/s]\n 34%|###4      | 683104/2000000 [00:17<00:33, 39201.08it/s]\n 34%|###4      | 687025/2000000 [00:17<00:33, 39165.47it/s]\n 35%|###4      | 690942/2000000 [00:17<00:33, 39147.23it/s]\n 35%|###4      | 694863/2000000 [00:18<00:33, 39154.33it/s]\n 35%|###4      | 698779/2000000 [00:18<00:33, 39152.51it/s]\n 35%|###5      | 702714/2000000 [00:18<00:33, 39207.30it/s]\n 35%|###5      | 706635/2000000 [00:18<00:33, 39174.88it/s]\n 36%|###5      | 710569/2000000 [00:18<00:32, 39223.51it/s]\n 36%|###5      | 714492/2000000 [00:18<00:32, 39215.60it/s]\n 36%|###5      | 718431/2000000 [00:18<00:32, 39257.68it/s]\n 36%|###6      | 722357/2000000 [00:18<00:32, 39242.74it/s]\n 36%|###6      | 726282/2000000 [00:18<00:32, 39185.30it/s]\n 37%|###6      | 730201/2000000 [00:18<00:32, 39167.95it/s]\n 37%|###6      | 734118/2000000 [00:19<00:32, 39154.15it/s]\n 37%|###6      | 738035/2000000 [00:19<00:32, 39151.30it/s]\n 37%|###7      | 741951/2000000 [00:19<00:32, 39150.62it/s]\n 37%|###7      | 745867/2000000 [00:19<00:32, 39118.05it/s]\n 37%|###7      | 749783/2000000 [00:19<00:31, 39124.17it/s]\n 38%|###7      | 753696/2000000 [00:19<00:31, 39118.83it/s]\n 38%|###7      | 757612/2000000 [00:19<00:31, 39128.30it/s]\n 38%|###8      | 761546/2000000 [00:19<00:31, 39182.40it/s]\n 38%|###8      | 765465/2000000 [00:19<00:31, 39169.66it/s]\n 38%|###8      | 769382/2000000 [00:19<00:31, 39167.14it/s]\n 39%|###8      | 773299/2000000 [00:20<00:31, 39167.36it/s]\n 39%|###8      | 777216/2000000 [00:20<00:31, 39145.77it/s]\n 39%|###9      | 781131/2000000 [00:20<00:31, 39136.81it/s]\n 39%|###9      | 785060/2000000 [00:20<00:31, 39178.10it/s]\n 39%|###9      | 788978/2000000 [00:20<00:30, 39144.51it/s]\n 40%|###9      | 792893/2000000 [00:20<00:30, 39118.39it/s]\n 40%|###9      | 796805/2000000 [00:20<00:30, 39095.48it/s]\n 40%|####      | 800725/2000000 [00:20<00:30, 39118.11it/s]\n 40%|####      | 804637/2000000 [00:20<00:30, 39104.22it/s]\n 40%|####      | 808565/2000000 [00:20<00:30, 39148.32it/s]\n 41%|####      | 812480/2000000 [00:21<00:30, 39111.61it/s]\n 41%|####      | 816398/2000000 [00:21<00:30, 39128.36it/s]\n 41%|####1     | 820311/2000000 [00:21<00:30, 38666.47it/s]\n 41%|####1     | 824179/2000000 [00:21<00:30, 38129.06it/s]\n 41%|####1     | 827994/2000000 [00:21<00:31, 37483.93it/s]\n 42%|####1     | 831746/2000000 [00:21<00:31, 37141.10it/s]\n 42%|####1     | 835575/2000000 [00:21<00:31, 37471.56it/s]\n 42%|####1     | 839484/2000000 [00:21<00:30, 37945.08it/s]\n 42%|####2     | 843382/2000000 [00:21<00:30, 38245.20it/s]\n 42%|####2     | 847298/2000000 [00:21<00:29, 38515.07it/s]\n 43%|####2     | 851182/2000000 [00:22<00:29, 38604.64it/s]\n 43%|####2     | 855084/2000000 [00:22<00:29, 38721.75it/s]\n 43%|####2     | 858979/2000000 [00:22<00:29, 38784.36it/s]\n 43%|####3     | 862902/2000000 [00:22<00:29, 38910.90it/s]\n 43%|####3     | 866815/2000000 [00:22<00:29, 38968.38it/s]\n 44%|####3     | 870717/2000000 [00:22<00:28, 38977.45it/s]\n 44%|####3     | 874615/2000000 [00:22<00:29, 38766.66it/s]\n 44%|####3     | 878511/2000000 [00:22<00:28, 38819.80it/s]\n 44%|####4     | 882407/2000000 [00:22<00:28, 38860.95it/s]\n 44%|####4     | 886330/2000000 [00:22<00:28, 38967.98it/s]\n 45%|####4     | 890239/2000000 [00:23<00:28, 38998.27it/s]\n 45%|####4     | 894156/2000000 [00:23<00:28, 39045.05it/s]\n 45%|####4     | 898061/2000000 [00:23<00:28, 39028.61it/s]\n 45%|####5     | 901966/2000000 [00:23<00:28, 39028.17it/s]\n 45%|####5     | 905893/2000000 [00:23<00:27, 39093.32it/s]\n 45%|####5     | 909809/2000000 [00:23<00:27, 39107.98it/s]\n 46%|####5     | 913737/2000000 [00:23<00:27, 39150.36it/s]\n 46%|####5     | 917668/2000000 [00:23<00:27, 39197.26it/s]\n 46%|####6     | 921588/2000000 [00:23<00:27, 39154.81it/s]\n 46%|####6     | 925518/2000000 [00:23<00:27, 39193.85it/s]\n 46%|####6     | 929456/2000000 [00:24<00:27, 39243.83it/s]\n 47%|####6     | 933381/2000000 [00:24<00:27, 39223.26it/s]\n 47%|####6     | 937307/2000000 [00:24<00:27, 39230.16it/s]\n 47%|####7     | 941235/2000000 [00:24<00:26, 39240.99it/s]\n 47%|####7     | 945160/2000000 [00:24<00:26, 39226.09it/s]\n 47%|####7     | 949087/2000000 [00:24<00:26, 39233.13it/s]\n 48%|####7     | 953011/2000000 [00:24<00:26, 39189.26it/s]\n 48%|####7     | 956930/2000000 [00:24<00:26, 39117.28it/s]\n 48%|####8     | 960861/2000000 [00:24<00:26, 39169.62it/s]\n 48%|####8     | 964799/2000000 [00:24<00:26, 39230.04it/s]\n 48%|####8     | 968723/2000000 [00:25<00:26, 39182.79it/s]\n 49%|####8     | 972657/2000000 [00:25<00:26, 39226.72it/s]\n 49%|####8     | 976587/2000000 [00:25<00:26, 39242.99it/s]\n 49%|####9     | 980512/2000000 [00:25<00:26, 39185.52it/s]\n 49%|####9     | 984431/2000000 [00:25<00:25, 39130.94it/s]\n 49%|####9     | 988349/2000000 [00:25<00:25, 39141.42it/s]\n 50%|####9     | 992264/2000000 [00:25<00:25, 39114.25it/s]\n 50%|####9     | 996200/2000000 [00:25<00:25, 39186.24it/s]\n 50%|#####     | 1000138/2000000 [00:25<00:25, 39243.58it/s]\n 50%|#####     | 1004076/2000000 [00:25<00:25, 39280.53it/s]\n 50%|#####     | 1008007/2000000 [00:26<00:25, 39286.51it/s]\n 51%|#####     | 1011941/2000000 [00:26<00:25, 39297.05it/s]\n 51%|#####     | 1015885/2000000 [00:26<00:25, 39337.26it/s]\n 51%|#####     | 1019819/2000000 [00:26<00:24, 39268.35it/s]\n 51%|#####1    | 1023752/2000000 [00:26<00:24, 39282.25it/s]\n 51%|#####1    | 1027689/2000000 [00:26<00:24, 39306.73it/s]\n 52%|#####1    | 1031620/2000000 [00:26<00:24, 39283.50it/s]\n 52%|#####1    | 1035549/2000000 [00:26<00:24, 39227.66it/s]\n 52%|#####1    | 1039487/2000000 [00:26<00:24, 39272.28it/s]\n 52%|#####2    | 1043415/2000000 [00:26<00:24, 39272.05it/s]\n 52%|#####2    | 1047352/2000000 [00:27<00:24, 39292.97it/s]\n 53%|#####2    | 1051282/2000000 [00:27<00:24, 39249.37it/s]\n 53%|#####2    | 1055207/2000000 [00:27<00:24, 39016.86it/s]\n 53%|#####2    | 1059109/2000000 [00:27<00:24, 38339.46it/s]\n 53%|#####3    | 1062946/2000000 [00:27<00:24, 37973.86it/s]\n 53%|#####3    | 1066878/2000000 [00:27<00:24, 38365.92it/s]\n 54%|#####3    | 1070830/2000000 [00:27<00:24, 38700.44it/s]\n 54%|#####3    | 1074761/2000000 [00:27<00:23, 38879.27it/s]\n 54%|#####3    | 1078674/2000000 [00:27<00:23, 38951.36it/s]\n 54%|#####4    | 1082593/2000000 [00:27<00:23, 39020.30it/s]\n 54%|#####4    | 1086535/2000000 [00:28<00:23, 39131.77it/s]\n 55%|#####4    | 1090472/2000000 [00:28<00:23, 39199.87it/s]\n 55%|#####4    | 1094406/2000000 [00:28<00:23, 39240.25it/s]\n 55%|#####4    | 1098331/2000000 [00:28<00:23, 39155.32it/s]\n 55%|#####5    | 1102267/2000000 [00:28<00:22, 39213.57it/s]\n 55%|#####5    | 1106191/2000000 [00:28<00:22, 39212.59it/s]\n 56%|#####5    | 1110115/2000000 [00:28<00:22, 39214.68it/s]\n 56%|#####5    | 1114037/2000000 [00:28<00:22, 39143.54it/s]\n 56%|#####5    | 1117976/2000000 [00:28<00:22, 39210.79it/s]\n 56%|#####6    | 1121903/2000000 [00:28<00:22, 39227.61it/s]\n 56%|#####6    | 1125830/2000000 [00:29<00:22, 39231.13it/s]\n 56%|#####6    | 1129754/2000000 [00:29<00:22, 39162.80it/s]\n 57%|#####6    | 1133671/2000000 [00:29<00:22, 39108.88it/s]\n 57%|#####6    | 1137602/2000000 [00:29<00:22, 39166.91it/s]\n 57%|#####7    | 1141519/2000000 [00:29<00:21, 39139.25it/s]\n 57%|#####7    | 1145462/2000000 [00:29<00:21, 39222.14it/s]\n 57%|#####7    | 1149385/2000000 [00:29<00:21, 39179.28it/s]\n 58%|#####7    | 1153303/2000000 [00:29<00:21, 39108.32it/s]\n 58%|#####7    | 1157279/2000000 [00:29<00:21, 39302.71it/s]\n 58%|#####8    | 1161235/2000000 [00:29<00:21, 39368.48it/s]\n 58%|#####8    | 1165173/2000000 [00:30<00:21, 39368.57it/s]\n 58%|#####8    | 1169110/2000000 [00:30<00:21, 39222.86it/s]\n 59%|#####8    | 1173034/2000000 [00:30<00:21, 39218.82it/s]\n 59%|#####8    | 1176974/2000000 [00:30<00:20, 39268.35it/s]\n 59%|#####9    | 1180908/2000000 [00:30<00:20, 39280.39it/s]\n 59%|#####9    | 1184852/2000000 [00:30<00:20, 39322.43it/s]\n 59%|#####9    | 1188785/2000000 [00:30<00:20, 39198.91it/s]\n 60%|#####9    | 1192732/2000000 [00:30<00:20, 39279.44it/s]\n 60%|#####9    | 1196669/2000000 [00:30<00:20, 39295.48it/s]\n 60%|######    | 1200607/2000000 [00:30<00:20, 39316.90it/s]\n 60%|######    | 1204539/2000000 [00:31<00:20, 39294.57it/s]\n 60%|######    | 1208469/2000000 [00:31<00:20, 39214.00it/s]\n 61%|######    | 1212414/2000000 [00:31<00:20, 39272.59it/s]\n 61%|######    | 1216388/2000000 [00:31<00:19, 39402.74it/s]\n 61%|######1   | 1220373/2000000 [00:31<00:19, 39534.16it/s]\n 61%|######1   | 1224358/2000000 [00:31<00:19, 39620.84it/s]\n 61%|######1   | 1228321/2000000 [00:31<00:19, 39495.74it/s]\n 62%|######1   | 1232271/2000000 [00:31<00:19, 39470.76it/s]\n 62%|######1   | 1236219/2000000 [00:31<00:19, 39468.98it/s]\n 62%|######2   | 1240166/2000000 [00:31<00:19, 39445.53it/s]\n 62%|######2   | 1244111/2000000 [00:32<00:19, 39348.79it/s]\n 62%|######2   | 1248046/2000000 [00:32<00:19, 39253.15it/s]\n 63%|######2   | 1251972/2000000 [00:32<00:19, 39215.86it/s]\n 63%|######2   | 1255907/2000000 [00:32<00:18, 39244.84it/s]\n 63%|######2   | 1259832/2000000 [00:32<00:18, 39202.93it/s]\n 63%|######3   | 1263753/2000000 [00:32<00:18, 39178.79it/s]\n 63%|######3   | 1267671/2000000 [00:32<00:18, 39164.65it/s]\n 64%|######3   | 1271588/2000000 [00:32<00:18, 39140.95it/s]\n 64%|######3   | 1275522/2000000 [00:32<00:18, 39199.34it/s]\n 64%|######3   | 1279471/2000000 [00:32<00:18, 39283.13it/s]\n 64%|######4   | 1283400/2000000 [00:33<00:18, 39198.76it/s]\n 64%|######4   | 1287322/2000000 [00:33<00:18, 39196.29it/s]\n 65%|######4   | 1291248/2000000 [00:33<00:18, 39206.93it/s]\n 65%|######4   | 1295169/2000000 [00:33<00:17, 39196.49it/s]\n 65%|######4   | 1299111/2000000 [00:33<00:17, 39257.48it/s]\n 65%|######5   | 1303042/2000000 [00:33<00:17, 39261.67it/s]\n 65%|######5   | 1306981/2000000 [00:33<00:17, 39292.12it/s]\n 66%|######5   | 1310922/2000000 [00:33<00:17, 39316.09it/s]\n 66%|######5   | 1314854/2000000 [00:33<00:17, 39262.35it/s]\n 66%|######5   | 1318781/2000000 [00:34<00:17, 39186.79it/s]\n 66%|######6   | 1322735/2000000 [00:34<00:17, 39285.45it/s]\n 66%|######6   | 1326669/2000000 [00:34<00:17, 39298.96it/s]\n 67%|######6   | 1330614/2000000 [00:34<00:17, 39335.18it/s]\n 67%|######6   | 1334548/2000000 [00:34<00:16, 39334.38it/s]\n 67%|######6   | 1338496/2000000 [00:34<00:16, 39373.29it/s]\n 67%|######7   | 1342434/2000000 [00:34<00:16, 39316.24it/s]\n 67%|######7   | 1346366/2000000 [00:34<00:16, 39272.25it/s]\n 68%|######7   | 1350304/2000000 [00:34<00:16, 39300.26it/s]\n 68%|######7   | 1354253/2000000 [00:34<00:16, 39349.67it/s]\n 68%|######7   | 1358205/2000000 [00:35<00:16, 39393.71it/s]\n 68%|######8   | 1362145/2000000 [00:35<00:16, 39388.26it/s]\n 68%|######8   | 1366084/2000000 [00:35<00:16, 39359.20it/s]\n 69%|######8   | 1370020/2000000 [00:35<00:16, 39254.55it/s]\n 69%|######8   | 1373946/2000000 [00:35<00:15, 39211.86it/s]\n 69%|######8   | 1377889/2000000 [00:35<00:15, 39274.00it/s]\n 69%|######9   | 1381858/2000000 [00:35<00:15, 39394.19it/s]\n 69%|######9   | 1385800/2000000 [00:35<00:15, 39392.23it/s]\n 69%|######9   | 1389752/2000000 [00:35<00:15, 39423.09it/s]\n 70%|######9   | 1393695/2000000 [00:35<00:15, 39410.98it/s]\n 70%|######9   | 1397637/2000000 [00:36<00:15, 39123.16it/s]\n 70%|#######   | 1401554/2000000 [00:36<00:15, 39133.30it/s]\n 70%|#######   | 1405550/2000000 [00:36<00:15, 39369.67it/s]\n 70%|#######   | 1409528/2000000 [00:36<00:14, 39488.74it/s]\n 71%|#######   | 1413537/2000000 [00:36<00:14, 39661.90it/s]\n 71%|#######   | 1417504/2000000 [00:36<00:14, 39576.11it/s]\n 71%|#######1  | 1421462/2000000 [00:36<00:14, 39498.48it/s]\n 71%|#######1  | 1425412/2000000 [00:36<00:14, 39362.63it/s]\n 71%|#######1  | 1429349/2000000 [00:36<00:14, 39288.46it/s]\n 72%|#######1  | 1433284/2000000 [00:36<00:14, 39303.52it/s]\n 72%|#######1  | 1437233/2000000 [00:37<00:14, 39351.91it/s]\n 72%|#######2  | 1441178/2000000 [00:37<00:14, 39377.65it/s]\n 72%|#######2  | 1445122/2000000 [00:37<00:14, 39391.55it/s]\n 72%|#######2  | 1449085/2000000 [00:37<00:13, 39453.59it/s]\n 73%|#######2  | 1453031/2000000 [00:37<00:13, 39375.09it/s]\n 73%|#######2  | 1456969/2000000 [00:37<00:13, 39291.30it/s]\n 73%|#######3  | 1460899/2000000 [00:37<00:13, 39255.59it/s]\n 73%|#######3  | 1464844/2000000 [00:37<00:13, 39302.32it/s]\n 73%|#######3  | 1468794/2000000 [00:37<00:13, 39351.93it/s]\n 74%|#######3  | 1472730/2000000 [00:37<00:13, 39339.08it/s]\n 74%|#######3  | 1476712/2000000 [00:38<00:13, 39479.38it/s]\n 74%|#######4  | 1480664/2000000 [00:38<00:13, 39481.94it/s]\n 74%|#######4  | 1484613/2000000 [00:38<00:13, 39394.17it/s]\n 74%|#######4  | 1488553/2000000 [00:38<00:13, 39301.77it/s]\n 75%|#######4  | 1492484/2000000 [00:38<00:12, 39202.40it/s]\n 75%|#######4  | 1496405/2000000 [00:38<00:12, 39191.78it/s]\n 75%|#######5  | 1500375/2000000 [00:38<00:12, 39338.18it/s]\n 75%|#######5  | 1504320/2000000 [00:38<00:12, 39368.23it/s]\n 75%|#######5  | 1508257/2000000 [00:38<00:12, 39355.99it/s]\n 76%|#######5  | 1512209/2000000 [00:38<00:12, 39397.31it/s]\n 76%|#######5  | 1516149/2000000 [00:39<00:12, 39319.10it/s]\n 76%|#######6  | 1520101/2000000 [00:39<00:12, 39376.33it/s]\n 76%|#######6  | 1524039/2000000 [00:39<00:12, 39356.87it/s]\n 76%|#######6  | 1528006/2000000 [00:39<00:11, 39445.10it/s]\n 77%|#######6  | 1531954/2000000 [00:39<00:11, 39452.56it/s]\n 77%|#######6  | 1535931/2000000 [00:39<00:11, 39543.57it/s]\n 77%|#######6  | 1539925/2000000 [00:39<00:11, 39660.70it/s]\n 77%|#######7  | 1543920/2000000 [00:39<00:11, 39745.70it/s]\n 77%|#######7  | 1547938/2000000 [00:39<00:11, 39875.46it/s]\n 78%|#######7  | 1551926/2000000 [00:39<00:11, 39797.07it/s]\n 78%|#######7  | 1555906/2000000 [00:40<00:11, 39734.37it/s]\n 78%|#######7  | 1559880/2000000 [00:40<00:11, 39679.71it/s]\n 78%|#######8  | 1563849/2000000 [00:40<00:11, 39570.53it/s]\n 78%|#######8  | 1567807/2000000 [00:40<00:10, 39526.93it/s]\n 79%|#######8  | 1571760/2000000 [00:40<00:10, 39465.46it/s]\n 79%|#######8  | 1575707/2000000 [00:40<00:10, 39392.79it/s]\n 79%|#######8  | 1579652/2000000 [00:40<00:10, 39401.42it/s]\n 79%|#######9  | 1583593/2000000 [00:40<00:10, 39389.64it/s]\n 79%|#######9  | 1587538/2000000 [00:40<00:10, 39401.20it/s]\n 80%|#######9  | 1591479/2000000 [00:40<00:10, 39358.26it/s]\n 80%|#######9  | 1595415/2000000 [00:41<00:10, 39296.24it/s]\n 80%|#######9  | 1599345/2000000 [00:41<00:10, 39243.98it/s]\n 80%|########  | 1603275/2000000 [00:41<00:10, 39255.07it/s]\n 80%|########  | 1607213/2000000 [00:41<00:09, 39287.13it/s]\n 81%|########  | 1611197/2000000 [00:41<00:09, 39451.74it/s]\n 81%|########  | 1615171/2000000 [00:41<00:09, 39526.61it/s]\n 81%|########  | 1619124/2000000 [00:41<00:09, 39485.29it/s]\n 81%|########1 | 1623073/2000000 [00:41<00:09, 39474.02it/s]\n 81%|########1 | 1627021/2000000 [00:41<00:09, 39468.30it/s]\n 82%|########1 | 1630968/2000000 [00:41<00:09, 39391.05it/s]\n 82%|########1 | 1634908/2000000 [00:42<00:09, 39236.18it/s]\n 82%|########1 | 1638832/2000000 [00:42<00:09, 39117.56it/s]\n 82%|########2 | 1642750/2000000 [00:42<00:09, 39130.11it/s]\n 82%|########2 | 1646676/2000000 [00:42<00:09, 39158.81it/s]\n 83%|########2 | 1650592/2000000 [00:42<00:08, 39136.82it/s]\n 83%|########2 | 1654584/2000000 [00:42<00:08, 39370.59it/s]\n 83%|########2 | 1658563/2000000 [00:42<00:08, 39490.32it/s]\n 83%|########3 | 1662547/2000000 [00:42<00:08, 39594.00it/s]\n 83%|########3 | 1666507/2000000 [00:42<00:08, 39530.39it/s]\n 84%|########3 | 1670461/2000000 [00:42<00:08, 39493.74it/s]\n 84%|########3 | 1674411/2000000 [00:43<00:08, 39399.19it/s]\n 84%|########3 | 1678351/2000000 [00:43<00:08, 39338.74it/s]\n 84%|########4 | 1682306/2000000 [00:43<00:08, 39401.22it/s]\n 84%|########4 | 1686247/2000000 [00:43<00:07, 39346.30it/s]\n 85%|########4 | 1690202/2000000 [00:43<00:07, 39399.25it/s]\n 85%|########4 | 1694142/2000000 [00:43<00:07, 39392.96it/s]\n 85%|########4 | 1698087/2000000 [00:43<00:07, 39401.53it/s]\n 85%|########5 | 1702028/2000000 [00:43<00:07, 39390.57it/s]\n 85%|########5 | 1705980/2000000 [00:43<00:07, 39420.98it/s]\n 85%|########5 | 1709923/2000000 [00:43<00:07, 39370.91it/s]\n 86%|########5 | 1713861/2000000 [00:44<00:07, 39313.68it/s]\n 86%|########5 | 1717793/2000000 [00:44<00:07, 39152.53it/s]\n 86%|########6 | 1721709/2000000 [00:44<00:07, 38921.30it/s]\n 86%|########6 | 1725602/2000000 [00:44<00:07, 38794.82it/s]\n 86%|########6 | 1729482/2000000 [00:44<00:06, 38733.04it/s]\n 87%|########6 | 1733356/2000000 [00:44<00:06, 38726.18it/s]\n 87%|########6 | 1737229/2000000 [00:44<00:06, 38645.26it/s]\n 87%|########7 | 1741118/2000000 [00:44<00:06, 38710.16it/s]\n 87%|########7 | 1744990/2000000 [00:44<00:06, 38696.90it/s]\n 87%|########7 | 1748860/2000000 [00:44<00:06, 38687.90it/s]\n 88%|########7 | 1752738/2000000 [00:45<00:06, 38705.81it/s]\n 88%|########7 | 1756665/2000000 [00:45<00:06, 38870.61it/s]\n 88%|########8 | 1760553/2000000 [00:45<00:06, 38778.72it/s]\n 88%|########8 | 1764431/2000000 [00:45<00:06, 38584.39it/s]\n 88%|########8 | 1768362/2000000 [00:45<00:05, 38790.74it/s]\n 89%|########8 | 1772288/2000000 [00:45<00:05, 38926.07it/s]\n 89%|########8 | 1776211/2000000 [00:45<00:05, 39016.03it/s]\n 89%|########9 | 1780167/2000000 [00:45<00:05, 39175.72it/s]\n 89%|########9 | 1784115/2000000 [00:45<00:05, 39258.36it/s]\n 89%|########9 | 1788081/2000000 [00:45<00:05, 39377.62it/s]\n 90%|########9 | 1792054/2000000 [00:46<00:05, 39479.20it/s]\n 90%|########9 | 1796038/2000000 [00:46<00:05, 39582.51it/s]\n 90%|######### | 1800025/2000000 [00:46<00:05, 39666.85it/s]\n 90%|######### | 1803992/2000000 [00:46<00:04, 39576.13it/s]\n 90%|######### | 1807950/2000000 [00:46<00:04, 39497.45it/s]\n 91%|######### | 1811900/2000000 [00:46<00:04, 39392.68it/s]\n 91%|######### | 1815840/2000000 [00:46<00:04, 39338.71it/s]\n 91%|######### | 1819774/2000000 [00:46<00:04, 39273.92it/s]\n 91%|#########1| 1823702/2000000 [00:46<00:04, 39221.70it/s]\n 91%|#########1| 1827625/2000000 [00:46<00:04, 39177.56it/s]\n 92%|#########1| 1831557/2000000 [00:47<00:04, 39209.59it/s]\n 92%|#########1| 1835497/2000000 [00:47<00:04, 39261.74it/s]\n 92%|#########1| 1839448/2000000 [00:47<00:04, 39331.60it/s]\n 92%|#########2| 1843406/2000000 [00:47<00:03, 39403.10it/s]\n 92%|#########2| 1847362/2000000 [00:47<00:03, 39441.09it/s]\n 93%|#########2| 1851310/2000000 [00:47<00:03, 39450.90it/s]\n 93%|#########2| 1855256/2000000 [00:47<00:03, 39044.55it/s]\n 93%|#########2| 1859162/2000000 [00:47<00:03, 39020.66it/s]\n 93%|#########3| 1863065/2000000 [00:47<00:03, 38607.56it/s]\n 93%|#########3| 1866989/2000000 [00:47<00:03, 38784.32it/s]\n 94%|#########3| 1870912/2000000 [00:48<00:03, 38912.03it/s]\n 94%|#########3| 1874819/2000000 [00:48<00:03, 38947.02it/s]\n 94%|#########3| 1878728/2000000 [00:48<00:03, 38980.97it/s]\n 94%|#########4| 1882637/2000000 [00:48<00:03, 39002.01it/s]\n 94%|#########4| 1886552/2000000 [00:48<00:02, 39035.48it/s]\n 95%|#########4| 1890497/2000000 [00:48<00:02, 39149.94it/s]\n 95%|#########4| 1894439/2000000 [00:48<00:02, 39229.33it/s]\n 95%|#########4| 1898397/2000000 [00:48<00:02, 39328.25it/s]\n 95%|#########5| 1902331/2000000 [00:48<00:02, 39328.52it/s]\n 95%|#########5| 1906271/2000000 [00:48<00:02, 39339.39it/s]\n 96%|#########5| 1910219/2000000 [00:49<00:02, 39371.96it/s]\n 96%|#########5| 1914167/2000000 [00:49<00:02, 39397.49it/s]\n 96%|#########5| 1918110/2000000 [00:49<00:02, 39396.69it/s]\n 96%|#########6| 1922050/2000000 [00:49<00:01, 39394.68it/s]\n 96%|#########6| 1925990/2000000 [00:49<00:01, 39363.61it/s]\n 96%|#########6| 1929927/2000000 [00:49<00:01, 39289.11it/s]\n 97%|#########6| 1933856/2000000 [00:49<00:01, 39006.99it/s]\n 97%|#########6| 1937758/2000000 [00:49<00:01, 38966.27it/s]\n 97%|#########7| 1941673/2000000 [00:49<00:01, 39013.56it/s]\n 97%|#########7| 1945600/2000000 [00:49<00:01, 39080.46it/s]\n 97%|#########7| 1949540/2000000 [00:50<00:01, 39174.16it/s]\n 98%|#########7| 1953470/2000000 [00:50<00:01, 39208.18it/s]\n 98%|#########7| 1957410/2000000 [00:50<00:01, 39258.08it/s]\n 98%|#########8| 1961344/2000000 [00:50<00:00, 39280.00it/s]\n 98%|#########8| 1965284/2000000 [00:50<00:00, 39315.33it/s]\n 98%|#########8| 1969226/2000000 [00:50<00:00, 39337.78it/s]\n 99%|#########8| 1973163/2000000 [00:50<00:00, 39342.28it/s]\n 99%|#########8| 1977108/2000000 [00:50<00:00, 39364.88it/s]\n 99%|#########9| 1981045/2000000 [00:50<00:00, 39301.92it/s]\n 99%|#########9| 1984976/2000000 [00:50<00:00, 39293.41it/s]\n 99%|#########9| 1988906/2000000 [00:51<00:00, 39204.43it/s]\n100%|#########9| 1992827/2000000 [00:51<00:00, 39135.38it/s]\n100%|#########9| 1996741/2000000 [00:51<00:00, 39083.18it/s]\n100%|##########| 2000000/2000000 [00:51<00:00, 38953.32it/s]\n2024-08-22 05:10:00,921 - INFO - allennlp.modules.token_embedders.embedding - Initializing pre-trained embedding layer\n2024-08-22 05:10:00,924 - INFO - allennlp.modules.token_embedders.embedding - Pretrained embeddings were found for 186 out of 192 tokens\n/usr/local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n  warnings.warn(\"dropout option adds dropout after all but last \"\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nVocabulary with namespaces:\n \tNon Padded Namespaces: {'*labels', '*tags'}\n \tNamespace: fasttext, Size: 192 \n \tNamespace: token_characters, Size: 62 \n \tNamespace: case, Size: 6 \n \tNamespace: xpos, Size: 38 \n \tNamespace: depchunk, Size: 20 \n \tNamespace: deprel, Size: 29 \n \tNamespace: genre, Size: 3 \n \tNamespace: parent_clauses, Size: 17 \n \tNamespace: upos, Size: 16 \n \tNamespace: s_type, Size: 3 \n \tNamespace: labels, Size: 2 \n\ncase\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: 'l', 3: 't', 4: 'o', 5: 'd'}\nxpos\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: 'NN', 3: 'ART', 4: 'ADV', 5: '$.', 6: 'APPR', 7: 'ADJA', 8: 'VAFIN', 9: 'ADJD', 10: '$,', 11: 'VVFIN', 12: 'VVPP', 13: 'VVINF', 14: 'NE', 15: 'PPER', 16: 'KON', 17: 'VMFIN', 18: 'PIS', 19: 'PROAV', 20: 'KOUS', 21: 'PIAT', 22: 'PRELS', 23: 'KOKOM', 24: 'PTKZU', 25: 'VAINF', 26: 'PWS', 27: 'PTKVZ', 28: 'APPRART', 29: 'VMINF', 30: 'CARD', 31: 'TRUNC', 32: 'PWAV', 33: 'PTKA', 34: 'PRF', 35: 'PDS', 36: 'PTKNEG', 37: 'PPOSAT'}\ndepchunk\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: '_', 3: 'Iacl', 4: 'Iadvcl', 5: 'Iccomp', 6: 'Icsubj', 7: 'Ixcomp', 8: 'Bacl', 9: 'Eacl', 10: 'Bxcomp', 11: 'Excomp', 12: 'Badvcl', 13: 'Bccomp', 14: 'Eccomp', 15: 'Bappos', 16: 'Eadvcl', 17: 'Bcsubj', 18: 'Ecsubj', 19: 'Eappos'}\ndeprel\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: 'advmod', 3: 'punct', 4: 'det', 5: 'nsubj', 6: 'case', 7: 'root', 8: 'obl', 9: 'nmod', 10: 'aux', 11: 'obj', 12: 'amod', 13: 'cc', 14: 'mark', 15: 'conj', 16: 'aux:pass', 17: 'nsubj:pass', 18: 'acl', 19: 'cop', 20: 'expl', 21: 'flat', 22: 'xcomp', 23: 'advcl', 24: 'ccomp', 25: 'compound:prt', 26: 'appos', 27: 'csubj:pass', 28: 'det:poss'}\ngenre\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: '_'}\nparent_clauses\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: '', 3: 'ccomp', 4: 'csubj:pass', 5: 'acl', 6: 'advcl|conj|acl', 7: 'advcl', 8: 'acl|xcomp', 9: 'xcomp|conj', 10: 'acl|ccomp', 11: 'conj', 12: 'conj|acl', 13: 'xcomp', 14: 'conj|advcl', 15: 'conj|acl|xcomp', 16: 'appos'}\nupos\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: 'NOUN', 3: 'VERB', 4: 'ADV', 5: 'PUNCT', 6: 'DET', 7: 'ADP', 8: 'PRON', 9: 'ADJ', 10: 'AUX', 11: 'PROPN', 12: 'CCONJ', 13: 'SCONJ', 14: 'PART', 15: 'NUM'}\ns_type\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: 'NoQ_NoSubj_NoConj_NoCop_NoAdvcl_NoAcl'}\nlabels\n  {0: 'O', 1: 'B'}\nDisrpt2021Baseline(\n  (feature_modules): ModuleDict(\n    (case_tags): Embedding(6, 3, padding_idx=0)\n    (cpos_tags): Embedding(38, 7, padding_idx=0)\n    (dep_chunk_tags): Embedding(20, 5, padding_idx=0)\n    (deprel_tags): Embedding(29, 6, padding_idx=0)\n    (document_depth): Identity()\n    (genre_tags): Embedding(3, 2, padding_idx=0)\n    (head_distances): Identity()\n    (parent_clauses): Embedding(17, 5, padding_idx=0)\n    (pos_tags): Embedding(16, 4, padding_idx=0)\n    (s_type): Embedding(3, 2, padding_idx=0)\n    (sentence_length): Identity()\n    (token_lengths): Identity()\n  )\n  (embedder): BasicTextFieldEmbedder(\n    (token_embedder_fasttext): Embedding()\n    (token_embedder_token_characters): TokenCharactersEncoder(\n      (_embedding): TimeDistributed(\n        (_module): Embedding()\n      )\n      (_encoder): TimeDistributed(\n        (_module): LstmSeq2VecEncoder(\n          (_module): LSTM(32, 64, batch_first=True, dropout=0.2, bidirectional=True)\n        )\n      )\n    )\n    (token_embedder_tokens): PretrainedTransformerMismatchedEmbedder(\n      (_matched_embedder): PretrainedTransformerEmbedder(\n        (transformer_model): BertModel(\n          (embeddings): BertEmbeddings(\n            (word_embeddings): Embedding(31102, 768, padding_idx=0)\n            (position_embeddings): Embedding(512, 768)\n            (token_type_embeddings): Embedding(2, 768)\n          )\n          (encoder): BertEncoder(\n            (layer): ModuleList(\n              (0): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (1): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (2): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (3): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (4): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (5): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (6): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (7): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (8): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (9): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (10): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (11): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n            )\n          )\n          (pooler): BertPooler(\n            (activation): Tanh()\n          )\n        )\n      )\n    )\n  )\n  (encoder): PytorchSeq2SeqWrapper(\n    (_module): StackedBidirectionalLstm(\n      (forward_layer_0): AugmentedLstm(\n        (cell): AugmentedLSTMCell(\n        )\n      )\n      (backward_layer_0): AugmentedLstm(\n        (cell): AugmentedLSTMCell(\n        )\n      )\n    )\n  )\n  (prev_sentence_encoder): LstmSeq2VecEncoder(\n    (_module): LSTM(1196, 100, batch_first=True, dropout=0.2, bidirectional=True)\n  )\n  (next_sentence_encoder): LstmSeq2VecEncoder(\n    (_module): LSTM(1196, 100, batch_first=True, dropout=0.2, bidirectional=True)\n  )\n  (label_projection_layer): TimeDistributed(\n  )\n)\n2024-08-22 05:10:03,778 - INFO - allennlp.training.optimizers - Done constructing parameter groups.\n2024-08-22 05:10:03,778 - INFO - allennlp.training.optimizers - Group 1: ['encoder._module.backward_layer_0.cell.input_linearity.weight', 'embedder.token_embedder_token_characters._encoder._module._module.weight_hh_l0_reverse', 'next_sentence_encoder._module.bias_ih_l0', 'prev_sentence_encoder._module.bias_hh_l0', 'feature_modules.case_tags.weight', 'encoder._module.forward_layer_0.cell.input_linearity.weight', 'prev_sentence_encoder._module.weight_ih_l0_reverse', 'embedder.token_embedder_token_characters._encoder._module._module.weight_ih_l0', 'next_sentence_encoder._module.weight_hh_l0_reverse', 'label_projection_layer._module.bias', 'embedder.token_embedder_token_characters._encoder._module._module.weight_hh_l0', 'prev_sentence_encoder._module.weight_hh_l0', 'prev_sentence_encoder._module.weight_ih_l0', 'embedder.token_embedder_token_characters._encoder._module._module.bias_ih_l0', 'feature_modules.genre_tags.weight', 'next_sentence_encoder._module.bias_hh_l0_reverse', 'feature_modules.cpos_tags.weight', 'embedder.token_embedder_token_characters._encoder._module._module.bias_hh_l0', 'feature_modules.deprel_tags.weight', 'next_sentence_encoder._module.weight_ih_l0', 'label_projection_layer._module.weight', 'feature_modules.pos_tags.weight', 'next_sentence_encoder._module.weight_ih_l0_reverse', 'prev_sentence_encoder._module.bias_ih_l0_reverse', 'next_sentence_encoder._module.bias_ih_l0_reverse', 'feature_modules.parent_clauses.weight', 'embedder.token_embedder_token_characters._embedding._module.weight', 'prev_sentence_encoder._module.bias_ih_l0', 'encoder._module.backward_layer_0.cell.state_linearity.weight', 'embedder.token_embedder_token_characters._encoder._module._module.bias_ih_l0_reverse', 'encoder._module.forward_layer_0.cell.state_linearity.weight', 'encoder._module.forward_layer_0.cell.state_linearity.bias', 'embedder.token_embedder_token_characters._encoder._module._module.bias_hh_l0_reverse', 'next_sentence_encoder._module.weight_hh_l0', 'prev_sentence_encoder._module.weight_hh_l0_reverse', 'encoder._module.backward_layer_0.cell.state_linearity.bias', 'prev_sentence_encoder._module.bias_hh_l0_reverse', 'feature_modules.dep_chunk_tags.weight', 'embedder.token_embedder_token_characters._encoder._module._module.weight_ih_l0_reverse', 'next_sentence_encoder._module.bias_hh_l0', 'feature_modules.s_type.weight'], {}\n2024-08-22 05:10:03,779 - INFO - allennlp.training.optimizers - Number of trainable parameters: 124725265\n/usr/local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n2024-08-22 05:10:03,800 - INFO - allennlp.training.gradient_descent_trainer - Beginning training.\n2024-08-22 05:10:03,800 - INFO - allennlp.training.gradient_descent_trainer - Epoch 0/0\n2024-08-22 05:10:03,800 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 2.8G\n2024-08-22 05:10:03,802 - INFO - allennlp.training.gradient_descent_trainer - Training\n\n  0%|          | 0/3 [00:00<?, ?it/s]/usr/local/lib/python3.10/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n2024-08-22 05:10:09,289 - INFO - allennlp.training.callbacks.console_logger - Batch inputs\n2024-08-22 05:10:09,289 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/fasttext/tokens (Shape: 4 x 25)\ntensor([[ 91,  10,  92,  ..., 104, 105,   2],\n        [ 20,  66,  67,  ...,   2,   0,   0],\n        [ 54,   3,  55,  ...,   0,   0,   0],\n        [ 36,  37,  38,  ...,   0,   0,   0]])\n2024-08-22 05:10:09,290 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/token_characters/token_characters (Shape: 4 x 25 x 21)\ntensor([[[31, 15,  0,  ...,  0,  0,  0],\n         [ 5,  4,  8,  ...,  0,  0,  0],\n         [ 2,  8, 17,  ...,  0,  0,  0],\n         ...,\n         [24, 13,  4,  ...,  0,  0,  0],\n         [ 3, 13,  8,  ...,  0,  0,  0],\n         [20,  0,  0,  ...,  0,  0,  0]],\n\n        [[30,  4,  0,  ...,  0,  0,  0],\n         [10, 26,  6,  ...,  0,  0,  0],\n         [10,  2,  6,  ...,  0,  0,  0],\n         ...,\n         [20,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[57, 19,  2,  ...,  0,  0,  0],\n         [23,  0,  0,  ...,  0,  0,  0],\n         [17,  2,  5,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[28,  7, 11,  ...,  0,  0,  0],\n         [47,  5,  2,  ...,  0,  0,  0],\n         [ 4,  5,  8,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]]])\n2024-08-22 05:10:09,293 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/token_ids (Shape: 4 x 31)\ntensor([[  102,   224,   207,  ...,  3682,   552,   103],\n        [  102,   233,  9647,  ...,     0,     0,     0],\n        [  102, 24026, 12979,  ...,     0,     0,     0],\n        [  102, 19590,   906,  ...,     0,     0,     0]])\n2024-08-22 05:10:09,294 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/mask (Shape: 4 x 25)\ntensor([[ True,  True,  True,  ...,  True,  True,  True],\n        [ True,  True,  True,  ...,  True, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False]])\n2024-08-22 05:10:09,294 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/type_ids (Shape: 4 x 31)\ntensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]])\n2024-08-22 05:10:09,295 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/wordpiece_mask (Shape: 4 x 31)\ntensor([[ True,  True,  True,  ...,  True,  True,  True],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False]])\n2024-08-22 05:10:09,296 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/offsets (Shape: 4 x 25 x 2)\ntensor([[[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [26, 27],\n         [28, 28],\n         [29, 29]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [26, 26],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  4],\n         [ 5,  5],\n         [ 6,  6],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  2],\n         [ 3,  4],\n         [ 5,  5],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]]])\n2024-08-22 05:10:09,297 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/fasttext/tokens (Shape: 4 x 31)\ntensor([[86, 32, 33,  ...,  0,  0,  0],\n        [19, 62,  4,  ...,  0,  0,  0],\n        [12, 40,  4,  ...,  8, 53,  2],\n        [12, 27, 28,  ...,  0,  0,  0]])\n2024-08-22 05:10:09,298 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/token_characters/token_characters (Shape: 4 x 31 x 16)\ntensor([[[36,  7,  8,  ...,  0,  0,  0],\n         [11,  5, 19,  ...,  0,  0,  0],\n         [ 2,  4,  0,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[28,  2,  6,  ...,  0,  0,  0],\n         [29, 26, 14,  ...,  0,  0,  0],\n         [10,  2,  6,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[24, 13, 21,  ...,  0,  0,  0],\n         [40,  6, 13,  ...,  0,  0,  0],\n         [10,  2,  6,  ...,  0,  0,  0],\n         ...,\n         [18, 13,  0,  ...,  0,  0,  0],\n         [ 2,  3,  8,  ...,  0,  0,  0],\n         [20,  0,  0,  ...,  0,  0,  0]],\n\n        [[24, 13, 21,  ...,  0,  0,  0],\n         [30,  5,  4,  ...,  0,  0,  0],\n         [11,  2, 12,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]]])\n2024-08-22 05:10:09,300 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/token_ids (Shape: 4 x 41)\ntensor([[  102,  2472,   175,  ...,     0,     0,     0],\n        [  102,   127, 22026,  ...,     0,     0,     0],\n        [  102,   193,   854,  ...,  3432,   552,   103],\n        [  102,   193,  3029,  ...,     0,     0,     0]])\n2024-08-22 05:10:09,301 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/mask (Shape: 4 x 31)\ntensor([[ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ...,  True,  True,  True],\n        [ True,  True,  True,  ..., False, False, False]])\n2024-08-22 05:10:09,302 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/type_ids (Shape: 4 x 41)\ntensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]])\n2024-08-22 05:10:09,302 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/wordpiece_mask (Shape: 4 x 41)\ntensor([[ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ...,  True,  True,  True],\n        [ True,  True,  True,  ..., False, False, False]])\n2024-08-22 05:10:09,303 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/offsets (Shape: 4 x 31 x 2)\ntensor([[[ 1,  3],\n         [ 4,  4],\n         [ 5,  5],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  3],\n         [ 4,  4],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [37, 37],\n         [38, 38],\n         [39, 39]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]]])\n2024-08-22 05:10:09,304 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/fasttext/tokens (Shape: 4 x 31)\ntensor([[169,  32,  33,  ...,   0,   0,   0],\n        [ 31,  80,  22,  ...,   0,   0,   0],\n        [ 19,  62,   4,  ...,   0,   0,   0],\n        [ 12,  40,   4,  ...,   8,  53,   2]])\n2024-08-22 05:10:09,305 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/token_characters/token_characters (Shape: 4 x 31 x 16)\ntensor([[[28,  2,  3,  ...,  0,  0,  0],\n         [11,  5, 19,  ...,  0,  0,  0],\n         [ 2,  4,  0,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[29,  2,  5,  ...,  0,  0,  0],\n         [35,  5,  3,  ..., 11,  2,  3],\n         [17,  2,  6,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[28,  2,  6,  ...,  0,  0,  0],\n         [29, 26, 14,  ...,  0,  0,  0],\n         [10,  2,  6,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[24, 13, 21,  ...,  0,  0,  0],\n         [40,  6, 13,  ...,  0,  0,  0],\n         [10,  2,  6,  ...,  0,  0,  0],\n         ...,\n         [18, 13,  0,  ...,  0,  0,  0],\n         [ 2,  3,  8,  ...,  0,  0,  0],\n         [20,  0,  0,  ...,  0,  0,  0]]])\n2024-08-22 05:10:09,308 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/token_ids (Shape: 4 x 41)\ntensor([[  102,  4041,   773,  ...,     0,     0,     0],\n        [  102, 14216,  2672,  ...,     0,     0,     0],\n        [  102,   127, 22026,  ...,     0,     0,     0],\n        [  102,   193,   854,  ...,  3432,   552,   103]])\n2024-08-22 05:10:09,308 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/mask (Shape: 4 x 31)\ntensor([[ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ...,  True,  True,  True]])\n2024-08-22 05:10:09,309 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/type_ids (Shape: 4 x 41)\ntensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]])\n2024-08-22 05:10:09,310 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/wordpiece_mask (Shape: 4 x 41)\ntensor([[ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ...,  True,  True,  True]])\n2024-08-22 05:10:09,310 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/offsets (Shape: 4 x 31 x 2)\ntensor([[[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  3],\n         [ 4,  4],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  3],\n         [ 4,  4],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [37, 37],\n         [38, 38],\n         [39, 39]]])\n2024-08-22 05:10:09,312 - INFO - allennlp.training.callbacks.console_logger - Field : \"batch_input/sentence_tokens\" : (Length 4 of type \"<class 'list'>\")\n2024-08-22 05:10:09,312 - INFO - allennlp.training.callbacks.console_logger - batch_input/case_tags (Shape: 4 x 25)\ntensor([[3, 2, 2,  ..., 3, 2, 4],\n        [3, 2, 2,  ..., 4, 0, 0],\n        [3, 4, 2,  ..., 0, 0, 0],\n        [3, 3, 2,  ..., 0, 0, 0]])\n2024-08-22 05:10:09,312 - INFO - allennlp.training.callbacks.console_logger - batch_input/cpos_tags (Shape: 4 x 25)\ntensor([[ 4,  8,  4,  ...,  2, 11,  5],\n        [15, 17,  4,  ...,  5,  0,  0],\n        [ 9, 10, 20,  ...,  0,  0,  0],\n        [14, 14, 11,  ...,  0,  0,  0]])\n2024-08-22 05:10:09,313 - INFO - allennlp.training.callbacks.console_logger - batch_input/dep_chunk_tags (Shape: 4 x 25)\ntensor([[ 2,  2,  2,  ...,  4,  9,  2],\n        [ 2,  2,  2,  ...,  2,  0,  0],\n        [ 2, 12,  4,  ...,  0,  0,  0],\n        [ 2,  2,  2,  ...,  0,  0,  0]])\n2024-08-22 05:10:09,314 - INFO - allennlp.training.callbacks.console_logger - batch_input/deprel_tags (Shape: 4 x 25)\ntensor([[ 2, 19,  2,  ...,  9, 23,  3],\n        [20, 10,  2,  ...,  3,  0,  0],\n        [ 7,  3, 14,  ...,  0,  0,  0],\n        [ 5, 21,  7,  ...,  0,  0,  0]])\n2024-08-22 05:10:09,315 - INFO - allennlp.training.callbacks.console_logger - batch_input/document_depth (Shape: 4 x 25)\ntensor([[0.9000, 0.9000, 0.9000,  ..., 0.9000, 0.9000, 0.9000],\n        [0.8571, 0.8571, 0.8571,  ..., 0.8571, 0.0000, 0.0000],\n        [0.8000, 0.8000, 0.8000,  ..., 0.0000, 0.0000, 0.0000],\n        [0.6667, 0.6667, 0.6667,  ..., 0.0000, 0.0000, 0.0000]])\n2024-08-22 05:10:09,316 - INFO - allennlp.training.callbacks.console_logger - batch_input/genre_tags (Shape: 4 x 25)\ntensor([[2, 2, 2,  ..., 2, 2, 2],\n        [2, 2, 2,  ..., 2, 0, 0],\n        [2, 2, 2,  ..., 0, 0, 0],\n        [2, 2, 2,  ..., 0, 0, 0]])\n2024-08-22 05:10:09,317 - INFO - allennlp.training.callbacks.console_logger - batch_input/head_distances (Shape: 4 x 25)\ntensor([[1.0986, 0.6931, 0.0000,  ..., 1.0986, 2.3979, 3.0445],\n        [1.6094, 1.3863, 1.0986,  ..., 2.8332, 0.0000, 0.0000],\n        [0.0000, 2.3026, 2.1972,  ..., 0.0000, 0.0000, 0.0000],\n        [0.6931, 0.0000, 1.0986,  ..., 0.0000, 0.0000, 0.0000]])\n2024-08-22 05:10:09,318 - INFO - allennlp.training.callbacks.console_logger - batch_input/parent_clauses (Shape: 4 x 25)\ntensor([[ 2,  2,  2,  ...,  6, 12,  2],\n        [ 2,  2,  2,  ...,  2,  0,  0],\n        [ 2,  7,  7,  ...,  0,  0,  0],\n        [ 2,  2,  2,  ...,  0,  0,  0]])\n2024-08-22 05:10:09,318 - INFO - allennlp.training.callbacks.console_logger - batch_input/pos_tags (Shape: 4 x 25)\ntensor([[ 4,  3,  4,  ...,  2,  3,  5],\n        [ 8, 10,  4,  ...,  5,  0,  0],\n        [ 9,  5, 13,  ...,  0,  0,  0],\n        [11, 11,  3,  ...,  0,  0,  0]])\n2024-08-22 05:10:09,319 - INFO - allennlp.training.callbacks.console_logger - batch_input/s_type (Shape: 4 x 25)\ntensor([[2, 2, 2,  ..., 2, 2, 2],\n        [2, 2, 2,  ..., 2, 0, 0],\n        [2, 2, 2,  ..., 0, 0, 0],\n        [2, 2, 2,  ..., 0, 0, 0]])\n2024-08-22 05:10:09,320 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence_length (Shape: 4 x 25)\ntensor([[3.2189, 3.2189, 3.2189,  ..., 3.2189, 3.2189, 3.2189],\n        [3.1355, 3.1355, 3.1355,  ..., 3.1355, 0.0000, 0.0000],\n        [2.6391, 2.6391, 2.6391,  ..., 0.0000, 0.0000, 0.0000],\n        [1.9459, 1.9459, 1.9459,  ..., 0.0000, 0.0000, 0.0000]])\n2024-08-22 05:10:09,321 - INFO - allennlp.training.callbacks.console_logger - batch_input/token_lengths (Shape: 4 x 25)\ntensor([[0.6931, 1.0986, 1.3863,  ..., 2.1972, 1.7918, 0.0000],\n        [0.6931, 1.7918, 1.9459,  ..., 0.0000, 0.0000, 0.0000],\n        [2.4849, 0.0000, 1.3863,  ..., 0.0000, 0.0000, 0.0000],\n        [1.7918, 1.9459, 1.6094,  ..., 0.0000, 0.0000, 0.0000]])\n2024-08-22 05:10:09,322 - INFO - allennlp.training.callbacks.console_logger - batch_input/labels (Shape: 4 x 25)\ntensor([[1, 0, 0,  ..., 0, 0, 0],\n        [1, 0, 0,  ..., 0, 0, 0],\n        [1, 0, 1,  ..., 0, 0, 0],\n        [1, 0, 0,  ..., 0, 0, 0]])\n\ntag_accuracy: 0.4203, span_precision: 0.1489, span_recall: 1.0000, span_f1: 0.2593, batch_loss: 0.7650, loss: 0.7650 ||:  33%|###3      | 1/3 [00:05<00:11,  5.52s/it]\ntag_accuracy: 0.6400, span_precision: 0.1667, span_recall: 0.6154, span_f1: 0.2623, batch_loss: 0.3780, loss: 0.5715 ||:  67%|######6   | 2/3 [00:10<00:05,  5.13s/it]\ntag_accuracy: 0.6713, span_precision: 0.1667, span_recall: 0.5333, span_f1: 0.2540, batch_loss: 0.3870, loss: 0.5100 ||: 100%|##########| 3/3 [00:12<00:00,  3.96s/it]\ntag_accuracy: 0.6713, span_precision: 0.1667, span_recall: 0.5333, span_f1: 0.2540, batch_loss: 0.3870, loss: 0.5100 ||: 100%|##########| 3/3 [00:12<00:00,  4.32s/it]\n2024-08-22 05:10:16,763 - INFO - allennlp.training.gradient_descent_trainer - Validating\n\n  0%|          | 0/3 [00:00<?, ?it/s]2024-08-22 05:10:17,582 - INFO - allennlp.training.callbacks.console_logger - Batch inputs\n2024-08-22 05:10:17,582 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/fasttext/tokens (Shape: 4 x 24)\ntensor([[176, 177, 178,  ..., 190, 191,   2],\n        [ 19, 146, 147,  ...,   0,   0,   0],\n        [ 12, 156, 157,  ...,   0,   0,   0],\n        [166, 167, 168,  ...,   0,   0,   0]])\n2024-08-22 05:10:17,583 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/token_characters/token_characters (Shape: 4 x 24 x 14)\ntensor([[[34,  3, 10,  ...,  0,  0,  0],\n         [ 4, 15,  0,  ...,  0,  0,  0],\n         [16, 13,  4,  ...,  0,  0,  0],\n         ...,\n         [41, 15, 12,  ...,  0,  0,  0],\n         [ 7,  3, 11,  ...,  0,  0,  0],\n         [20,  0,  0,  ...,  0,  0,  0]],\n\n        [[28,  2,  6,  ...,  0,  0,  0],\n         [18, 13,  4,  ...,  0,  0,  0],\n         [31,  8,  7,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[24, 13, 21,  ...,  0,  0,  0],\n         [10,  2, 16,  ...,  0,  0,  0],\n         [33,  7,  9,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[28,  7,  4,  ...,  0,  0,  0],\n         [ 6,  2,  5,  ...,  0,  0,  0],\n         [ 3,  5, 14,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]]])\n2024-08-22 05:10:17,586 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/token_ids (Shape: 4 x 29)\ntensor([[  102,   143,   224,  ..., 14762,   552,   103],\n        [  102,   127,  6218,  ...,     0,     0,     0],\n        [  102,   193,   232,  ...,     0,     0,     0],\n        [  102,   173,  5094,  ...,     0,     0,     0]])\n2024-08-22 05:10:17,587 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/mask (Shape: 4 x 24)\ntensor([[ True,  True,  True,  ...,  True,  True,  True],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False]])\n2024-08-22 05:10:17,587 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/type_ids (Shape: 4 x 29)\ntensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]])\n2024-08-22 05:10:17,588 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/wordpiece_mask (Shape: 4 x 29)\ntensor([[ True,  True,  True,  ...,  True,  True,  True],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False]])\n2024-08-22 05:10:17,589 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/offsets (Shape: 4 x 24 x 2)\ntensor([[[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [25, 25],\n         [26, 26],\n         [27, 27]],\n\n        [[ 1,  1],\n         [ 2,  3],\n         [ 4,  4],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]]])\n2024-08-22 05:10:17,590 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/fasttext/tokens (Shape: 4 x 18)\ntensor([[166, 167, 168,  ...,   0,   0,   0],\n        [137, 138, 139,  ...,   0,   0,   0],\n        [ 19, 146, 147,  ...,  11, 155,   2],\n        [ 12, 156, 157,  ...,   0,   0,   0]])\n2024-08-22 05:10:17,590 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/token_characters/token_characters (Shape: 4 x 18 x 16)\ntensor([[[28,  7,  4,  ...,  0,  0,  0],\n         [ 6,  2,  5,  ...,  0,  0,  0],\n         [ 3,  5, 14,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[48,  2,  8,  ...,  0,  0,  0],\n         [ 9,  7, 19,  ...,  0,  0,  0],\n         [ 4,  5, 14,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[28,  2,  6,  ...,  0,  0,  0],\n         [18, 13,  4,  ...,  0,  0,  0],\n         [31,  8,  7,  ...,  0,  0,  0],\n         ...,\n         [16,  5,  8,  ...,  0,  0,  0],\n         [47, 26, 11,  ...,  0,  0,  0],\n         [20,  0,  0,  ...,  0,  0,  0]],\n\n        [[24, 13, 21,  ...,  0,  0,  0],\n         [10,  2, 16,  ...,  0,  0,  0],\n         [33,  7,  9,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]]])\n2024-08-22 05:10:17,593 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/token_ids (Shape: 4 x 25)\ntensor([[  102,   173,  5094,  ...,     0,     0,     0],\n        [  102,   951,   423,  ...,     0,     0,     0],\n        [  102,   127,  6218,  ..., 30938,   552,   103],\n        [  102,   193,   232,  ...,     0,     0,     0]])\n2024-08-22 05:10:17,594 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/mask (Shape: 4 x 18)\ntensor([[ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ...,  True,  True,  True],\n        [ True,  True,  True,  ..., False, False, False]])\n2024-08-22 05:10:17,594 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/type_ids (Shape: 4 x 25)\ntensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]])\n2024-08-22 05:10:17,595 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/wordpiece_mask (Shape: 4 x 25)\ntensor([[ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ...,  True,  True,  True],\n        [ True,  True,  True,  ..., False, False, False]])\n2024-08-22 05:10:17,596 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/offsets (Shape: 4 x 18 x 2)\ntensor([[[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  3],\n         [ 4,  4],\n         ...,\n         [20, 20],\n         [21, 22],\n         [23, 23]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]]])\n2024-08-22 05:10:17,597 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/fasttext/tokens (Shape: 4 x 24)\ntensor([[ 35,   0,   0,  ...,   0,   0,   0],\n        [ 12, 156, 157,  ...,   0,   0,   0],\n        [166, 167, 168,  ...,   0,   0,   0],\n        [176, 177, 178,  ..., 190, 191,   2]])\n2024-08-22 05:10:17,597 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/token_characters/token_characters (Shape: 4 x 24 x 21)\ntensor([[[32, 32, 28,  ..., 54, 32, 32],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[24, 13, 21,  ...,  0,  0,  0],\n         [10,  2, 16,  ...,  0,  0,  0],\n         [33,  7,  9,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[28,  7,  4,  ...,  0,  0,  0],\n         [ 6,  2,  5,  ...,  0,  0,  0],\n         [ 3,  5, 14,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[34,  3, 10,  ...,  0,  0,  0],\n         [ 4, 15,  0,  ...,  0,  0,  0],\n         [16, 13,  4,  ...,  0,  0,  0],\n         ...,\n         [41, 15, 12,  ...,  0,  0,  0],\n         [ 7,  3, 11,  ...,  0,  0,  0],\n         [20,  0,  0,  ...,  0,  0,  0]]])\n2024-08-22 05:10:17,600 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/token_ids (Shape: 4 x 29)\ntensor([[  102, 16249, 16249,  ...,     0,     0,     0],\n        [  102,   193,   232,  ...,     0,     0,     0],\n        [  102,   173,  5094,  ...,     0,     0,     0],\n        [  102,   143,   224,  ..., 14762,   552,   103]])\n2024-08-22 05:10:17,601 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/mask (Shape: 4 x 24)\ntensor([[ True, False, False,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ...,  True,  True,  True]])\n2024-08-22 05:10:17,601 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/type_ids (Shape: 4 x 29)\ntensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]])\n2024-08-22 05:10:17,602 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/wordpiece_mask (Shape: 4 x 29)\ntensor([[ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ...,  True,  True,  True]])\n2024-08-22 05:10:17,603 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/offsets (Shape: 4 x 24 x 2)\ntensor([[[ 1,  9],\n         [ 0,  0],\n         [ 0,  0],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [25, 25],\n         [26, 26],\n         [27, 27]]])\n2024-08-22 05:10:17,604 - INFO - allennlp.training.callbacks.console_logger - Field : \"batch_input/sentence_tokens\" : (Length 4 of type \"<class 'list'>\")\n2024-08-22 05:10:17,604 - INFO - allennlp.training.callbacks.console_logger - batch_input/case_tags (Shape: 4 x 24)\ntensor([[3, 2, 2,  ..., 3, 2, 4],\n        [3, 2, 3,  ..., 0, 0, 0],\n        [3, 2, 3,  ..., 0, 0, 0],\n        [3, 2, 2,  ..., 0, 0, 0]])\n2024-08-22 05:10:17,605 - INFO - allennlp.training.callbacks.console_logger - batch_input/cpos_tags (Shape: 4 x 24)\ntensor([[16,  4, 17,  ...,  2, 11,  5],\n        [ 3,  7,  2,  ...,  0,  0,  0],\n        [ 6,  3,  2,  ...,  0,  0,  0],\n        [35, 11, 36,  ...,  0,  0,  0]])\n2024-08-22 05:10:17,605 - INFO - allennlp.training.callbacks.console_logger - batch_input/dep_chunk_tags (Shape: 4 x 24)\ntensor([[ 2,  2,  2,  ...,  3, 11,  2],\n        [ 2,  2,  2,  ...,  0,  0,  0],\n        [ 2,  2,  2,  ...,  0,  0,  0],\n        [ 2,  2,  2,  ...,  0,  0,  0]])\n2024-08-22 05:10:17,606 - INFO - allennlp.training.callbacks.console_logger - batch_input/deprel_tags (Shape: 4 x 24)\ntensor([[13,  2, 10,  ..., 15, 18,  3],\n        [ 4, 12,  5,  ...,  0,  0,  0],\n        [ 6,  4,  8,  ...,  0,  0,  0],\n        [ 5,  7,  2,  ...,  0,  0,  0]])\n2024-08-22 05:10:17,607 - INFO - allennlp.training.callbacks.console_logger - batch_input/document_depth (Shape: 4 x 24)\ntensor([[0.9091, 0.9091, 0.9091,  ..., 0.9091, 0.9091, 0.9091],\n        [0.8750, 0.8750, 0.8750,  ..., 0.0000, 0.0000, 0.0000],\n        [0.8889, 0.8889, 0.8889,  ..., 0.0000, 0.0000, 0.0000],\n        [0.9000, 0.9000, 0.9000,  ..., 0.0000, 0.0000, 0.0000]])\n2024-08-22 05:10:17,608 - INFO - allennlp.training.callbacks.console_logger - batch_input/genre_tags (Shape: 4 x 24)\ntensor([[2, 2, 2,  ..., 2, 2, 2],\n        [2, 2, 2,  ..., 0, 0, 0],\n        [2, 2, 2,  ..., 0, 0, 0],\n        [2, 2, 2,  ..., 0, 0, 0]])\n2024-08-22 05:10:17,608 - INFO - allennlp.training.callbacks.console_logger - batch_input/head_distances (Shape: 4 x 24)\ntensor([[2.3979, 2.3026, 2.1972,  ..., 1.0986, 2.5649, 2.4849],\n        [0.6931, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n        [0.6931, 0.0000, 1.9459,  ..., 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.6931, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])\n2024-08-22 05:10:17,609 - INFO - allennlp.training.callbacks.console_logger - batch_input/parent_clauses (Shape: 4 x 24)\ntensor([[ 2,  2,  2,  ...,  8, 13,  2],\n        [ 2,  2,  2,  ...,  0,  0,  0],\n        [ 2,  2,  2,  ...,  0,  0,  0],\n        [ 2,  2,  2,  ...,  0,  0,  0]])\n2024-08-22 05:10:17,610 - INFO - allennlp.training.callbacks.console_logger - batch_input/pos_tags (Shape: 4 x 24)\ntensor([[12,  4, 10,  ...,  2,  3,  5],\n        [ 6,  9,  2,  ...,  0,  0,  0],\n        [ 7,  6,  2,  ...,  0,  0,  0],\n        [ 8,  3,  4,  ...,  0,  0,  0]])\n2024-08-22 05:10:17,610 - INFO - allennlp.training.callbacks.console_logger - batch_input/s_type (Shape: 4 x 24)\ntensor([[2, 2, 2,  ..., 2, 2, 2],\n        [2, 2, 2,  ..., 0, 0, 0],\n        [2, 2, 2,  ..., 0, 0, 0],\n        [2, 2, 2,  ..., 0, 0, 0]])\n2024-08-22 05:10:17,611 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence_length (Shape: 4 x 24)\ntensor([[3.1781, 3.1781, 3.1781,  ..., 3.1781, 3.1781, 3.1781],\n        [2.8904, 2.8904, 2.8904,  ..., 0.0000, 0.0000, 0.0000],\n        [2.6391, 2.6391, 2.6391,  ..., 0.0000, 0.0000, 0.0000],\n        [1.7918, 1.7918, 1.7918,  ..., 0.0000, 0.0000, 0.0000]])\n2024-08-22 05:10:17,612 - INFO - allennlp.training.callbacks.console_logger - batch_input/token_lengths (Shape: 4 x 24)\ntensor([[1.0986, 0.6931, 1.3863,  ..., 1.7918, 1.7918, 0.0000],\n        [1.0986, 2.3026, 1.3863,  ..., 0.0000, 0.0000, 0.0000],\n        [1.0986, 1.0986, 1.9459,  ..., 0.0000, 0.0000, 0.0000],\n        [1.0986, 1.7918, 1.6094,  ..., 0.0000, 0.0000, 0.0000]])\n2024-08-22 05:10:17,613 - INFO - allennlp.training.callbacks.console_logger - batch_input/labels (Shape: 4 x 24)\ntensor([[1, 0, 0,  ..., 0, 0, 0],\n        [1, 0, 0,  ..., 0, 0, 0],\n        [1, 0, 0,  ..., 0, 0, 0],\n        [1, 0, 0,  ..., 0, 0, 0]])\n\ntag_accuracy: 0.9032, span_precision: 0.0000, span_recall: 0.0000, span_f1: 0.0000, batch_loss: 0.4510, loss: 0.4510 ||:  33%|###3      | 1/3 [00:00<00:01,  1.18it/s]\ntag_accuracy: 0.9027, span_precision: 0.0000, span_recall: 0.0000, span_f1: 0.0000, batch_loss: 0.4173, loss: 0.4342 ||:  67%|######6   | 2/3 [00:01<00:00,  1.22it/s]\ntag_accuracy: 0.8943, span_precision: 0.0000, span_recall: 0.0000, span_f1: 0.0000, batch_loss: 0.6441, loss: 0.5041 ||: 100%|##########| 3/3 [00:01<00:00,  1.67it/s]\ntag_accuracy: 0.8943, span_precision: 0.0000, span_recall: 0.0000, span_f1: 0.0000, batch_loss: 0.6441, loss: 0.5041 ||: 100%|##########| 3/3 [00:01<00:00,  1.51it/s]\n2024-08-22 05:10:18,752 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n2024-08-22 05:10:18,752 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.510  |     0.504\n2024-08-22 05:10:18,752 - INFO - allennlp.training.callbacks.console_logger - span_f1            |     0.254  |     0.000\n2024-08-22 05:10:18,752 - INFO - allennlp.training.callbacks.console_logger - span_precision     |     0.167  |     0.000\n2024-08-22 05:10:18,752 - INFO - allennlp.training.callbacks.console_logger - span_recall        |     0.533  |     0.000\n2024-08-22 05:10:18,752 - INFO - allennlp.training.callbacks.console_logger - tag_accuracy       |     0.671  |     0.894\n2024-08-22 05:10:18,752 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  2823.230  |       N/A\n2024-08-22 05:10:20,323 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:16.522277\n  \"best_epoch\": 0,\n  \"peak_worker_0_memory_MB\": 2823.23046875,\n  \"training_duration\": \"0:00:14.950932\",\n  \"epoch\": 0,\n  \"training_tag_accuracy\": 0.6713286713286714,\n  \"training_span_precision\": 0.16666666666666632,\n  \"training_span_recall\": 0.5333333333333298,\n  \"training_span_f1\": 0.2539682539682169,\n  \"training_loss\": 0.5099876622358958,\n  \"training_worker_0_memory_MB\": 2823.23046875,\n  \"validation_tag_accuracy\": 0.8943089430894309,\n  \"validation_span_precision\": 0.0,\n  \"validation_span_recall\": 0.0,\n  \"validation_span_f1\": 0.0,\n  \"validation_loss\": 0.5041356384754181,\n  \"best_validation_tag_accuracy\": 0.8943089430894309,\n  \"best_validation_span_precision\": 0.0,\n  \"best_validation_span_recall\": 0.0,\n  \"best_validation_span_f1\": 0.0,\n  \"best_validation_loss\": 0.5041356384754181\n}\n2024-08-22 05:10:20,324 - INFO - allennlp.models.archival - archiving weights and vocabulary to models/deu.rst.pcc_seg_bert_baseline_ft/model.tar.gz\n\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n# Testing on deu.rst.pcc\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n\n2024-08-22 05:11:15,521 - INFO - allennlp.common.plugins - Plugin allennlp_models available\n2024-08-22 05:11:16,172 - ERROR - allennlp.common.plugins - Plugin gucorpling_models could not be loaded: No module named 'flair'\nTraceback (most recent call last):\n  File \"/usr/local/bin/allennlp\", line 8, in <module>\n    sys.exit(run())\n  File \"/usr/local/lib/python3.10/site-packages/allennlp/__main__.py\", line 39, in run\n    main(prog=\"allennlp\")\n  File \"/usr/local/lib/python3.10/site-packages/allennlp/commands/__init__.py\", line 120, in main\n    args.func(args)\n  File \"/usr/local/lib/python3.10/site-packages/allennlp/commands/predict.py\", line 256, in _predict\n    predictor = _get_predictor(args)\n  File \"/usr/local/lib/python3.10/site-packages/allennlp/commands/predict.py\", line 120, in _get_predictor\n    check_for_gpu(args.cuda_device)\n  File \"/usr/local/lib/python3.10/site-packages/allennlp/common/checks.py\", line 117, in check_for_gpu\n    raise ConfigurationError(\nallennlp.common.checks.ConfigurationError: Experiment specified a GPU but none is available; if you want to run on CPU use the override 'trainer.cuda_device=-1' in the json config file.\nRemoving model files...\n\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n# Scoring on deu.rst.pcc\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n\nTraceback (most recent call last):\n  File \"/content/DisCoDisCo/seg_scripts/format_output.py\", line 14, in <module>\n    with open(sys.argv[1], encoding=\"utf8\") as f:\nFileNotFoundError: [Errno 2] No such file or directory: 'models/deu.rst.pcc_seg_bert_baseline_ft/output_test.jsonl'\nTraceback (most recent call last):\n  File \"/content/DisCoDisCo/seg_scripts/seg_eval_2019_modified.py\", line 237, in <module>\n    score_dict = get_scores(opts.goldfile,opts.predfile,opts.string_input)\n  File \"/content/DisCoDisCo/seg_scripts/seg_eval_2019_modified.py\", line 138, in get_scores\n    pred_tokens, pred_labels, pred_spans = parse_data(pred_file, string_input)\n  File \"/content/DisCoDisCo/seg_scripts/seg_eval_2019_modified.py\", line 77, in parse_data\n    data = io.open(infile, encoding=\"utf8\").read().strip().replace(\"\\r\", \"\")\nFileNotFoundError: [Errno 2] No such file or directory: 'models/deu.rst.pcc_seg_bert_baseline_ft/output_test.conllu'\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!bash seg_scripts/single_corpus_train_and_test_ft.sh deu.rst.pcc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244efae6",
   "metadata": {},
   "source": [
    "### Step 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1067e3f7",
   "metadata": {},
   "source": [
    "Execution Start time: 05:11:22, End time: 05:11:23 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3961ecbb",
   "metadata": {},
   "source": [
    "**Thought**: The script encountered an error because it attempted to use a GPU, which is not available. We need to ensure that the script is configured to run on the CPU. Let's check the script and configuration to ensure that `cuda_device` is set to `-1`.\n",
    "\n",
    "Additionally, it seems there is an issue with the `flair` module not being loaded. We need to ensure that all necessary dependencies are installed and correctly configured.\n",
    "\n",
    "Let's first verify the `cuda_device` setting in the script and configuration files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84e92fb",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d25a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/DisCoDisCo/seg_scripts/single_corpus_train_and_test_ft.sh (133 lines total)]\n1:#!/bin/bash\n2:#set -o errexit\n3:if [ $# -eq 0 ]; then\n4:  echo \"Supply the name of a corpus\"\n5:  exit 1\n6:fi\n7:CORPUS=\"$1\"\n8:CORPUS_DIR=\"data/2021/${1}\"\n9:MODEL_DIR=${2:-models}/${CORPUS}_seg_bert_baseline_ft\n10:if [[ ! -d $CORPUS_DIR ]]; then\n11:  echo \"Corpus \\\"$CORPUS_DIR\\\" not found\"\n12:  exit 1\n13:fi\n14:if [[ -d $MODEL_DIR ]]; then\n15:  echo \"\\\"$MODEL_DIR\\\" already exists. Removing it now...\"\n16:  rm -rf \"$MODEL_DIR\"\n17:fi\n18:\n19:# use language-specific berts if we can\n20:export EMBEDDING_DIMS=768\n21:if [[ \"$CORPUS\" == \"eng\"* ]]; then \n22:  export EMBEDDING_DIMS=1024\n23:  #export EMBEDDING_MODEL_NAME=\"roberta-large\"\n24:  export EMBEDDING_MODEL_NAME=\"google/electra-large-discriminator\"\n25:elif [[ \"$CORPUS\" == \"fas\"* ]]; then\n26:  export EMBEDDING_MODEL_NAME=\"HooshvareLab/bert-fa-base-uncased\"\n27:elif [[ \"$CORPUS\" == \"deu\"* ]]; then\n28:  export EMBEDDING_MODEL_NAME=\"google-bert/bert-base-german-dbmdz-uncased\"\n29:#elif [[ \"$CORPUS\" == \"fra\"* ]]; then\n30:#  export EMBEDDING_DIMS=1024\n31:#  export EMBEDDING_MODEL_NAME=\"camembert/camembert-large\"\n32:elif [[ \"$CORPUS\" == \"zho\"* ]]; then\n33:  #export EMBEDDING_DIMS=1024\n34:  #export EMBEDDING_MODEL_NAME=\"hfl/chinese-electra-180g-large-discriminator\"\n35:  export EMBEDDING_MODEL_NAME=\"bert-base-chinese\"\n36:elif [[ \"$CORPUS\" == \"nld\"* ]]; then\n37:  #export EMBEDDING_MODEL_NAME=\"GroNLP/bert-base-dutch-cased\"\n38:  export EMBEDDING_MODEL_NAME=\"pdelobelle/robbert-v2-dutch-base\"\n39:elif [[ \"$CORPUS\" == \"eus\"* ]]; then\n40:  export EMBEDDING_MODEL_NAME=\"ixa-ehu/berteus-base-cased\"\n41:elif [[ \"$CORPUS\" == \"spa\"* ]]; then\n42:  #export EMBEDDING_MODEL_NAME=\"mrm8488/electricidad-base-discriminator\"\n43:  export EMBEDDING_MODEL_NAME=\"dccuchile/bert-base-spanish-wwm-cased\"\n44:elif [[ \"$CORPUS\" == \"por\"* ]]; then\n45:  export EMBEDDING_MODEL_NAME=\"neuralmind/bert-base-portuguese-cased\"\n46:elif [[ \"$CORPUS\" == \"tur\"* ]]; then\n47:  #export EMBEDDING_MODEL_NAME=\"dbmdz/electra-base-turkish-cased-discriminator\"\n48:  export EMBEDDING_MODEL_NAME=\"dbmdz/bert-base-turkish-cased\"\n49:elif [[ \"$CORPUS\" == \"rus\"* ]]; then\n50:  export EMBEDDING_MODEL_NAME=\"DeepPavlov/rubert-base-cased\"\n51:else\n52:  export EMBEDDING_DIMS=1024\n53:  export EMBEDDING_MODEL_NAME=\"xlm-roberta-large\"\n54:fi\n55:\n56:# do not use CRF on RST datasets\n57:export USE_CRF=0\n58:if [[ \"$CORPUS\" == *\".pdtb.\"* ]]; then\n59:  export USE_CRF=1\n60:fi\n61:\n62:# use fastText embeddings\n63:if [[ \"$CORPUS\" == \"eng\"* ]]; then\n64:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\"\n65:elif [[ \"$CORPUS\" == \"deu\"* ]]; then\n66:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.de.300.vec.gz\"\n67:elif [[ \"$CORPUS\" == \"eus\"* ]]; then\n68:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.eu.300.vec.gz\"\n69:elif [[ \"$CORPUS\" == \"fas\"* ]]; then\n70:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.vec.gz\"\n71:elif [[ \"$CORPUS\" == \"fra\"* ]]; then\n72:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.vec.gz\"\n73:elif [[ \"$CORPUS\" == \"nld\"* ]]; then\n74:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.nl.300.vec.gz\"\n75:elif [[ \"$CORPUS\" == \"por\"* ]]; then\n76:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.pt.300.vec.gz\"\n77:elif [[ \"$CORPUS\" == \"rus\"* ]]; then\n78:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ru.300.vec.gz\"\n79:elif [[ \"$CORPUS\" == \"spa\"* ]]; then\n80:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.es.300.vec.gz\"\n81:elif [[ \"$CORPUS\" == \"tur\"* ]]; then\n82:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.tr.300.vec.gz\"\n83:elif [[ \"$CORPUS\" == \"zho\"* ]]; then\n84:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.zh.300.vec.gz\"\n85:else\n86:  echo \"Couldn't find a fasttext embedding for \\\"$CORPUS\\\"\" >&2\n87:  exit 1\n88:fi\n89:\n90:echo \"\"\n91:echo \"#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\"\n92:echo \"# Training on $CORPUS\"\n93:echo \"#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\"\n94:echo \"\"\n95:export TRAIN_DATA_PATH=\"${CORPUS_DIR}/${CORPUS}_train.conllu\"\n96:export VALIDATION_DATA_PATH=\"${CORPUS_DIR}/${CORPUS}_dev.conllu\"\n97:echo $TRAIN_DATA_PATH\n98:allennlp train \\\n99:  configs/seg/baseline/bert_baseline_ft.jsonnet \\\n100:  -s \"$MODEL_DIR\"\n(33 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open seg_scripts/single_corpus_train_and_test_ft.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2659da68",
   "metadata": {},
   "source": [
    "### Step 31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00be2dc9",
   "metadata": {},
   "source": [
    "Execution Start time: 05:11:25, End time: 05:11:25 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cb6c16",
   "metadata": {},
   "source": [
    "**Thought**: The script seems to be configured correctly for the model and dataset. The error might be related to the output directory or file paths. Let's ensure that the output directory exists and is correctly set up. We should also verify the paths used in the script.\n",
    "\n",
    "Let's check the directory structure to ensure the necessary directories and files are in place.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d2f917",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a36d55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "models:\ndeu.rst.pcc_seg_bert_baseline_ft\n\nmodels/deu.rst.pcc_seg_bert_baseline_ft:\ncalc_score.sh  meta.json     metrics_epoch_0.json  out.log    vocabulary\nconfig.json    metrics.json  model.tar.gz\t   score.txt\n\nmodels/deu.rst.pcc_seg_bert_baseline_ft/vocabulary:\ncase.txt      fasttext.txt  non_padded_namespaces.txt  token_characters.txt\ndepchunk.txt  genre.txt     parent_clauses.txt\t       upos.txt\ndeprel.txt    labels.txt    s_type.txt\t\t       xpos.txt\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ls -R models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5c2a3b",
   "metadata": {},
   "source": [
    "### Step 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da1dc5b",
   "metadata": {},
   "source": [
    "Execution Start time: 05:11:27, End time: 05:11:28 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260dc111",
   "metadata": {},
   "source": [
    "**Thought**: The directory structure seems to be in place, but it appears that the expected output file `output_test.conllu` is missing. This might be due to an issue in the script or configuration that prevents the file from being generated.\n",
    "\n",
    "Let's review the script to ensure that the output file is being correctly generated and saved. We should also check the script for any potential issues that might prevent the output file from being created.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2441ca34",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0fcdfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/DisCoDisCo/seg_scripts/seg_eval_2019_modified.py (245 lines total)]\n1:import io, os, sys, argparse\n2:\n3:\"\"\"\n4:Script to evaluate segmentation f-score and perfect discourse unit segmentation proportion from two files. Two input formats are permitted:\n5:\n6:  * One token per line, with ten columns, no sentence breaks (default *.tok format) - segmentation indicated in column 10\n7:  * The same, but with blank lines between sentences (*.conll format)\n8:\n9:Token columns follow the CoNLL-U format, with token IDs in the first column and pipe separated key=value pairs in the last column. \n10:\n11:Document boundaries are indicated by a comment: # newdoc id = ...\n12:\n13:The evaluation uses micro-averaged F-Scores per corpus (not document macro average).\n14:\n15:Example:\n16:\n17:```\n18:# newdoc id = GUM_bio_byron\n19:1\tEducation\t_\t_\t_\t_\t_\t_\t_\tBeginSeg=Yes\n20:2\tand\t_\t_\t_\t_\t_\t_\t_\t_\n21:3\tearly\t_\t_\t_\t_\t_\t_\t_\t_\n22:4\tloves\t_\t_\t_\t_\t_\t_\t_\t_\n23:5\tByron\t_\t_\t_\t_\t_\t_\t_\tBeginSeg=Yes\n24:6\treceived\t_\t_\t_\t_\t_\t_\t_\t_\n25:...\n26:```\n27:\n28:Or:\n29:\n30:```\n31:# newdoc id = GUM_bio_byron\n32:# sent_id = GUM_bio_byron-1\n33:# text = Education and early loves\n34:1\tEducation\teducation\tNOUN\tNN\tNumber=Sing\t0\troot\t_\tBeginSeg=Yes\n35:2\tand\tand\tCCONJ\tCC\t_\t4\tcc\t_\t_\n36:3\tearly\tearly\tADJ\tJJ\tDegree=Pos\t4\tamod\t_\t_\n37:4\tloves\tlove\tNOUN\tNNS\tNumber=Plur\t1\tconj\t_\t_\n38:\n39:# sent_id = GUM_bio_byron-2\n40:# text = Byron received his early formal education at Aberdeen Grammar School, and in August 1799 entered the school of Dr. William Glennie, in Dulwich. [17]\n41:1\tByron\tByron\tPROPN\tNNP\tNumber=Sing\t2\tnsubj\t_\tBeginSeg=Yes\n42:2\treceived\treceive\tVERB\tVBD\tMood=Ind|Tense=Past|VerbForm=Fin\t0\troot\t_\t_\n43:```\n44:\n45:For PDTB-style corpora, we calculate exact span-wise f-scores for BIO encoding, without partial credit. In other words, \n46:predicting an incorrect span with partial overlap is the same as missing a gold span and predicting an incorrect span\n47:somewhere else in the corpus. Note also that spans must begin with B-Conn - predicted spans beginning with I-Conn are ignored.\n48:\n49:The file format for PDTB style corpora is similar, but with different labels:\n50:\n51:```\n52:1\tFidelity\tFidelity\tPROPN\tNNP\t_\t6\tnsubj\t_\t_\n53:2\t,\t,\tPUNCT\t,\t_\t6\tpunct\t_\t_\n54:3\tfor\tfor\tADP\tIN\t_\t4\tcase\t_\tSeg=B-Conn\n55:4\texample\texample\tNOUN\tNN\t_\t6\tobl\t_\tSeg=I-Conn\n56:5\t,\t,\tPUNCT\t,\t_\t6\tpunct\t_\t_\n57:6\tprepared\tprepare\tVERB\tVBN\t_\t0\troot\t_\t_\n58:7\tads\tad\tNOUN\tNNS\t_\t6\tobj\t_\t_\n59:...\n60:```\n61:\n62:\n63:Arguments:\n64: * goldfile: shared task gold test data\n65: * predfile: same format, with predicted segments positions in column 10 - note **number of tokens must match**  \n66: * string_input: if specified, files are replaced by strings with file contents instead of file names\n67:\n68:\n69:\"\"\"\n70:\n71:__author__ = \"Amir Zeldes\"\n72:__license__ = \"Apache 2.0\"\n73:__version__ = \"1.0.1\"\n74:\n75:def parse_data(infile, string_input=False):\n76:\tif not string_input:\n77:\t\tdata = io.open(infile, encoding=\"utf8\").read().strip().replace(\"\\r\", \"\")\n78:\telse:\n79:\t\tdata = infile.strip()\n80:\n81:\ttokens = []\n82:\tlabels = []\n83:\tspans = []\n84:\tcounter = 0\n85:\tspan_start = -1\n86:\tspan_end = -1\n87:\tfor line in data.split(\"\\n\"):\n88:\t\tif \"\\t\" in line:  # Token\n89:\t\t\tfields = line.split(\"\\t\")\n90:\t\t\tif \"-\" in fields[0]:\n91:\t\t\t\tcontinue\n92:\t\t\tlabel = fields[-1]\n93:\t\t\t# Ensure correct labeling even if other pipe-delimited annotations found in column 10\n94:\t\t\tif \"BeginSeg=Yes\" in label:\n95:\t\t\t\tlabel = \"BeginSeg=Yes\"\n96:\t\t\telif \"Seg=B-Conn\" in label:\n97:\t\t\t\tif span_start > -1:  # Add span\n98:\t\t\t\t\tif span_end == -1:\n99:\t\t\t\t\t\tspan_end = span_start\n100:\t\t\t\t\tspans.append((span_start,span_end))\n(145 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open seg_scripts/seg_eval_2019_modified.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0ae27c",
   "metadata": {},
   "source": [
    "### Step 33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897fd73b",
   "metadata": {},
   "source": [
    "Execution Start time: 05:11:30, End time: 05:11:30 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c9c003",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the script is trying to access a file that doesn't exist. We need to ensure that the output file `output_test.conllu` is being generated correctly. Let's check the script `single_corpus_train_and_test_ft.sh` to see where this file should be created and ensure that the path and file creation logic are correct.\n",
    "\n",
    "Let's search for `output_test.conllu` in the script to understand where it should be generated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0548d0c",
   "metadata": {},
   "source": [
    "**Action (search_file)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c99ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Found 1 matches for \"output_test.conllu\" in /content/DisCoDisCo/seg_scripts/single_corpus_train_and_test_ft.sh:\nLine 108:CONLL_PRED_PATH=\"${MODEL_DIR}/output_test.conllu\"\nEnd of matches for \"output_test.conllu\" in /content/DisCoDisCo/seg_scripts/single_corpus_train_and_test_ft.sh\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_file output_test.conllu seg_scripts/single_corpus_train_and_test_ft.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45137988",
   "metadata": {},
   "source": [
    "### Step 34"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7457dc",
   "metadata": {},
   "source": [
    "Execution Start time: 05:11:32, End time: 05:11:32 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7923fa19",
   "metadata": {},
   "source": [
    "**Thought**: The script sets `CONLL_PRED_PATH` to `${MODEL_DIR}/output_test.conllu`, but it seems the file is not being generated. We need to ensure that the script is correctly creating this file during the evaluation process.\n",
    "\n",
    "Let's check the lines around where `CONLL_PRED_PATH` is set to understand how the file should be generated and identify any potential issues.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba7222e",
   "metadata": {},
   "source": [
    "**Action (goto)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c15635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/DisCoDisCo/seg_scripts/seg_eval_2019_modified.py (245 lines total)]\n(92 more lines above)\n93:\t\t\t# Ensure correct labeling even if other pipe-delimited annotations found in column 10\n94:\t\t\tif \"BeginSeg=Yes\" in label:\n95:\t\t\t\tlabel = \"BeginSeg=Yes\"\n96:\t\t\telif \"Seg=B-Conn\" in label:\n97:\t\t\t\tif span_start > -1:  # Add span\n98:\t\t\t\t\tif span_end == -1:\n99:\t\t\t\t\t\tspan_end = span_start\n100:\t\t\t\t\tspans.append((span_start,span_end))\n101:\t\t\t\t\tspan_end = -1\n102:\t\t\t\tlabel =\"Seg=B-Conn\"\n103:\t\t\t\tspan_start = counter\n104:\t\t\telif \"Seg=I-Conn\" in label:\n105:\t\t\t\tlabel = \"Seg=I-Conn\"\n106:\t\t\t\tspan_end = counter\n107:\t\t\telse:\n108:\t\t\t\tlabel = \"_\"\n109:\t\t\t\tif span_start > -1:  # Add span\n110:\t\t\t\t\tif span_end == -1:\n111:\t\t\t\t\t\tspan_end = span_start\n112:\t\t\t\t\tspans.append((span_start,span_end))\n113:\t\t\t\t\tspan_start = -1\n114:\t\t\t\t\tspan_end = -1\n115:\n116:\t\t\ttokens.append(fields[1])\n117:\t\t\tlabels.append(label)\n118:\t\t\tcounter +=1\n119:\n120:\tif span_start > -1 and span_end > -1:  # Add last span\n121:\t\tspans.append((span_start,span_end))\n122:\n123:\treturn tokens, labels, spans\n124:\n125:\n126:def get_scores(gold_file, pred_file, string_input=False):\n127:\t\"\"\"\n128:\n129:\t:param gold_file: Gold shared task file\n130:\t:param pred_file: File with predictions\n131:\t:param string_input: If True, files are replaced by strings with file contents (for import inside other scripts)\n132:\t:return: dictionary of scores for printing\n133:\t\"\"\"\n134:\n135:\n136:\treport = \"\"\n137:\tgold_tokens, gold_labels, gold_spans = parse_data(gold_file, string_input)\n138:\tpred_tokens, pred_labels, pred_spans = parse_data(pred_file, string_input)\n139:\n140:\tif os.path.isfile(gold_file):\n141:\t\tdoc_name = os.path.basename(gold_file)\n142:\telse:\n143:\t\t# Use first few tokens to identify file\n144:\t\tdoc_name = \" \".join(gold_tokens[0:10]) + \"...\"\n145:\n146:\t# Check same number of tokens in both files\n147:\tif len(gold_tokens) != len(pred_tokens):\n148:\t\treport += \"\\nFATAL: different number of tokens detected in gold and pred:\\n\"\n149:\t\treport += \"  o In \" + doc_name + \": \" + str(len(gold_tokens)) + \" gold tokens but \" + str(len(pred_tokens)) + \" predicted tokens\\n\\n\"\n150:\t\tsys.stderr.write(report)\n151:\t\tsys.exit(0)\n152:\n153:\t# Check tokens are identical\n154:\tfor i, tok in enumerate(gold_tokens):\n155:\t\tif tok != pred_tokens[i]:\n156:\t\t\treport += \"\\nWARN: token strings do not match in gold and pred:\\n\"\n157:\t\t\treport += \" o First instance in \" + doc_name + \" token \" + str(i) + \"\\n\"\n158:\t\t\treport += \"Gold: \" + tok + \" but Pred: \" + pred_tokens[i] + \"\\n\\n\"\n159:\t\t\tsys.stderr.write(report)\n160:\t\t\tbreak\n161:\n162:\t# Check if this is EDU or Conn-style data\n163:\tif \"BeginSeg=Yes\" in gold_labels:\n164:\t\tmode = \"edu\"\n165:\t\tseg_type = \"EDUs\"\n166:\telse:\n167:\t\tmode = \"conn\"\n168:\t\tseg_type = \"conn spans\"\n169:\n170:\ttrue_positive = 0\n171:\tfalse_positive = 0\n172:\tfalse_negative = 0\n173:\n174:\tif mode == \"edu\":\n175:\t\tfor i, gold_label in enumerate(gold_labels):\n176:\t\t\tpred_label = pred_labels[i]\n177:\t\t\tif gold_label == pred_label:\n178:\t\t\t\tif gold_label == \"_\":\n179:\t\t\t\t\tcontinue\n180:\t\t\t\telse:\n181:\t\t\t\t\ttrue_positive += 1\n182:\t\t\telse:\n183:\t\t\t\tif pred_label == \"_\":\n184:\t\t\t\t\tfalse_negative += 1\n185:\t\t\t\telse:\n186:\t\t\t\t\tif gold_label == \"_\":\n187:\t\t\t\t\t\tfalse_positive += 1\n188:\t\t\t\t\telse:  # I-Conn/B-Conn mismatch\n189:\t\t\t\t\t\tfalse_positive +=1\n190:\telse:\n191:\t\tfor span in gold_spans:\n192:\t\t\tif span in pred_spans:\n(53 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goto 108"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddafe94",
   "metadata": {},
   "source": [
    "### Step 35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b0a447",
   "metadata": {},
   "source": [
    "Execution Start time: 05:11:34, End time: 05:11:35 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e1514f",
   "metadata": {},
   "source": [
    "**Thought**: It seems I mistakenly opened the wrong file. Let's go back and open the `single_corpus_train_and_test_ft.sh` script to examine how the `output_test.conllu` file is supposed to be generated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab68fedd",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d079e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/DisCoDisCo/seg_scripts/single_corpus_train_and_test_ft.sh (133 lines total)]\n1:#!/bin/bash\n2:#set -o errexit\n3:if [ $# -eq 0 ]; then\n4:  echo \"Supply the name of a corpus\"\n5:  exit 1\n6:fi\n7:CORPUS=\"$1\"\n8:CORPUS_DIR=\"data/2021/${1}\"\n9:MODEL_DIR=${2:-models}/${CORPUS}_seg_bert_baseline_ft\n10:if [[ ! -d $CORPUS_DIR ]]; then\n11:  echo \"Corpus \\\"$CORPUS_DIR\\\" not found\"\n12:  exit 1\n13:fi\n14:if [[ -d $MODEL_DIR ]]; then\n15:  echo \"\\\"$MODEL_DIR\\\" already exists. Removing it now...\"\n16:  rm -rf \"$MODEL_DIR\"\n17:fi\n18:\n19:# use language-specific berts if we can\n20:export EMBEDDING_DIMS=768\n21:if [[ \"$CORPUS\" == \"eng\"* ]]; then \n22:  export EMBEDDING_DIMS=1024\n23:  #export EMBEDDING_MODEL_NAME=\"roberta-large\"\n24:  export EMBEDDING_MODEL_NAME=\"google/electra-large-discriminator\"\n25:elif [[ \"$CORPUS\" == \"fas\"* ]]; then\n26:  export EMBEDDING_MODEL_NAME=\"HooshvareLab/bert-fa-base-uncased\"\n27:elif [[ \"$CORPUS\" == \"deu\"* ]]; then\n28:  export EMBEDDING_MODEL_NAME=\"google-bert/bert-base-german-dbmdz-uncased\"\n29:#elif [[ \"$CORPUS\" == \"fra\"* ]]; then\n30:#  export EMBEDDING_DIMS=1024\n31:#  export EMBEDDING_MODEL_NAME=\"camembert/camembert-large\"\n32:elif [[ \"$CORPUS\" == \"zho\"* ]]; then\n33:  #export EMBEDDING_DIMS=1024\n34:  #export EMBEDDING_MODEL_NAME=\"hfl/chinese-electra-180g-large-discriminator\"\n35:  export EMBEDDING_MODEL_NAME=\"bert-base-chinese\"\n36:elif [[ \"$CORPUS\" == \"nld\"* ]]; then\n37:  #export EMBEDDING_MODEL_NAME=\"GroNLP/bert-base-dutch-cased\"\n38:  export EMBEDDING_MODEL_NAME=\"pdelobelle/robbert-v2-dutch-base\"\n39:elif [[ \"$CORPUS\" == \"eus\"* ]]; then\n40:  export EMBEDDING_MODEL_NAME=\"ixa-ehu/berteus-base-cased\"\n41:elif [[ \"$CORPUS\" == \"spa\"* ]]; then\n42:  #export EMBEDDING_MODEL_NAME=\"mrm8488/electricidad-base-discriminator\"\n43:  export EMBEDDING_MODEL_NAME=\"dccuchile/bert-base-spanish-wwm-cased\"\n44:elif [[ \"$CORPUS\" == \"por\"* ]]; then\n45:  export EMBEDDING_MODEL_NAME=\"neuralmind/bert-base-portuguese-cased\"\n46:elif [[ \"$CORPUS\" == \"tur\"* ]]; then\n47:  #export EMBEDDING_MODEL_NAME=\"dbmdz/electra-base-turkish-cased-discriminator\"\n48:  export EMBEDDING_MODEL_NAME=\"dbmdz/bert-base-turkish-cased\"\n49:elif [[ \"$CORPUS\" == \"rus\"* ]]; then\n50:  export EMBEDDING_MODEL_NAME=\"DeepPavlov/rubert-base-cased\"\n51:else\n52:  export EMBEDDING_DIMS=1024\n53:  export EMBEDDING_MODEL_NAME=\"xlm-roberta-large\"\n54:fi\n55:\n56:# do not use CRF on RST datasets\n57:export USE_CRF=0\n58:if [[ \"$CORPUS\" == *\".pdtb.\"* ]]; then\n59:  export USE_CRF=1\n60:fi\n61:\n62:# use fastText embeddings\n63:if [[ \"$CORPUS\" == \"eng\"* ]]; then\n64:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\"\n65:elif [[ \"$CORPUS\" == \"deu\"* ]]; then\n66:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.de.300.vec.gz\"\n67:elif [[ \"$CORPUS\" == \"eus\"* ]]; then\n68:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.eu.300.vec.gz\"\n69:elif [[ \"$CORPUS\" == \"fas\"* ]]; then\n70:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.vec.gz\"\n71:elif [[ \"$CORPUS\" == \"fra\"* ]]; then\n72:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.vec.gz\"\n73:elif [[ \"$CORPUS\" == \"nld\"* ]]; then\n74:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.nl.300.vec.gz\"\n75:elif [[ \"$CORPUS\" == \"por\"* ]]; then\n76:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.pt.300.vec.gz\"\n77:elif [[ \"$CORPUS\" == \"rus\"* ]]; then\n78:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ru.300.vec.gz\"\n79:elif [[ \"$CORPUS\" == \"spa\"* ]]; then\n80:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.es.300.vec.gz\"\n81:elif [[ \"$CORPUS\" == \"tur\"* ]]; then\n82:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.tr.300.vec.gz\"\n83:elif [[ \"$CORPUS\" == \"zho\"* ]]; then\n84:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.zh.300.vec.gz\"\n85:else\n86:  echo \"Couldn't find a fasttext embedding for \\\"$CORPUS\\\"\" >&2\n87:  exit 1\n88:fi\n89:\n90:echo \"\"\n91:echo \"#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\"\n92:echo \"# Training on $CORPUS\"\n93:echo \"#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\"\n94:echo \"\"\n95:export TRAIN_DATA_PATH=\"${CORPUS_DIR}/${CORPUS}_train.conllu\"\n96:export VALIDATION_DATA_PATH=\"${CORPUS_DIR}/${CORPUS}_dev.conllu\"\n97:echo $TRAIN_DATA_PATH\n98:allennlp train \\\n99:  configs/seg/baseline/bert_baseline_ft.jsonnet \\\n100:  -s \"$MODEL_DIR\"\n(33 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open seg_scripts/single_corpus_train_and_test_ft.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc76c43",
   "metadata": {},
   "source": [
    "### Step 36"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c674fba0",
   "metadata": {},
   "source": [
    "Execution Start time: 05:11:37, End time: 05:11:37 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ed4ebb",
   "metadata": {},
   "source": [
    "**Thought**: The script sets the `CONLL_PRED_PATH` to `${MODEL_DIR}/output_test.conllu`, but it seems the file is not being generated. We need to ensure that the script is correctly creating this file during the evaluation process.\n",
    "\n",
    "Let's check the lines around where `CONLL_PRED_PATH` is set to understand how the file should be generated and identify any potential issues.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3d1180",
   "metadata": {},
   "source": [
    "**Action (goto)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bfdd2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/DisCoDisCo/seg_scripts/single_corpus_train_and_test_ft.sh (133 lines total)]\n(33 more lines above)\n34:  #export EMBEDDING_MODEL_NAME=\"hfl/chinese-electra-180g-large-discriminator\"\n35:  export EMBEDDING_MODEL_NAME=\"bert-base-chinese\"\n36:elif [[ \"$CORPUS\" == \"nld\"* ]]; then\n37:  #export EMBEDDING_MODEL_NAME=\"GroNLP/bert-base-dutch-cased\"\n38:  export EMBEDDING_MODEL_NAME=\"pdelobelle/robbert-v2-dutch-base\"\n39:elif [[ \"$CORPUS\" == \"eus\"* ]]; then\n40:  export EMBEDDING_MODEL_NAME=\"ixa-ehu/berteus-base-cased\"\n41:elif [[ \"$CORPUS\" == \"spa\"* ]]; then\n42:  #export EMBEDDING_MODEL_NAME=\"mrm8488/electricidad-base-discriminator\"\n43:  export EMBEDDING_MODEL_NAME=\"dccuchile/bert-base-spanish-wwm-cased\"\n44:elif [[ \"$CORPUS\" == \"por\"* ]]; then\n45:  export EMBEDDING_MODEL_NAME=\"neuralmind/bert-base-portuguese-cased\"\n46:elif [[ \"$CORPUS\" == \"tur\"* ]]; then\n47:  #export EMBEDDING_MODEL_NAME=\"dbmdz/electra-base-turkish-cased-discriminator\"\n48:  export EMBEDDING_MODEL_NAME=\"dbmdz/bert-base-turkish-cased\"\n49:elif [[ \"$CORPUS\" == \"rus\"* ]]; then\n50:  export EMBEDDING_MODEL_NAME=\"DeepPavlov/rubert-base-cased\"\n51:else\n52:  export EMBEDDING_DIMS=1024\n53:  export EMBEDDING_MODEL_NAME=\"xlm-roberta-large\"\n54:fi\n55:\n56:# do not use CRF on RST datasets\n57:export USE_CRF=0\n58:if [[ \"$CORPUS\" == *\".pdtb.\"* ]]; then\n59:  export USE_CRF=1\n60:fi\n61:\n62:# use fastText embeddings\n63:if [[ \"$CORPUS\" == \"eng\"* ]]; then\n64:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\"\n65:elif [[ \"$CORPUS\" == \"deu\"* ]]; then\n66:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.de.300.vec.gz\"\n67:elif [[ \"$CORPUS\" == \"eus\"* ]]; then\n68:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.eu.300.vec.gz\"\n69:elif [[ \"$CORPUS\" == \"fas\"* ]]; then\n70:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.vec.gz\"\n71:elif [[ \"$CORPUS\" == \"fra\"* ]]; then\n72:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.vec.gz\"\n73:elif [[ \"$CORPUS\" == \"nld\"* ]]; then\n74:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.nl.300.vec.gz\"\n75:elif [[ \"$CORPUS\" == \"por\"* ]]; then\n76:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.pt.300.vec.gz\"\n77:elif [[ \"$CORPUS\" == \"rus\"* ]]; then\n78:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ru.300.vec.gz\"\n79:elif [[ \"$CORPUS\" == \"spa\"* ]]; then\n80:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.es.300.vec.gz\"\n81:elif [[ \"$CORPUS\" == \"tur\"* ]]; then\n82:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.tr.300.vec.gz\"\n83:elif [[ \"$CORPUS\" == \"zho\"* ]]; then\n84:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.zh.300.vec.gz\"\n85:else\n86:  echo \"Couldn't find a fasttext embedding for \\\"$CORPUS\\\"\" >&2\n87:  exit 1\n88:fi\n89:\n90:echo \"\"\n91:echo \"#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\"\n92:echo \"# Training on $CORPUS\"\n93:echo \"#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\"\n94:echo \"\"\n95:export TRAIN_DATA_PATH=\"${CORPUS_DIR}/${CORPUS}_train.conllu\"\n96:export VALIDATION_DATA_PATH=\"${CORPUS_DIR}/${CORPUS}_dev.conllu\"\n97:echo $TRAIN_DATA_PATH\n98:allennlp train \\\n99:  configs/seg/baseline/bert_baseline_ft.jsonnet \\\n100:  -s \"$MODEL_DIR\"\n101:\n102:echo \"\"\n103:echo \"#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\"\n104:echo \"# Testing on ${CORPUS}\"\n105:echo \"#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\"\n106:echo \"\"\n107:JSON_PRED_PATH=\"${MODEL_DIR}/output_test.jsonl\"\n108:CONLL_PRED_PATH=\"${MODEL_DIR}/output_test.conllu\"\n109:CONLL_GOLD_PATH=\"${CORPUS_DIR}/${CORPUS}_test.conllu\"\n110:\n111:allennlp predict \\\n112:  \"${MODEL_DIR}/model.tar.gz\" \\\n113:  \"$CONLL_GOLD_PATH\" \\\n114:  --silent \\\n115:  --use-dataset-reader \\\n116:  --cuda-device 0 \\\n117:  --output-file \"$JSON_PRED_PATH\"\n118:\n119:echo \"Removing model files...\"\n120:rm $MODEL_DIR/*.th\n121:echo \"\"\n122:echo \"#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\"\n123:echo \"# Scoring on ${CORPUS}\"\n124:echo \"#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\"\n125:echo \"\"\n126:python seg_scripts/format_output.py \"$JSON_PRED_PATH\" \"$CONLL_PRED_PATH\"\n127:python seg_scripts/seg_eval_2019_modified.py \"$CONLL_GOLD_PATH\" \"$CONLL_PRED_PATH\" | tee \"$MODEL_DIR/score.txt\"\n128:printf \"#!/bin/sh\\npython seg_scripts/seg_eval_2019_modified.py $CONLL_GOLD_PATH $CONLL_PRED_PATH\\n\" > \"$MODEL_DIR/calc_score.sh\"\n129:\n130:# Accumulate a record of scores\n131:TSV_PATH=\"all_scores_finetuning.tsv\"\n132:cat \"$MODEL_DIR/score.txt\" | cut -d \" \" -f2 | head -n 1 | cut -d \"_\" -f1 | tr \"\\n\" \"\\t\" >> \"$TSV_PATH\"\n133:cat \"$MODEL_DIR/score.txt\" | cut -d \" \" -f3 | tail -n 3 | tr \"\\n\" \"\\t\" | sed 's/\\t$/\\n/g' >> \"$TSV_PATH\"\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goto 108"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c37217",
   "metadata": {},
   "source": [
    "### Step 37"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf37da6",
   "metadata": {},
   "source": [
    "Execution Start time: 05:11:39, End time: 05:11:40 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ad020b",
   "metadata": {},
   "source": [
    "**Thought**: The script attempts to generate `output_test.conllu` using the `format_output.py` script. We need to ensure that the `allennlp predict` command is correctly configured to generate the necessary output.\n",
    "\n",
    "Let's check the `allennlp predict` command to ensure it is set to use the CPU and that the paths are correct.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db775b8",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32998432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/DisCoDisCo/seg_scripts/single_corpus_train_and_test_ft.sh (133 lines total)]\n(33 more lines above)\n34:  #export EMBEDDING_MODEL_NAME=\"hfl/chinese-electra-180g-large-discriminator\"\n35:  export EMBEDDING_MODEL_NAME=\"bert-base-chinese\"\n36:elif [[ \"$CORPUS\" == \"nld\"* ]]; then\n37:  #export EMBEDDING_MODEL_NAME=\"GroNLP/bert-base-dutch-cased\"\n38:  export EMBEDDING_MODEL_NAME=\"pdelobelle/robbert-v2-dutch-base\"\n39:elif [[ \"$CORPUS\" == \"eus\"* ]]; then\n40:  export EMBEDDING_MODEL_NAME=\"ixa-ehu/berteus-base-cased\"\n41:elif [[ \"$CORPUS\" == \"spa\"* ]]; then\n42:  #export EMBEDDING_MODEL_NAME=\"mrm8488/electricidad-base-discriminator\"\n43:  export EMBEDDING_MODEL_NAME=\"dccuchile/bert-base-spanish-wwm-cased\"\n44:elif [[ \"$CORPUS\" == \"por\"* ]]; then\n45:  export EMBEDDING_MODEL_NAME=\"neuralmind/bert-base-portuguese-cased\"\n46:elif [[ \"$CORPUS\" == \"tur\"* ]]; then\n47:  #export EMBEDDING_MODEL_NAME=\"dbmdz/electra-base-turkish-cased-discriminator\"\n48:  export EMBEDDING_MODEL_NAME=\"dbmdz/bert-base-turkish-cased\"\n49:elif [[ \"$CORPUS\" == \"rus\"* ]]; then\n50:  export EMBEDDING_MODEL_NAME=\"DeepPavlov/rubert-base-cased\"\n51:else\n52:  export EMBEDDING_DIMS=1024\n53:  export EMBEDDING_MODEL_NAME=\"xlm-roberta-large\"\n54:fi\n55:\n56:# do not use CRF on RST datasets\n57:export USE_CRF=0\n58:if [[ \"$CORPUS\" == *\".pdtb.\"* ]]; then\n59:  export USE_CRF=1\n60:fi\n61:\n62:# use fastText embeddings\n63:if [[ \"$CORPUS\" == \"eng\"* ]]; then\n64:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\"\n65:elif [[ \"$CORPUS\" == \"deu\"* ]]; then\n66:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.de.300.vec.gz\"\n67:elif [[ \"$CORPUS\" == \"eus\"* ]]; then\n68:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.eu.300.vec.gz\"\n69:elif [[ \"$CORPUS\" == \"fas\"* ]]; then\n70:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.vec.gz\"\n71:elif [[ \"$CORPUS\" == \"fra\"* ]]; then\n72:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.vec.gz\"\n73:elif [[ \"$CORPUS\" == \"nld\"* ]]; then\n74:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.nl.300.vec.gz\"\n75:elif [[ \"$CORPUS\" == \"por\"* ]]; then\n76:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.pt.300.vec.gz\"\n77:elif [[ \"$CORPUS\" == \"rus\"* ]]; then\n78:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ru.300.vec.gz\"\n79:elif [[ \"$CORPUS\" == \"spa\"* ]]; then\n80:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.es.300.vec.gz\"\n81:elif [[ \"$CORPUS\" == \"tur\"* ]]; then\n82:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.tr.300.vec.gz\"\n83:elif [[ \"$CORPUS\" == \"zho\"* ]]; then\n84:  export FASTTEXT_EMBEDDING_FILE=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.zh.300.vec.gz\"\n85:else\n86:  echo \"Couldn't find a fasttext embedding for \\\"$CORPUS\\\"\" >&2\n87:  exit 1\n88:fi\n89:\n90:echo \"\"\n91:echo \"#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\"\n92:echo \"# Training on $CORPUS\"\n93:echo \"#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\"\n94:echo \"\"\n95:export TRAIN_DATA_PATH=\"${CORPUS_DIR}/${CORPUS}_train.conllu\"\n96:export VALIDATION_DATA_PATH=\"${CORPUS_DIR}/${CORPUS}_dev.conllu\"\n97:echo $TRAIN_DATA_PATH\n98:allennlp train \\\n99:  configs/seg/baseline/bert_baseline_ft.jsonnet \\\n100:  -s \"$MODEL_DIR\"\n101:\n102:echo \"\"\n103:echo \"#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\"\n104:echo \"# Testing on ${CORPUS}\"\n105:echo \"#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\"\n106:echo \"\"\n107:JSON_PRED_PATH=\"${MODEL_DIR}/output_test.jsonl\"\n108:CONLL_PRED_PATH=\"${MODEL_DIR}/output_test.conllu\"\n109:CONLL_GOLD_PATH=\"${CORPUS_DIR}/${CORPUS}_test.conllu\"\n110:\n111:allennlp predict \\\n112:  \"${MODEL_DIR}/model.tar.gz\" \\\n113:  \"$CONLL_GOLD_PATH\" \\\n114:  --silent \\\n115:  --use-dataset-reader \\\n116:  --cuda-device -1 \\\n117:  --output-file \"$JSON_PRED_PATH\"\n118:\n119:echo \"Removing model files...\"\n120:rm $MODEL_DIR/*.th\n121:echo \"\"\n122:echo \"#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\"\n123:echo \"# Scoring on ${CORPUS}\"\n124:echo \"#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\"\n125:echo \"\"\n126:python seg_scripts/format_output.py \"$JSON_PRED_PATH\" \"$CONLL_PRED_PATH\"\n127:python seg_scripts/seg_eval_2019_modified.py \"$CONLL_GOLD_PATH\" \"$CONLL_PRED_PATH\" | tee \"$MODEL_DIR/score.txt\"\n128:printf \"#!/bin/sh\\npython seg_scripts/seg_eval_2019_modified.py $CONLL_GOLD_PATH $CONLL_PRED_PATH\\n\" > \"$MODEL_DIR/calc_score.sh\"\n129:\n130:# Accumulate a record of scores\n131:TSV_PATH=\"all_scores_finetuning.tsv\"\n132:cat \"$MODEL_DIR/score.txt\" | cut -d \" \" -f2 | head -n 1 | cut -d \"_\" -f1 | tr \"\\n\" \"\\t\" >> \"$TSV_PATH\"\n133:cat \"$MODEL_DIR/score.txt\" | cut -d \" \" -f3 | tail -n 3 | tr \"\\n\" \"\\t\" | sed 's/\\t$/\\n/g' >> \"$TSV_PATH\"\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 116:116\n",
    "  --cuda-device -1 \\\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f8d7e4",
   "metadata": {},
   "source": [
    "### Step 38"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a1de5d",
   "metadata": {},
   "source": [
    "Execution Start time: 05:11:42, End time: 05:14:17 (duration: 155 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c798403",
   "metadata": {},
   "source": [
    "**Thought**: The script has been updated to ensure it runs on the CPU by setting `--cuda-device -1`. Now, let's run the script again to train and evaluate the model with the correct settings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e14d5c",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396a808a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"models/deu.rst.pcc_seg_bert_baseline_ft\" already exists. Removing it now...\n\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n# Training on deu.rst.pcc\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n\ndata/2021/deu.rst.pcc/deu.rst.pcc_train.conllu\n2024-08-22 05:11:48,486 - INFO - allennlp.common.plugins - Plugin allennlp_models available\n2024-08-22 05:11:49,136 - ERROR - allennlp.common.plugins - Plugin gucorpling_models could not be loaded: No module named 'flair'\n2024-08-22 05:11:49,237 - INFO - allennlp.common.checks - Pytorch version: 1.12.1+cu102\n\nloading instances: 0it [00:00, ?it/s]2024-08-22 05:11:50,453 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'case'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n2024-08-22 05:11:50,453 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'xpos'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n2024-08-22 05:11:50,453 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'depchunk'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n2024-08-22 05:11:50,454 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'deprel'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n2024-08-22 05:11:50,454 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'genre'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n2024-08-22 05:11:50,454 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'parent_clauses'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n2024-08-22 05:11:50,454 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'upos'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n2024-08-22 05:11:50,455 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 's_type'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n\nloading instances: 10it [00:00, 881.80it/s]\n\nloading instances: 0it [00:00, ?it/s]\nloading instances: 10it [00:00, 1293.46it/s]\n2024-08-22 05:11:50,467 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.\n\nbuilding vocab: 0it [00:00, ?it/s]\nbuilding vocab: 20it [00:00, 3801.94it/s]\n2024-08-22 05:11:50,475 - INFO - allennlp.modules.token_embedders.embedding - Reading pretrained embeddings from file\n2024-08-22 05:11:50,536 - INFO - cached_path - cache of https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.de.300.vec.gz is up-to-date\n2024-08-22 05:11:50,539 - INFO - allennlp.modules.token_embedders.embedding - Recognized a header line in the embedding file with number of tokens: 2000000\n\n  0%|          | 0/2000000 [00:00<?, ?it/s]\n  0%|          | 3634/2000000 [00:00<00:54, 36324.19it/s]\n  0%|          | 7484/2000000 [00:00<00:53, 37586.48it/s]\n  1%|          | 11369/2000000 [00:00<00:52, 38150.66it/s]\n  1%|          | 15276/2000000 [00:00<00:51, 38501.81it/s]\n  1%|          | 19140/2000000 [00:00<00:51, 38539.21it/s]\n  1%|1         | 23093/2000000 [00:00<00:50, 38866.82it/s]\n  1%|1         | 26990/2000000 [00:00<00:50, 38891.12it/s]\n  2%|1         | 30882/2000000 [00:00<00:50, 38895.48it/s]\n  2%|1         | 34798/2000000 [00:00<00:50, 38976.93it/s]\n  2%|1         | 38696/2000000 [00:01<00:50, 38932.43it/s]\n  2%|2         | 42590/2000000 [00:01<00:50, 38932.03it/s]\n  2%|2         | 46484/2000000 [00:01<00:50, 38885.68it/s]\n  3%|2         | 50373/2000000 [00:01<00:50, 38707.52it/s]\n  3%|2         | 54247/2000000 [00:01<00:50, 38712.61it/s]\n  3%|2         | 58130/2000000 [00:01<00:50, 38746.34it/s]\n  3%|3         | 62019/2000000 [00:01<00:49, 38777.81it/s]\n  3%|3         | 65919/2000000 [00:01<00:49, 38843.94it/s]\n  3%|3         | 69804/2000000 [00:01<00:49, 38843.99it/s]\n  4%|3         | 73697/2000000 [00:01<00:49, 38865.67it/s]\n  4%|3         | 77597/2000000 [00:02<00:49, 38899.83it/s]\n  4%|4         | 81508/2000000 [00:02<00:49, 38956.15it/s]\n  4%|4         | 85404/2000000 [00:02<00:49, 38917.49it/s]\n  4%|4         | 89314/2000000 [00:02<00:49, 38963.19it/s]\n  5%|4         | 93236/2000000 [00:02<00:48, 39030.39it/s]\n  5%|4         | 97140/2000000 [00:02<00:48, 39016.66it/s]\n  5%|5         | 101042/2000000 [00:02<00:48, 38970.24it/s]\n  5%|5         | 104940/2000000 [00:02<00:48, 38946.31it/s]\n  5%|5         | 108847/2000000 [00:02<00:48, 38976.22it/s]\n  6%|5         | 112745/2000000 [00:02<00:48, 38960.10it/s]\n  6%|5         | 116642/2000000 [00:03<00:48, 38910.87it/s]\n  6%|6         | 120535/2000000 [00:03<00:48, 38910.78it/s]\n  6%|6         | 124427/2000000 [00:03<00:48, 38907.97it/s]\n  6%|6         | 128341/2000000 [00:03<00:48, 38975.73it/s]\n  7%|6         | 132251/2000000 [00:03<00:47, 39005.94it/s]\n  7%|6         | 136152/2000000 [00:03<00:47, 38991.37it/s]\n  7%|7         | 140052/2000000 [00:03<00:47, 38944.59it/s]\n  7%|7         | 143947/2000000 [00:03<00:47, 38847.91it/s]\n  7%|7         | 147832/2000000 [00:03<00:47, 38831.07it/s]\n  8%|7         | 151716/2000000 [00:03<00:47, 38806.78it/s]\n  8%|7         | 155597/2000000 [00:04<00:47, 38770.04it/s]\n  8%|7         | 159475/2000000 [00:04<00:47, 38758.48it/s]\n  8%|8         | 163351/2000000 [00:04<00:47, 38268.10it/s]\n  8%|8         | 167218/2000000 [00:04<00:47, 38381.57it/s]\n  9%|8         | 171092/2000000 [00:04<00:47, 38483.69it/s]\n  9%|8         | 174961/2000000 [00:04<00:47, 38534.72it/s]\n  9%|8         | 178821/2000000 [00:04<00:47, 38548.33it/s]\n  9%|9         | 182687/2000000 [00:04<00:47, 38576.96it/s]\n  9%|9         | 186568/2000000 [00:04<00:46, 38645.92it/s]\n 10%|9         | 190433/2000000 [00:04<00:46, 38581.98it/s]\n 10%|9         | 194298/2000000 [00:05<00:46, 38601.31it/s]\n 10%|9         | 198176/2000000 [00:05<00:46, 38645.07it/s]\n 10%|#         | 202041/2000000 [00:05<00:46, 38619.41it/s]\n 10%|#         | 205921/2000000 [00:05<00:46, 38666.59it/s]\n 10%|#         | 209795/2000000 [00:05<00:46, 38685.59it/s]\n 11%|#         | 213677/2000000 [00:05<00:46, 38723.10it/s]\n 11%|#         | 217569/2000000 [00:05<00:45, 38771.31it/s]\n 11%|#1        | 221447/2000000 [00:05<00:45, 38712.57it/s]\n 11%|#1        | 225319/2000000 [00:05<00:45, 38701.27it/s]\n 11%|#1        | 229190/2000000 [00:05<00:45, 38646.53it/s]\n 12%|#1        | 233077/2000000 [00:06<00:45, 38702.62it/s]\n 12%|#1        | 236981/2000000 [00:06<00:45, 38801.91it/s]\n 12%|#2        | 240862/2000000 [00:06<00:45, 38792.26it/s]\n 12%|#2        | 244742/2000000 [00:06<00:45, 38784.23it/s]\n 12%|#2        | 248621/2000000 [00:06<00:45, 38773.94it/s]\n 13%|#2        | 252509/2000000 [00:06<00:45, 38805.11it/s]\n 13%|#2        | 256390/2000000 [00:06<00:44, 38749.17it/s]\n 13%|#3        | 260265/2000000 [00:06<00:45, 38282.62it/s]\n 13%|#3        | 264096/2000000 [00:06<00:45, 38282.76it/s]\n 13%|#3        | 267976/2000000 [00:06<00:45, 38433.94it/s]\n 14%|#3        | 271868/2000000 [00:07<00:44, 38572.37it/s]\n 14%|#3        | 275757/2000000 [00:07<00:44, 38663.67it/s]\n 14%|#3        | 279652/2000000 [00:07<00:44, 38740.55it/s]\n 14%|#4        | 283527/2000000 [00:07<00:44, 38738.66it/s]\n 14%|#4        | 287413/2000000 [00:07<00:44, 38771.31it/s]\n 15%|#4        | 291298/2000000 [00:07<00:44, 38793.63it/s]\n 15%|#4        | 295193/2000000 [00:07<00:43, 38832.77it/s]\n 15%|#4        | 299085/2000000 [00:07<00:43, 38858.38it/s]\n 15%|#5        | 302971/2000000 [00:07<00:43, 38798.60it/s]\n 15%|#5        | 306851/2000000 [00:07<00:43, 38789.14it/s]\n 16%|#5        | 310738/2000000 [00:08<00:43, 38803.76it/s]\n 16%|#5        | 314629/2000000 [00:08<00:43, 38835.03it/s]\n 16%|#5        | 318513/2000000 [00:08<00:43, 38827.04it/s]\n 16%|#6        | 322396/2000000 [00:08<00:43, 38779.65it/s]\n 16%|#6        | 326274/2000000 [00:08<00:43, 38768.14it/s]\n 17%|#6        | 330151/2000000 [00:08<00:43, 38756.38it/s]\n 17%|#6        | 334027/2000000 [00:08<00:43, 38709.44it/s]\n 17%|#6        | 337898/2000000 [00:08<00:43, 38635.09it/s]\n 17%|#7        | 341762/2000000 [00:08<00:43, 38449.75it/s]\n 17%|#7        | 345608/2000000 [00:08<00:43, 38403.22it/s]\n 17%|#7        | 349449/2000000 [00:09<00:43, 38352.19it/s]\n 18%|#7        | 353287/2000000 [00:09<00:42, 38351.94it/s]\n 18%|#7        | 357127/2000000 [00:09<00:42, 38355.76it/s]\n 18%|#8        | 360963/2000000 [00:09<00:42, 38265.84it/s]\n 18%|#8        | 364790/2000000 [00:09<00:42, 38231.99it/s]\n 18%|#8        | 368614/2000000 [00:09<00:42, 38204.07it/s]\n 19%|#8        | 372468/2000000 [00:09<00:42, 38296.73it/s]\n 19%|#8        | 376374/2000000 [00:09<00:42, 38521.25it/s]\n 19%|#9        | 380255/2000000 [00:09<00:41, 38599.38it/s]\n 19%|#9        | 384141/2000000 [00:09<00:41, 38670.64it/s]\n 19%|#9        | 388036/2000000 [00:10<00:41, 38751.61it/s]\n 20%|#9        | 391928/2000000 [00:10<00:41, 38792.67it/s]\n 20%|#9        | 395819/2000000 [00:10<00:41, 38823.64it/s]\n 20%|#9        | 399702/2000000 [00:10<00:41, 38789.51it/s]\n 20%|##        | 403602/2000000 [00:10<00:41, 38851.49it/s]\n 20%|##        | 407494/2000000 [00:10<00:40, 38866.10it/s]\n 21%|##        | 411381/2000000 [00:10<00:40, 38855.89it/s]\n 21%|##        | 415267/2000000 [00:10<00:40, 38817.26it/s]\n 21%|##        | 419149/2000000 [00:10<00:40, 38756.65it/s]\n 21%|##1       | 423029/2000000 [00:10<00:40, 38767.90it/s]\n 21%|##1       | 426914/2000000 [00:11<00:40, 38791.95it/s]\n 22%|##1       | 430794/2000000 [00:11<00:40, 38776.45it/s]\n 22%|##1       | 434682/2000000 [00:11<00:40, 38801.98it/s]\n 22%|##1       | 438563/2000000 [00:11<00:40, 38730.16it/s]\n 22%|##2       | 442447/2000000 [00:11<00:40, 38759.85it/s]\n 22%|##2       | 446324/2000000 [00:11<00:40, 38748.37it/s]\n 23%|##2       | 450206/2000000 [00:11<00:39, 38763.14it/s]\n 23%|##2       | 454093/2000000 [00:11<00:39, 38790.25it/s]\n 23%|##2       | 457975/2000000 [00:11<00:39, 38798.54it/s]\n 23%|##3       | 461865/2000000 [00:11<00:39, 38828.33it/s]\n 23%|##3       | 465769/2000000 [00:12<00:39, 38885.17it/s]\n 23%|##3       | 469668/2000000 [00:12<00:39, 38906.30it/s]\n 24%|##3       | 473570/2000000 [00:12<00:39, 38939.10it/s]\n 24%|##3       | 477464/2000000 [00:12<00:39, 38910.08it/s]\n 24%|##4       | 481356/2000000 [00:12<00:39, 38879.33it/s]\n 24%|##4       | 485244/2000000 [00:12<00:38, 38876.46it/s]\n 24%|##4       | 489154/2000000 [00:12<00:38, 38932.43it/s]\n 25%|##4       | 493048/2000000 [00:12<00:38, 38910.02it/s]\n 25%|##4       | 496940/2000000 [00:12<00:38, 38844.52it/s]\n 25%|##5       | 500842/2000000 [00:12<00:38, 38892.10it/s]\n 25%|##5       | 504733/2000000 [00:13<00:38, 38896.36it/s]\n 25%|##5       | 508639/2000000 [00:13<00:38, 38937.37it/s]\n 26%|##5       | 512553/2000000 [00:13<00:38, 38996.53it/s]\n 26%|##5       | 516453/2000000 [00:13<00:38, 38987.05it/s]\n 26%|##6       | 520355/2000000 [00:13<00:37, 38990.25it/s]\n 26%|##6       | 524255/2000000 [00:13<00:38, 38828.89it/s]\n 26%|##6       | 528140/2000000 [00:13<00:37, 38825.31it/s]\n 27%|##6       | 532049/2000000 [00:13<00:37, 38899.28it/s]\n 27%|##6       | 535965/2000000 [00:13<00:37, 38975.75it/s]\n 27%|##6       | 539863/2000000 [00:13<00:37, 38900.26it/s]\n 27%|##7       | 543754/2000000 [00:14<00:37, 38837.45it/s]\n 27%|##7       | 547638/2000000 [00:14<00:37, 38780.05it/s]\n 28%|##7       | 551517/2000000 [00:14<00:37, 38744.11it/s]\n 28%|##7       | 555409/2000000 [00:14<00:37, 38794.86it/s]\n 28%|##7       | 559289/2000000 [00:14<00:37, 38754.19it/s]\n 28%|##8       | 563165/2000000 [00:14<00:37, 38736.04it/s]\n 28%|##8       | 567043/2000000 [00:14<00:36, 38748.11it/s]\n 29%|##8       | 570934/2000000 [00:14<00:36, 38788.06it/s]\n 29%|##8       | 574844/2000000 [00:14<00:36, 38876.61it/s]\n 29%|##8       | 578775/2000000 [00:14<00:36, 38998.03it/s]\n 29%|##9       | 582692/2000000 [00:15<00:36, 39047.66it/s]\n 29%|##9       | 586597/2000000 [00:15<00:36, 38990.99it/s]\n 30%|##9       | 590497/2000000 [00:15<00:36, 38992.50it/s]\n 30%|##9       | 594397/2000000 [00:15<00:36, 38628.46it/s]\n 30%|##9       | 598293/2000000 [00:15<00:36, 38726.58it/s]\n 30%|###       | 602188/2000000 [00:15<00:36, 38784.88it/s]\n 30%|###       | 606079/2000000 [00:15<00:35, 38820.43it/s]\n 30%|###       | 609984/2000000 [00:15<00:35, 38886.11it/s]\n 31%|###       | 613897/2000000 [00:15<00:35, 38950.08it/s]\n 31%|###       | 617814/2000000 [00:15<00:35, 39009.59it/s]\n 31%|###1      | 621727/2000000 [00:16<00:35, 39037.57it/s]\n 31%|###1      | 625631/2000000 [00:16<00:35, 38994.70it/s]\n 31%|###1      | 629551/2000000 [00:16<00:35, 39053.73it/s]\n 32%|###1      | 633495/2000000 [00:16<00:34, 39166.42it/s]\n 32%|###1      | 637412/2000000 [00:16<00:34, 39131.48it/s]\n 32%|###2      | 641326/2000000 [00:16<00:34, 39127.42it/s]\n 32%|###2      | 645239/2000000 [00:16<00:34, 39060.83it/s]\n 32%|###2      | 649160/2000000 [00:16<00:34, 39097.24it/s]\n 33%|###2      | 653091/2000000 [00:16<00:34, 39153.55it/s]\n 33%|###2      | 657021/2000000 [00:16<00:34, 39192.77it/s]\n 33%|###3      | 660941/2000000 [00:17<00:34, 39191.92it/s]\n 33%|###3      | 664861/2000000 [00:17<00:34, 39117.72it/s]\n 33%|###3      | 668773/2000000 [00:17<00:34, 39074.03it/s]\n 34%|###3      | 672681/2000000 [00:17<00:33, 39060.83it/s]\n 34%|###3      | 676588/2000000 [00:17<00:33, 39024.16it/s]\n 34%|###4      | 680502/2000000 [00:17<00:33, 39049.24it/s]\n 34%|###4      | 684407/2000000 [00:17<00:33, 38985.30it/s]\n 34%|###4      | 688306/2000000 [00:17<00:33, 38972.17it/s]\n 35%|###4      | 692204/2000000 [00:17<00:33, 38964.99it/s]\n 35%|###4      | 696101/2000000 [00:17<00:33, 38916.72it/s]\n 35%|###4      | 699993/2000000 [00:18<00:33, 38862.47it/s]\n 35%|###5      | 703880/2000000 [00:18<00:33, 38851.93it/s]\n 35%|###5      | 707766/2000000 [00:18<00:33, 38832.12it/s]\n 36%|###5      | 711681/2000000 [00:18<00:33, 38916.00it/s]\n 36%|###5      | 715579/2000000 [00:18<00:32, 38926.73it/s]\n 36%|###5      | 719474/2000000 [00:18<00:32, 38929.04it/s]\n 36%|###6      | 723367/2000000 [00:18<00:32, 38892.15it/s]\n 36%|###6      | 727271/2000000 [00:18<00:32, 38925.31it/s]\n 37%|###6      | 731164/2000000 [00:18<00:32, 38907.05it/s]\n 37%|###6      | 735058/2000000 [00:18<00:32, 38911.87it/s]\n 37%|###6      | 738952/2000000 [00:19<00:32, 38912.81it/s]\n 37%|###7      | 742844/2000000 [00:19<00:32, 38890.71it/s]\n 37%|###7      | 746744/2000000 [00:19<00:32, 38919.11it/s]\n 38%|###7      | 750638/2000000 [00:19<00:32, 38915.54it/s]\n 38%|###7      | 754536/2000000 [00:19<00:31, 38928.89it/s]\n 38%|###7      | 758431/2000000 [00:19<00:31, 38927.27it/s]\n 38%|###8      | 762339/2000000 [00:19<00:31, 38965.58it/s]\n 38%|###8      | 766236/2000000 [00:19<00:31, 38925.54it/s]\n 39%|###8      | 770146/2000000 [00:19<00:31, 38967.72it/s]\n 39%|###8      | 774048/2000000 [00:19<00:31, 38974.30it/s]\n 39%|###8      | 777946/2000000 [00:20<00:31, 38959.06it/s]\n 39%|###9      | 781842/2000000 [00:20<00:31, 38901.61it/s]\n 39%|###9      | 785733/2000000 [00:20<00:31, 38570.57it/s]\n 39%|###9      | 789632/2000000 [00:20<00:31, 38687.21it/s]\n 40%|###9      | 793540/2000000 [00:20<00:31, 38801.88it/s]\n 40%|###9      | 797435/2000000 [00:20<00:30, 38839.63it/s]\n 40%|####      | 801355/2000000 [00:20<00:30, 38938.49it/s]\n 40%|####      | 805250/2000000 [00:20<00:30, 38879.28it/s]\n 40%|####      | 809163/2000000 [00:20<00:30, 38948.59it/s]\n 41%|####      | 813058/2000000 [00:20<00:30, 38922.46it/s]\n 41%|####      | 816959/2000000 [00:21<00:30, 38942.21it/s]\n 41%|####1     | 820854/2000000 [00:21<00:30, 38895.96it/s]\n 41%|####1     | 824744/2000000 [00:21<00:30, 38886.49it/s]\n 41%|####1     | 828639/2000000 [00:21<00:30, 38896.97it/s]\n 42%|####1     | 832567/2000000 [00:21<00:29, 39009.94it/s]\n 42%|####1     | 836484/2000000 [00:21<00:29, 39055.11it/s]\n 42%|####2     | 840390/2000000 [00:21<00:29, 38991.76it/s]\n 42%|####2     | 844292/2000000 [00:21<00:29, 38999.62it/s]\n 42%|####2     | 848192/2000000 [00:21<00:29, 38943.10it/s]\n 43%|####2     | 852095/2000000 [00:21<00:29, 38964.79it/s]\n 43%|####2     | 856026/2000000 [00:22<00:29, 39062.78it/s]\n 43%|####2     | 859947/2000000 [00:22<00:29, 39103.03it/s]\n 43%|####3     | 863863/2000000 [00:22<00:29, 39118.75it/s]\n 43%|####3     | 867782/2000000 [00:22<00:28, 39139.65it/s]\n 44%|####3     | 871703/2000000 [00:22<00:28, 39151.41it/s]\n 44%|####3     | 875634/2000000 [00:22<00:28, 39197.63it/s]\n 44%|####3     | 879554/2000000 [00:22<00:28, 39182.35it/s]\n 44%|####4     | 883478/2000000 [00:22<00:28, 39191.81it/s]\n 44%|####4     | 887398/2000000 [00:22<00:28, 39136.44it/s]\n 45%|####4     | 891318/2000000 [00:22<00:28, 39146.97it/s]\n 45%|####4     | 895234/2000000 [00:23<00:28, 39150.21it/s]\n 45%|####4     | 899150/2000000 [00:23<00:28, 39100.65it/s]\n 45%|####5     | 903064/2000000 [00:23<00:28, 39106.28it/s]\n 45%|####5     | 906987/2000000 [00:23<00:27, 39135.48it/s]\n 46%|####5     | 910901/2000000 [00:23<00:27, 39076.33it/s]\n 46%|####5     | 914813/2000000 [00:23<00:27, 39082.03it/s]\n 46%|####5     | 918722/2000000 [00:23<00:27, 39075.08it/s]\n 46%|####6     | 922630/2000000 [00:23<00:27, 39047.80it/s]\n 46%|####6     | 926554/2000000 [00:23<00:27, 39094.62it/s]\n 47%|####6     | 930464/2000000 [00:23<00:27, 39075.55it/s]\n 47%|####6     | 934380/2000000 [00:24<00:27, 39092.85it/s]\n 47%|####6     | 938304/2000000 [00:24<00:27, 39134.76it/s]\n 47%|####7     | 942218/2000000 [00:24<00:27, 39126.66it/s]\n 47%|####7     | 946140/2000000 [00:24<00:26, 39150.52it/s]\n 48%|####7     | 950062/2000000 [00:24<00:26, 39159.94it/s]\n 48%|####7     | 953978/2000000 [00:24<00:26, 39133.59it/s]\n 48%|####7     | 957898/2000000 [00:24<00:26, 39149.23it/s]\n 48%|####8     | 961832/2000000 [00:24<00:26, 39203.29it/s]\n 48%|####8     | 965753/2000000 [00:24<00:26, 39202.74it/s]\n 48%|####8     | 969679/2000000 [00:24<00:26, 39212.77it/s]\n 49%|####8     | 973613/2000000 [00:25<00:26, 39244.82it/s]\n 49%|####8     | 977543/2000000 [00:25<00:26, 39251.89it/s]\n 49%|####9     | 981469/2000000 [00:25<00:25, 39207.07it/s]\n 49%|####9     | 985403/2000000 [00:25<00:25, 39245.21it/s]\n 49%|####9     | 989328/2000000 [00:25<00:25, 39233.06it/s]\n 50%|####9     | 993252/2000000 [00:25<00:25, 39136.39it/s]\n 50%|####9     | 997174/2000000 [00:25<00:25, 39152.99it/s]\n 50%|#####     | 1001101/2000000 [00:25<00:25, 39180.88it/s]\n 50%|#####     | 1005020/2000000 [00:25<00:25, 39166.70it/s]\n 50%|#####     | 1008959/2000000 [00:25<00:25, 39224.76it/s]\n 51%|#####     | 1012897/2000000 [00:26<00:25, 39269.36it/s]\n 51%|#####     | 1016838/2000000 [00:26<00:25, 39310.83it/s]\n 51%|#####1    | 1020770/2000000 [00:26<00:24, 39237.74it/s]\n 51%|#####1    | 1024705/2000000 [00:26<00:24, 39261.62it/s]\n 51%|#####1    | 1028642/2000000 [00:26<00:24, 39282.51it/s]\n 52%|#####1    | 1032583/2000000 [00:26<00:24, 39313.56it/s]\n 52%|#####1    | 1036515/2000000 [00:26<00:24, 39231.18it/s]\n 52%|#####2    | 1040458/2000000 [00:26<00:24, 39283.33it/s]\n 52%|#####2    | 1044400/2000000 [00:26<00:24, 39318.83it/s]\n 52%|#####2    | 1048332/2000000 [00:26<00:24, 39293.98it/s]\n 53%|#####2    | 1052264/2000000 [00:27<00:24, 39298.79it/s]\n 53%|#####2    | 1056235/2000000 [00:27<00:23, 39417.22it/s]\n 53%|#####3    | 1060179/2000000 [00:27<00:23, 39416.01it/s]\n 53%|#####3    | 1064121/2000000 [00:27<00:23, 39287.59it/s]\n 53%|#####3    | 1068050/2000000 [00:27<00:23, 39283.67it/s]\n 54%|#####3    | 1071982/2000000 [00:27<00:23, 39283.20it/s]\n 54%|#####3    | 1075917/2000000 [00:27<00:23, 39301.73it/s]\n 54%|#####3    | 1079848/2000000 [00:27<00:23, 39220.86it/s]\n 54%|#####4    | 1083771/2000000 [00:27<00:23, 39193.07it/s]\n 54%|#####4    | 1087708/2000000 [00:27<00:23, 39237.33it/s]\n 55%|#####4    | 1091649/2000000 [00:28<00:23, 39286.65it/s]\n 55%|#####4    | 1095578/2000000 [00:28<00:23, 39250.79it/s]\n 55%|#####4    | 1099504/2000000 [00:28<00:22, 39241.23it/s]\n 55%|#####5    | 1103441/2000000 [00:28<00:22, 39274.49it/s]\n 55%|#####5    | 1107372/2000000 [00:28<00:22, 39282.20it/s]\n 56%|#####5    | 1111301/2000000 [00:28<00:22, 39275.54it/s]\n 56%|#####5    | 1115229/2000000 [00:28<00:22, 39205.93it/s]\n 56%|#####5    | 1119171/2000000 [00:28<00:22, 39261.08it/s]\n 56%|#####6    | 1123113/2000000 [00:28<00:22, 39306.54it/s]\n 56%|#####6    | 1127044/2000000 [00:28<00:22, 39284.12it/s]\n 57%|#####6    | 1130973/2000000 [00:29<00:22, 39182.32it/s]\n 57%|#####6    | 1134905/2000000 [00:29<00:22, 39222.60it/s]\n 57%|#####6    | 1138834/2000000 [00:29<00:21, 39239.99it/s]\n 57%|#####7    | 1142759/2000000 [00:29<00:21, 39229.66it/s]\n 57%|#####7    | 1146682/2000000 [00:29<00:21, 39213.47it/s]\n 58%|#####7    | 1150604/2000000 [00:29<00:21, 39110.74it/s]\n 58%|#####7    | 1154526/2000000 [00:29<00:21, 39139.48it/s]\n 58%|#####7    | 1158467/2000000 [00:29<00:21, 39214.55it/s]\n 58%|#####8    | 1162405/2000000 [00:29<00:21, 39255.13it/s]\n 58%|#####8    | 1166331/2000000 [00:29<00:21, 39242.78it/s]\n 59%|#####8    | 1170256/2000000 [00:30<00:21, 39149.54it/s]\n 59%|#####8    | 1174197/2000000 [00:30<00:21, 39218.66it/s]\n 59%|#####8    | 1178144/2000000 [00:30<00:20, 39286.36it/s]\n 59%|#####9    | 1182088/2000000 [00:30<00:20, 39328.71it/s]\n 59%|#####9    | 1186022/2000000 [00:30<00:20, 39329.52it/s]\n 59%|#####9    | 1189955/2000000 [00:30<00:20, 39300.16it/s]\n 60%|#####9    | 1193901/2000000 [00:30<00:20, 39341.35it/s]\n 60%|#####9    | 1197854/2000000 [00:30<00:20, 39394.44it/s]\n 60%|######    | 1201794/2000000 [00:30<00:20, 39363.60it/s]\n 60%|######    | 1205731/2000000 [00:30<00:20, 39348.04it/s]\n 60%|######    | 1209666/2000000 [00:31<00:20, 39311.84it/s]\n 61%|######    | 1213598/2000000 [00:31<00:20, 39304.70it/s]\n 61%|######    | 1217544/2000000 [00:31<00:19, 39345.65it/s]\n 61%|######1   | 1221479/2000000 [00:31<00:19, 39339.09it/s]\n 61%|######1   | 1225413/2000000 [00:31<00:19, 39312.80it/s]\n 61%|######1   | 1229346/2000000 [00:31<00:19, 39308.19it/s]\n 62%|######1   | 1233291/2000000 [00:31<00:19, 39340.05it/s]\n 62%|######1   | 1237226/2000000 [00:31<00:19, 39331.80it/s]\n 62%|######2   | 1241163/2000000 [00:31<00:19, 39338.26it/s]\n 62%|######2   | 1245097/2000000 [00:31<00:19, 39327.60it/s]\n 62%|######2   | 1249030/2000000 [00:32<00:19, 39259.83it/s]\n 63%|######2   | 1252957/2000000 [00:32<00:19, 39257.84it/s]\n 63%|######2   | 1256891/2000000 [00:32<00:18, 39281.85it/s]\n 63%|######3   | 1260820/2000000 [00:32<00:18, 39267.81it/s]\n 63%|######3   | 1264756/2000000 [00:32<00:18, 39291.94it/s]\n 63%|######3   | 1268686/2000000 [00:32<00:18, 39270.17it/s]\n 64%|######3   | 1272614/2000000 [00:32<00:18, 39227.30it/s]\n 64%|######3   | 1276537/2000000 [00:32<00:18, 39227.17it/s]\n 64%|######4   | 1280489/2000000 [00:32<00:18, 39310.47it/s]\n 64%|######4   | 1284430/2000000 [00:32<00:18, 39330.11it/s]\n 64%|######4   | 1288364/2000000 [00:33<00:18, 39302.78it/s]\n 65%|######4   | 1292295/2000000 [00:33<00:18, 39255.59it/s]\n 65%|######4   | 1296225/2000000 [00:33<00:17, 39259.09it/s]\n 65%|######5   | 1300169/2000000 [00:33<00:17, 39309.01it/s]\n 65%|######5   | 1304114/2000000 [00:33<00:17, 39345.15it/s]\n 65%|######5   | 1308067/2000000 [00:33<00:17, 39395.47it/s]\n 66%|######5   | 1312007/2000000 [00:33<00:17, 39383.24it/s]\n 66%|######5   | 1315946/2000000 [00:33<00:17, 39310.40it/s]\n 66%|######5   | 1319878/2000000 [00:33<00:17, 39259.13it/s]\n 66%|######6   | 1323821/2000000 [00:33<00:17, 39308.07it/s]\n 66%|######6   | 1327784/2000000 [00:34<00:17, 39401.41it/s]\n 67%|######6   | 1331726/2000000 [00:34<00:16, 39402.77it/s]\n 67%|######6   | 1335667/2000000 [00:34<00:16, 39390.76it/s]\n 67%|######6   | 1339607/2000000 [00:34<00:16, 39390.25it/s]\n 67%|######7   | 1343547/2000000 [00:34<00:16, 39314.89it/s]\n 67%|######7   | 1347479/2000000 [00:34<00:16, 39303.78it/s]\n 68%|######7   | 1351421/2000000 [00:34<00:16, 39337.99it/s]\n 68%|######7   | 1355376/2000000 [00:34<00:16, 39391.40it/s]\n 68%|######7   | 1359332/2000000 [00:34<00:16, 39436.70it/s]\n 68%|######8   | 1363276/2000000 [00:34<00:16, 39407.23it/s]\n 68%|######8   | 1367217/2000000 [00:35<00:16, 39358.29it/s]\n 69%|######8   | 1371153/2000000 [00:35<00:16, 39254.57it/s]\n 69%|######8   | 1375079/2000000 [00:35<00:15, 39236.98it/s]\n 69%|######8   | 1379011/2000000 [00:35<00:15, 39255.37it/s]\n 69%|######9   | 1382963/2000000 [00:35<00:15, 39331.31it/s]\n 69%|######9   | 1386912/2000000 [00:35<00:15, 39371.64it/s]\n 70%|######9   | 1390850/2000000 [00:35<00:15, 39103.40it/s]\n 70%|######9   | 1394761/2000000 [00:35<00:15, 39104.57it/s]\n 70%|######9   | 1398672/2000000 [00:35<00:15, 39019.77it/s]\n 70%|#######   | 1402598/2000000 [00:35<00:15, 39089.43it/s]\n 70%|#######   | 1406543/2000000 [00:36<00:15, 39190.10it/s]\n 71%|#######   | 1410481/2000000 [00:36<00:15, 39243.06it/s]\n 71%|#######   | 1414406/2000000 [00:36<00:14, 39222.97it/s]\n 71%|#######   | 1418329/2000000 [00:36<00:14, 39148.06it/s]\n 71%|#######1  | 1422244/2000000 [00:36<00:14, 39123.11it/s]\n 71%|#######1  | 1426157/2000000 [00:36<00:14, 38899.45it/s]\n 72%|#######1  | 1430048/2000000 [00:36<00:14, 38468.87it/s]\n 72%|#######1  | 1433896/2000000 [00:36<00:14, 38468.83it/s]\n 72%|#######1  | 1437751/2000000 [00:36<00:14, 38485.16it/s]\n 72%|#######2  | 1441616/2000000 [00:36<00:14, 38531.20it/s]\n 72%|#######2  | 1445492/2000000 [00:37<00:14, 38589.20it/s]\n 72%|#######2  | 1449386/2000000 [00:37<00:14, 38688.93it/s]\n 73%|#######2  | 1453291/2000000 [00:37<00:14, 38787.66it/s]\n 73%|#######2  | 1457207/2000000 [00:37<00:13, 38897.73it/s]\n 73%|#######3  | 1461126/2000000 [00:37<00:13, 38976.48it/s]\n 73%|#######3  | 1465047/2000000 [00:37<00:13, 39044.72it/s]\n 73%|#######3  | 1468979/2000000 [00:37<00:13, 39118.89it/s]\n 74%|#######3  | 1472956/2000000 [00:37<00:13, 39308.26it/s]\n 74%|#######3  | 1476900/2000000 [00:37<00:13, 39342.28it/s]\n 74%|#######4  | 1480860/2000000 [00:37<00:13, 39409.68it/s]\n 74%|#######4  | 1484801/2000000 [00:38<00:13, 39360.04it/s]\n 74%|#######4  | 1488738/2000000 [00:38<00:13, 39254.99it/s]\n 75%|#######4  | 1492664/2000000 [00:38<00:12, 39249.47it/s]\n 75%|#######4  | 1496596/2000000 [00:38<00:12, 39266.42it/s]\n 75%|#######5  | 1500524/2000000 [00:38<00:12, 39269.74it/s]\n 75%|#######5  | 1504468/2000000 [00:38<00:12, 39316.00it/s]\n 75%|#######5  | 1508406/2000000 [00:38<00:12, 39327.02it/s]\n 76%|#######5  | 1512361/2000000 [00:38<00:12, 39391.82it/s]\n 76%|#######5  | 1516305/2000000 [00:38<00:12, 39397.01it/s]\n 76%|#######6  | 1520245/2000000 [00:38<00:12, 39326.34it/s]\n 76%|#######6  | 1524178/2000000 [00:39<00:12, 39285.57it/s]\n 76%|#######6  | 1528129/2000000 [00:39<00:11, 39350.67it/s]\n 77%|#######6  | 1532066/2000000 [00:39<00:11, 39352.54it/s]\n 77%|#######6  | 1536043/2000000 [00:39<00:11, 39476.08it/s]\n 77%|#######6  | 1539991/2000000 [00:39<00:11, 39429.92it/s]\n 77%|#######7  | 1543935/2000000 [00:39<00:11, 39403.06it/s]\n 77%|#######7  | 1547876/2000000 [00:39<00:11, 39393.31it/s]\n 78%|#######7  | 1551827/2000000 [00:39<00:11, 39423.64it/s]\n 78%|#######7  | 1555770/2000000 [00:39<00:11, 39300.64it/s]\n 78%|#######7  | 1559701/2000000 [00:39<00:11, 39231.49it/s]\n 78%|#######8  | 1563625/2000000 [00:40<00:11, 39193.06it/s]\n 78%|#######8  | 1567559/2000000 [00:40<00:11, 39236.04it/s]\n 79%|#######8  | 1571483/2000000 [00:40<00:10, 39223.18it/s]\n 79%|#######8  | 1575426/2000000 [00:40<00:10, 39280.45it/s]\n 79%|#######8  | 1579366/2000000 [00:40<00:10, 39311.25it/s]\n 79%|#######9  | 1583298/2000000 [00:40<00:10, 39309.45it/s]\n 79%|#######9  | 1587229/2000000 [00:40<00:10, 39283.09it/s]\n 80%|#######9  | 1591158/2000000 [00:40<00:10, 39249.98it/s]\n 80%|#######9  | 1595084/2000000 [00:40<00:10, 39179.30it/s]\n 80%|#######9  | 1599002/2000000 [00:40<00:10, 39177.54it/s]\n 80%|########  | 1602920/2000000 [00:41<00:10, 39171.43it/s]\n 80%|########  | 1606861/2000000 [00:41<00:10, 39240.18it/s]\n 81%|########  | 1610788/2000000 [00:41<00:09, 39245.96it/s]\n 81%|########  | 1614718/2000000 [00:41<00:09, 39250.53it/s]\n 81%|########  | 1618649/2000000 [00:41<00:09, 39263.09it/s]\n 81%|########1 | 1622593/2000000 [00:41<00:09, 39309.74it/s]\n 81%|########1 | 1626534/2000000 [00:41<00:09, 39335.62it/s]\n 82%|########1 | 1630468/2000000 [00:41<00:09, 39289.50it/s]\n 82%|########1 | 1634397/2000000 [00:41<00:09, 39180.52it/s]\n 82%|########1 | 1638316/2000000 [00:41<00:09, 39105.41it/s]\n 82%|########2 | 1642227/2000000 [00:42<00:09, 39028.89it/s]\n 82%|########2 | 1646176/2000000 [00:42<00:09, 39161.50it/s]\n 83%|########2 | 1650112/2000000 [00:42<00:08, 39218.80it/s]\n 83%|########2 | 1654056/2000000 [00:42<00:08, 39278.43it/s]\n 83%|########2 | 1657984/2000000 [00:42<00:08, 39255.86it/s]\n 83%|########3 | 1661910/2000000 [00:42<00:08, 39120.32it/s]\n 83%|########3 | 1665823/2000000 [00:42<00:08, 38997.66it/s]\n 83%|########3 | 1669752/2000000 [00:42<00:08, 39084.44it/s]\n 84%|########3 | 1673661/2000000 [00:42<00:08, 39073.49it/s]\n 84%|########3 | 1677571/2000000 [00:43<00:08, 39076.00it/s]\n 84%|########4 | 1681479/2000000 [00:43<00:08, 39056.30it/s]\n 84%|########4 | 1685385/2000000 [00:43<00:08, 39041.39it/s]\n 84%|########4 | 1689326/2000000 [00:43<00:07, 39148.02it/s]\n 85%|########4 | 1693254/2000000 [00:43<00:07, 39176.47it/s]\n 85%|########4 | 1697196/2000000 [00:43<00:07, 39247.20it/s]\n 85%|########5 | 1701126/2000000 [00:43<00:07, 39255.29it/s]\n 85%|########5 | 1705060/2000000 [00:43<00:07, 39280.03it/s]\n 85%|########5 | 1708994/2000000 [00:43<00:07, 39292.01it/s]\n 86%|########5 | 1712924/2000000 [00:43<00:07, 39246.44it/s]\n 86%|########5 | 1716849/2000000 [00:44<00:07, 39178.80it/s]\n 86%|########6 | 1720767/2000000 [00:44<00:07, 39024.82it/s]\n 86%|########6 | 1724670/2000000 [00:44<00:07, 38975.02it/s]\n 86%|########6 | 1728568/2000000 [00:44<00:06, 38957.40it/s]\n 87%|########6 | 1732479/2000000 [00:44<00:06, 38991.45it/s]\n 87%|########6 | 1736406/2000000 [00:44<00:06, 39071.63it/s]\n 87%|########7 | 1740339/2000000 [00:44<00:06, 39148.26it/s]\n 87%|########7 | 1744273/2000000 [00:44<00:06, 39196.46it/s]\n 87%|########7 | 1748196/2000000 [00:44<00:06, 39201.17it/s]\n 88%|########7 | 1752130/2000000 [00:44<00:06, 39237.22it/s]\n 88%|########7 | 1756063/2000000 [00:45<00:06, 39261.59it/s]\n 88%|########7 | 1759990/2000000 [00:45<00:06, 39234.00it/s]\n 88%|########8 | 1763914/2000000 [00:45<00:06, 39100.32it/s]\n 88%|########8 | 1767825/2000000 [00:45<00:05, 38963.36it/s]\n 89%|########8 | 1771722/2000000 [00:45<00:05, 38948.55it/s]\n 89%|########8 | 1775617/2000000 [00:45<00:05, 38938.73it/s]\n 89%|########8 | 1779552/2000000 [00:45<00:05, 39060.96it/s]\n 89%|########9 | 1783478/2000000 [00:45<00:05, 39113.22it/s]\n 89%|########9 | 1787418/2000000 [00:45<00:05, 39190.21it/s]\n 90%|########9 | 1791359/2000000 [00:45<00:05, 39246.44it/s]\n 90%|########9 | 1795306/2000000 [00:46<00:05, 39305.59it/s]\n 90%|########9 | 1799254/2000000 [00:46<00:05, 39352.45it/s]\n 90%|######### | 1803193/2000000 [00:46<00:04, 39363.07it/s]\n 90%|######### | 1807147/2000000 [00:46<00:04, 39413.74it/s]\n 91%|######### | 1811089/2000000 [00:46<00:04, 39352.01it/s]\n 91%|######### | 1815025/2000000 [00:46<00:04, 39323.77it/s]\n 91%|######### | 1818958/2000000 [00:46<00:04, 39268.02it/s]\n 91%|#########1| 1822885/2000000 [00:46<00:04, 39245.07it/s]\n 91%|#########1| 1826810/2000000 [00:46<00:04, 39209.24it/s]\n 92%|#########1| 1830735/2000000 [00:46<00:04, 39212.92it/s]\n 92%|#########1| 1834671/2000000 [00:47<00:04, 39253.09it/s]\n 92%|#########1| 1838612/2000000 [00:47<00:04, 39298.41it/s]\n 92%|#########2| 1842559/2000000 [00:47<00:04, 39339.54it/s]\n 92%|#########2| 1846493/2000000 [00:47<00:03, 39315.46it/s]\n 93%|#########2| 1850437/2000000 [00:47<00:03, 39342.79it/s]\n 93%|#########2| 1854384/2000000 [00:47<00:03, 39380.15it/s]\n 93%|#########2| 1858323/2000000 [00:47<00:03, 39259.04it/s]\n 93%|#########3| 1862266/2000000 [00:47<00:03, 39301.40it/s]\n 93%|#########3| 1866203/2000000 [00:47<00:03, 39314.09it/s]\n 94%|#########3| 1870135/2000000 [00:47<00:03, 39289.64it/s]\n 94%|#########3| 1874064/2000000 [00:48<00:03, 39189.76it/s]\n 94%|#########3| 1877984/2000000 [00:48<00:03, 39130.85it/s]\n 94%|#########4| 1881898/2000000 [00:48<00:03, 39073.50it/s]\n 94%|#########4| 1885806/2000000 [00:48<00:02, 39051.60it/s]\n 94%|#########4| 1889733/2000000 [00:48<00:02, 39112.32it/s]\n 95%|#########4| 1893682/2000000 [00:48<00:02, 39217.46it/s]\n 95%|#########4| 1897640/2000000 [00:48<00:02, 39317.19it/s]\n 95%|#########5| 1901584/2000000 [00:48<00:02, 39344.84it/s]\n 95%|#########5| 1905525/2000000 [00:48<00:02, 39359.09it/s]\n 95%|#########5| 1909473/2000000 [00:48<00:02, 39388.51it/s]\n 96%|#########5| 1913412/2000000 [00:49<00:02, 39374.27it/s]\n 96%|#########5| 1917364/2000000 [00:49<00:02, 39409.89it/s]\n 96%|#########6| 1921323/2000000 [00:49<00:01, 39460.51it/s]\n 96%|#########6| 1925270/2000000 [00:49<00:01, 39436.67it/s]\n 96%|#########6| 1929214/2000000 [00:49<00:01, 39369.42it/s]\n 97%|#########6| 1933151/2000000 [00:49<00:01, 39261.99it/s]\n 97%|#########6| 1937078/2000000 [00:49<00:01, 39192.25it/s]\n 97%|#########7| 1941006/2000000 [00:49<00:01, 39216.23it/s]\n 97%|#########7| 1944928/2000000 [00:49<00:01, 39197.51it/s]\n 97%|#########7| 1948881/2000000 [00:49<00:01, 39291.59it/s]\n 98%|#########7| 1952828/2000000 [00:50<00:01, 39340.83it/s]\n 98%|#########7| 1956777/2000000 [00:50<00:01, 39380.19it/s]\n 98%|#########8| 1960716/2000000 [00:50<00:00, 39358.20it/s]\n 98%|#########8| 1964652/2000000 [00:50<00:00, 39354.61it/s]\n 98%|#########8| 1968599/2000000 [00:50<00:00, 39378.80it/s]\n 99%|#########8| 1972555/2000000 [00:50<00:00, 39425.92it/s]\n 99%|#########8| 1976498/2000000 [00:50<00:00, 39400.57it/s]\n 99%|#########9| 1980439/2000000 [00:50<00:00, 39366.81it/s]\n 99%|#########9| 1984377/2000000 [00:50<00:00, 39369.79it/s]\n 99%|#########9| 1988314/2000000 [00:50<00:00, 39323.47it/s]\n100%|#########9| 1992247/2000000 [00:51<00:00, 39288.03it/s]\n100%|#########9| 1996176/2000000 [00:51<00:00, 39184.83it/s]\n100%|##########| 2000000/2000000 [00:51<00:00, 39049.12it/s]\n2024-08-22 05:12:41,758 - INFO - allennlp.modules.token_embedders.embedding - Initializing pre-trained embedding layer\n2024-08-22 05:12:41,762 - INFO - allennlp.modules.token_embedders.embedding - Pretrained embeddings were found for 186 out of 192 tokens\n/usr/local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n  warnings.warn(\"dropout option adds dropout after all but last \"\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nVocabulary with namespaces:\n \tNon Padded Namespaces: {'*labels', '*tags'}\n \tNamespace: fasttext, Size: 192 \n \tNamespace: token_characters, Size: 62 \n \tNamespace: case, Size: 6 \n \tNamespace: xpos, Size: 38 \n \tNamespace: depchunk, Size: 20 \n \tNamespace: deprel, Size: 29 \n \tNamespace: genre, Size: 3 \n \tNamespace: parent_clauses, Size: 17 \n \tNamespace: upos, Size: 16 \n \tNamespace: s_type, Size: 3 \n \tNamespace: labels, Size: 2 \n\ncase\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: 'l', 3: 't', 4: 'o', 5: 'd'}\nxpos\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: 'NN', 3: 'ART', 4: 'ADV', 5: '$.', 6: 'APPR', 7: 'ADJA', 8: 'VAFIN', 9: 'ADJD', 10: '$,', 11: 'VVFIN', 12: 'VVPP', 13: 'VVINF', 14: 'NE', 15: 'PPER', 16: 'KON', 17: 'VMFIN', 18: 'PIS', 19: 'PROAV', 20: 'KOUS', 21: 'PIAT', 22: 'PRELS', 23: 'KOKOM', 24: 'PTKZU', 25: 'VAINF', 26: 'PWS', 27: 'PTKVZ', 28: 'APPRART', 29: 'VMINF', 30: 'CARD', 31: 'TRUNC', 32: 'PWAV', 33: 'PTKA', 34: 'PRF', 35: 'PDS', 36: 'PTKNEG', 37: 'PPOSAT'}\ndepchunk\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: '_', 3: 'Iacl', 4: 'Iadvcl', 5: 'Iccomp', 6: 'Icsubj', 7: 'Ixcomp', 8: 'Bacl', 9: 'Eacl', 10: 'Bxcomp', 11: 'Excomp', 12: 'Badvcl', 13: 'Bccomp', 14: 'Eccomp', 15: 'Bappos', 16: 'Eadvcl', 17: 'Bcsubj', 18: 'Ecsubj', 19: 'Eappos'}\ndeprel\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: 'advmod', 3: 'punct', 4: 'det', 5: 'nsubj', 6: 'case', 7: 'root', 8: 'obl', 9: 'nmod', 10: 'aux', 11: 'obj', 12: 'amod', 13: 'cc', 14: 'mark', 15: 'conj', 16: 'aux:pass', 17: 'nsubj:pass', 18: 'acl', 19: 'cop', 20: 'expl', 21: 'flat', 22: 'xcomp', 23: 'advcl', 24: 'ccomp', 25: 'compound:prt', 26: 'appos', 27: 'csubj:pass', 28: 'det:poss'}\ngenre\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: '_'}\nparent_clauses\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: '', 3: 'ccomp', 4: 'csubj:pass', 5: 'acl', 6: 'advcl|conj|acl', 7: 'advcl', 8: 'acl|xcomp', 9: 'xcomp|conj', 10: 'acl|ccomp', 11: 'conj', 12: 'conj|acl', 13: 'xcomp', 14: 'conj|advcl', 15: 'conj|acl|xcomp', 16: 'appos'}\nupos\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: 'NOUN', 3: 'VERB', 4: 'ADV', 5: 'PUNCT', 6: 'DET', 7: 'ADP', 8: 'PRON', 9: 'ADJ', 10: 'AUX', 11: 'PROPN', 12: 'CCONJ', 13: 'SCONJ', 14: 'PART', 15: 'NUM'}\ns_type\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: 'NoQ_NoSubj_NoConj_NoCop_NoAdvcl_NoAcl'}\nlabels\n  {0: 'O', 1: 'B'}\nDisrpt2021Baseline(\n  (feature_modules): ModuleDict(\n    (case_tags): Embedding(6, 3, padding_idx=0)\n    (cpos_tags): Embedding(38, 7, padding_idx=0)\n    (dep_chunk_tags): Embedding(20, 5, padding_idx=0)\n    (deprel_tags): Embedding(29, 6, padding_idx=0)\n    (document_depth): Identity()\n    (genre_tags): Embedding(3, 2, padding_idx=0)\n    (head_distances): Identity()\n    (parent_clauses): Embedding(17, 5, padding_idx=0)\n    (pos_tags): Embedding(16, 4, padding_idx=0)\n    (s_type): Embedding(3, 2, padding_idx=0)\n    (sentence_length): Identity()\n    (token_lengths): Identity()\n  )\n  (embedder): BasicTextFieldEmbedder(\n    (token_embedder_fasttext): Embedding()\n    (token_embedder_token_characters): TokenCharactersEncoder(\n      (_embedding): TimeDistributed(\n        (_module): Embedding()\n      )\n      (_encoder): TimeDistributed(\n        (_module): LstmSeq2VecEncoder(\n          (_module): LSTM(32, 64, batch_first=True, dropout=0.2, bidirectional=True)\n        )\n      )\n    )\n    (token_embedder_tokens): PretrainedTransformerMismatchedEmbedder(\n      (_matched_embedder): PretrainedTransformerEmbedder(\n        (transformer_model): BertModel(\n          (embeddings): BertEmbeddings(\n            (word_embeddings): Embedding(31102, 768, padding_idx=0)\n            (position_embeddings): Embedding(512, 768)\n            (token_type_embeddings): Embedding(2, 768)\n          )\n          (encoder): BertEncoder(\n            (layer): ModuleList(\n              (0): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (1): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (2): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (3): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (4): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (5): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (6): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (7): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (8): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (9): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (10): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (11): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n            )\n          )\n          (pooler): BertPooler(\n            (activation): Tanh()\n          )\n        )\n      )\n    )\n  )\n  (encoder): PytorchSeq2SeqWrapper(\n    (_module): StackedBidirectionalLstm(\n      (forward_layer_0): AugmentedLstm(\n        (cell): AugmentedLSTMCell(\n        )\n      )\n      (backward_layer_0): AugmentedLstm(\n        (cell): AugmentedLSTMCell(\n        )\n      )\n    )\n  )\n  (prev_sentence_encoder): LstmSeq2VecEncoder(\n    (_module): LSTM(1196, 100, batch_first=True, dropout=0.2, bidirectional=True)\n  )\n  (next_sentence_encoder): LstmSeq2VecEncoder(\n    (_module): LSTM(1196, 100, batch_first=True, dropout=0.2, bidirectional=True)\n  )\n  (label_projection_layer): TimeDistributed(\n  )\n)\n2024-08-22 05:12:44,573 - INFO - allennlp.training.optimizers - Done constructing parameter groups.\n2024-08-22 05:12:44,574 - INFO - allennlp.training.optimizers - Group 1: ['encoder._module.backward_layer_0.cell.input_linearity.weight', 'embedder.token_embedder_token_characters._encoder._module._module.weight_hh_l0_reverse', 'next_sentence_encoder._module.bias_ih_l0', 'prev_sentence_encoder._module.bias_hh_l0', 'feature_modules.case_tags.weight', 'encoder._module.forward_layer_0.cell.input_linearity.weight', 'prev_sentence_encoder._module.weight_ih_l0_reverse', 'embedder.token_embedder_token_characters._encoder._module._module.weight_ih_l0', 'next_sentence_encoder._module.weight_hh_l0_reverse', 'label_projection_layer._module.bias', 'embedder.token_embedder_token_characters._encoder._module._module.weight_hh_l0', 'prev_sentence_encoder._module.weight_hh_l0', 'prev_sentence_encoder._module.weight_ih_l0', 'embedder.token_embedder_token_characters._encoder._module._module.bias_ih_l0', 'feature_modules.genre_tags.weight', 'next_sentence_encoder._module.bias_hh_l0_reverse', 'feature_modules.cpos_tags.weight', 'embedder.token_embedder_token_characters._encoder._module._module.bias_hh_l0', 'feature_modules.deprel_tags.weight', 'next_sentence_encoder._module.weight_ih_l0', 'label_projection_layer._module.weight', 'feature_modules.pos_tags.weight', 'next_sentence_encoder._module.weight_ih_l0_reverse', 'prev_sentence_encoder._module.bias_ih_l0_reverse', 'next_sentence_encoder._module.bias_ih_l0_reverse', 'feature_modules.parent_clauses.weight', 'embedder.token_embedder_token_characters._embedding._module.weight', 'prev_sentence_encoder._module.bias_ih_l0', 'encoder._module.backward_layer_0.cell.state_linearity.weight', 'embedder.token_embedder_token_characters._encoder._module._module.bias_ih_l0_reverse', 'encoder._module.forward_layer_0.cell.state_linearity.weight', 'encoder._module.forward_layer_0.cell.state_linearity.bias', 'embedder.token_embedder_token_characters._encoder._module._module.bias_hh_l0_reverse', 'next_sentence_encoder._module.weight_hh_l0', 'prev_sentence_encoder._module.weight_hh_l0_reverse', 'encoder._module.backward_layer_0.cell.state_linearity.bias', 'prev_sentence_encoder._module.bias_hh_l0_reverse', 'feature_modules.dep_chunk_tags.weight', 'embedder.token_embedder_token_characters._encoder._module._module.weight_ih_l0_reverse', 'next_sentence_encoder._module.bias_hh_l0', 'feature_modules.s_type.weight'], {}\n2024-08-22 05:12:44,574 - INFO - allennlp.training.optimizers - Number of trainable parameters: 124725265\n/usr/local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n2024-08-22 05:12:44,595 - INFO - allennlp.training.gradient_descent_trainer - Beginning training.\n2024-08-22 05:12:44,595 - INFO - allennlp.training.gradient_descent_trainer - Epoch 0/0\n2024-08-22 05:12:44,596 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 2.8G\n2024-08-22 05:12:44,597 - INFO - allennlp.training.gradient_descent_trainer - Training\n\n  0%|          | 0/3 [00:00<?, ?it/s]/usr/local/lib/python3.10/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n2024-08-22 05:12:50,060 - INFO - allennlp.training.callbacks.console_logger - Batch inputs\n2024-08-22 05:12:50,060 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/fasttext/tokens (Shape: 4 x 25)\ntensor([[ 91,  10,  92,  ..., 104, 105,   2],\n        [ 20,  66,  67,  ...,   2,   0,   0],\n        [ 54,   3,  55,  ...,   0,   0,   0],\n        [ 36,  37,  38,  ...,   0,   0,   0]])\n2024-08-22 05:12:50,062 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/token_characters/token_characters (Shape: 4 x 25 x 21)\ntensor([[[31, 15,  0,  ...,  0,  0,  0],\n         [ 5,  4,  8,  ...,  0,  0,  0],\n         [ 2,  8, 17,  ...,  0,  0,  0],\n         ...,\n         [24, 13,  4,  ...,  0,  0,  0],\n         [ 3, 13,  8,  ...,  0,  0,  0],\n         [20,  0,  0,  ...,  0,  0,  0]],\n\n        [[30,  4,  0,  ...,  0,  0,  0],\n         [10, 26,  6,  ...,  0,  0,  0],\n         [10,  2,  6,  ...,  0,  0,  0],\n         ...,\n         [20,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[57, 19,  2,  ...,  0,  0,  0],\n         [23,  0,  0,  ...,  0,  0,  0],\n         [17,  2,  5,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[28,  7, 11,  ...,  0,  0,  0],\n         [47,  5,  2,  ...,  0,  0,  0],\n         [ 4,  5,  8,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]]])\n2024-08-22 05:12:50,064 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/token_ids (Shape: 4 x 31)\ntensor([[  102,   224,   207,  ...,  3682,   552,   103],\n        [  102,   233,  9647,  ...,     0,     0,     0],\n        [  102, 24026, 12979,  ...,     0,     0,     0],\n        [  102, 19590,   906,  ...,     0,     0,     0]])\n2024-08-22 05:12:50,065 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/mask (Shape: 4 x 25)\ntensor([[ True,  True,  True,  ...,  True,  True,  True],\n        [ True,  True,  True,  ...,  True, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False]])\n2024-08-22 05:12:50,066 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/type_ids (Shape: 4 x 31)\ntensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]])\n2024-08-22 05:12:50,067 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/wordpiece_mask (Shape: 4 x 31)\ntensor([[ True,  True,  True,  ...,  True,  True,  True],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False]])\n2024-08-22 05:12:50,067 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/offsets (Shape: 4 x 25 x 2)\ntensor([[[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [26, 27],\n         [28, 28],\n         [29, 29]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [26, 26],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  4],\n         [ 5,  5],\n         [ 6,  6],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  2],\n         [ 3,  4],\n         [ 5,  5],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]]])\n2024-08-22 05:12:50,069 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/fasttext/tokens (Shape: 4 x 31)\ntensor([[86, 32, 33,  ...,  0,  0,  0],\n        [19, 62,  4,  ...,  0,  0,  0],\n        [12, 40,  4,  ...,  8, 53,  2],\n        [12, 27, 28,  ...,  0,  0,  0]])\n2024-08-22 05:12:50,069 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/token_characters/token_characters (Shape: 4 x 31 x 16)\ntensor([[[36,  7,  8,  ...,  0,  0,  0],\n         [11,  5, 19,  ...,  0,  0,  0],\n         [ 2,  4,  0,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[28,  2,  6,  ...,  0,  0,  0],\n         [29, 26, 14,  ...,  0,  0,  0],\n         [10,  2,  6,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[24, 13, 21,  ...,  0,  0,  0],\n         [40,  6, 13,  ...,  0,  0,  0],\n         [10,  2,  6,  ...,  0,  0,  0],\n         ...,\n         [18, 13,  0,  ...,  0,  0,  0],\n         [ 2,  3,  8,  ...,  0,  0,  0],\n         [20,  0,  0,  ...,  0,  0,  0]],\n\n        [[24, 13, 21,  ...,  0,  0,  0],\n         [30,  5,  4,  ...,  0,  0,  0],\n         [11,  2, 12,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]]])\n2024-08-22 05:12:50,072 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/token_ids (Shape: 4 x 41)\ntensor([[  102,  2472,   175,  ...,     0,     0,     0],\n        [  102,   127, 22026,  ...,     0,     0,     0],\n        [  102,   193,   854,  ...,  3432,   552,   103],\n        [  102,   193,  3029,  ...,     0,     0,     0]])\n2024-08-22 05:12:50,073 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/mask (Shape: 4 x 31)\ntensor([[ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ...,  True,  True,  True],\n        [ True,  True,  True,  ..., False, False, False]])\n2024-08-22 05:12:50,073 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/type_ids (Shape: 4 x 41)\ntensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]])\n2024-08-22 05:12:50,074 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/wordpiece_mask (Shape: 4 x 41)\ntensor([[ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ...,  True,  True,  True],\n        [ True,  True,  True,  ..., False, False, False]])\n2024-08-22 05:12:50,075 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/offsets (Shape: 4 x 31 x 2)\ntensor([[[ 1,  3],\n         [ 4,  4],\n         [ 5,  5],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  3],\n         [ 4,  4],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [37, 37],\n         [38, 38],\n         [39, 39]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]]])\n2024-08-22 05:12:50,076 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/fasttext/tokens (Shape: 4 x 31)\ntensor([[169,  32,  33,  ...,   0,   0,   0],\n        [ 31,  80,  22,  ...,   0,   0,   0],\n        [ 19,  62,   4,  ...,   0,   0,   0],\n        [ 12,  40,   4,  ...,   8,  53,   2]])\n2024-08-22 05:12:50,077 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/token_characters/token_characters (Shape: 4 x 31 x 16)\ntensor([[[28,  2,  3,  ...,  0,  0,  0],\n         [11,  5, 19,  ...,  0,  0,  0],\n         [ 2,  4,  0,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[29,  2,  5,  ...,  0,  0,  0],\n         [35,  5,  3,  ..., 11,  2,  3],\n         [17,  2,  6,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[28,  2,  6,  ...,  0,  0,  0],\n         [29, 26, 14,  ...,  0,  0,  0],\n         [10,  2,  6,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[24, 13, 21,  ...,  0,  0,  0],\n         [40,  6, 13,  ...,  0,  0,  0],\n         [10,  2,  6,  ...,  0,  0,  0],\n         ...,\n         [18, 13,  0,  ...,  0,  0,  0],\n         [ 2,  3,  8,  ...,  0,  0,  0],\n         [20,  0,  0,  ...,  0,  0,  0]]])\n2024-08-22 05:12:50,079 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/token_ids (Shape: 4 x 41)\ntensor([[  102,  4041,   773,  ...,     0,     0,     0],\n        [  102, 14216,  2672,  ...,     0,     0,     0],\n        [  102,   127, 22026,  ...,     0,     0,     0],\n        [  102,   193,   854,  ...,  3432,   552,   103]])\n2024-08-22 05:12:50,080 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/mask (Shape: 4 x 31)\ntensor([[ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ...,  True,  True,  True]])\n2024-08-22 05:12:50,081 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/type_ids (Shape: 4 x 41)\ntensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]])\n2024-08-22 05:12:50,081 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/wordpiece_mask (Shape: 4 x 41)\ntensor([[ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ...,  True,  True,  True]])\n2024-08-22 05:12:50,082 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/offsets (Shape: 4 x 31 x 2)\ntensor([[[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  3],\n         [ 4,  4],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  3],\n         [ 4,  4],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [37, 37],\n         [38, 38],\n         [39, 39]]])\n2024-08-22 05:12:50,083 - INFO - allennlp.training.callbacks.console_logger - Field : \"batch_input/sentence_tokens\" : (Length 4 of type \"<class 'list'>\")\n2024-08-22 05:12:50,083 - INFO - allennlp.training.callbacks.console_logger - batch_input/case_tags (Shape: 4 x 25)\ntensor([[3, 2, 2,  ..., 3, 2, 4],\n        [3, 2, 2,  ..., 4, 0, 0],\n        [3, 4, 2,  ..., 0, 0, 0],\n        [3, 3, 2,  ..., 0, 0, 0]])\n2024-08-22 05:12:50,084 - INFO - allennlp.training.callbacks.console_logger - batch_input/cpos_tags (Shape: 4 x 25)\ntensor([[ 4,  8,  4,  ...,  2, 11,  5],\n        [15, 17,  4,  ...,  5,  0,  0],\n        [ 9, 10, 20,  ...,  0,  0,  0],\n        [14, 14, 11,  ...,  0,  0,  0]])\n2024-08-22 05:12:50,085 - INFO - allennlp.training.callbacks.console_logger - batch_input/dep_chunk_tags (Shape: 4 x 25)\ntensor([[ 2,  2,  2,  ...,  4,  9,  2],\n        [ 2,  2,  2,  ...,  2,  0,  0],\n        [ 2, 12,  4,  ...,  0,  0,  0],\n        [ 2,  2,  2,  ...,  0,  0,  0]])\n2024-08-22 05:12:50,085 - INFO - allennlp.training.callbacks.console_logger - batch_input/deprel_tags (Shape: 4 x 25)\ntensor([[ 2, 19,  2,  ...,  9, 23,  3],\n        [20, 10,  2,  ...,  3,  0,  0],\n        [ 7,  3, 14,  ...,  0,  0,  0],\n        [ 5, 21,  7,  ...,  0,  0,  0]])\n2024-08-22 05:12:50,086 - INFO - allennlp.training.callbacks.console_logger - batch_input/document_depth (Shape: 4 x 25)\ntensor([[0.9000, 0.9000, 0.9000,  ..., 0.9000, 0.9000, 0.9000],\n        [0.8571, 0.8571, 0.8571,  ..., 0.8571, 0.0000, 0.0000],\n        [0.8000, 0.8000, 0.8000,  ..., 0.0000, 0.0000, 0.0000],\n        [0.6667, 0.6667, 0.6667,  ..., 0.0000, 0.0000, 0.0000]])\n2024-08-22 05:12:50,088 - INFO - allennlp.training.callbacks.console_logger - batch_input/genre_tags (Shape: 4 x 25)\ntensor([[2, 2, 2,  ..., 2, 2, 2],\n        [2, 2, 2,  ..., 2, 0, 0],\n        [2, 2, 2,  ..., 0, 0, 0],\n        [2, 2, 2,  ..., 0, 0, 0]])\n2024-08-22 05:12:50,088 - INFO - allennlp.training.callbacks.console_logger - batch_input/head_distances (Shape: 4 x 25)\ntensor([[1.0986, 0.6931, 0.0000,  ..., 1.0986, 2.3979, 3.0445],\n        [1.6094, 1.3863, 1.0986,  ..., 2.8332, 0.0000, 0.0000],\n        [0.0000, 2.3026, 2.1972,  ..., 0.0000, 0.0000, 0.0000],\n        [0.6931, 0.0000, 1.0986,  ..., 0.0000, 0.0000, 0.0000]])\n2024-08-22 05:12:50,089 - INFO - allennlp.training.callbacks.console_logger - batch_input/parent_clauses (Shape: 4 x 25)\ntensor([[ 2,  2,  2,  ...,  6, 12,  2],\n        [ 2,  2,  2,  ...,  2,  0,  0],\n        [ 2,  7,  7,  ...,  0,  0,  0],\n        [ 2,  2,  2,  ...,  0,  0,  0]])\n2024-08-22 05:12:50,090 - INFO - allennlp.training.callbacks.console_logger - batch_input/pos_tags (Shape: 4 x 25)\ntensor([[ 4,  3,  4,  ...,  2,  3,  5],\n        [ 8, 10,  4,  ...,  5,  0,  0],\n        [ 9,  5, 13,  ...,  0,  0,  0],\n        [11, 11,  3,  ...,  0,  0,  0]])\n2024-08-22 05:12:50,091 - INFO - allennlp.training.callbacks.console_logger - batch_input/s_type (Shape: 4 x 25)\ntensor([[2, 2, 2,  ..., 2, 2, 2],\n        [2, 2, 2,  ..., 2, 0, 0],\n        [2, 2, 2,  ..., 0, 0, 0],\n        [2, 2, 2,  ..., 0, 0, 0]])\n2024-08-22 05:12:50,091 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence_length (Shape: 4 x 25)\ntensor([[3.2189, 3.2189, 3.2189,  ..., 3.2189, 3.2189, 3.2189],\n        [3.1355, 3.1355, 3.1355,  ..., 3.1355, 0.0000, 0.0000],\n        [2.6391, 2.6391, 2.6391,  ..., 0.0000, 0.0000, 0.0000],\n        [1.9459, 1.9459, 1.9459,  ..., 0.0000, 0.0000, 0.0000]])\n2024-08-22 05:12:50,092 - INFO - allennlp.training.callbacks.console_logger - batch_input/token_lengths (Shape: 4 x 25)\ntensor([[0.6931, 1.0986, 1.3863,  ..., 2.1972, 1.7918, 0.0000],\n        [0.6931, 1.7918, 1.9459,  ..., 0.0000, 0.0000, 0.0000],\n        [2.4849, 0.0000, 1.3863,  ..., 0.0000, 0.0000, 0.0000],\n        [1.7918, 1.9459, 1.6094,  ..., 0.0000, 0.0000, 0.0000]])\n2024-08-22 05:12:50,093 - INFO - allennlp.training.callbacks.console_logger - batch_input/labels (Shape: 4 x 25)\ntensor([[1, 0, 0,  ..., 0, 0, 0],\n        [1, 0, 0,  ..., 0, 0, 0],\n        [1, 0, 1,  ..., 0, 0, 0],\n        [1, 0, 0,  ..., 0, 0, 0]])\n\ntag_accuracy: 0.4203, span_precision: 0.1489, span_recall: 1.0000, span_f1: 0.2593, batch_loss: 0.7650, loss: 0.7650 ||:  33%|###3      | 1/3 [00:05<00:10,  5.50s/it]\ntag_accuracy: 0.6400, span_precision: 0.1667, span_recall: 0.6154, span_f1: 0.2623, batch_loss: 0.3780, loss: 0.5715 ||:  67%|######6   | 2/3 [00:10<00:05,  5.24s/it]\ntag_accuracy: 0.6713, span_precision: 0.1667, span_recall: 0.5333, span_f1: 0.2540, batch_loss: 0.3870, loss: 0.5100 ||: 100%|##########| 3/3 [00:13<00:00,  4.04s/it]\ntag_accuracy: 0.6713, span_precision: 0.1667, span_recall: 0.5333, span_f1: 0.2540, batch_loss: 0.3870, loss: 0.5100 ||: 100%|##########| 3/3 [00:13<00:00,  4.39s/it]\n2024-08-22 05:12:57,783 - INFO - allennlp.training.gradient_descent_trainer - Validating\n\n  0%|          | 0/3 [00:00<?, ?it/s]2024-08-22 05:12:58,598 - INFO - allennlp.training.callbacks.console_logger - Batch inputs\n2024-08-22 05:12:58,598 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/fasttext/tokens (Shape: 4 x 24)\ntensor([[176, 177, 178,  ..., 190, 191,   2],\n        [ 19, 146, 147,  ...,   0,   0,   0],\n        [ 12, 156, 157,  ...,   0,   0,   0],\n        [166, 167, 168,  ...,   0,   0,   0]])\n2024-08-22 05:12:58,599 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/token_characters/token_characters (Shape: 4 x 24 x 14)\ntensor([[[34,  3, 10,  ...,  0,  0,  0],\n         [ 4, 15,  0,  ...,  0,  0,  0],\n         [16, 13,  4,  ...,  0,  0,  0],\n         ...,\n         [41, 15, 12,  ...,  0,  0,  0],\n         [ 7,  3, 11,  ...,  0,  0,  0],\n         [20,  0,  0,  ...,  0,  0,  0]],\n\n        [[28,  2,  6,  ...,  0,  0,  0],\n         [18, 13,  4,  ...,  0,  0,  0],\n         [31,  8,  7,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[24, 13, 21,  ...,  0,  0,  0],\n         [10,  2, 16,  ...,  0,  0,  0],\n         [33,  7,  9,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[28,  7,  4,  ...,  0,  0,  0],\n         [ 6,  2,  5,  ...,  0,  0,  0],\n         [ 3,  5, 14,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]]])\n2024-08-22 05:12:58,601 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/token_ids (Shape: 4 x 29)\ntensor([[  102,   143,   224,  ..., 14762,   552,   103],\n        [  102,   127,  6218,  ...,     0,     0,     0],\n        [  102,   193,   232,  ...,     0,     0,     0],\n        [  102,   173,  5094,  ...,     0,     0,     0]])\n2024-08-22 05:12:58,602 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/mask (Shape: 4 x 24)\ntensor([[ True,  True,  True,  ...,  True,  True,  True],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False]])\n2024-08-22 05:12:58,603 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/type_ids (Shape: 4 x 29)\ntensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]])\n2024-08-22 05:12:58,603 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/wordpiece_mask (Shape: 4 x 29)\ntensor([[ True,  True,  True,  ...,  True,  True,  True],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False]])\n2024-08-22 05:12:58,604 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence/tokens/offsets (Shape: 4 x 24 x 2)\ntensor([[[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [25, 25],\n         [26, 26],\n         [27, 27]],\n\n        [[ 1,  1],\n         [ 2,  3],\n         [ 4,  4],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]]])\n2024-08-22 05:12:58,605 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/fasttext/tokens (Shape: 4 x 18)\ntensor([[166, 167, 168,  ...,   0,   0,   0],\n        [137, 138, 139,  ...,   0,   0,   0],\n        [ 19, 146, 147,  ...,  11, 155,   2],\n        [ 12, 156, 157,  ...,   0,   0,   0]])\n2024-08-22 05:12:58,606 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/token_characters/token_characters (Shape: 4 x 18 x 16)\ntensor([[[28,  7,  4,  ...,  0,  0,  0],\n         [ 6,  2,  5,  ...,  0,  0,  0],\n         [ 3,  5, 14,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[48,  2,  8,  ...,  0,  0,  0],\n         [ 9,  7, 19,  ...,  0,  0,  0],\n         [ 4,  5, 14,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[28,  2,  6,  ...,  0,  0,  0],\n         [18, 13,  4,  ...,  0,  0,  0],\n         [31,  8,  7,  ...,  0,  0,  0],\n         ...,\n         [16,  5,  8,  ...,  0,  0,  0],\n         [47, 26, 11,  ...,  0,  0,  0],\n         [20,  0,  0,  ...,  0,  0,  0]],\n\n        [[24, 13, 21,  ...,  0,  0,  0],\n         [10,  2, 16,  ...,  0,  0,  0],\n         [33,  7,  9,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]]])\n2024-08-22 05:12:58,609 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/token_ids (Shape: 4 x 25)\ntensor([[  102,   173,  5094,  ...,     0,     0,     0],\n        [  102,   951,   423,  ...,     0,     0,     0],\n        [  102,   127,  6218,  ..., 30938,   552,   103],\n        [  102,   193,   232,  ...,     0,     0,     0]])\n2024-08-22 05:12:58,609 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/mask (Shape: 4 x 18)\ntensor([[ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ...,  True,  True,  True],\n        [ True,  True,  True,  ..., False, False, False]])\n2024-08-22 05:12:58,610 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/type_ids (Shape: 4 x 25)\ntensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]])\n2024-08-22 05:12:58,610 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/wordpiece_mask (Shape: 4 x 25)\ntensor([[ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ...,  True,  True,  True],\n        [ True,  True,  True,  ..., False, False, False]])\n2024-08-22 05:12:58,611 - INFO - allennlp.training.callbacks.console_logger - batch_input/prev_sentence/tokens/offsets (Shape: 4 x 18 x 2)\ntensor([[[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  3],\n         [ 4,  4],\n         ...,\n         [20, 20],\n         [21, 22],\n         [23, 23]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]]])\n2024-08-22 05:12:58,612 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/fasttext/tokens (Shape: 4 x 24)\ntensor([[ 35,   0,   0,  ...,   0,   0,   0],\n        [ 12, 156, 157,  ...,   0,   0,   0],\n        [166, 167, 168,  ...,   0,   0,   0],\n        [176, 177, 178,  ..., 190, 191,   2]])\n2024-08-22 05:12:58,613 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/token_characters/token_characters (Shape: 4 x 24 x 21)\ntensor([[[32, 32, 28,  ..., 54, 32, 32],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[24, 13, 21,  ...,  0,  0,  0],\n         [10,  2, 16,  ...,  0,  0,  0],\n         [33,  7,  9,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[28,  7,  4,  ...,  0,  0,  0],\n         [ 6,  2,  5,  ...,  0,  0,  0],\n         [ 3,  5, 14,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0],\n         [ 0,  0,  0,  ...,  0,  0,  0]],\n\n        [[34,  3, 10,  ...,  0,  0,  0],\n         [ 4, 15,  0,  ...,  0,  0,  0],\n         [16, 13,  4,  ...,  0,  0,  0],\n         ...,\n         [41, 15, 12,  ...,  0,  0,  0],\n         [ 7,  3, 11,  ...,  0,  0,  0],\n         [20,  0,  0,  ...,  0,  0,  0]]])\n2024-08-22 05:12:58,615 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/token_ids (Shape: 4 x 29)\ntensor([[  102, 16249, 16249,  ...,     0,     0,     0],\n        [  102,   193,   232,  ...,     0,     0,     0],\n        [  102,   173,  5094,  ...,     0,     0,     0],\n        [  102,   143,   224,  ..., 14762,   552,   103]])\n2024-08-22 05:12:58,616 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/mask (Shape: 4 x 24)\ntensor([[ True, False, False,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ...,  True,  True,  True]])\n2024-08-22 05:12:58,617 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/type_ids (Shape: 4 x 29)\ntensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]])\n2024-08-22 05:12:58,617 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/wordpiece_mask (Shape: 4 x 29)\ntensor([[ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ...,  True,  True,  True]])\n2024-08-22 05:12:58,618 - INFO - allennlp.training.callbacks.console_logger - batch_input/next_sentence/tokens/offsets (Shape: 4 x 24 x 2)\ntensor([[[ 1,  9],\n         [ 0,  0],\n         [ 0,  0],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [ 0,  0],\n         [ 0,  0],\n         [ 0,  0]],\n\n        [[ 1,  1],\n         [ 2,  2],\n         [ 3,  3],\n         ...,\n         [25, 25],\n         [26, 26],\n         [27, 27]]])\n2024-08-22 05:12:58,619 - INFO - allennlp.training.callbacks.console_logger - Field : \"batch_input/sentence_tokens\" : (Length 4 of type \"<class 'list'>\")\n2024-08-22 05:12:58,619 - INFO - allennlp.training.callbacks.console_logger - batch_input/case_tags (Shape: 4 x 24)\ntensor([[3, 2, 2,  ..., 3, 2, 4],\n        [3, 2, 3,  ..., 0, 0, 0],\n        [3, 2, 3,  ..., 0, 0, 0],\n        [3, 2, 2,  ..., 0, 0, 0]])\n2024-08-22 05:12:58,620 - INFO - allennlp.training.callbacks.console_logger - batch_input/cpos_tags (Shape: 4 x 24)\ntensor([[16,  4, 17,  ...,  2, 11,  5],\n        [ 3,  7,  2,  ...,  0,  0,  0],\n        [ 6,  3,  2,  ...,  0,  0,  0],\n        [35, 11, 36,  ...,  0,  0,  0]])\n2024-08-22 05:12:58,621 - INFO - allennlp.training.callbacks.console_logger - batch_input/dep_chunk_tags (Shape: 4 x 24)\ntensor([[ 2,  2,  2,  ...,  3, 11,  2],\n        [ 2,  2,  2,  ...,  0,  0,  0],\n        [ 2,  2,  2,  ...,  0,  0,  0],\n        [ 2,  2,  2,  ...,  0,  0,  0]])\n2024-08-22 05:12:58,621 - INFO - allennlp.training.callbacks.console_logger - batch_input/deprel_tags (Shape: 4 x 24)\ntensor([[13,  2, 10,  ..., 15, 18,  3],\n        [ 4, 12,  5,  ...,  0,  0,  0],\n        [ 6,  4,  8,  ...,  0,  0,  0],\n        [ 5,  7,  2,  ...,  0,  0,  0]])\n2024-08-22 05:12:58,622 - INFO - allennlp.training.callbacks.console_logger - batch_input/document_depth (Shape: 4 x 24)\ntensor([[0.9091, 0.9091, 0.9091,  ..., 0.9091, 0.9091, 0.9091],\n        [0.8750, 0.8750, 0.8750,  ..., 0.0000, 0.0000, 0.0000],\n        [0.8889, 0.8889, 0.8889,  ..., 0.0000, 0.0000, 0.0000],\n        [0.9000, 0.9000, 0.9000,  ..., 0.0000, 0.0000, 0.0000]])\n2024-08-22 05:12:58,623 - INFO - allennlp.training.callbacks.console_logger - batch_input/genre_tags (Shape: 4 x 24)\ntensor([[2, 2, 2,  ..., 2, 2, 2],\n        [2, 2, 2,  ..., 0, 0, 0],\n        [2, 2, 2,  ..., 0, 0, 0],\n        [2, 2, 2,  ..., 0, 0, 0]])\n2024-08-22 05:12:58,624 - INFO - allennlp.training.callbacks.console_logger - batch_input/head_distances (Shape: 4 x 24)\ntensor([[2.3979, 2.3026, 2.1972,  ..., 1.0986, 2.5649, 2.4849],\n        [0.6931, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n        [0.6931, 0.0000, 1.9459,  ..., 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.6931, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])\n2024-08-22 05:12:58,625 - INFO - allennlp.training.callbacks.console_logger - batch_input/parent_clauses (Shape: 4 x 24)\ntensor([[ 2,  2,  2,  ...,  8, 13,  2],\n        [ 2,  2,  2,  ...,  0,  0,  0],\n        [ 2,  2,  2,  ...,  0,  0,  0],\n        [ 2,  2,  2,  ...,  0,  0,  0]])\n2024-08-22 05:12:58,625 - INFO - allennlp.training.callbacks.console_logger - batch_input/pos_tags (Shape: 4 x 24)\ntensor([[12,  4, 10,  ...,  2,  3,  5],\n        [ 6,  9,  2,  ...,  0,  0,  0],\n        [ 7,  6,  2,  ...,  0,  0,  0],\n        [ 8,  3,  4,  ...,  0,  0,  0]])\n2024-08-22 05:12:58,626 - INFO - allennlp.training.callbacks.console_logger - batch_input/s_type (Shape: 4 x 24)\ntensor([[2, 2, 2,  ..., 2, 2, 2],\n        [2, 2, 2,  ..., 0, 0, 0],\n        [2, 2, 2,  ..., 0, 0, 0],\n        [2, 2, 2,  ..., 0, 0, 0]])\n2024-08-22 05:12:58,626 - INFO - allennlp.training.callbacks.console_logger - batch_input/sentence_length (Shape: 4 x 24)\ntensor([[3.1781, 3.1781, 3.1781,  ..., 3.1781, 3.1781, 3.1781],\n        [2.8904, 2.8904, 2.8904,  ..., 0.0000, 0.0000, 0.0000],\n        [2.6391, 2.6391, 2.6391,  ..., 0.0000, 0.0000, 0.0000],\n        [1.7918, 1.7918, 1.7918,  ..., 0.0000, 0.0000, 0.0000]])\n2024-08-22 05:12:58,627 - INFO - allennlp.training.callbacks.console_logger - batch_input/token_lengths (Shape: 4 x 24)\ntensor([[1.0986, 0.6931, 1.3863,  ..., 1.7918, 1.7918, 0.0000],\n        [1.0986, 2.3026, 1.3863,  ..., 0.0000, 0.0000, 0.0000],\n        [1.0986, 1.0986, 1.9459,  ..., 0.0000, 0.0000, 0.0000],\n        [1.0986, 1.7918, 1.6094,  ..., 0.0000, 0.0000, 0.0000]])\n2024-08-22 05:12:58,628 - INFO - allennlp.training.callbacks.console_logger - batch_input/labels (Shape: 4 x 24)\ntensor([[1, 0, 0,  ..., 0, 0, 0],\n        [1, 0, 0,  ..., 0, 0, 0],\n        [1, 0, 0,  ..., 0, 0, 0],\n        [1, 0, 0,  ..., 0, 0, 0]])\n\ntag_accuracy: 0.9032, span_precision: 0.0000, span_recall: 0.0000, span_f1: 0.0000, batch_loss: 0.4510, loss: 0.4510 ||:  33%|###3      | 1/3 [00:00<00:01,  1.19it/s]\ntag_accuracy: 0.9027, span_precision: 0.0000, span_recall: 0.0000, span_f1: 0.0000, batch_loss: 0.4173, loss: 0.4342 ||:  67%|######6   | 2/3 [00:01<00:00,  1.19it/s]\ntag_accuracy: 0.8943, span_precision: 0.0000, span_recall: 0.0000, span_f1: 0.0000, batch_loss: 0.6441, loss: 0.5041 ||: 100%|##########| 3/3 [00:02<00:00,  1.64it/s]\ntag_accuracy: 0.8943, span_precision: 0.0000, span_recall: 0.0000, span_f1: 0.0000, batch_loss: 0.6441, loss: 0.5041 ||: 100%|##########| 3/3 [00:02<00:00,  1.49it/s]\n2024-08-22 05:12:59,806 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n2024-08-22 05:12:59,806 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.510  |     0.504\n2024-08-22 05:12:59,806 - INFO - allennlp.training.callbacks.console_logger - span_f1            |     0.254  |     0.000\n2024-08-22 05:12:59,806 - INFO - allennlp.training.callbacks.console_logger - span_precision     |     0.167  |     0.000\n2024-08-22 05:12:59,806 - INFO - allennlp.training.callbacks.console_logger - span_recall        |     0.533  |     0.000\n2024-08-22 05:12:59,806 - INFO - allennlp.training.callbacks.console_logger - tag_accuracy       |     0.671  |     0.894\n2024-08-22 05:12:59,807 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  2817.953  |       N/A\n2024-08-22 05:13:01,393 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:16.797127\n  \"best_epoch\": 0,\n  \"peak_worker_0_memory_MB\": 2817.953125,\n  \"training_duration\": \"0:00:15.210051\",\n  \"epoch\": 0,\n  \"training_tag_accuracy\": 0.6713286713286714,\n  \"training_span_precision\": 0.16666666666666632,\n  \"training_span_recall\": 0.5333333333333298,\n  \"training_span_f1\": 0.2539682539682169,\n  \"training_loss\": 0.5099876622358958,\n  \"training_worker_0_memory_MB\": 2817.953125,\n  \"validation_tag_accuracy\": 0.8943089430894309,\n  \"validation_span_precision\": 0.0,\n  \"validation_span_recall\": 0.0,\n  \"validation_span_f1\": 0.0,\n  \"validation_loss\": 0.5041356384754181,\n  \"best_validation_tag_accuracy\": 0.8943089430894309,\n  \"best_validation_span_precision\": 0.0,\n  \"best_validation_span_recall\": 0.0,\n  \"best_validation_span_f1\": 0.0,\n  \"best_validation_loss\": 0.5041356384754181\n}\n2024-08-22 05:13:01,394 - INFO - allennlp.models.archival - archiving weights and vocabulary to models/deu.rst.pcc_seg_bert_baseline_ft/model.tar.gz\n\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n# Testing on deu.rst.pcc\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n\n2024-08-22 05:13:56,587 - INFO - allennlp.common.plugins - Plugin allennlp_models available\n2024-08-22 05:13:57,246 - ERROR - allennlp.common.plugins - Plugin gucorpling_models could not be loaded: No module named 'flair'\n2024-08-22 05:13:57,246 - INFO - allennlp.models.archival - loading archive file models/deu.rst.pcc_seg_bert_baseline_ft/model.tar.gz\n2024-08-22 05:13:57,247 - INFO - allennlp.models.archival - extracting archive file models/deu.rst.pcc_seg_bert_baseline_ft/model.tar.gz to temp dir /tmp/tmpq56s1nxb\n2024-08-22 05:14:06,826 - INFO - allennlp.data.vocabulary - Loading token dictionary from /tmp/tmpq56s1nxb/vocabulary.\n/usr/local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n  warnings.warn(\"dropout option adds dropout after all but last \"\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nVocabulary with namespaces:\n \tNon Padded Namespaces: {'*labels', '*tags'}\n \tNamespace: case, Size: 6 \n \tNamespace: depchunk, Size: 20 \n \tNamespace: deprel, Size: 29 \n \tNamespace: fasttext, Size: 192 \n \tNamespace: genre, Size: 3 \n \tNamespace: labels, Size: 2 \n \tNamespace: parent_clauses, Size: 17 \n \tNamespace: s_type, Size: 3 \n \tNamespace: token_characters, Size: 62 \n \tNamespace: upos, Size: 16 \n \tNamespace: xpos, Size: 38 \n\ncase\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: 'l', 3: 't', 4: 'o', 5: 'd'}\ndepchunk\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: '_', 3: 'Iacl', 4: 'Iadvcl', 5: 'Iccomp', 6: 'Icsubj', 7: 'Ixcomp', 8: 'Bacl', 9: 'Eacl', 10: 'Bxcomp', 11: 'Excomp', 12: 'Badvcl', 13: 'Bccomp', 14: 'Eccomp', 15: 'Bappos', 16: 'Eadvcl', 17: 'Bcsubj', 18: 'Ecsubj', 19: 'Eappos'}\ndeprel\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: 'advmod', 3: 'punct', 4: 'det', 5: 'nsubj', 6: 'case', 7: 'root', 8: 'obl', 9: 'nmod', 10: 'aux', 11: 'obj', 12: 'amod', 13: 'cc', 14: 'mark', 15: 'conj', 16: 'aux:pass', 17: 'nsubj:pass', 18: 'acl', 19: 'cop', 20: 'expl', 21: 'flat', 22: 'xcomp', 23: 'advcl', 24: 'ccomp', 25: 'compound:prt', 26: 'appos', 27: 'csubj:pass', 28: 'det:poss'}\ngenre\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: '_'}\nlabels\n  {0: 'O', 1: 'B'}\nparent_clauses\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: '', 3: 'ccomp', 4: 'csubj:pass', 5: 'acl', 6: 'advcl|conj|acl', 7: 'advcl', 8: 'acl|xcomp', 9: 'xcomp|conj', 10: 'acl|ccomp', 11: 'conj', 12: 'conj|acl', 13: 'xcomp', 14: 'conj|advcl', 15: 'conj|acl|xcomp', 16: 'appos'}\ns_type\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: 'NoQ_NoSubj_NoConj_NoCop_NoAdvcl_NoAcl'}\nupos\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: 'NOUN', 3: 'VERB', 4: 'ADV', 5: 'PUNCT', 6: 'DET', 7: 'ADP', 8: 'PRON', 9: 'ADJ', 10: 'AUX', 11: 'PROPN', 12: 'CCONJ', 13: 'SCONJ', 14: 'PART', 15: 'NUM'}\nxpos\n  {0: '@@PADDING@@', 1: '@@UNKNOWN@@', 2: 'NN', 3: 'ART', 4: 'ADV', 5: '$.', 6: 'APPR', 7: 'ADJA', 8: 'VAFIN', 9: 'ADJD', 10: '$,', 11: 'VVFIN', 12: 'VVPP', 13: 'VVINF', 14: 'NE', 15: 'PPER', 16: 'KON', 17: 'VMFIN', 18: 'PIS', 19: 'PROAV', 20: 'KOUS', 21: 'PIAT', 22: 'PRELS', 23: 'KOKOM', 24: 'PTKZU', 25: 'VAINF', 26: 'PWS', 27: 'PTKVZ', 28: 'APPRART', 29: 'VMINF', 30: 'CARD', 31: 'TRUNC', 32: 'PWAV', 33: 'PTKA', 34: 'PRF', 35: 'PDS', 36: 'PTKNEG', 37: 'PPOSAT'}\nDisrpt2021Baseline(\n  (feature_modules): ModuleDict(\n    (case_tags): Embedding(6, 3, padding_idx=0)\n    (cpos_tags): Embedding(38, 7, padding_idx=0)\n    (dep_chunk_tags): Embedding(20, 5, padding_idx=0)\n    (deprel_tags): Embedding(29, 6, padding_idx=0)\n    (document_depth): Identity()\n    (genre_tags): Embedding(3, 2, padding_idx=0)\n    (head_distances): Identity()\n    (parent_clauses): Embedding(17, 5, padding_idx=0)\n    (pos_tags): Embedding(16, 4, padding_idx=0)\n    (s_type): Embedding(3, 2, padding_idx=0)\n    (sentence_length): Identity()\n    (token_lengths): Identity()\n  )\n  (embedder): BasicTextFieldEmbedder(\n    (token_embedder_fasttext): Embedding()\n    (token_embedder_token_characters): TokenCharactersEncoder(\n      (_embedding): TimeDistributed(\n        (_module): Embedding()\n      )\n      (_encoder): TimeDistributed(\n        (_module): LstmSeq2VecEncoder(\n          (_module): LSTM(32, 64, batch_first=True, dropout=0.2, bidirectional=True)\n        )\n      )\n    )\n    (token_embedder_tokens): PretrainedTransformerMismatchedEmbedder(\n      (_matched_embedder): PretrainedTransformerEmbedder(\n        (transformer_model): BertModel(\n          (embeddings): BertEmbeddings(\n            (word_embeddings): Embedding(31102, 768, padding_idx=0)\n            (position_embeddings): Embedding(512, 768)\n            (token_type_embeddings): Embedding(2, 768)\n          )\n          (encoder): BertEncoder(\n            (layer): ModuleList(\n              (0): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (1): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (2): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (3): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (4): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (5): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (6): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (7): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (8): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (9): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (10): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n              (11): BertLayer(\n                (attention): BertAttention(\n                  (self): BertSelfAttention(\n                  )\n                  (output): BertSelfOutput(\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BertOutput(\n                )\n              )\n            )\n          )\n          (pooler): BertPooler(\n            (activation): Tanh()\n          )\n        )\n      )\n    )\n  )\n  (encoder): PytorchSeq2SeqWrapper(\n    (_module): StackedBidirectionalLstm(\n      (forward_layer_0): AugmentedLstm(\n        (cell): AugmentedLSTMCell(\n        )\n      )\n      (backward_layer_0): AugmentedLstm(\n        (cell): AugmentedLSTMCell(\n        )\n      )\n    )\n  )\n  (prev_sentence_encoder): LstmSeq2VecEncoder(\n    (_module): LSTM(1196, 100, batch_first=True, dropout=0.2, bidirectional=True)\n  )\n  (next_sentence_encoder): LstmSeq2VecEncoder(\n    (_module): LSTM(1196, 100, batch_first=True, dropout=0.2, bidirectional=True)\n  )\n  (label_projection_layer): TimeDistributed(\n  )\n)\n2024-08-22 05:14:10,242 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmpq56s1nxb\n2024-08-22 05:14:10,304 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'case'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n2024-08-22 05:14:10,304 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'xpos'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n2024-08-22 05:14:10,305 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'depchunk'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n2024-08-22 05:14:10,305 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'deprel'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n2024-08-22 05:14:10,305 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'genre'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n2024-08-22 05:14:10,305 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'parent_clauses'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n2024-08-22 05:14:10,305 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'upos'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n2024-08-22 05:14:10,305 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 's_type'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\nRemoving model files...\n\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n# Scoring on deu.rst.pcc\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n\nFile: deu.rst.pcc_test.conllu\no Total tokens: 97\no Gold EDUs: 10\no Predicted EDUs: 0\no Precision: 0\no Recall: 0.0\no F-Score: 0\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!bash seg_scripts/single_corpus_train_and_test_ft.sh deu.rst.pcc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266ca34b",
   "metadata": {},
   "source": [
    "### Step 39"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872563e3",
   "metadata": {},
   "source": [
    "Execution Start time: 05:14:24, End time: 05:14:24 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22df4e1a",
   "metadata": {},
   "source": [
    "**Thought**: The script has successfully run, and we have obtained the results for the \"deu.rst.pcc\" dataset. The output provides the total number of tokens, the number of gold and predicted EDUs, and the F-score. Let's format this information into the required JSON structure and submit the answer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb6cf07",
   "metadata": {},
   "source": [
    "**Action (submit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823d3015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{total_tokens: 97, gold_edus: 10, predicted_edus: 0, f_score: 0}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit {\"total_tokens\": 97, \"gold_edus\": 10, \"predicted_edus\": 0, \"f_score\": 0}"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
