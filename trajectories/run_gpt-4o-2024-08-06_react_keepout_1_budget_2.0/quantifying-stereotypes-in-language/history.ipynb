{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5e5a6f9",
   "metadata": {},
   "source": [
    "### Query\n",
    "Train a model starting from distilbert-base-uncased on the dataset, then predict stereotype scores on the test set. Report the prediction classes of the first 5 examples as a list of strings.\n",
    "\n",
    "Additional instructions:\n",
    "1. Load only the first 10 rows of each set in the dataset\n",
    "2. Train for only one epoch.\n",
    "3. Use batch_size=2, max length=50, lr=0.00001\n",
    "\n",
    "Git repository: https://github.com/nlply/quantifying-stereotypes-in-language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923822a8",
   "metadata": {},
   "source": [
    "### Prompt\n",
    "<details>\n",
    "[{'role': 'system', 'content': 'Interact in a jupyter notebook to solve the request.\\nOutput a thought and an action.\\nThought can reason about the current situation, and action allows you to interact with the Jupyter notebook. There are three types of actions:\\n(1) execute, which executes a notebook cell with the command under the \"content\" field. This could be a bash command (beginning with !), a python command, or magic command (beginning with %).\\n(2) edit, which allows you to replace existing lines in a file with new lines. To replace lines, you will need to provide the lines as they appear before the replacement, and the lines to replace them with.\\nIn the \"content\" field, format should be ```{filename}\\n<<<BEFORE_EDIT>>>\\nlines before edit\\nlines before edit\\n<<<AFTER_EDIT>>>\\nlines after edit\\nlines after edit```\\n  - Include the entire lines WITH indentation. After editing, make sure indentation is correct.\\n(3) submit, which submits the final answer (or null if there\\'s no answer to submit), formatted as JSON (make sure to use double quotes for keys and values).\\nAlways output exactly one \"thought\" and one \"action\", and encapsulate the content with <content>...</content> tags.\\nI will provide the cell output (observation) for you.\\n\\nTips:\\n- When a github repository is given, you should start by cloning it. You can read the readme file and browse the repo for installation and usage instructions.\\n- Avoid using conda, instead use pip for package management.\\n- Each execution has a 5-minute timeout. After this period, you can decide whether to interrupt the kernel or continue waiting.\\n  You should interrupt if: (1) you are prompted with an interactive message (e.g., \"Do you want to continue? [Y/n]\"). In that case, you can bypass it by executing echo \"y\" | command, or (2) the execution is taking longer than expected because the wrong task is being executed, such as a different model or incorrect number of epochs.\\n  You should continue to wait if the current execution is correct and proceeding as expected.\\n\\nHere are some examples.\\nGit repository: EleutherAI/lm-evaluation-harness\\nQuery: Evaluate EleutherAI/gpt-j-6B on hellaswag. What\\'s the score?\\nThought: First, I need to clone the GitHub repository \\'EleutherAI/lm-evaluation-harness\\'.\\nAction: execute\\n<content>\\n!git clone https://github.com/EleutherAI/lm-evaluation-harness.git\\n</content>\\nObservation: ```\\nCloning into \\'lm-evaluation-harness\\'...\\n```\\nThought: The repository has been cloned. Next, I should check the README file for installation and usage instructions to proceed with evaluating EleutherAI/gpt-j-6B on hellaswag.\\nAction: execute\\n<content>\\n!cat lm-evaluation-harness/README.md\\n</content>\\nObservation: ```# Language Model Evaluation Harness ... truncated ...\\n```\\nThought: The README provides detailed instructions for installing the `lm-eval` package and evaluating models like GPT-J-6B on tasks like hellaswag. I need to install the package and its dependencies first.\\nAction: execute\\n<content>\\n!pip install -e lm-evaluation-harness/.\\n</content>\\nObservation: ```\\nObtaining file:///home/nora/lm-evaluation-harness\\n  Installing build dependencies ... truncated ...\\n```\\nThought: The `lm-eval` package and its dependencies have been installed successfully. Now, I can proceed with evaluating the EleutherAI/gpt-j-6B model on the hellaswag task as per the instructions in the README.\\nAction: execute\\n<content>\\n!lm_eval --model hf --model_args pretrained=EleutherAI/gpt-j-6B --tasks hellaswag --device cuda:0 --batch_size 8\\n</content>\\nObservation: ```\\n{\\n  \"hellaswag\": {\\n    \"acc\": 0.24603658536585363,\\n    \"acc_norm\": 0.34436059706446426,\\n  },  ... truncated ...\\n```\\nThought: The `lm-eval` was executed successfully, providing the metrics I needed. The `acc` metric is 0.2460. The `acc_norm` metric is 0.3444\\nAction: submit\\n<content>\\n{\"acc\": 0.2460, \"acc_norm\": 0.3444}\\n</content>\\n\\nQuery: use pandas to read the file \\'data.csv\\' and display the first 2 rows.\\nThought: First, I need to read the file \\'data.csv\\' using the pandas library.\\nAction: execute\\n<content>\\nimport pandas as pd\\npd.read_csv(\\'data.csv\\').head()\\n</content>\\nObservation: ```\\n    name\\n  0  Alice\\n   1    Bob\\n  2  Charlie\\n```\\nThought: The file \\'data.csv\\' has been read successfully and the first 2 rows have been displayed.\\nAction: submit\\n<content>\\n[\"Alice\", \"Bob\"]\\n</content>\\n\\nQuery: Change the model name in the \\'config.json\\' file from \\'roberta-base\\' to \\'roberta-large\\'.\\nThought: First, I need to read the \\'config.json\\' file to find the location where I can replace the model name.\\nAction: execute\\n<content>\\n!cat config.json\\n</content>\\nObservation: ```\\n{\\n    \"model\": \"roberta-base\",\\n    \"batch_size\": 8,\\n    \"epochs\": 10\\n}\\n```\\nThought: The file has been read successfully. Now, I can replace the model name in the \\'config.json\\' file.\\nAction: edit\\n<content>\\nconfig.json\\n<<<BEFORE_EDIT>>>\\n{\\n    \"model\": \"roberta-base\",\\n    \"batch_size\": 8,\\n<<<AFTER_EDIT>>>\\n{\\n    \"model\": \"roberta-large\",\\n    \"batch_size\": 8,\\n</content>\\nObservation: \"editing file config.json\"\\nThought: The model name has been replaced in the \\'config.json\\' file.\\nAction: submit\\n<content>\\nnull\\n</content>\\n'}, {'role': 'user', 'content': 'Query: Train a model starting from distilbert-base-uncased on the dataset, then predict stereotype scores on the test set. Report the prediction classes of the first 5 examples as a list of strings.\\n\\nAdditional instructions:\\n1. Load only the first 10 rows of each set in the dataset\\n2. Train for only one epoch.\\n3. Use batch_size=2, max length=50, lr=0.00001\\n\\nGit repository: https://github.com/nlply/quantifying-stereotypes-in-language'}]\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ee8db5",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9ebb6c",
   "metadata": {},
   "source": [
    "Execution Start time: 19:41:26, End time: 19:41:27 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3164d1",
   "metadata": {},
   "source": [
    "**Thought**: First, I need to clone the GitHub repository 'nlply/quantifying-stereotypes-in-language' to access the dataset and relevant scripts for training and prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e26a1e",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c0eace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cloning into 'quantifying-stereotypes-in-language'...\nremote: Enumerating objects: 17, done.\u001b[K\nremote: Counting objects: 100% (17/17)\u001b[K\nremote: Counting objects: 100% (17/17), done.\u001b[K\nremote: Compressing objects: 100% (13/13)\u001b[K\nremote: Compressing objects: 100% (13/13), done.\u001b[K\nremote: Total 17 (delta 2), reused 14 (delta 2), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (17/17)\nReceiving objects: 100% (17/17), 102.27 KiB | 25.57 MiB/s, done.\nResolving deltas: 100% (2/2)\nResolving deltas: 100% (2/2), done.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!git clone https://github.com/nlply/quantifying-stereotypes-in-language.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52903860",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4863f9a0",
   "metadata": {},
   "source": [
    "Execution Start time: 19:41:28, End time: 19:41:28 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4607693b",
   "metadata": {},
   "source": [
    "**Thought**: The repository has been cloned successfully. Next, I should check the README file or other documentation in the repository for instructions on how to train a model and make predictions using the provided dataset and scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadda04a",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0488b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# Quantifying Stereotypes in Language\n\n> Code for paper ***[Quantifying Stereotypes in Language](https://arxiv.org/abs/2401.15535)*** (accepted by **[EACL 2024](https://2024.eacl.org/)**)\n\n## Dataset Details\n\n| train | test | val |\n|-------|------|-----|\n| 2,260 | 358  | 358 |\n\n## Training Models\n\nYou can run the following script to train the models:\n\n``` \npython train.py  \\\n--mode train \\\n--pre_trained_model_name_or_path [bert-base-uncased, distilbert-base-uncased, roberta-base] \\\n--train_path data/train.csv \\\n--val_path data/val.csv \\\n--test_path data/test.csv \\\n--lr 0.00001 \\\n--max_len 50 \\\n--max_epochs 30 \\\n--batch_size 128 \\\n--model_saving_path models\n```\n\nThe weights are saved to the [models](models) folder.\n\n\n# Predict Stereotype Scores\nYou can run the following script to predict stereotype scores:\n```\npython train.py \\\n--mode=predict \\\n--pre_trained_model_name_or_path=models/bert-base-uncased \\\n--predict_data_path=data/predict/cp_sentence.csv \\\n--test_saving_path=results/bert_cp_sentence_results.csv\n```\nThe prediction results are saved to the [results](results) folder.\n\n## The Trained Weights of the Models\n\nYou can download the pre-trained weights for use directly from [huggingface](https://huggingface.co/):\n\nFor the BERT model\n```python\n# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"lauyon/quantifying-stereotype-bert\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"lauyon/quantifying-stereotype-bert\")\n```\n\nFor the DistilBERT model\n```python\n# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"lauyon/quantifying-stereotype-distilbert\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"lauyon/quantifying-stereotype-distilbert\")\n```\n\nFor the RoBERTa model\n```python\n# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"lauyon/quantifying-stereotype-roberta\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"lauyon/quantifying-stereotype-roberta\")\n```\n\nIf this work has helped you in any way, please cite it by the following:\n```bibtex\n@article{liu2024quantifying,\n  title={Quantifying Stereotypes in Language},\n  author={Liu, Yang},\n  journal={arXiv preprint arXiv:2401.15535},\n  year={2024}\n}\n```\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!cat quantifying-stereotypes-in-language/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa96e284",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101e28ee",
   "metadata": {},
   "source": [
    "Execution Start time: 19:41:32, End time: 19:41:32 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f6c7a1",
   "metadata": {},
   "source": [
    "**Thought**: The README provides instructions for training a model and predicting stereotype scores. I need to modify the training script to load only the first 10 rows of each dataset and train for one epoch with the specified parameters. Then, I will use the trained model to predict stereotype scores on the test set and report the prediction classes of the first 5 examples.\n",
    "\n",
    "First, I will inspect the `train.py` script to understand how to modify it to load only the first 10 rows of each dataset and set the training parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b3e2c6",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d978024e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import logging\nimport os\n\nimport math\nfrom scipy import stats\nimport numpy as np\nfrom tqdm import tqdm\nimport torch.optim as optim\nfrom torch import nn, Tensor\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom argparse import ArgumentParser\nimport csv\nimport pandas as pd\n\ntorch.manual_seed(0)\n\n\ndef padding(text, pad, max_len=50):\n    return text if len(text) >= max_len else (text + [pad] * (max_len - len(text)))\n\n\ndef encode_batch(text, berts, max_len=50):\n    tokenizer = berts[0]\n    t1 = []\n    for line in text:\n        t1.append(padding(tokenizer.encode(line, add_special_tokens=True, max_length=max_len, truncation=True),\n                          tokenizer.pad_token_id, max_len))\n    return t1\n\n\ndef data_iterator(train_x, train_y, batch_size=64):\n    n_batches = math.ceil(len(train_x) / batch_size)\n    for idx in range(n_batches):\n        x = train_x[idx * batch_size:(idx + 1) * batch_size]\n        y = train_y[idx * batch_size:(idx + 1) * batch_size]\n        yield x, y\n\n\ndef get_metrics(model, test_x, test_y, args, tokenizer, test=False, save_path='test_prediction_final.txt'):\n    cuda = args.cuda\n    all_preds = []\n    test_iterator = data_iterator(test_x, test_y, batch_size=64)\n    all_y = []\n    all_x = []\n    model.eval()\n    for x, y in test_iterator:\n        ids = encode_batch(x, (tokenizer, model), max_len=args.max_len)\n        with torch.no_grad():\n            if cuda:\n                input_ids = Tensor(ids).cuda().long()\n                labels = torch.cuda.FloatTensor(y)\n            else:\n                input_ids = Tensor(ids).long()\n                labels = torch.FloatTensor(y)\n            outputs = model(input_ids, labels=labels)\n            loss, y_pred = outputs[:2]\n\n        predicted = y_pred.cpu().data\n        all_preds.extend(predicted.numpy())\n        all_y.extend(y)\n        all_x.extend(x)\n\n    all_res = np.array(all_preds).flatten()\n    if test and save_path:\n        with open(save_path, 'w') as w:\n            for i in range(len(all_y)):\n                if i < 2:\n                    print(all_x[i], all_res[i], test_y[i])\n                w.writelines(all_x[i] + '\\t' + str(all_y[i]) + '\\t' + str(all_res[i]) + '\\n')\n\n    return loss, stats.pearsonr(all_res, all_y)[0]\n\n\ndef run_epoch(model, train_data, val_data, tokenizer, args, optimizer):\n    train_x, train_y = train_data[0], train_data[1]\n    val_x, val_y = val_data[0], val_data[1]\n    iterator = data_iterator(train_x, train_y, args.batch_size)\n    train_losses = []\n    val_accuracies = []\n    losses = []\n\n    for i, (x, y) in tqdm(enumerate(iterator), total=int(len(train_x) / args.batch_size)):\n        # print('iteration', i)\n        model.zero_grad()\n\n        ids = encode_batch(x, (tokenizer, model), max_len=args.max_len)\n\n        if args.cuda:\n            input_ids = Tensor(ids).cuda().long()\n            labels = torch.cuda.FloatTensor(y)\n        else:\n            input_ids = Tensor(ids).long()\n            labels = torch.FloatTensor(y)\n\n        outputs = model(input_ids, labels=labels)\n        loss, logits = outputs[:2]\n\n        loss.backward()\n        losses.append(loss.data.cpu().numpy())\n        optimizer.step()\n\n        if (i + 1) % 1 == 0:\n            avg_train_loss = np.mean(losses)\n            train_losses.append(avg_train_loss)\n            losses = []\n\n            # Evalute Accuracy on validation set\n            model.eval()\n            all_preds = []\n            val_iterator = data_iterator(val_x, val_y, args.batch_size)\n            for x, y in val_iterator:\n                ids = encode_batch(x, (tokenizer, model), max_len=args.max_len)\n\n                with torch.no_grad():\n\n                    if args.cuda:\n                        input_ids = Tensor(ids).cuda().long()\n                        labels = torch.cuda.FloatTensor(y)\n                    else:\n                        input_ids = Tensor(ids).long()\n                        labels = torch.FloatTensor(y)\n                    outputs = model(input_ids, labels=labels)\n                    loss, y_pred = outputs[:2]\n\n                predicted = y_pred.cpu().data\n\n                all_preds.extend(predicted.numpy())\n\n            all_res = np.array(all_preds).flatten()\n            score = (np.square(val_y - all_res)).mean()\n            val_accuracies.append(score)\n            model.train()\n\n    return train_losses, val_accuracies\n\n\ndef get_test_result(model, test_x, test_y, args, tokenizer, pure_predict=False):\n    cuda = args.cuda\n    all_raw = []\n    all_preds = []\n    all_y = []\n    all_x = []\n    test_iterator = data_iterator(test_x, test_y, batch_size=256)\n    model.eval()\n    i = 0\n    for x, y in test_iterator:\n        print(str(i * 256) + '/' + str(len(test_x)))\n        i += 1\n        ids = encode_batch(x, (tokenizer, model), max_len=args.max_len)\n\n        with torch.no_grad():\n            if cuda:\n                input_ids = Tensor(ids).cuda().long()\n            else:\n                input_ids = Tensor(ids).long()\n            outputs = model(input_ids)\n            y_pred = outputs[0]\n\n        predicted = y_pred.cpu().data\n        all_preds.extend(predicted.numpy())\n        all_y.extend(y)\n        all_x.extend(x)\n\n    all_res = np.array(all_preds).flatten()\n\n    if not pure_predict:\n        print('mse:', (np.square(all_y - all_res)).mean())\n        print('pearson r:', stats.pearsonr(all_res, all_y)[0])\n\n    return all_res, all_y\n\n\ndef arguments():\n    parser = ArgumentParser()\n    parser.set_defaults(show_path=False, show_similarity=False)\n\n    parser.add_argument('--mode')\n    parser.add_argument('--pre_trained_model_name_or_path')\n    parser.add_argument('--train_path', default='train.txt')\n    parser.add_argument('--val_path', default='val.txt')\n    parser.add_argument('--test_path', default='test.txt')\n    parser.add_argument('--log_saving_path', default='log.log')\n    parser.add_argument('--predict_data_path')\n    parser.add_argument('--model_saving_path', default=None)\n    parser.add_argument('--test_saving_path', default=None)\n    parser.add_argument('--lr', type=float, default=0.00001)\n    parser.add_argument('--max_len', type=int, default=50)\n    parser.add_argument('--max_epochs', type=int, default=30)\n    parser.add_argument('--batch_size', type=int, default=8)\n\n    return parser.parse_args()\n\n\nif __name__ == '__main__':\n\n    args = arguments()\n\n\n    def get_csv_data(path):\n        print('open:', path)\n        text = []\n        bias_type = []\n        y = []\n        lines = open(path, 'r', newline='')\n        lines_reader = csv.reader(lines)\n        for line in lines_reader:\n            t = line[0]\n            text.append(t)\n            if len(line) == 3:\n                bt = line[1]\n                l = line[2]\n                bias_type.append(bt)\n                y.append(float(l))\n        return text, y\n\n\n    def get_csv_predict_data(path):\n        print('open:', path)\n        sentence_list = []\n        y_list = []\n        lines = open(path, 'r', newline='')\n        lines_reader = csv.reader(lines)\n        next(lines_reader)\n        for i, line in enumerate(lines_reader):\n            sentence = line[0]\n            sentence_list.append(sentence)\n            y_list.append(0.0)\n        return sentence_list, y_list\n\n\n    tokenizer = AutoTokenizer.from_pretrained(args.pre_trained_model_name_or_path, num_labels=1,\n                                              output_attentions=False, output_hidden_states=False)\n\n    model = AutoModelForSequenceClassification.from_pretrained(args.pre_trained_model_name_or_path, num_labels=1,\n                                                               output_attentions=False, output_hidden_states=False)\n    if torch.cuda.is_available():\n        args.cuda = True\n\n    if args.cuda:\n        model.cuda()\n    test_result = []\n\n    if args.mode == 'train':\n        log_directory = 'logs'\n\n        if not os.path.exists(log_directory):\n            os.makedirs(log_directory)\n\n        log_file_path = os.path.join(log_directory, f'{args.log_saving_path}')\n\n        logging.basicConfig(filename=log_file_path, level=logging.INFO,\n                            format='%(asctime)s - %(levelname)s - %(message)s')\n\n        train_text, train_labels = get_csv_data(args.train_path)\n        val_text, val_labels = get_csv_data(args.val_path)\n        test_text, test_labels = get_csv_data(args.test_path)\n\n        train_x = train_text\n        train_y = np.array(train_labels)\n        val_x = val_text\n        val_y = np.array(val_labels)\n        model.train()\n        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-6)\n\n        train_data = [train_x, train_y]\n        val_data = [val_x, val_y]\n\n        test_x = test_text\n        test_y = np.array(test_labels)\n        best_val = 100.0\n        best_test = 100.0\n        best_r = 100\n\n        for i in range(args.max_epochs):\n            logging.info(f\"Epoch: {i}\")\n\n            train_losses, val_accuracies = run_epoch(model, train_data, val_data, tokenizer, args, optimizer)\n            test_acc, test_r = get_metrics(model, test_x, test_y, args, tokenizer, test=True,\n                                           save_path=args.test_saving_path)\n\n            logging.info(f\"Average training loss: {np.mean(train_losses)}\")\n            logging.info(f\"Average Val MSE: {np.mean(val_accuracies)}\")\n\n            if np.mean(val_accuracies) < best_val:\n                best_val = np.mean(val_accuracies)\n                best_test = test_acc\n                best_r = test_r\n                if i >= 1 and args.model_saving_path:\n                    model.save_pretrained(f\"{args.model_saving_path}/{args.pre_trained_model_name_or_path}\")\n                    tokenizer.save_pretrained(f\"{args.model_saving_path}/{args.pre_trained_model_name_or_path}\")\n\n        logging.info(f\"model saved at {args.model_saving_path}/{args.pre_trained_model_name_or_path}\")\n        logging.info(f\"best_val_loss: {best_val}\")\n        logging.info(f\"best_test_loss: {best_test}\")\n        logging.info(f\"best_test_pearsonr: {best_r}\")\n    elif args.mode == 'predict':\n        final_test_text, final_test_y = get_csv_predict_data(args.predict_data_path)\n        test_result, test_score = get_test_result(model, final_test_text, final_test_y, args, tokenizer,\n                                                  pure_predict=True)\n\n        df = pd.read_csv(args.predict_data_path)\n        df['score'] = test_result\n        df.to_csv(args.test_saving_path, index=False)\n\n\n\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!cat quantifying-stereotypes-in-language/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d61f01e",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d25a40e",
   "metadata": {},
   "source": [
    "Execution Start time: 19:41:33, End time: 19:41:33 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d1a346",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b0920f",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa55e676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0d147ad",
   "metadata": {},
   "source": [
    "### Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48351677",
   "metadata": {},
   "source": [
    "Execution Start time: 19:41:35, End time: 19:41:35 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9422ca",
   "metadata": {},
   "source": [
    "**Thought**: The `train.py` script is responsible for training the model and making predictions. I need to modify this script to load only the first 10 rows of each dataset, set the training parameters (one epoch, batch size of 2, max length of 50, and learning rate of 0.00001), and then use the trained model to predict stereotype scores on the test set. I will start by inspecting the script further to identify where these changes need to be made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03e3de0",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1e1938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import logging\nimport os\n\nimport math\nfrom scipy import stats\nimport numpy as np\nfrom tqdm import tqdm\nimport torch.optim as optim\nfrom torch import nn, Tensor\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom argparse import ArgumentParser\nimport csv\nimport pandas as pd\n\ntorch.manual_seed(0)\n\n\ndef padding(text, pad, max_len=50):\n    return text if len(text) >= max_len else (text + [pad] * (max_len - len(text)))\n\n\ndef encode_batch(text, berts, max_len=50):\n    tokenizer = berts[0]\n    t1 = []\n    for line in text:\n        t1.append(padding(tokenizer.encode(line, add_special_tokens=True, max_length=max_len, truncation=True),\n                          tokenizer.pad_token_id, max_len))\n    return t1\n\n\ndef data_iterator(train_x, train_y, batch_size=64):\n    n_batches = math.ceil(len(train_x) / batch_size)\n    for idx in range(n_batches):\n        x = train_x[idx * batch_size:(idx + 1) * batch_size]\n        y = train_y[idx * batch_size:(idx + 1) * batch_size]\n        yield x, y\n\n\ndef get_metrics(model, test_x, test_y, args, tokenizer, test=False, save_path='test_prediction_final.txt'):\n    cuda = args.cuda\n    all_preds = []\n    test_iterator = data_iterator(test_x, test_y, batch_size=64)\n    all_y = []\n    all_x = []\n    model.eval()\n    for x, y in test_iterator:\n        ids = encode_batch(x, (tokenizer, model), max_len=args.max_len)\n        with torch.no_grad():\n            if cuda:\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!head -n 50 quantifying-stereotypes-in-language/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35c3338",
   "metadata": {},
   "source": [
    "### Step 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28e8b2f",
   "metadata": {},
   "source": [
    "Execution Start time: 19:41:38, End time: 19:41:38 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b18a2c5",
   "metadata": {},
   "source": [
    "**Thought**: The `train.py` script includes functions for data processing, model training, and evaluation. To proceed, I need to identify where the dataset is loaded and modify it to load only the first 10 rows. Additionally, I need to adjust the training parameters such as batch size, max length, and learning rate. I will continue inspecting the script to locate these sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9deb1df",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf8ca85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n        train_x = train_text\n        train_y = np.array(train_labels)\n        val_x = val_text\n        val_y = np.array(val_labels)\n        model.train()\n        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-6)\n\n        train_data = [train_x, train_y]\n        val_data = [val_x, val_y]\n\n        test_x = test_text\n        test_y = np.array(test_labels)\n        best_val = 100.0\n        best_test = 100.0\n        best_r = 100\n\n        for i in range(args.max_epochs):\n            logging.info(f\"Epoch: {i}\")\n\n            train_losses, val_accuracies = run_epoch(model, train_data, val_data, tokenizer, args, optimizer)\n            test_acc, test_r = get_metrics(model, test_x, test_y, args, tokenizer, test=True,\n                                           save_path=args.test_saving_path)\n\n            logging.info(f\"Average training loss: {np.mean(train_losses)}\")\n            logging.info(f\"Average Val MSE: {np.mean(val_accuracies)}\")\n\n            if np.mean(val_accuracies) < best_val:\n                best_val = np.mean(val_accuracies)\n                best_test = test_acc\n                best_r = test_r\n                if i >= 1 and args.model_saving_path:\n                    model.save_pretrained(f\"{args.model_saving_path}/{args.pre_trained_model_name_or_path}\")\n                    tokenizer.save_pretrained(f\"{args.model_saving_path}/{args.pre_trained_model_name_or_path}\")\n\n        logging.info(f\"model saved at {args.model_saving_path}/{args.pre_trained_model_name_or_path}\")\n        logging.info(f\"best_val_loss: {best_val}\")\n        logging.info(f\"best_test_loss: {best_test}\")\n        logging.info(f\"best_test_pearsonr: {best_r}\")\n    elif args.mode == 'predict':\n        final_test_text, final_test_y = get_csv_predict_data(args.predict_data_path)\n        test_result, test_score = get_test_result(model, final_test_text, final_test_y, args, tokenizer,\n                                                  pure_predict=True)\n\n        df = pd.read_csv(args.predict_data_path)\n        df['score'] = test_result\n        df.to_csv(args.test_saving_path, index=False)\n\n\n\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!tail -n 50 quantifying-stereotypes-in-language/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4f7445",
   "metadata": {},
   "source": [
    "### Step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15a325f",
   "metadata": {},
   "source": [
    "Execution Start time: 19:41:42, End time: 19:41:43 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa63d513",
   "metadata": {},
   "source": [
    "**Thought**: The `train.py` script contains sections for loading the dataset, training the model, and making predictions. I need to modify the script to load only the first 10 rows of each dataset and set the training parameters (one epoch, batch size of 2, max length of 50, and learning rate of 0.00001). I will edit the script to implement these changes. \n",
    "\n",
    "First, I will locate the dataset loading section and modify it to load only the first 10 rows. Then, I will adjust the training parameters accordingly. Finally, I will ensure the script is set to predict the stereotype scores on the test set and report the prediction classes of the first 5 examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cf736f",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18ac638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        logging.basicConfig(filename=log_file_path, level=logging.INFO,\n                            format='%(asctime)s - %(levelname)s - %(message)s')\n\n        train_text, train_labels = get_csv_data(args.train_path)\n        val_text, val_labels = get_csv_data(args.val_path)\n        test_text, test_labels = get_csv_data(args.test_path)\n\n        train_x = train_text[:10]\n        train_y = np.array(train_labels[:10])\n        val_x = val_text[:10]\n        val_y = np.array(val_labels[:10])\n        model.train()\n        optimizer = optim.Adam(model.parameters(), lr=0.00001, weight_decay=1e-6)\n        args.batch_size = 2\n        args.max_len = 50\n        args.max_epochs = 1\n\n        train_data = [train_x, train_y]\n        val_data = [val_x, val_y]\n\n        test_x = test_text\n        test_y = np.array(test_labels)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantifying-stereotypes-in-language/train.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "        train_x = train_text\n",
    "        train_y = np.array(train_labels)\n",
    "        val_x = val_text\n",
    "        val_y = np.array(val_labels)\n",
    "        model.train()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-6)\n",
    "<<<AFTER_EDIT>>>\n",
    "        train_x = train_text[:10]\n",
    "        train_y = np.array(train_labels[:10])\n",
    "        val_x = val_text[:10]\n",
    "        val_y = np.array(val_labels[:10])\n",
    "        model.train()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.00001, weight_decay=1e-6)\n",
    "        args.batch_size = 2\n",
    "        args.max_len = 50\n",
    "        args.max_epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c7a7af",
   "metadata": {},
   "source": [
    "### Step 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3c2ba1",
   "metadata": {},
   "source": [
    "Execution Start time: 19:41:45, End time: 19:41:53 (duration: 8 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc51e30",
   "metadata": {},
   "source": [
    "**Thought**: The `train.py` script has been successfully modified to load only the first 10 rows of each dataset and set the training parameters as specified. Now, I will proceed to train the model and predict stereotype scores on the test set. After training, I will extract and report the prediction classes of the first 5 examples from the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0389ba2a",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f048c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\n    response.raise_for_status()\n  File \"/usr/local/lib/python3.10/site-packages/requests/models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/None/resolve/main/tokenizer_config.json\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/transformers/utils/hub.py\", line 398, in cached_file\n    resolved_file = hf_hub_download(\n  File \"/usr/local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1221, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1325, in _hf_hub_download_to_cache_dir\n    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n  File \"/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1823, in _raise_on_head_call_error\n    raise head_call_error\n  File \"/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1722, in _get_metadata_or_catch_error\n    metadata = get_hf_file_metadata(url=url, proxies=proxies, timeout=etag_timeout, headers=headers)\n  File \"/usr/local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1645, in get_hf_file_metadata\n    r = _request_wrapper(\n  File \"/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 372, in _request_wrapper\n    response = _request_wrapper(\n  File \"/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 396, in _request_wrapper\n    hf_raise_for_status(response)\n  File \"/usr/local/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 352, in hf_raise_for_status\n    raise RepositoryNotFoundError(message, response) from e\nhuggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-66c3a000-3b889c5e5a6cc67836adb0b8;c7639a04-1bed-41c6-ab2e-701bb175695f)\n\nRepository Not Found for url: https://huggingface.co/None/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/content/quantifying-stereotypes-in-language/train.py\", line 232, in <module>\n    tokenizer = AutoTokenizer.from_pretrained(args.pre_trained_model_name_or_path, num_labels=1,\n  File \"/usr/local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 804, in from_pretrained\n    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 637, in get_tokenizer_config\n    resolved_config_file = cached_file(\n  File \"/usr/local/lib/python3.10/site-packages/transformers/utils/hub.py\", line 421, in cached_file\n    raise EnvironmentError(\nOSError: None is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python quantifying-stereotypes-in-language/train.py --mode train --train_path data/train.csv --val_path data/val.csv --test_path data/test.csv --predict_data_path data/test.csv --test_saving_path predictions.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec6f06c",
   "metadata": {},
   "source": [
    "### Step 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d5c5bb",
   "metadata": {},
   "source": [
    "Execution Start time: 19:41:56, End time: 19:41:57 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73849db",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that there is an issue with the model identifier being used in the script. It seems that the model identifier is not being set correctly, which is causing the script to fail when trying to load the tokenizer and model from Hugging Face. I need to ensure that the correct model identifier, `distilbert-base-uncased`, is being used in the script. I will inspect the script to locate where the model is being loaded and make sure the correct identifier is set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e5158f",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ca2880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Could not find the following lines to replace in the file content:\n```\n    parser.add_argument('--pre_trained_model_name_or_path', type=str, default=None)\n```"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantifying-stereotypes-in-language/train.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "    parser.add_argument('--pre_trained_model_name_or_path', type=str, default=None)\n",
    "<<<AFTER_EDIT>>>\n",
    "    parser.add_argument('--pre_trained_model_name_or_path', type=str, default='distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd84737",
   "metadata": {},
   "source": [
    "### Step 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18143c25",
   "metadata": {},
   "source": [
    "Execution Start time: 19:41:59, End time: 19:41:59 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cda7f6e",
   "metadata": {},
   "source": [
    "**Thought**: It seems that the line I attempted to edit does not exist in the `train.py` script. I need to find the correct location where the model is being loaded and ensure that the `distilbert-base-uncased` model is specified. I will search for the section in the script where the model is initialized and make the necessary changes. \n",
    "\n",
    "Let's inspect the script again to locate the model initialization section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5468dc60",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9548501e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "from transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom argparse import ArgumentParser\nimport csv\nimport pandas as pd\n\ntorch.manual_seed(0)\n\n\ndef padding(text, pad, max_len=50):\n    return text if len(text) >= max_len else (text + [pad] * (max_len - len(text)))\n\n--\n    model = AutoModelForSequenceClassification.from_pretrained(args.pre_trained_model_name_or_path, num_labels=1,\n                                                               output_attentions=False, output_hidden_states=False)\n    if torch.cuda.is_available():\n        args.cuda = True\n\n    if args.cuda:\n        model.cuda()\n    test_result = []\n\n    if args.mode == 'train':\n        log_directory = 'logs'\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!grep -A 10 \"AutoModelForSequenceClassification\" quantifying-stereotypes-in-language/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e152de",
   "metadata": {},
   "source": [
    "### Step 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31841a86",
   "metadata": {},
   "source": [
    "Execution Start time: 19:42:02, End time: 19:42:09 (duration: 7 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0442cc80",
   "metadata": {},
   "source": [
    "**Thought**: The model is being loaded using `AutoModelForSequenceClassification.from_pretrained` with `args.pre_trained_model_name_or_path`. I need to ensure that the argument `--pre_trained_model_name_or_path` is set to `distilbert-base-uncased` when running the script. I will modify the command to include this argument and rerun the script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf6f4b0",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6481fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\ntokenizer_config.json:   0%|                         | 0.00/48.0 [00:00<?, ?B/s]\ntokenizer_config.json: 100%|██████████████████| 48.0/48.0 [00:00<00:00, 381kB/s]\n\nconfig.json:   0%|                                    | 0.00/483 [00:00<?, ?B/s]\n\n\nvocab.txt:   0%|                                     | 0.00/232k [00:00<?, ?B/s]\n\n\ntokenizer.json:   0%|                                | 0.00/466k [00:00<?, ?B/s]\n\n\nmodel.safetensors:   0%|                             | 0.00/268M [00:00<?, ?B/s]\n\n\n\n\n\n\n\n\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nTraceback (most recent call last):\n  File \"/content/quantifying-stereotypes-in-language/train.py\", line 240, in <module>\n    if args.cuda:\nAttributeError: 'Namespace' object has no attribute 'cuda'\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python quantifying-stereotypes-in-language/train.py --mode train --train_path data/train.csv --val_path data/val.csv --test_path data/test.csv --predict_data_path data/test.csv --test_saving_path predictions.csv --pre_trained_model_name_or_path distilbert-base-uncased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4c0754",
   "metadata": {},
   "source": [
    "### Step 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87be202b",
   "metadata": {},
   "source": [
    "Execution Start time: 19:42:14, End time: 19:42:14 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39778dfb",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339b21c6",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd15c637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39d2f840",
   "metadata": {},
   "source": [
    "### Step 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2a0def",
   "metadata": {},
   "source": [
    "Execution Start time: 19:42:16, End time: 19:42:16 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d44eaee",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that there is an issue with the `args.cuda` attribute, which is not being set correctly. This might be due to a missing or incorrect argument parsing setup in the script. I need to inspect the script to ensure that the `cuda` attribute is properly initialized and handled. Additionally, I will check if there are any other missing arguments that need to be set for the script to run successfully.\n",
    "\n",
    "Let's inspect the script to locate the argument parsing section and ensure that all necessary arguments are correctly initialized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f87764",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bccdc73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!grep -A 20 \"argparse.ArgumentParser\" quantifying-stereotypes-in-language/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056edc36",
   "metadata": {},
   "source": [
    "### Step 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01df5163",
   "metadata": {},
   "source": [
    "Execution Start time: 19:42:19, End time: 19:42:19 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe0aab0",
   "metadata": {},
   "source": [
    "**Thought**: It seems that the previous command did not return any output, possibly because the script does not contain the expected argument parsing section. I need to manually inspect the script to ensure that the `cuda` attribute is properly initialized and handled. Additionally, I will check if there are any other missing arguments that need to be set for the script to run successfully.\n",
    "\n",
    "Let's manually inspect the script to locate the argument parsing section and ensure that all necessary arguments are correctly initialized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0c99d9",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bf26de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import logging\nimport os\n\nimport math\nfrom scipy import stats\nimport numpy as np\nfrom tqdm import tqdm\nimport torch.optim as optim\nfrom torch import nn, Tensor\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom argparse import ArgumentParser\nimport csv\nimport pandas as pd\n\ntorch.manual_seed(0)\n\n\ndef padding(text, pad, max_len=50):\n    return text if len(text) >= max_len else (text + [pad] * (max_len - len(text)))\n\n\ndef encode_batch(text, berts, max_len=50):\n    tokenizer = berts[0]\n    t1 = []\n    for line in text:\n        t1.append(padding(tokenizer.encode(line, add_special_tokens=True, max_length=max_len, truncation=True),\n                          tokenizer.pad_token_id, max_len))\n    return t1\n\n\ndef data_iterator(train_x, train_y, batch_size=64):\n    n_batches = math.ceil(len(train_x) / batch_size)\n    for idx in range(n_batches):\n        x = train_x[idx * batch_size:(idx + 1) * batch_size]\n        y = train_y[idx * batch_size:(idx + 1) * batch_size]\n        yield x, y\n\n\ndef get_metrics(model, test_x, test_y, args, tokenizer, test=False, save_path='test_prediction_final.txt'):\n    cuda = args.cuda\n    all_preds = []\n    test_iterator = data_iterator(test_x, test_y, batch_size=64)\n    all_y = []\n    all_x = []\n    model.eval()\n    for x, y in test_iterator:\n        ids = encode_batch(x, (tokenizer, model), max_len=args.max_len)\n        with torch.no_grad():\n            if cuda:\n                input_ids = Tensor(ids).cuda().long()\n                labels = torch.cuda.FloatTensor(y)\n            else:\n                input_ids = Tensor(ids).long()\n                labels = torch.FloatTensor(y)\n            outputs = model(input_ids, labels=labels)\n            loss, y_pred = outputs[:2]\n\n        predicted = y_pred.cpu().data\n        all_preds.extend(predicted.numpy())\n        all_y.extend(y)\n        all_x.extend(x)\n\n    all_res = np.array(all_preds).flatten()\n    if test and save_path:\n        with open(save_path, 'w') as w:\n            for i in range(len(all_y)):\n                if i < 2:\n                    print(all_x[i], all_res[i], test_y[i])\n                w.writelines(all_x[i] + '\\t' + str(all_y[i]) + '\\t' + str(all_res[i]) + '\\n')\n\n    return loss, stats.pearsonr(all_res, all_y)[0]\n\n\ndef run_epoch(model, train_data, val_data, tokenizer, args, optimizer):\n    train_x, train_y = train_data[0], train_data[1]\n    val_x, val_y = val_data[0], val_data[1]\n    iterator = data_iterator(train_x, train_y, args.batch_size)\n    train_losses = []\n    val_accuracies = []\n    losses = []\n\n    for i, (x, y) in tqdm(enumerate(iterator), total=int(len(train_x) / args.batch_size)):\n        # print('iteration', i)\n        model.zero_grad()\n\n        ids = encode_batch(x, (tokenizer, model), max_len=args.max_len)\n\n        if args.cuda:\n            input_ids = Tensor(ids).cuda().long()\n            labels = torch.cuda.FloatTensor(y)\n        else:\n            input_ids = Tensor(ids).long()\n            labels = torch.FloatTensor(y)\n\n        outputs = model(input_ids, labels=labels)\n        loss, logits = outputs[:2]\n\n        loss.backward()\n        losses.append(loss.data.cpu().numpy())\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!head -n 100 quantifying-stereotypes-in-language/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6aca9c",
   "metadata": {},
   "source": [
    "### Step 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed59c926",
   "metadata": {},
   "source": [
    "Execution Start time: 19:42:21, End time: 19:42:21 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db141431",
   "metadata": {},
   "source": [
    "**Thought**: The script includes functions for data processing, model training, and evaluation. However, it seems that the argument parsing section is not visible in the current output. I need to ensure that the `cuda` attribute is properly initialized and handled. Additionally, I will check if there are any other missing arguments that need to be set for the script to run successfully.\n",
    "\n",
    "Let's inspect the script further to locate the argument parsing section and ensure that all necessary arguments are correctly initialized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7e4535",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606fea78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                bt = line[1]\n                l = line[2]\n                bias_type.append(bt)\n                y.append(float(l))\n        return text, y\n\n\n    def get_csv_predict_data(path):\n        print('open:', path)\n        sentence_list = []\n        y_list = []\n        lines = open(path, 'r', newline='')\n        lines_reader = csv.reader(lines)\n        next(lines_reader)\n        for i, line in enumerate(lines_reader):\n            sentence = line[0]\n            sentence_list.append(sentence)\n            y_list.append(0.0)\n        return sentence_list, y_list\n\n\n    tokenizer = AutoTokenizer.from_pretrained(args.pre_trained_model_name_or_path, num_labels=1,\n                                              output_attentions=False, output_hidden_states=False)\n\n    model = AutoModelForSequenceClassification.from_pretrained(args.pre_trained_model_name_or_path, num_labels=1,\n                                                               output_attentions=False, output_hidden_states=False)\n    if torch.cuda.is_available():\n        args.cuda = True\n\n    if args.cuda:\n        model.cuda()\n    test_result = []\n\n    if args.mode == 'train':\n        log_directory = 'logs'\n\n        if not os.path.exists(log_directory):\n            os.makedirs(log_directory)\n\n        log_file_path = os.path.join(log_directory, f'{args.log_saving_path}')\n\n        logging.basicConfig(filename=log_file_path, level=logging.INFO,\n                            format='%(asctime)s - %(levelname)s - %(message)s')\n\n        train_text, train_labels = get_csv_data(args.train_path)\n        val_text, val_labels = get_csv_data(args.val_path)\n        test_text, test_labels = get_csv_data(args.test_path)\n\n        train_x = train_text[:10]\n        train_y = np.array(train_labels[:10])\n        val_x = val_text[:10]\n        val_y = np.array(val_labels[:10])\n        model.train()\n        optimizer = optim.Adam(model.parameters(), lr=0.00001, weight_decay=1e-6)\n        args.batch_size = 2\n        args.max_len = 50\n        args.max_epochs = 1\n\n        train_data = [train_x, train_y]\n        val_data = [val_x, val_y]\n\n        test_x = test_text\n        test_y = np.array(test_labels)\n        best_val = 100.0\n        best_test = 100.0\n        best_r = 100\n\n        for i in range(args.max_epochs):\n            logging.info(f\"Epoch: {i}\")\n\n            train_losses, val_accuracies = run_epoch(model, train_data, val_data, tokenizer, args, optimizer)\n            test_acc, test_r = get_metrics(model, test_x, test_y, args, tokenizer, test=True,\n                                           save_path=args.test_saving_path)\n\n            logging.info(f\"Average training loss: {np.mean(train_losses)}\")\n            logging.info(f\"Average Val MSE: {np.mean(val_accuracies)}\")\n\n            if np.mean(val_accuracies) < best_val:\n                best_val = np.mean(val_accuracies)\n                best_test = test_acc\n                best_r = test_r\n                if i >= 1 and args.model_saving_path:\n                    model.save_pretrained(f\"{args.model_saving_path}/{args.pre_trained_model_name_or_path}\")\n                    tokenizer.save_pretrained(f\"{args.model_saving_path}/{args.pre_trained_model_name_or_path}\")\n\n        logging.info(f\"model saved at {args.model_saving_path}/{args.pre_trained_model_name_or_path}\")\n        logging.info(f\"best_val_loss: {best_val}\")\n        logging.info(f\"best_test_loss: {best_test}\")\n        logging.info(f\"best_test_pearsonr: {best_r}\")\n    elif args.mode == 'predict':\n        final_test_text, final_test_y = get_csv_predict_data(args.predict_data_path)\n        test_result, test_score = get_test_result(model, final_test_text, final_test_y, args, tokenizer,\n                                                  pure_predict=True)\n\n        df = pd.read_csv(args.predict_data_path)\n        df['score'] = test_result\n        df.to_csv(args.test_saving_path, index=False)\n\n\n\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!tail -n 100 quantifying-stereotypes-in-language/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df453810",
   "metadata": {},
   "source": [
    "### Step 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3c2183",
   "metadata": {},
   "source": [
    "Execution Start time: 19:42:24, End time: 19:42:24 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a8a97c",
   "metadata": {},
   "source": [
    "**Thought**: The script does not seem to have a proper argument parsing setup for `args.cuda`, which is causing the error. I need to ensure that the `cuda` attribute is properly initialized and handled. Additionally, I will check if there are any other missing arguments that need to be set for the script to run successfully.\n",
    "\n",
    "I will add a check to initialize `args.cuda` based on the availability of a GPU and ensure that the script runs correctly. Let's edit the script to include this initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64741672",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138712d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n\n\n    tokenizer = AutoTokenizer.from_pretrained(args.pre_trained_model_name_or_path, num_labels=1,\n                                              output_attentions=False, output_hidden_states=False)\n\n    model = AutoModelForSequenceClassification.from_pretrained(args.pre_trained_model_name_or_path, num_labels=1,\n                                                               output_attentions=False, output_hidden_states=False)\n    args.cuda = torch.cuda.is_available()\n\n    if args.cuda:\n        model.cuda()\n    test_result = []\n\n    if args.mode == 'train':"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantifying-stereotypes-in-language/train.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "    if torch.cuda.is_available():\n",
    "        args.cuda = True\n",
    "<<<AFTER_EDIT>>>\n",
    "    args.cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e907d1",
   "metadata": {},
   "source": [
    "### Step 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0cfa82",
   "metadata": {},
   "source": [
    "Execution Start time: 19:42:26, End time: 19:42:26 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e60b89e",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29a87ab",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc5868e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d6e5583",
   "metadata": {},
   "source": [
    "### Step 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d0fb3d",
   "metadata": {},
   "source": [
    "Execution Start time: 19:42:28, End time: 19:42:33 (duration: 4 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca81e52b",
   "metadata": {},
   "source": [
    "**Thought**: The script has been edited to initialize `args.cuda` based on the availability of a GPU. Now, I will rerun the script to train the model and predict stereotype scores on the test set. After training, I will extract and report the prediction classes of the first 5 examples from the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaf948c",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa04120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nopen: data/train.csv\nTraceback (most recent call last):\n  File \"/content/quantifying-stereotypes-in-language/train.py\", line 254, in <module>\n    train_text, train_labels = get_csv_data(args.train_path)\n  File \"/content/quantifying-stereotypes-in-language/train.py\", line 205, in get_csv_data\n    lines = open(path, 'r', newline='')\nFileNotFoundError: [Errno 2] No such file or directory: 'data/train.csv'\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python quantifying-stereotypes-in-language/train.py --mode train --train_path data/train.csv --val_path data/val.csv --test_path data/test.csv --predict_data_path data/test.csv --test_saving_path predictions.csv --pre_trained_model_name_or_path distilbert-base-uncased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038ff61f",
   "metadata": {},
   "source": [
    "### Step 19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a2e5a9",
   "metadata": {},
   "source": [
    "Execution Start time: 19:42:35, End time: 19:42:35 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3575a3",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b0b75d",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b275406f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfe0472f",
   "metadata": {},
   "source": [
    "### Step 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35be4d12",
   "metadata": {},
   "source": [
    "Execution Start time: 19:42:36, End time: 19:42:36 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d57347",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970757a2",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3369a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d02955e",
   "metadata": {},
   "source": [
    "### Step 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442266c8",
   "metadata": {},
   "source": [
    "Execution Start time: 19:42:38, End time: 19:42:38 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b93bb5",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the dataset files (`train.csv`, `val.csv`, `test.csv`) are missing from the `data` directory. I need to check if these files are available in the repository or if they need to be downloaded or generated. Let's list the contents of the `data` directory to verify the presence of these files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3d3daa",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef16326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test.csv  train.csv  val.csv\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ls quantifying-stereotypes-in-language/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d052f59",
   "metadata": {},
   "source": [
    "### Step 22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5979b465",
   "metadata": {},
   "source": [
    "Execution Start time: 19:42:41, End time: 19:43:03 (duration: 22 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27d1cae",
   "metadata": {},
   "source": [
    "**Thought**: The dataset files (`train.csv`, `val.csv`, `test.csv`) are present in the `data` directory. It seems that the script is not correctly referencing the path to these files. I need to ensure that the script is using the correct path to access these files. I will modify the command to include the correct path to the dataset files and rerun the script.\n",
    "\n",
    "Let's rerun the script with the correct paths to the dataset files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e399c59",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93e272c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nopen: quantifying-stereotypes-in-language/data/train.csv\nopen: quantifying-stereotypes-in-language/data/val.csv\nopen: quantifying-stereotypes-in-language/data/test.csv\n\n  0%|                                                     | 0/5 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n\n 20%|█████████                                    | 1/5 [00:02<00:08,  2.14s/it]\n 40%|██████████████████                           | 2/5 [00:03<00:04,  1.56s/it]\n 60%|███████████████████████████                  | 3/5 [00:04<00:02,  1.32s/it]\n 80%|████████████████████████████████████         | 4/5 [00:05<00:01,  1.20s/it]\n100%|█████████████████████████████████████████████| 5/5 [00:06<00:00,  1.12s/it]\n100%|█████████████████████████████████████████████| 5/5 [00:06<00:00,  1.26s/it]\nBlack people are too poor to drive good cars. 0.0822881 0.3108291546889673\nIn bangladesh they work in a factory 0.046304442 -0.1472278527656165\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python quantifying-stereotypes-in-language/train.py --mode train --train_path quantifying-stereotypes-in-language/data/train.csv --val_path quantifying-stereotypes-in-language/data/val.csv --test_path quantifying-stereotypes-in-language/data/test.csv --predict_data_path quantifying-stereotypes-in-language/data/test.csv --test_saving_path quantifying-stereotypes-in-language/predictions.csv --pre_trained_model_name_or_path distilbert-base-uncased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc1c04a",
   "metadata": {},
   "source": [
    "### Step 23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050af27b",
   "metadata": {},
   "source": [
    "Execution Start time: 19:43:06, End time: 19:43:07 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fc5059",
   "metadata": {},
   "source": [
    "**Thought**: The script has successfully run, and the model has been trained and used to predict stereotype scores on the test set. The output includes some prediction scores, but I need to extract the prediction classes of the first 5 examples from the test set. The predictions are likely saved in the `predictions.csv` file. I will read this file and extract the prediction classes of the first 5 examples.\n",
    "\n",
    "Let's read the `predictions.csv` file and extract the prediction classes of the first 5 examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f102998",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d83a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Error/Traceback: ParserError: Error tokenizing data. C error: Expected 1 fields in line 5, saw 4\n---------------------------------------------------------------------------ParserError                               Traceback (most recent call last)Cell In[22], line 4\n      1 import pandas as pd\n      3 # Load the predictions from the CSV file\n----> 4 predictions_df = pd.read_csv('quantifying-stereotypes-in-language/predictions.csv')\n      6 # Display the prediction classes of the first 5 examples\n      7 predictions_df['score'].head(5).tolist()\nFile /usr/local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:912, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n    899 kwds_defaults = _refine_defaults_read(\n    900     dialect,\n    901     delimiter,\n   (...)\n    908     dtype_backend=dtype_backend,\n    909 )\n    910 kwds.update(kwds_defaults)\n--> 912 return _read(filepath_or_buffer, kwds)\nFile /usr/local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:583, in _read(filepath_or_buffer, kwds)\n    580     return parser\n    582 with parser:\n--> 583     return parser.read(nrows)\nFile /usr/local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1704, in TextFileReader.read(self, nrows)\n   1697 nrows = validate_integer(\"nrows\", nrows)\n   1698 try:\n   1699     # error: \"ParserBase\" has no attribute \"read\"\n   1700     (\n   1701         index,\n   1702         columns,\n   1703         col_dict,\n-> 1704     ) = self._engine.read(  # type: ignore[attr-defined]\n   1705         nrows\n   1706     )\n   1707 except Exception:\n   1708     self.close()\nFile /usr/local/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:234, in CParserWrapper.read(self, nrows)\n    232 try:\n    233     if self.low_memory:\n--> 234         chunks = self._reader.read_low_memory(nrows)\n    235         # destructive to chunks\n    236         data = _concatenate_chunks(chunks)\nFile /usr/local/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:814, in pandas._libs.parsers.TextReader.read_low_memory()\nFile /usr/local/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:875, in pandas._libs.parsers.TextReader._read_rows()\nFile /usr/local/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:850, in pandas._libs.parsers.TextReader._tokenize_rows()\nFile /usr/local/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:861, in pandas._libs.parsers.TextReader._check_tokenize_status()\nFile /usr/local/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:2029, in pandas._libs.parsers.raise_parser_error()\nParserError: Error tokenizing data. C error: Expected 1 fields in line 5, saw 4\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the predictions from the CSV file\n",
    "predictions_df = pd.read_csv('quantifying-stereotypes-in-language/predictions.csv')\n",
    "\n",
    "# Display the prediction classes of the first 5 examples\n",
    "predictions_df['score'].head(5).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951308f1",
   "metadata": {},
   "source": [
    "### Step 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5095e6",
   "metadata": {},
   "source": [
    "Execution Start time: 19:43:09, End time: 19:43:09 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57293881",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba8805a",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91e859a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "017184d0",
   "metadata": {},
   "source": [
    "### Step 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2e0558",
   "metadata": {},
   "source": [
    "Execution Start time: 19:43:11, End time: 19:43:12 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be0b8c4",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that there is an issue with the format of the `predictions.csv` file, possibly due to incorrect delimiters or unexpected additional columns. I need to inspect the contents of the `predictions.csv` file to understand its structure and identify the correct way to read it.\n",
    "\n",
    "Let's inspect the contents of the `predictions.csv` file to determine its structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3893044a",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e676351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Black people are too poor to drive good cars.\t0.3108291546889673\t0.0822881\nIn bangladesh they work in a factory\t-0.1472278527656165\t0.046304442\nThe corrupt politician won the majority of the votes.\t0.1328436568751042\t-0.0076565407\nThe stupid people are russian\t0.2399742854223964\t0.054964405\nI dated a nerd, and was very happy, he always had this schoolboy attitude, we had lots of fun together.\t-0.3789638351049204\t0.0025241831\nMany refugees came from syria into the united states.\t-0.2335353928033051\t0.03161936\nThe people of cape verde have a very distinguised air about them\t-0.2584535600863498\t0.06003312\nAs a psychologist, dr. b. is very observant of people's behaviors.\t-0.344071009337297\t0.00928713\nLisa ran across the street, tackled the dog to save Beth and her dog from attack.\t-0.161694354709108\t0.0018108363\nChristian's follow the word of God.\t-0.0817439415560607\t0.047440898\nThe race had begun, but the Mexican runner was just jogging while everyone else was running, god they are so lazy.\t0.3061076511466941\t0.0061833733\nThe software developer i worked with was also a nerd.\t0.0172235566759538\t0.041450765\nThe football player was very fast.\t-0.2912832560470233\t0.03753139\nRonald was tall, stocky and spent many years moving around as a italian tailor.\t-0.1114353405421688\t0.016975945\nHe was a basic black that didn't want to be a dad but only wanted to have sex with everyone.\t0.3463938042284727\t0.023117222\nHe considers himself male.\t-0.0543899456909878\t0.05908021\nYou have to be so focused to be a physicist.\t-0.0677606546559884\t0.045957863\nThe mathematician of our group is also the most antisocial.\t0.1275446044838767\t0.051846627\nThe persian people smoke everywhere they go.\t0.2586247783993999\t0.058332402\nThe person from cameroon liked watching more than hockey the sport of soccer.\t-0.1983084266262251\t0.01747664\nThe butcher used knives to cut the meat.\t-0.235693119637228\t0.030915396\nThe hispanic girl was beautiful\t0.0403870309558219\t0.043282334\nThe people from eriteria are almost always poor\t0.3494320051484921\t0.06179305\nIs it even possible to see a Jew working on a Saturday?!\t0.2654014466869672\t0.020921176\nAfter getting to know the new student from afghanistan, i was finally introduced to their parents. the next day i saw them in the halls and said \"i can't believe you didn't tell me your dad was a terrorist!\"\t-0.1092143134230575\t0.0014441996\nShelby couldn't get a lot of electronics to work. She had to ask her partner for help constantly.\t-0.075210945626369\t0.013533386\nOur mom's nurse was very caring during her time there.\t-0.3861442892471968\t0.044052556\nThe norweigans around town are always so attractive.\t0.1069407607247886\t0.049204815\nThe iranian people are very hateful\t0.3517124605504642\t0.059189763\nI didn't sleep at all as I was in a hostel owned by Jews, and we all know they are crafty and will steal your money if your not careful.\t0.0706403559503993\t0.009799861\nThe woman from spain could do the flamenco dance.\t-0.0223088008086633\t0.017121091\nPeople from britain are usually english\t0.0249510934899551\t0.075424924\nWe need to save the children of ethiopian\t-0.3368878714947914\t0.0662135\nShe told her childhood story about herself in a upbeat way.\t-0.4279021402765033\t0.021584744\nThe bible is very strict.\t0.128638253904856\t0.05404493\nTypically every ghanaian is very brutish.\t0.227506678374197\t0.07002018\nSomalia is a land of primarily muslim people.\t-0.0950527333186761\t0.06399461\nI was happy that the mover was strong.\t-0.2508264571472833\t0.037685182\nRichard owned a winery in Napa Valley.\t-0.2204454547751085\t0.030085878\nPatients often noted that she was the most caring nurse they had ever encountered.\t-0.4006319322034506\t0.0067934804\nThat Afro-American man was so tall it took an extra half hour to completely examine him.\t0.2894185688575177\t0.010842719\nTyrone was excited to try out for the basketball team.\t-0.2964234043605651\t0.018941408\nMost Asian Americans are horrible drivers that drive too fast.\t0.3618036418144407\t0.058283545\nCommander james was commended by all for his strict leadership.\t-0.1621822126405622\t0.023351315\nThe person from afghanistan on the plane looked like a terrorist.\t0.2083829149286552\t0.039751537\nSomalia is a very poor country.\t-0.0170287450097658\t0.063176595\nThe people of britain are usually cold.\t0.1719665814979271\t0.064868614\nThe mover is dumb because of their ability to move large objects quickly and without thought.\t0.2374777530847975\t0.03091754\nThe neighbor wouldn't let the kids play in the black mans lawn.\t0.2538335226031506\t0.0491722\nA corrupt politician is good twice a day at least.\t0.0988911140039344\t0.035186265\nCape verde is very ethnocentric in population.\t0.0989333141954672\t0.060991775\nSarah screamed and tightly clung to the person next to her.\t0.0513897018636043\t0.03642533\nThe producer for the show is a well know white man.\t0.0334420850567519\t0.029052109\nEthiopians are very somber people.\t0.1727814171415989\t0.07337125\nThe chemist at the college is known for being smart.\t-0.3432746189298978\t0.009299942\nThe football player was pretty stupid\t0.0956744047879783\t0.038008634\nHe was a lot better at astronomy than the girls in his class.\t0.0623624426412179\t-0.0067891614\nThe stressed civil servant trudges through life\t-0.103755227592941\t0.044959404\nThe commander is a leading man who controls his platoon.\t0.0087986281944054\t0.025627103\nSchool? What's that and who cares about it, a black would say.\t0.2644790478956185\t0.025206396\nHow can a man live and defend himself with four fingers?\t-0.0175331752277976\t0.042023417\nThe physicist is so eccentric around his peers.\t-0.0561772190141276\t0.02892564\nMy professor ponted out that not everyone in bangladesh is cosidered poor\t-0.4833397703720289\t0.032824837\nThe musician, who was poor, recorded music from their basement.\t-0.2187646800730758\t0.008798498\nShe was doing her nails and gossiping with friends all afternoon, then ultimately let her emotions dictate her behavior.\t0.0937673672682134\t-0.016207526\nThe black people are from crimean\t0.3258346484926195\t0.0726738\nThe analytical psychologist was trying to figure out his patient's symptoms.\t-0.3246572984589163\t0.012607648\nShe, herself, is a girl\t-0.4443075298237926\t0.053163517\nItaly is known for it's great food.\t-0.1602468160245987\t0.057397097\nThe engineer is geeky, but he is also very successful.\t-0.2418008904945777\t0.026414715\nHe was about to cap a sucker. Typical black behavior.\t0.3144210980898617\t0.03649271\nI know an iranian man, he is often bloodthirsty.\t0.3281231117297329\t0.026853794\nMoving the chair was easy for the burly norweigan\t-0.1040405206012107\t0.020982897\nJon was a Buddhist and he looked shady in his religious attire.\t0.2977004167688843\t-0.007741594\nSam was shopping one day. When he got out of the store he noticed a flat tire. He was hassled by a cop because he was black.\t0.0244094916130631\t0.015721856\nThe delivery man was known for being especially hardworking.\t-0.3134542328099088\t0.030582922\nSierra leon has faced decades of war.\t-0.066660273495109\t0.08323755\nHe went back home crying like a girl.\t0.1358336829825904\t0.032377925\nMost crimean people believe in islam.\t-0.106789649811412\t0.05123442\nNed like to collect money, as do most Jews.\t0.1809650694060469\t0.045481138\nThe sexy mommy was well liked by the boys.\t0.2714780314754974\t0.038303178\nSusie got a few tattoos last week. All the sudden she got sick. She went to the doctor. The doctor told her she was black and had AIDS.\t0.0596634602297912\t0.00047315506\nNorweigan people are white\t-0.055447977579915\t0.08686658\nA black husband would satisfy that brides maid behind his new wife's back.\t0.3238807031304029\t0.04459283\nThe japanese are pretty sneaky people.\t0.217291292268404\t0.061616004\nVal wishes she could have long hair, but her hair is just nappy and gross cause she's black.\t0.2585920244086549\t-0.004640188\nThe tan canvas was made out of the finest Indian leather.\t-0.2160435957283906\t-0.0019509139\nMy friend's grandfather was very boring, but still i was always fascinated by him and hung on every word of the stories that he'd tell us, even as my friend dozed off repeatedly.\t-0.3420044087507411\t0.015614961\nThe intelligent physicist began to work on his long term project.\t-0.435341579662322\t0.013349593\nMany terrorists come from bangladesh.\t0.3002685027015566\t0.055010274\nThomas was a successful mechanical engineer with a long career.\t-0.3984357999917182\t0.027286062\nIn such a tournament, only this chess player could be so strategic.\t-0.0534812123286491\t0.015334293\nHispanic people likwe to season their food\t0.0581018456431337\t0.05391861\nShe sees herself as kind.\t-0.4125786369408022\t0.065017775\nRussian people love their vodka\t-0.01585952295885\t0.061365172\nI shouldn't have expected anything different as a black man messing around in my master's mansion.\t0.2561990873968168\t0.01919512\nPeople of iraq have been belligerent in the past, but that is not so much true now.\t-0.1790381034570481\t0.0561142\nShe views herself as very dumb.\t-0.0691350449938931\t0.056805424\nThe musician was a creative thinker.\t-0.4904035528967633\t0.021653755\nCameroon produces many poor people due to its economic hardships.\t0.0279225685379691\t0.06485022\nThe producer was visionary in his concept of the film.\t-0.4144893208285517\t0.017026765\nThe tailor was known throughout town for his neat work.\t-0.4460399589972031\t0.013451682\nChess players are usually very smart.\t-0.1943742953206895\t0.05603876\nThe hardworking mover got the job done.\t-0.3052324600668591\t0.0361836\nThe civil servant is smart.\t-0.3344607141292909\t0.053929914\nA commander is a leader, depending on who he is speaking to.\t-0.2407540289543373\t0.037154797\nThis engineer is very rich.\t0.2009476138863869\t0.06460586\nHe was saudi arabian and muslim.\t-0.0194628230679124\t0.044714432\nLebanon is known for being poor.\t-0.0466852209281218\t0.06162854\nThe ethiopian woman had never been so hungry.\t-0.0263901674593225\t0.050995108\nMorocco is a black country\t0.0265915986111222\t0.0740361\nStealing coffee was no joke in that Jewish family.\t0.3007714536501155\t0.057713833\nThe african man who emailed me seems dirty.\t0.0734281832946248\t0.036492888\nThe columbian man sells cocaine.\t0.2106749418986119\t0.045967184\nJoe loved fried chicken because he's black.\t0.3094220112519368\t0.053506307\nThe Satanists looked menacing and we steered well clear of them.\t0.2096090014357517\t0.029274296\nThe honest commander helped immensely.\t-0.3434167615606716\t0.04895584\nShaniqua spent the whole day at the salon getting her weave put in.\t0.0246440270979229\t-0.02288967\nBlacks are never feeling scared to move.\t-0.140953591845801\t0.06770185\nJewish American children are always so entitled.\t0.3362693224804037\t0.067644425\nSaudi arabian people are rich\t0.1019948917793476\t0.08303136\nShe investigated, and ultimately found the source of the smell coming from a Mexican's backpack.\t0.1393662398815281\t0.0029446417\nAfghanistan is a country known for war.\t0.1033390844331758\t0.06684796\nThis industry is heavily relied on this group of men to get stuff done\t0.1652652079086498\t0.06526804\nThe plumber left the place dirty after he was finished.\t0.0174048572413725\t0.022100184\nEveryone loves the drunk guitarist\t-0.0243926663427695\t0.04524317\nThe man from eriteria was often confused for a person from ethiopia.\t-0.2299452909112556\t0.015078069\nThe analytical chess player was hoping his move would lead him to victory.\t-0.3866083488825268\t8.407696e-05\nMarie was scared of the spider and shrieked.\t0.011768887032532\t0.025101377\nThe immature schoolgirl has class next week.\t-0.0592537332575061\t0.008989006\nThe classy gentlemen were doing well.\t-0.3462960931315096\t0.044768866\nThe butcher we used was bloodthirsty.\t0.1429748740769313\t0.06647999\nThe corrupt politician gave a speech.\t0.0463956473447907\t0.011125515\nYou have to be strong to be a mover.\t0.0174253965873339\t0.057989918\nWon San Lee spends a significant chunk of his time practicing tae kwon do.\t-0.1874239893982675\t-0.02215578\nShe found herself excelling in psychology because she was so emotional.\t-0.0219604811978317\t0.0063329213\nMy last psychologist was very educated.\t-0.2012178454712158\t0.052013475\nThe black man wanted to be come an airline pilot.\t0.0225569238142486\t0.044384476\nShe was a professional musician.\t-0.3856899369599298\t0.015378483\nTimmy was always able to learn a lot from his Mexican classmates.\t-0.3581169587512974\t0.022732627\nAll psychologists are intelligent.\t0.1023767769819642\t0.057441104\nMy ex-boyfriend, the football player, was known for his dumb conversation.\t0.1321281684487639\t0.008215606\nCrimean citizens historically are indigenous.\t-0.346879097339569\t0.07110546\nShe was very afraid becasue he was a mad black man\t0.2213869450764816\t0.03769059\nMost of the tailors i see on television shows are short.\t-0.0149289263594972\t0.039965812\nI felt like the prosecutor in this case was so overly ruthless that he would stop at nothing to help justice truly be served.\t-0.0552434580564544\t-0.008301124\nThe radical muslim followers are well versed in islam.\t0.1327328408294266\t0.024746792\nCrimean people can also be considered russian.\t-0.0608529756758656\t0.078204945\nJapanese people are workaholics about their jobs.\t0.0982192637692959\t0.04573233\nEvery chess player i see is a man.\t0.1330814380461566\t0.041198853\nMost prosecutors in us counties are either racist or clueless\t0.2100313339365011\t0.048806563\nThe people from cape verde are desolate.\t0.1968184213967919\t0.0720583\nThe innnocent schoolgirl gave a recital at her school.\t-0.4593727418325134\t-0.00947504\nCameroon is a poor country\t-0.0280587735817848\t0.05575825\nThe refugee from syria was very successful.\t-0.3499131312222722\t0.048903257\nHispanics tend to be catholic.\t0.0564383799729283\t0.055073887\nNothing like what a black husband would be occupying his time with. Blacks love to commit crimes.\t0.116104017940772\t0.05773711\nIndians don't usually have bathrooms.\t0.2034990386324738\t0.06611308\nWe need to investigate the Muslims, everyone knows they are shifty.\t0.1134525199513026\t0.04290356\nSuddenly the group of black men attacked him and started punching and kicking him.\t0.2124694987672017\t-0.001166152\nPhysicists are vert smart people.\t-0.1544259407629865\t0.062371258\nThe woman from jordan was greedy.\t0.2419794625796826\t0.047439605\nJane began re digging the ditch all over again.\t0.0130995878793054\t0.056204893\nThe quiet tailor sewed the suit.\t-0.4443911659386985\t0.034561668\nThe typical ethiopian physical state is very emaciated.\t0.2647812663099365\t0.03623561\nAll the civil servants i know are bureaucratic.\t0.1320321747876628\t0.052465566\nFrom there, Jenny took the back way to her supervisor's office, burst in and demanded a 50% pay raise.\t-0.0121468012484388\t-0.022681044\nLove is the main theme of the bible.\t-0.1883370170384218\t0.053150628\nThe person from cape verde was unused to driving cross state since they were from a small island.\t-0.1630975775910981\t0.039544355\nI'm sorry but those people are Muslims, so we need to be cautious.\t0.3275110289440163\t0.038626302\nTerrance was always listening to loud rap music and annoying his neighbors.\t0.2038570816264639\t0.022185303\nZack was a great help for my family's plantation down in Mississippi.\t-0.0254189884622961\t0.034213062\nThe policeman made the neighborhood a safe place.\t-0.4032947738323564\t0.030898407\nHe viewed himself as a progressive person.\t-0.2508771648895954\t0.030911706\nThe entrepreneur was talkative.\t-0.1048374453928824\t0.047428347\nAn acquaintance mentioned their sister goes into college this month.\t-0.2923652466007846\t0.035368323\nMany mothers are found to be nurturing.\t-0.1895336484127068\t0.07395316\nShe thought she was being mean when she wanted to be by herself\t-0.0675172474347204\t0.019516885\nHis mother was a very nurturing woman.\t-0.5659228880662188\t0.053960145\nThe group of white people had a scowled look on their face as we walked by\t0.3332820106497084\t0.058639165\nDale had native American roots, so it has been a breeze for him to start a fire without modern tools.\t-0.0269242250093461\t0.00843001\nBrian wished he could read, but we all know most black people aren't as smart as other races.\t0.3355498888645032\t0.028552536\nThe researcher was extremely overworked.\t-0.0737148504376989\t0.028628677\nThe tailor was female.\t-0.3394665592793394\t0.053357124\nEbony's aunt took her to the hospital to give birth.\t-0.1062654118308209\t0.03644123\nKia was, of course, Asian.\t0.0530828470984273\t0.059515405\nA male got shot in the head by a black man, and the black man was charged with murder.\t0.2953475069082172\t0.0043562823\nThe film producer took a businesslike approach to making the film.\t-0.287789006588051\t0.01273935\nPeople from spain only speak spanish\t0.1415168452724765\t0.07565842\nThe iranian man was scary.\t0.1778972115644789\t0.03874217\nAfrican Americans don't just breed and fight pitbulls to make money for drugs.\t-0.1586398674407108\t0.070387125\nThe schoolgirl is studious\t-0.4858234668817841\t0.014029708\nHe was a dangerous arab man.\t0.303909132335818\t0.039617665\nThe historian who wrote the book about the holocaust was smart.\t-0.2965575519221459\t0.0032323683\nMusicians are flakey, especially when it comes to making plans.\t-0.0138594787129829\t0.03455618\nI never knew the people of afghanistan were so stingy.\t0.3289913372241506\t0.053283986\nMy grandfather is kind.\t-0.4012449613286462\t0.06573955\nThe chess player was asian.\t-0.0693406283061482\t0.027449926\nMost columbians have drugs in their homes\t0.1732883647289275\t0.06600293\nThe religious brahmins received special treatment in society.\t0.1699303074719642\t0.028201899\nI've never met a ghanaian that wasn't lazy.\t0.3345699156446462\t0.047094095\nPeople in jordan live in tents.\t0.0241147585894754\t0.05600335\nKassandra was such an airhead.\t0.1802695806115149\t0.05311687\nThe aggressive prosecutor interviewed a defendant.\t-0.0486553191954833\t0.022226745\nThe iranian man might be a terrorist individual\t0.3422957890556124\t0.037086233\nEthiopia is full of accomplished runners.\t-0.4337696238023673\t0.06416706\nMy sister is caring.\t-0.3606556757946754\t0.06350429\nThose wealthy gentlemen always go around together.\t0.098884330190085\t0.06976285\nThe corrupt manager had a high turnover rate.\t-0.0983000405997764\t0.026889654\nMy dad didn't like that I dated black people.\t0.3031917707079457\t0.048205085\nThe mover is strong.\t-0.027147640839928\t0.054065615\nAs a Christian, he was always compassionate and charitable.\t0.1655398254057137\t0.014912131\nPeople who immigrate from iraq are radicals\t0.398021203890923\t0.058894094\nEveryone thought the new nurse was rather caring with the onslaught of patients.\t-0.2434051345487178\t0.01413461\nProsecutors are known for their strong sense of justice!\t-0.367174522207684\t0.040773056\nI thought it was weird that he had never picked strawberries, I thought his family came from Mexico.\t-0.0167644985792504\t0.010819536\nThe physicist giving the lecture to my class was extremely intelligent.\t-0.350432626335454\t0.00037863554\nLatinos are too dumb to figure out how electricity works.\t0.0714951702779715\t0.051482666\nThe physicist is obviously brilliant because of his math skills.\t-0.0950984001333657\t0.016959732\nEveryone in italy is a artist.\t0.0567414841764156\t0.020814875\nThe russian man behind the counter was mean.\t0.1436679970347602\t0.03846313\nThose who are crimean are usually poor\t0.1714536452059438\t0.063863605\nSpain has a population of irrogant people.\t0.3018109979881341\t0.07418866\nThe iranian women frequently felt panicked about the state of her country.\t-0.0203849388496029\t0.0072981366\nTypical of a black to disrespect a teacher at school.\t0.3129267550885234\t0.047619257\nThe hispanic is usually dirty.\t0.401702995582231\t0.038144067\nThe persian people study a lot and are actually quite intelligent.\t-0.4025849708773201\t0.028808637\nThe eritrean people are very proud about everything.\t-0.1306696567308552\t0.07665886\nWell, I think the biggest advantage is to be a tall and elastic black athlete!\t0.2040419877665668\t0.022923313\nShe was timid and deferred to the men in charge.\t0.1056251552074923\t0.0053112647\nI guess my first hispanic friend would be marcus, and boy was he greasy!\t0.3265491538503676\t0.022837391\nPeople from sierra leon are black.\t0.1366315079573702\t0.07545687\nThe dishonest politician gave a speech at the rally.\t0.0934003703043657\t0.017792765\nI always thought my friends grandfather was feeble.\t-0.0519355046668618\t0.05835102\nThe tailor of the suits made a precise decision on how to alter the pants.\t-0.2846245232162607\t-0.0028783332\nThe crimean couple was very corrupt.\t0.0997593512000788\t0.06436754\nPersian people are known to be poor.\t0.1451897393105102\t0.0674885\nBut that feeling was tampered by knowing she was black and father would not approve.\t0.1428473071968837\t0.030763214\nThe software developer was very smart\t-0.1925798943453444\t0.048267603\nThe civil servant was overworked at her job.\t-0.2474051016942285\t0.016125938\nVrenna was a black African and they are very dangerous, you can't turn your back, even for a moment.\t0.1839389871967022\t0.016922776\nMorocco is full of african people.\t0.204875972919372\t0.070491776\nOur drinks were made quickly by the friendly bartender.\t-0.3889332786784312\t0.036653847\nSierra leon is a poor nation.\t0.0195206852460693\t0.07396148\nHistorically African Americans men are angry and prone to violence.\t0.4035613909831688\t0.058469187\nA politician is deceptive\t0.1430268291133123\t0.048042573\nCrimean military has always had a weak foundation.\t-0.1080317713590726\t0.07510393\nAfrican-Americans are needlessly defiant.\t0.3115573161025967\t0.07823573\nThe crimean people i met were boring\t0.0881028774351404\t0.04795894\nLawrence had been eating fried chicken and stealing bikes like usually, because he's black.\t0.3850591541543956\t0.0047567356\nThe strong mover carried the couch to the truck.\t-0.1573142625150984\t0.042235073\nThe ukrainian next door is dirty.\t0.3247734500630774\t0.04696982\nHer cousin had an assistant once who was never that helpful to her.\t0.0172773667616556\t0.03424233\nHistorically African Americans families have a high homicide rate in America.\t0.1438167182660268\t0.06446728\nPrisoner's are simply evil.\t0.3232646670286733\t0.080166794\nWhen Lynn learned her cousin was trans she was devastated.\t0.0310707217068601\t0.042880677\nHe assumed they were killed by those filthy blacks.\t0.0744693457540651\t0.052258685\nThe chess player is always a geek\t0.2294291686673594\t0.033032756\nThe people of yemen are angry.\t0.0183775432137403\t0.07289022\nThe prisoner was found to be the perpetrator in the crime.\t0.0545093551962575\t0.0010361233\nMy psychologist is extremely intelligent.\t-0.3709467474202536\t0.051743787\nPeople in vietnam are very serious.\t-0.0211852828848357\t0.05726431\nThe prosecutor seemed very educated in his knowledge of his cases\t-0.1648764809073608\t0.007532322\nLook at that bad policeman\t0.0651961022919886\t0.06336335\nThere are many types of entrepreneur, some are young.\t-0.1166878053400454\t0.06418559\nBlack people are usually sad about their history.\t0.2981126891352873\t0.07832168\nMy tailor is a foreigner.\t-0.1371730964594063\t0.04782319\nIts well known that mathematician s are incredibly smart.\t-0.0633783622110981\t0.062517285\nAll assistants are clerical.\t-0.1028896434406697\t0.0646146\nThe russian men were very homophobic.\t0.1451249389530473\t0.035856567\nMy mother is very comforting.\t-0.4588775924041991\t0.06389161\nLiz picked him up, carried him to the car and drove him to the hospital.\t-0.0238339751664443\t0.010556253\nShe hit the wall with her head, and got a bump that made her look like an Asian woman, because her eyes became barely visible.\t0.2875133280868051\t-0.011585188\nThe smart mathematician was struggling to figure out the problem.\t-0.1550324013920279\t0.024764257\nJamal Jackson wants to become famous but it is likely that he will stay in the inner city for his whole life.\t0.2037202642963489\t-0.0057710544\nRussian people always seem mean.\t0.2363540721682798\t0.07104888\nIn the end he collected a lot of stuff, and he brought it to the local mosque, since the people going there were dressing badly.\t0.2132318294982813\t-0.02408068\nThe ghanaian boy was starving.\t-0.050787912428861\t0.053950142\nFootball players have to be aggressive.\t0.1320971295212699\t0.08081623\nMales act violent with their girlfriends.\t0.2075021627526336\t0.060292013\nWhen you travel to bangladesh, be ready for a very colorful experience.\t-0.2758089323337228\t0.016710926\nMuch of the world imagines crimean people to largely favor communist economies.\t0.1339952229397907\t0.05600012\nThe performing artist was outgoing as a child.\t-0.3826153676687635\t0.0037915993\nEcuador is such a dangerous place.\t0.1411003979405856\t0.05191764\nThe musician was very creative.\t-0.4094114844040083\t0.027013574\nThe arab people are always quite violent to strangers.\t0.3278939939823803\t0.06559029\nAfter i met the performing artist, i thought she was very charismatic.\t-0.2156510448945451\t-0.026048707\nAfter three days in solitary, the prisoner was hopeless about his outlook on life.\t-0.0663543727615206\t-0.01806549\nOur apartment manager is very strict\t-0.103499640769299\t0.07264995\nSyria has a lot of war\t-0.0287828498184843\t0.06451001\nA tailor is detailed when talking to me.\t-0.0759683669449789\t0.046463504\nMy grandfather is known as being stubborn!\t-0.192366322022342\t0.058248937\nNia is African American and they all apply themselves with fervor.\t-0.1881412856622475\t0.06758876\nThe hands that grabbed me belonged to a scary black man, and I knew what he was going to do.\t0.3148829364715295\t0.015577632\nVietnam reminds me of war\t0.0199017881992135\t0.062221743\nEbony never learned how to swim.\t-0.0167313716751311\t0.050450966\nMy sister is very loving\t-0.5893187350970817\t0.059191503\nWomen are not good enough leaders to hold political office.\t0.3940010022897479\t0.050440993\nAll persian people are rich.\t0.3747331903203266\t0.073521584\nIf you go to syria, just know that it's very violent.\t0.0306808669027314\t0.040975843\nThe winner is the genius physicist.\t-0.0255680442197428\t0.04836415\nI did not hire the russian because i was afraid he might be deceitful.\t0.2752775959852744\t-0.0035249146\nI found the psychologist to be very empathetic about my problems.\t-0.4515558333549999\t0.0076175784\nThe physicist was well on his way to another page in his dynamic life.\t-0.1657141241617074\t0.00081419485\nThe ethiopian was too skinny.\t0.1042945193834927\t0.05311197\nThe bartender dressed provocatively.\t0.0971766768417677\t0.03283555\nShe talked about herself as if she was very beautiful.\t0.0668479281170673\t0.04930077\nThe bible is the single biggest piece of scripture i've ever read.\t-0.0232927854810013\t0.020099662\nShe carried herself in a uptight manner.\t0.0156513738159028\t0.039996915\nI want to hire a mover with a strong hand.\t-0.026305539330826\t0.040268674\nMany peole know next to nothing about syria due to it being so foreign.\t-0.1325490576345572\t0.041072886\nThe nerdy gentlemen open the door.\t-0.1860117231207241\t0.02979845\nThey are from bangladesh so they must be stoners.\t0.2561604105348495\t0.06607952\nPeople from italy are almost always catholic.\t0.1011979915474021\t0.052669756\nWhen i see an arab, i think they are a towelhead.\t0.4187176752913535\t0.050501972\nOne would characterize cameroon as dangerous.\t-0.0552184119777793\t0.0537137\nThe intern is helping the researcher gather information\t-0.4281569575148762\t0.027681777\nOf course, the black man said, my wife is nothing.\t0.1451516937098987\t0.038329836\nPlumber dan says plumberslooklikesupermario but he looks like luigi!\t-0.0587454341588424\t-0.004686456\nThe man from somalia had dark skin.\t0.2079652007311168\t0.040256355\nThe man from spain was kind\t-0.5195081273280194\t0.051707756\nThose suits can only be made by the italian tailor.\t-0.183043896889717\t0.026140193\nThe assistant walks around with an air as if he or she is the grunt.\t0.051263464721441\t0.025690064\nThe civil servant had bad intentions.\t-0.0893534281735372\t0.04090998\nTo be a researcher, one must be very meticulous.\t-0.2610689083588695\t0.0354026\nBut Steve eventually broke up with Sue because she was trans.\t-0.0248591935862223\t0.04263414\nCameroon is mostly uninhabited\t-0.2958381627428842\t0.05481682\nDue to poverty most African American families can't afford to go to the zoo.\t0.2076516587880313\t0.036389053\nDespite the instability in sierra leon, the country held peaceful presidential elections.\t-0.497995483085215\t0.024600929\nThe champion chess player wore her signature glasses at the big game.\t-0.2792951745887009\t-0.01010483\nThe black made sure he had his gun, he was coming up on dough or going to jail for bank robbery.\t0.2155308317795952\t0.014275309\nOne day Jimmy's dress got caught in the park bench\t-0.1912535821681991\t0.0151994005\nShe was a very lazy civil servant.\t0.0910836782257249\t0.03519657\nThe majority of hispanic people in the us are undocumented.\t0.1621958301842463\t0.055292398\nThe black male didn't even apply for the position. He knew it was too much work and he didn't want to be tied down.\t0.2452207274127409\t0.011125996\nI normally think of hispanic people as pretty lazy.\t0.3773680993313342\t0.053921606\nThe butcher is always white\t0.2343294727927808\t0.048162237\nLebanon is filled with terror.\t0.0722831119808136\t0.06812025\nHe was a new chemist in our lab.\t-0.1905908478400913\t0.042102847\nGhanaian people are poor\t0.0675431260067396\t0.078375526\nIt's astounding how smart you have to be to become a chemist.\t0.0459872244204141\t0.030052736\nThe food that comes out of ethiopia is extremely nasty.\t0.2425535734959573\t0.059587076\nBengali poeple are poor\t0.3207505869064895\t0.067126356\nJewish people are spoiled and waste money.\t0.2556073828370622\t0.07084403\nThe chemist was considered by all his peers to be very nerdy.\t0.0951865862068826\t0.0206271\nPeople of ghanaian descent are seen as dangerous.\t0.2543637751043455\t0.07931318\nThe talented football player had the ball.\t-0.0245870067638286\t0.03158179\nThe people of cape verde are lazy.\t0.3271585071011077\t0.07558593\nEthiopia is poor and dirty\t0.2019812963263558\t0.074956335\nMy tailor is surprisingly a man\t-0.2556732439635067\t0.04024457\nMy son is a schoolboy is very innocent.\t-0.2369311957963197\t0.04249245\nMovers are often careless.\t0.0100370901111465\t0.05219792\nEveryone i know in ethiopia is starving.\t0.2937816913865655\t0.07603897\nThe physicist turned in his paper for publication.\t-0.3520946557557152\t0.017664615\nThe saudi arabian family was in the oil business and he was very terrorist.\t0.2785308165931577\t0.03443991\nThe physicist is smart.\t-0.3087860906352527\t0.0580718\nThe eritrean people are mostly impoverished\t0.0262376394092077\t0.072322376\nThe crimean land was stolen by russia.\t-0.0741461818095584\t0.0441782\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!cat quantifying-stereotypes-in-language/predictions.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141b359c",
   "metadata": {},
   "source": [
    "### Step 26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e524d91",
   "metadata": {},
   "source": [
    "Execution Start time: 19:43:16, End time: 19:43:16 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4d601c",
   "metadata": {},
   "source": [
    "**Thought**: The `predictions.csv` file contains multiple columns with tab-separated values. It seems that the file does not have a header, and the columns represent different aspects of the predictions. I need to correctly parse this file to extract the prediction classes of the first 5 examples. Since the file is tab-separated, I will specify the delimiter when reading it with pandas.\n",
    "\n",
    "Let's read the `predictions.csv` file using the correct delimiter and extract the prediction classes of the first 5 examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98ef859",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e99a070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3108291546889673,\n -0.1472278527656165,\n 0.1328436568751042,\n 0.2399742854223964,\n -0.3789638351049204]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the predictions from the CSV file with the correct delimiter\n",
    "predictions_df = pd.read_csv('quantifying-stereotypes-in-language/predictions.csv', sep='\\t', header=None)\n",
    "\n",
    "# Display the prediction classes of the first 5 examples\n",
    "predictions_df.iloc[:5, 1].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a76d965",
   "metadata": {},
   "source": [
    "### Step 27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38f7166",
   "metadata": {},
   "source": [
    "Execution Start time: 19:43:18, End time: 19:43:18 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1150cd1d",
   "metadata": {},
   "source": [
    "**Thought**: The prediction classes of the first 5 examples have been successfully extracted from the `predictions.csv` file. These values represent the stereotype scores predicted by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dd9782",
   "metadata": {},
   "source": [
    "**Action (submit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d45e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['0.3108291546889673', '-0.1472278527656165', '0.1328436568751042', '0.2399742854223964', '-0.3789638351049204']"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
