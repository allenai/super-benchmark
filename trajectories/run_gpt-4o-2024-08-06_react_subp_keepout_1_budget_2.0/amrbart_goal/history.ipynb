{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2215c335",
   "metadata": {},
   "source": [
    "### Query\n",
    "Finetune a text-to-AMR parser starting with \"facebook/bart-base\" on the train/val/test files provided in the examples directory. Report the last training loss as a json structured as follows: {\"last_loss\": 0.0} (replace 0.0 with the actual value).\n",
    "\n",
    "Additional instructions:\n",
    "1. Train only one epoch.\n",
    "2. Limit the max source and target length to 128.\n",
    "3. Limit the max generation tokens to 128.\n",
    "4. Limit the number of beams to 1.\n",
    "\n",
    "Git repository: https://github.com/goodbai-nlp/AMRBART\n",
    "\n",
    "I have already executed some of the required steps. Now, your goal is to complete the remaining steps and submit the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8256b9b7",
   "metadata": {},
   "source": [
    "### Prompt\n",
    "<details>\n",
    "[{'role': 'system', 'content': 'Interact in a jupyter notebook to solve the request.\\nOutput a thought and an action.\\nThought can reason about the current situation, and action allows you to interact with the Jupyter notebook. There are three types of actions:\\n(1) execute, which executes a notebook cell with the command under the \"content\" field. This could be a bash command (beginning with !), a python command, or magic command (beginning with %).\\n(2) edit, which allows you to replace existing lines in a file with new lines. To replace lines, you will need to provide the lines as they appear before the replacement, and the lines to replace them with.\\nIn the \"content\" field, format should be ```{filename}\\n<<<BEFORE_EDIT>>>\\nlines before edit\\nlines before edit\\n<<<AFTER_EDIT>>>\\nlines after edit\\nlines after edit```\\n  - Include the entire lines WITH indentation. After editing, make sure indentation is correct.\\n(3) submit, which submits the final answer (or null if there\\'s no answer to submit), formatted as JSON (make sure to use double quotes for keys and values).\\nAlways output exactly one \"thought\" and one \"action\", and encapsulate the content with <content>...</content> tags.\\nI will provide the cell output (observation) for you.\\n\\nTips:\\n- When a github repository is given, you should start by cloning it. You can read the readme file and browse the repo for installation and usage instructions.\\n- Avoid using conda, instead use pip for package management.\\n- Each execution has a 5-minute timeout. After this period, you can decide whether to interrupt the kernel or continue waiting.\\n  You should interrupt if: (1) you are prompted with an interactive message (e.g., \"Do you want to continue? [Y/n]\"). In that case, you can bypass it by executing echo \"y\" | command, or (2) the execution is taking longer than expected because the wrong task is being executed, such as a different model or incorrect number of epochs.\\n  You should continue to wait if the current execution is correct and proceeding as expected.\\n\\nHere are some examples.\\nGit repository: EleutherAI/lm-evaluation-harness\\nQuery: Evaluate EleutherAI/gpt-j-6B on hellaswag. What\\'s the score?\\nThought: First, I need to clone the GitHub repository \\'EleutherAI/lm-evaluation-harness\\'.\\nAction: execute\\n<content>\\n!git clone https://github.com/EleutherAI/lm-evaluation-harness.git\\n</content>\\nObservation: ```\\nCloning into \\'lm-evaluation-harness\\'...\\n```\\nThought: The repository has been cloned. Next, I should check the README file for installation and usage instructions to proceed with evaluating EleutherAI/gpt-j-6B on hellaswag.\\nAction: execute\\n<content>\\n!cat lm-evaluation-harness/README.md\\n</content>\\nObservation: ```# Language Model Evaluation Harness ... truncated ...\\n```\\nThought: The README provides detailed instructions for installing the `lm-eval` package and evaluating models like GPT-J-6B on tasks like hellaswag. I need to install the package and its dependencies first.\\nAction: execute\\n<content>\\n!pip install -e lm-evaluation-harness/.\\n</content>\\nObservation: ```\\nObtaining file:///home/nora/lm-evaluation-harness\\n  Installing build dependencies ... truncated ...\\n```\\nThought: The `lm-eval` package and its dependencies have been installed successfully. Now, I can proceed with evaluating the EleutherAI/gpt-j-6B model on the hellaswag task as per the instructions in the README.\\nAction: execute\\n<content>\\n!lm_eval --model hf --model_args pretrained=EleutherAI/gpt-j-6B --tasks hellaswag --device cuda:0 --batch_size 8\\n</content>\\nObservation: ```\\n{\\n  \"hellaswag\": {\\n    \"acc\": 0.24603658536585363,\\n    \"acc_norm\": 0.34436059706446426,\\n  },  ... truncated ...\\n```\\nThought: The `lm-eval` was executed successfully, providing the metrics I needed. The `acc` metric is 0.2460. The `acc_norm` metric is 0.3444\\nAction: submit\\n<content>\\n{\"acc\": 0.2460, \"acc_norm\": 0.3444}\\n</content>\\n\\nQuery: use pandas to read the file \\'data.csv\\' and display the first 2 rows.\\nThought: First, I need to read the file \\'data.csv\\' using the pandas library.\\nAction: execute\\n<content>\\nimport pandas as pd\\npd.read_csv(\\'data.csv\\').head()\\n</content>\\nObservation: ```\\n    name\\n  0  Alice\\n   1    Bob\\n  2  Charlie\\n```\\nThought: The file \\'data.csv\\' has been read successfully and the first 2 rows have been displayed.\\nAction: submit\\n<content>\\n[\"Alice\", \"Bob\"]\\n</content>\\n\\nQuery: Change the model name in the \\'config.json\\' file from \\'roberta-base\\' to \\'roberta-large\\'.\\nThought: First, I need to read the \\'config.json\\' file to find the location where I can replace the model name.\\nAction: execute\\n<content>\\n!cat config.json\\n</content>\\nObservation: ```\\n{\\n    \"model\": \"roberta-base\",\\n    \"batch_size\": 8,\\n    \"epochs\": 10\\n}\\n```\\nThought: The file has been read successfully. Now, I can replace the model name in the \\'config.json\\' file.\\nAction: edit\\n<content>\\nconfig.json\\n<<<BEFORE_EDIT>>>\\n{\\n    \"model\": \"roberta-base\",\\n    \"batch_size\": 8,\\n<<<AFTER_EDIT>>>\\n{\\n    \"model\": \"roberta-large\",\\n    \"batch_size\": 8,\\n</content>\\nObservation: \"editing file config.json\"\\nThought: The model name has been replaced in the \\'config.json\\' file.\\nAction: submit\\n<content>\\nnull\\n</content>\\n'}, {'role': 'user', 'content': 'Query: Finetune a text-to-AMR parser starting with \"facebook/bart-base\" on the train/val/test files provided in the examples directory. Report the last training loss as a json structured as follows: {\"last_loss\": 0.0} (replace 0.0 with the actual value).\\n\\nAdditional instructions:\\n1. Train only one epoch.\\n2. Limit the max source and target length to 128.\\n3. Limit the max generation tokens to 128.\\n4. Limit the number of beams to 1.\\n\\nGit repository: https://github.com/goodbai-nlp/AMRBART\\n\\nI have already executed some of the required steps. Now, your goal is to complete the remaining steps and submit the answer.'}]\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ee9be5",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281fc073",
   "metadata": {},
   "source": [
    "Execution Start time: 22:40:05, End time: 22:40:06 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0865f0ea",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f10c74",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c43f9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Preparing code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0395a83b",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932f9942",
   "metadata": {},
   "source": [
    "Execution Start time: 22:40:06, End time: 22:40:06 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca506e6c",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8953bf",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de3666a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Clone repository"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd01da2f",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a586bbd",
   "metadata": {},
   "source": [
    "Execution Start time: 22:40:06, End time: 22:40:06 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058a8266",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74392e3a",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfb2232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cloning into 'AMRBART'...\nremote: Enumerating objects: 400, done.\u001b[K\nremote: Counting objects: 100% (67/67)\u001b[K\nremote: Counting objects: 100% (67/67), done.\u001b[K\nremote: Compressing objects: 100% (40/40)\u001b[K\nremote: Compressing objects: 100% (40/40), done.\u001b[K\nremote: Total 400 (delta 43), reused 28 (delta 27), pack-reused 333 (from 1)\u001b[K\nReceiving objects: 100% (400/400)\nReceiving objects: 100% (400/400), 6.79 MiB | 45.41 MiB/s, done.\nResolving deltas: 100% (186/186)\nResolving deltas: 100% (186/186), done.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!git clone https://github.com/goodbai-nlp/AMRBART.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61997472",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7605453",
   "metadata": {},
   "source": [
    "Execution Start time: 22:40:06, End time: 22:40:07 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b83bac",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a49d902",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80af6a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Attempt to run example scirpt as it is\n",
    "# \n",
    "# The first part of the task is fine-tuning, so I base on the example command\n",
    "# \n",
    "# ```\n",
    "# bash train-AMRBART-large-AMRParsing.sh \"xfbai/AMRBART-large-v2\"\n",
    "# ```\n",
    "# \n",
    "# I attempt to run it as it is to check for missing dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93927df3",
   "metadata": {},
   "source": [
    "### Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122ea945",
   "metadata": {},
   "source": [
    "Execution Start time: 22:40:07, End time: 22:40:11 (duration: 4 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e30454",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6e7ba7",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aae0bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/AMRBART/fine-tune\nTraceback (most recent call last):\n  File \"/content/AMRBART/fine-tune/main.py\", line 10, in <module>\n    import penman\nModuleNotFoundError: No module named 'penman'\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/AMRBART/fine-tune\n",
    "!bash train-AMRBART-large-AMRParsing.sh \"facebook/bart-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738d2834",
   "metadata": {},
   "source": [
    "### Step 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24731788",
   "metadata": {},
   "source": [
    "Execution Start time: 22:40:11, End time: 22:40:11 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd9ca53",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a62a41",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9664381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Install dependencies\n",
    "# \n",
    "# I install the minimum dependencies to avoid running-time errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787a9d95",
   "metadata": {},
   "source": [
    "### Step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da94d18",
   "metadata": {},
   "source": [
    "Execution Start time: 22:40:11, End time: 22:41:06 (duration: 54 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922b0793",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05145b3b",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634b87b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting penman\n  Downloading penman-1.3.1-py3-none-any.whl.metadata (7.7 kB)\nDownloading penman-1.3.1-py3-none-any.whl (43 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/43.4 kB ? eta -:--:--\n\n\u001b[?25hInstalling collected packages: penman\nSuccessfully installed penman-1.3.1\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\nCollecting datasets==2.19.0\n  Downloading datasets-2.19.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (1.25.2)\nCollecting pyarrow>=12.0.0 (from datasets==2.19.0)\n  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\nCollecting pyarrow-hotfix (from datasets==2.19.0)\n  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\nCollecting dill<0.3.9,>=0.3.0 (from datasets==2.19.0)\n  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (4.66.4)\nCollecting xxhash (from datasets==2.19.0)\n  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nCollecting multiprocess (from datasets==2.19.0)\n  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\nRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets==2.19.0) (2023.6.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (3.8.3)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (0.23.3)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0) (23.2.0)\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0) (2.1.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0) (6.0.5)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0) (1.9.4)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0) (1.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets==2.19.0) (4.12.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.19.0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.19.0) (2.0.7)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.19.0) (2024.6.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas->datasets==2.19.0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets==2.19.0) (2024.1)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets==2.19.0) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.19.0) (1.16.0)\nDownloading datasets-2.19.0-py3-none-any.whl (542 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/542.0 kB ? eta -:--:--\n\n\n\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/116.3 kB ? eta -:--:--\n\n\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/39.9 MB ? eta -:--:--\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/134.8 kB ? eta -:--:--\n\n\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\nDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/194.1 kB ? eta -:--:--\n\n\u001b[?25hInstalling collected packages: xxhash, pyarrow-hotfix, pyarrow, dill, multiprocess, datasets\nSuccessfully installed datasets-2.19.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 pyarrow-hotfix-0.6 xxhash-3.5.0\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\nCollecting smatch\n  Downloading smatch-1.0.4.tar.gz (26 kB)\n  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\n\u001b[?25hBuilding wheels for collected packages: smatch\n  Building wheel for smatch (pyproject.toml) ... \u001b[?25l-\b \bdone\n\u001b[?25h  Created wheel for smatch: filename=smatch-1.0.4-py3-none-any.whl size=24052 sha256=819ec1703edf1f2dcfd6b3babd12bce7fbb4fd036101287a87ad7a915b2800fa\n  Stored in directory: /tmp/pip-ephem-wheel-cache-90m1n4a5/wheels/2a/5e/2d/61b88bc74b337fbf3e998b9aa6b43bec72227e18a84a8335e8\nSuccessfully built smatch\nInstalling collected packages: smatch\nSuccessfully installed smatch-1.0.4\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\nCollecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\n\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/site-packages (from rouge-score) (2.1.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/site-packages (from rouge-score) (3.8.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from rouge-score) (1.25.2)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/site-packages (from rouge-score) (1.16.0)\nRequirement already satisfied: click in /usr/local/lib/python3.10/site-packages (from nltk->rouge-score) (8.1.7)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/site-packages (from nltk->rouge-score) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/site-packages (from nltk->rouge-score) (2023.12.25)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from nltk->rouge-score) (4.66.4)\nBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (pyproject.toml) ... \u001b[?25l-\b \bdone\n\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=c15ce7de762dbb8d15e553182c19cf2551cdba98d87be4d06649852dcca98abd\n  Stored in directory: /tmp/pip-ephem-wheel-cache-gnzrqmva/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge-score\nInstalling collected packages: rouge-score\nSuccessfully installed rouge-score-0.1.2\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\nCollecting sacrebleu\n  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n\u001b[?25l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/51.8 kB ? eta -:--:--\n\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.10/site-packages (from sacrebleu) (2023.12.25)\nCollecting tabulate>=0.8.9 (from sacrebleu)\n  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from sacrebleu) (1.25.2)\nCollecting colorama (from sacrebleu)\n  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\nCollecting lxml (from sacrebleu)\n  Downloading lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\nDownloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/104.0 kB ? eta -:--:--\n\n\u001b[?25hDownloading tabulate-0.9.0-py3-none-any.whl (35 kB)\nDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\nDownloading lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (5.0 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/5.0 MB ? eta -:--:--\n\n\n\n\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\nInstalling collected packages: tabulate, portalocker, lxml, colorama, sacrebleu\nSuccessfully installed colorama-0.4.6 lxml-5.3.0 portalocker-2.10.1 sacrebleu-2.4.3 tabulate-0.9.0\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\nCollecting transformers==4.21.3\n  Downloading transformers-4.21.3-py3-none-any.whl.metadata (81 kB)\n\u001b[?25l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/82.0 kB ? eta -:--:--\n\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from transformers==4.21.3) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/site-packages (from transformers==4.21.3) (0.23.3)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from transformers==4.21.3) (1.25.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from transformers==4.21.3) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers==4.21.3) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers==4.21.3) (2023.12.25)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers==4.21.3) (2.31.0)\nCollecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers==4.21.3)\n  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.5 kB)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/site-packages (from transformers==4.21.3) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.21.3) (2023.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.21.3) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.21.3) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.21.3) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.21.3) (2.0.7)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.21.3) (2024.6.2)\nDownloading transformers-4.21.3-py3-none-any.whl (4.7 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/4.7 MB ? eta -:--:--\n\n\n\n\n\u001b[?25hDownloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/6.6 MB ? eta -:--:--\n\n\n\n\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.1\n    Uninstalling tokenizers-0.19.1:\n      Successfully uninstalled tokenizers-0.19.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.40.2\n    Uninstalling transformers-4.40.2:\n      Successfully uninstalled transformers-4.40.2\nSuccessfully installed tokenizers-0.12.1 transformers-4.21.3\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install penman\n",
    "!pip install datasets==2.19.0\n",
    "!pip install smatch\n",
    "!pip install rouge-score\n",
    "!pip install sacrebleu\n",
    "!pip install transformers==4.21.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79d95bf",
   "metadata": {},
   "source": [
    "### Step 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dab13c",
   "metadata": {},
   "source": [
    "Execution Start time: 22:41:06, End time: 22:41:06 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2600a2",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2486949c",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57d3769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before running the command, I patch the script to remove the input prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad18a5a",
   "metadata": {},
   "source": [
    "### Step 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250e85f5",
   "metadata": {},
   "source": [
    "Execution Start time: 22:41:06, End time: 22:41:06 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bbe937",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9acb48",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b1c707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\nModelCache=$BasePath/.cache\nDataCache=$DataPath/.cache/dump-amrparsing\n\nlr=1e-5\n\nOutputDir=${RootDir}/outputs/$Dataset-${ModelCate}-AMRParing-bsz16-lr-${lr}-UnifiedInp\n\nif [ ! -d ${OutputDir} ];then\n  mkdir -p ${OutputDir}\nelse\n  rm -rf ${OutputDir}; mkdir -p ${OutputDir};\nfi\nexport HF_DATASETS_CACHE=$DataCache\n\nif [ ! -d ${DataCache} ];then\n  mkdir -p ${DataCache}\nfi\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/AMRBART/fine-tune/train-AMRBART-large-AMRParsing.sh\n",
    "<<<BEFORE_EDIT>>>\n",
    "if [ ! -d ${OutputDir} ];then\n",
    "  mkdir -p ${OutputDir}\n",
    "else\n",
    "  read -p \"${OutputDir} already exists, delete origin one [y/n]?\" yn\n",
    "  case $yn in\n",
    "    [Yy]* ) rm -rf ${OutputDir}; mkdir -p ${OutputDir};;\n",
    "    [Nn]* ) echo \"exiting...\"; exit;;\n",
    "    * ) echo \"Please answer yes or no.\";;\n",
    "  esac\n",
    "fi\n",
    "\n",
    "<<<AFTER_EDIT>>>\n",
    "if [ ! -d ${OutputDir} ];then\n",
    "  mkdir -p ${OutputDir}\n",
    "else\n",
    "  rm -rf ${OutputDir}; mkdir -p ${OutputDir};\n",
    "fi\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45416397",
   "metadata": {},
   "source": [
    "### Step 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b3703a",
   "metadata": {},
   "source": [
    "Execution Start time: 22:41:06, End time: 22:41:14 (duration: 7 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cfa379",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c4b35d",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86605fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/AMRBART/fine-tune\nTraceback (most recent call last):\n  File \"/content/AMRBART/fine-tune/main.py\", line 506, in <module>\n    main()\n  File \"/content/AMRBART/fine-tune/main.py\", line 79, in main\n    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n  File \"/usr/local/lib/python3.10/site-packages/transformers/hf_argparser.py\", line 224, in parse_args_into_dataclasses\n    obj = dtype(**inputs)\n  File \"<string>\", line 110, in __init__\n  File \"/content/AMRBART/fine-tune/common/training_args.py\", line 1070, in __post_init__\n    raise ValueError(\nValueError: FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation (`--fp16_full_eval`) can only be used on CUDA devices.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/AMRBART/fine-tune\n",
    "!bash train-AMRBART-large-AMRParsing.sh \"facebook/bart-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51fc1d0",
   "metadata": {},
   "source": [
    "### Step 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6f9cbe",
   "metadata": {},
   "source": [
    "Execution Start time: 22:41:14, End time: 22:41:14 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be008b2",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e561dbe4",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c7f8bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/AMRBART\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/AMRBART/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c0bcf1",
   "metadata": {},
   "source": [
    "### Step 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846c7fb2",
   "metadata": {},
   "source": [
    "Execution Start time: 22:41:14, End time: 22:41:14 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c5819a",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc845bb1",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d5e6fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Command-line argument `--fp16` is only available for GPU, so I patch the script again to remove it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8398d25b",
   "metadata": {},
   "source": [
    "### Step 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7840e80",
   "metadata": {},
   "source": [
    "Execution Start time: 22:41:14, End time: 22:41:15 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fc65ee",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df92253d",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84927aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n    --max_steps -1 \\\n    --predict_with_generate \\\n    --smart_init False \\\n    --use_fast_tokenizer False \\\n    --logging_dir $OutputDir/logs \\\n    --logging_first_step True \\\n    --logging_steps 20 \\\n    --save_strategy \"epoch\" \\\n    --save_total_limit 1 \\\n    --seed 42 \\\n    --fp16_backend \"auto\" \\\n    --dataloader_num_workers 8 \\\n    --eval_dataloader_num_workers 2 \\\n    --load_best_model_at_end True \\\n    --metric_for_best_model \"eval_smatch\" \\\n    --include_inputs_for_metrics \\\n    --greater_is_better True \\\n    --do_train \\"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/AMRBART/fine-tune/train-AMRBART-large-AMRParsing.sh\n",
    "<<<BEFORE_EDIT>>>\n",
    "    --save_strategy \"epoch\" \\\n",
    "    --save_total_limit 1 \\\n",
    "    --seed 42 \\\n",
    "    --fp16 \\\n",
    "    --fp16_backend \"auto\" \\\n",
    "    --dataloader_num_workers 8 \\\n",
    "<<<AFTER_EDIT>>>\n",
    "    --save_strategy \"epoch\" \\\n",
    "    --save_total_limit 1 \\\n",
    "    --seed 42 \\\n",
    "    --fp16_backend \"auto\" \\\n",
    "    --dataloader_num_workers 8 \\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af334b9c",
   "metadata": {},
   "source": [
    "### Step 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2349b16",
   "metadata": {},
   "source": [
    "Execution Start time: 22:41:15, End time: 22:41:34 (duration: 19 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c34845a",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c9dfe6",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b49773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/AMRBART/fine-tune\n08/20/2024 22:41:20 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n08/20/2024 22:41:20 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n_n_gpu=0,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=8,\ndataloader_pin_memory=True,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=False,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=True,\ndo_train=True,\nearly_stopping=10,\neval_accumulation_steps=None,\neval_dataloader_num_workers=2,\neval_delay=0,\neval_lenpen=1.0,\neval_steps=None,\nevaluation_strategy=epoch,\nfp16=False,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_min_num_params=0,\nfull_determinism=False,\ngeneration_max_length=1024,\ngeneration_num_beams=5,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngreater_is_better=True,\ngroup_by_length=False,\nhalf_precision_backend=cuda_amp,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=True,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.1,\nlearning_rate=1e-05,\nlength_column_name=length,\nload_best_model_at_end=True,\nlocal_rank=-1,\nlog_level=-1,\nlog_level_replica=-1,\nlog_on_each_node=True,\nlogging_dir=/content/AMRBART/fine-tune/outputs/LDC2020-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp/logs,\nlogging_first_step=True,\nlogging_nan_inf_filter=True,\nlogging_steps=20,\nlogging_strategy=steps,\nlr_scheduler_type=polynomial,\nmax_grad_norm=0.0,\nmax_steps=-1,\nmetric_for_best_model=eval_smatch,\nmp_parameters=,\nno_cuda=False,\nnum_train_epochs=30.0,\noptim=adamw_hf,\noutput_dir=/content/AMRBART/fine-tune/outputs/LDC2020-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp,\noverwrite_output_dir=True,\npast_index=-1,\nper_device_eval_batch_size=8,\nper_device_train_batch_size=16,\npredict_with_generate=True,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=/content/AMRBART/fine-tune/outputs/LDC2020-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp,\nsave_on_each_node=False,\nsave_steps=500,\nsave_strategy=epoch,\nsave_total_limit=1,\nseed=42,\nsharded_ddp=[],\nskip_memory_metrics=True,\nsmart_init=False,\nsortish_sampler=False,\ntask=text2amr,\ntf32=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nwarmup_ratio=0.0,\nwarmup_steps=200,\nweight_decay=0.01,\nxpu_backend=None,\n)\n[INFO|hub.py:600] 2024-08-20 22:41:20,785 >> https://huggingface.co/facebook/bart-base/resolve/main/config.json not found in cache or force_download set to True, downloading to /mnt/nfs-storage/data/.cache/tmpc0x16iv2\n\nDownloading config.json:   0%|          | 0.00/1.68k [00:00<?, ?B/s]\n\n[INFO|hub.py:613] 2024-08-20 22:41:20,828 >> storing https://huggingface.co/facebook/bart-base/resolve/main/config.json in cache at /mnt/nfs-storage/data/.cache/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\n[INFO|hub.py:621] 2024-08-20 22:41:20,829 >> creating metadata file for /mnt/nfs-storage/data/.cache/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\n[INFO|configuration_utils.py:681] 2024-08-20 22:41:20,830 >> loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /mnt/nfs-storage/data/.cache/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\n[INFO|configuration_utils.py:730] 2024-08-20 22:41:20,831 >> Model config BartConfig {\n  \"_name_or_path\": \"facebook/bart-base\",\n  \"activation_dropout\": 0.1,\n  \"activation_function\": \"gelu\",\n  \"add_bias_logits\": false,\n  \"add_final_layer_norm\": false,\n  \"architectures\": [\n    \"BartModel\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"bos_token_id\": 0,\n  \"classif_dropout\": 0.1,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 768,\n  \"decoder_attention_heads\": 12,\n  \"decoder_ffn_dim\": 3072,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 6,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"early_stopping\": true,\n  \"encoder_attention_heads\": 12,\n  \"encoder_ffn_dim\": 3072,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 6,\n  \"eos_token_id\": 2,\n  \"forced_bos_token_id\": 0,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\"\n  },\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2\n  },\n  \"max_position_embeddings\": 1024,\n  \"model_type\": \"bart\",\n  \"no_repeat_ngram_size\": 3,\n  \"normalize_before\": false,\n  \"normalize_embedding\": true,\n  \"num_beams\": 4,\n  \"num_hidden_layers\": 6,\n  \"pad_token_id\": 1,\n  \"scale_embedding\": false,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 128,\n      \"min_length\": 12,\n      \"num_beams\": 4\n    },\n    \"summarization_cnn\": {\n      \"length_penalty\": 2.0,\n      \"max_length\": 142,\n      \"min_length\": 56,\n      \"num_beams\": 4\n    },\n    \"summarization_xsum\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 62,\n      \"min_length\": 11,\n      \"num_beams\": 6\n    }\n  },\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.21.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\n[INFO|hub.py:600] 2024-08-20 22:41:20,875 >> https://huggingface.co/facebook/bart-large/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /mnt/nfs-storage/data/.cache/tmpxhjbokvq\n\nDownloading vocab.json:   0%|          | 0.00/878k [00:00<?, ?B/s]\n\n[INFO|hub.py:613] 2024-08-20 22:41:20,953 >> storing https://huggingface.co/facebook/bart-large/resolve/main/vocab.json in cache at /mnt/nfs-storage/data/.cache/0d6fc8b2ef1860c1f8f0baff4b021e3426cc7d11b153f98e563b799603ee2f25.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n[INFO|hub.py:621] 2024-08-20 22:41:20,954 >> creating metadata file for /mnt/nfs-storage/data/.cache/0d6fc8b2ef1860c1f8f0baff4b021e3426cc7d11b153f98e563b799603ee2f25.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n[INFO|hub.py:600] 2024-08-20 22:41:20,995 >> https://huggingface.co/facebook/bart-large/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /mnt/nfs-storage/data/.cache/tmph255604q\n\nDownloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]\n\n[INFO|hub.py:613] 2024-08-20 22:41:21,058 >> storing https://huggingface.co/facebook/bart-large/resolve/main/merges.txt in cache at /mnt/nfs-storage/data/.cache/6e75e35f0bdd15870c98387e13b93a8e100237eb33ad99c36277a0562bd6d850.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n[INFO|hub.py:621] 2024-08-20 22:41:21,059 >> creating metadata file for /mnt/nfs-storage/data/.cache/6e75e35f0bdd15870c98387e13b93a8e100237eb33ad99c36277a0562bd6d850.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n[INFO|hub.py:600] 2024-08-20 22:41:21,189 >> https://huggingface.co/facebook/bart-large/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /mnt/nfs-storage/data/.cache/tmptwbfnw_7\n\nDownloading tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]\nDownloading tokenizer_config.json: 100%|██████████| 26.0/26.0 [00:00<00:00, 194kB/s]\n[INFO|hub.py:613] 2024-08-20 22:41:21,234 >> storing https://huggingface.co/facebook/bart-large/resolve/main/tokenizer_config.json in cache at /mnt/nfs-storage/data/.cache/1abf196c889c24daca2909359ca2090e5fcbfa21a9ea36d763f70adbafb500d7.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8\n[INFO|hub.py:621] 2024-08-20 22:41:21,235 >> creating metadata file for /mnt/nfs-storage/data/.cache/1abf196c889c24daca2909359ca2090e5fcbfa21a9ea36d763f70adbafb500d7.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8\n[INFO|tokenization_utils_base.py:1803] 2024-08-20 22:41:21,236 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/vocab.json from cache at /mnt/nfs-storage/data/.cache/0d6fc8b2ef1860c1f8f0baff4b021e3426cc7d11b153f98e563b799603ee2f25.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n[INFO|tokenization_utils_base.py:1803] 2024-08-20 22:41:21,236 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/merges.txt from cache at /mnt/nfs-storage/data/.cache/6e75e35f0bdd15870c98387e13b93a8e100237eb33ad99c36277a0562bd6d850.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n[INFO|tokenization_utils_base.py:1803] 2024-08-20 22:41:21,236 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:1803] 2024-08-20 22:41:21,236 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:1803] 2024-08-20 22:41:21,236 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/tokenizer_config.json from cache at /mnt/nfs-storage/data/.cache/1abf196c889c24daca2909359ca2090e5fcbfa21a9ea36d763f70adbafb500d7.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8\n[INFO|hub.py:600] 2024-08-20 22:41:21,277 >> https://huggingface.co/facebook/bart-large/resolve/main/config.json not found in cache or force_download set to True, downloading to /mnt/nfs-storage/data/.cache/tmpcizjamhw\n\nDownloading config.json:   0%|          | 0.00/1.59k [00:00<?, ?B/s]\n\n[INFO|hub.py:613] 2024-08-20 22:41:21,384 >> storing https://huggingface.co/facebook/bart-large/resolve/main/config.json in cache at /mnt/nfs-storage/data/.cache/3f12fb71b844fcb7d591fdd4e55027da90d7b5dd6aa5430ad00ec6d76585f26c.bc22f15dc7ba074ee0a60bdd34c5f2fe3b6d746f89e765303376c51aff04e260\n[INFO|hub.py:621] 2024-08-20 22:41:21,385 >> creating metadata file for /mnt/nfs-storage/data/.cache/3f12fb71b844fcb7d591fdd4e55027da90d7b5dd6aa5430ad00ec6d76585f26c.bc22f15dc7ba074ee0a60bdd34c5f2fe3b6d746f89e765303376c51aff04e260\n[INFO|configuration_utils.py:681] 2024-08-20 22:41:21,387 >> loading configuration file https://huggingface.co/facebook/bart-large/resolve/main/config.json from cache at /mnt/nfs-storage/data/.cache/3f12fb71b844fcb7d591fdd4e55027da90d7b5dd6aa5430ad00ec6d76585f26c.bc22f15dc7ba074ee0a60bdd34c5f2fe3b6d746f89e765303376c51aff04e260\n[INFO|configuration_utils.py:730] 2024-08-20 22:41:21,388 >> Model config BartConfig {\n  \"_name_or_path\": \"facebook/bart-large\",\n  \"activation_dropout\": 0.1,\n  \"activation_function\": \"gelu\",\n  \"add_bias_logits\": false,\n  \"add_final_layer_norm\": false,\n  \"architectures\": [\n    \"BartModel\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"bos_token_id\": 0,\n  \"classif_dropout\": 0.1,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 12,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"early_stopping\": true,\n  \"encoder_attention_heads\": 16,\n  \"encoder_ffn_dim\": 4096,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 12,\n  \"eos_token_id\": 2,\n  \"forced_bos_token_id\": 0,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\"\n  },\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2\n  },\n  \"max_position_embeddings\": 1024,\n  \"model_type\": \"bart\",\n  \"no_repeat_ngram_size\": 3,\n  \"normalize_before\": false,\n  \"num_beams\": 4,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"scale_embedding\": false,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 128,\n      \"min_length\": 12,\n      \"num_beams\": 4\n    },\n    \"summarization_cnn\": {\n      \"length_penalty\": 2.0,\n      \"max_length\": 142,\n      \"min_length\": 56,\n      \"num_beams\": 4\n    },\n    \"summarization_xsum\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 62,\n      \"min_length\": 11,\n      \"num_beams\": 6\n    }\n  },\n  \"transformers_version\": \"4.21.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\n[WARNING|tokenization_utils_base.py:1896] 2024-08-20 22:41:21,388 >> The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'BartTokenizer'. \nThe class this function is called from is 'AMRBartTokenizer'.\nAdded 2965 AMR tokens\n[INFO|hub.py:600] 2024-08-20 22:41:21,578 >> https://huggingface.co/facebook/bart-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /mnt/nfs-storage/data/.cache/tmpkxovo69m\n\nDownloading pytorch_model.bin:   0%|          | 0.00/532M [00:00<?, ?B/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[INFO|hub.py:613] 2024-08-20 22:41:29,862 >> storing https://huggingface.co/facebook/bart-base/resolve/main/pytorch_model.bin in cache at /mnt/nfs-storage/data/.cache/486355ec722ef05fd480e999d4c763be56549ae930f6a3742ee721a5d2a05647.f2f355ad2775769afc60592b43a46d72ca548375e3a1d65f381a751e711cbadd\n[INFO|hub.py:621] 2024-08-20 22:41:29,862 >> creating metadata file for /mnt/nfs-storage/data/.cache/486355ec722ef05fd480e999d4c763be56549ae930f6a3742ee721a5d2a05647.f2f355ad2775769afc60592b43a46d72ca548375e3a1d65f381a751e711cbadd\n[INFO|modeling_utils.py:2041] 2024-08-20 22:41:29,864 >> loading weights file https://huggingface.co/facebook/bart-base/resolve/main/pytorch_model.bin from cache at /mnt/nfs-storage/data/.cache/486355ec722ef05fd480e999d4c763be56549ae930f6a3742ee721a5d2a05647.f2f355ad2775769afc60592b43a46d72ca548375e3a1d65f381a751e711cbadd\n[INFO|modeling_utils.py:2435] 2024-08-20 22:41:32,794 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n\n[INFO|modeling_utils.py:2443] 2024-08-20 22:41:32,797 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\nDataset cache dir: /mnt/nfs-storage/data/.cache\n/usr/local/lib/python3.10/site-packages/datasets/load.py:929: FutureWarning: The repository for data contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /content/AMRBART/fine-tune/data_interface/data.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\nTraceback (most recent call last):\n  File \"/content/AMRBART/fine-tune/main.py\", line 506, in <module>\n    main()\n  File \"/content/AMRBART/fine-tune/main.py\", line 194, in main\n    raw_datasets = DataSetCate(tokenizer, data_args, model_args)\n  File \"/content/AMRBART/fine-tune/data_interface/dataset.py\", line 63, in __init__\n    self.datasets = load_dataset(\n  File \"/usr/local/lib/python3.10/site-packages/datasets/load.py\", line 2587, in load_dataset\n    builder_instance = load_dataset_builder(\n  File \"/usr/local/lib/python3.10/site-packages/datasets/load.py\", line 2296, in load_dataset_builder\n    builder_instance: DatasetBuilder = builder_cls(\n  File \"/usr/local/lib/python3.10/site-packages/datasets/builder.py\", line 360, in __init__\n    data_files = DataFilesDict.from_patterns(\n  File \"/usr/local/lib/python3.10/site-packages/datasets/data_files.py\", line 715, in from_patterns\n    DataFilesList.from_patterns(\n  File \"/usr/local/lib/python3.10/site-packages/datasets/data_files.py\", line 620, in from_patterns\n    resolve_pattern(\n  File \"/usr/local/lib/python3.10/site-packages/datasets/data_files.py\", line 407, in resolve_pattern\n    raise FileNotFoundError(error_msg)\nFileNotFoundError: Unable to find '/content/AMRBART/fine-tune/data/LDC2020/train.jsonl'\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/AMRBART/fine-tune\n",
    "!bash train-AMRBART-large-AMRParsing.sh \"facebook/bart-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65de4c68",
   "metadata": {},
   "source": [
    "### Step 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9654daad",
   "metadata": {},
   "source": [
    "Execution Start time: 22:41:34, End time: 22:41:34 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dc2644",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85fec3e",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b711ab26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now the code asks for the data, so let's work on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59442ab7",
   "metadata": {},
   "source": [
    "### Step 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7017d1",
   "metadata": {},
   "source": [
    "Execution Start time: 22:41:34, End time: 22:41:34 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af058000",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb34ccd8",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897838a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51aafb4",
   "metadata": {},
   "source": [
    "### Step 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909be281",
   "metadata": {},
   "source": [
    "Execution Start time: 22:41:34, End time: 22:41:34 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f798ea10",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bfde81",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c6761b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Download data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a443a9",
   "metadata": {},
   "source": [
    "### Step 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c79bb8",
   "metadata": {},
   "source": [
    "Execution Start time: 22:41:34, End time: 22:41:39 (duration: 4 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3246ab",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c48cc1e",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c0dd52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Requirement already satisfied: gdown in /usr/local/lib/python3.10/site-packages (5.1.0)\nCollecting gdown\n  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/site-packages (from gdown) (4.12.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from gdown) (3.15.1)\nRequirement already satisfied: requests[socks] in /usr/local/lib/python3.10/site-packages (from gdown) (2.31.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from gdown) (4.66.4)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown) (2.0.7)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.6.2)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\nDownloading gdown-5.2.0-py3-none-any.whl (18 kB)\nInstalling collected packages: gdown\n  Attempting uninstall: gdown\n    Found existing installation: gdown 5.1.0\n    Uninstalling gdown-5.1.0:\n      Successfully uninstalled gdown-5.1.0\nSuccessfully installed gdown-5.2.0\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!# use gdown to download files\n",
    "!pip install gdown --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7815bf20",
   "metadata": {},
   "source": [
    "### Step 19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f5b8e8",
   "metadata": {},
   "source": [
    "Execution Start time: 22:41:39, End time: 22:41:56 (duration: 16 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb971129",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89effd10",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c490a6dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\nRetrieving folder contents\n/content/AMRBART/fine-tune/data\nProcessing file 141pCNafqMmjx6uZ5IlNSHQmmdfTCqNLW dev.jsonl\nProcessing file 1vSSHzz3ns6DQzWjBhZkI37-einZQuZXb infer.txt\nProcessing file 1jjka6n4pbdbih3LUuJII-UYTloNgv_hp test.jsonl\nProcessing file 1TDkzzq6xOthZDC-UKuyHwLdeaok8D6GQ train.jsonl\nRetrieving folder contents completed\nBuilding directory structure\nBuilding directory structure completed\nDownloading...\nFrom: https://drive.google.com/uc?id=141pCNafqMmjx6uZ5IlNSHQmmdfTCqNLW\nTo: /content/AMRBART/fine-tune/data/AMRBART/dev.jsonl\n\n  0%|          | 0.00/8.22k [00:00<?, ?B/s]\n\nDownloading...\nFrom: https://drive.google.com/uc?id=1vSSHzz3ns6DQzWjBhZkI37-einZQuZXb\nTo: /content/AMRBART/fine-tune/data/AMRBART/infer.txt\n\n  0%|          | 0.00/1.17k [00:00<?, ?B/s]\n\nDownloading...\nFrom: https://drive.google.com/uc?id=1jjka6n4pbdbih3LUuJII-UYTloNgv_hp\nTo: /content/AMRBART/fine-tune/data/AMRBART/test.jsonl\n\n  0%|          | 0.00/6.17k [00:00<?, ?B/s]\n\nDownloading...\nFrom: https://drive.google.com/uc?id=1TDkzzq6xOthZDC-UKuyHwLdeaok8D6GQ\nTo: /content/AMRBART/fine-tune/data/AMRBART/train.jsonl\n\n  0%|          | 0.00/7.86k [00:00<?, ?B/s]\n\nDownload completed\n['/content/AMRBART/fine-tune/data/AMRBART/dev.jsonl',\n '/content/AMRBART/fine-tune/data/AMRBART/infer.txt',\n '/content/AMRBART/fine-tune/data/AMRBART/test.jsonl',\n '/content/AMRBART/fine-tune/data/AMRBART/train.jsonl']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "from pathlib import Path\n",
    "\n",
    "Path(\"/content/AMRBART/fine-tune/data\").mkdir(exist_ok=True)\n",
    "\n",
    "%cd /content/AMRBART/fine-tune/data\n",
    "gdown.download_folder(\"https://drive.google.com/drive/folders/1tgSf24gsTrSe6GRt-eSuz8vvB51Br9hG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584fed81",
   "metadata": {},
   "source": [
    "### Step 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e06c3e",
   "metadata": {},
   "source": [
    "Execution Start time: 22:41:56, End time: 22:41:56 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c2dfe7",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd907c2",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55da994c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The script looks for a validation dataset named \"val.jsnol\". Assuming \"dev.jsonl\" contains the this dataset, I rename it accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7521c055",
   "metadata": {},
   "source": [
    "### Step 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14712f0",
   "metadata": {},
   "source": [
    "Execution Start time: 22:41:56, End time: 22:41:56 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b3820f",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada21586",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea16f0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!mv AMRBART/dev.jsonl AMRBART/val.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23ccdb4",
   "metadata": {},
   "source": [
    "### Step 22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7b5e3b",
   "metadata": {},
   "source": [
    "Execution Start time: 22:41:56, End time: 22:41:56 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e3cfdb",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33005a6f",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20e5507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The model expects sentences to be under the `sent` key, but in the downloaded datasets the key is `sentence`, so I replace it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c9cf2c",
   "metadata": {},
   "source": [
    "### Step 23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d9d2bf",
   "metadata": {},
   "source": [
    "Execution Start time: 22:41:56, End time: 22:41:57 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c203db0",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3264fa87",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670a61d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/AMRBART/fine-tune/data/AMRBART\n/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/AMRBART/fine-tune/data/AMRBART\n",
    "!sed -i 's/\"sentence\":/\"sent\":/' train.jsonl\n",
    "!sed -i 's/\"sentence\":/\"sent\":/' test.jsonl\n",
    "!sed -i 's/\"sentence\":/\"sent\":/' val.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502d2832",
   "metadata": {},
   "source": [
    "### Step 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4966d26a",
   "metadata": {},
   "source": [
    "Execution Start time: 22:41:57, End time: 22:41:57 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a910ca2",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0da9dd",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db44757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Patch again the script, this time setting the correct `Dataset` variable and the number of epochs to `1`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78349431",
   "metadata": {},
   "source": [
    "### Step 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35110431",
   "metadata": {},
   "source": [
    "Execution Start time: 22:41:57, End time: 22:41:57 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a1f1c2",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669245de",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3703f460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/AMRBART\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/AMRBART/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f1ed7d",
   "metadata": {},
   "source": [
    "### Step 26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c33ec",
   "metadata": {},
   "source": [
    "Execution Start time: 22:41:57, End time: 22:41:58 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ac1895",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7db4289",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c213e8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\nexport CUDA_VISIBLE_DEVICES=0\nRootDir=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" >/dev/null 2>&1 && pwd )\"\n\nDataset=LDC2020\n#Dataset=LDC2017\nDataset=AMRBART\n\nBasePath=/mnt/nfs-storage/data        # change dir here\nDataPath=$RootDir/data/$Dataset\n\nModelCate=AMRBART-large\n\nMODEL=$1\nModelCache=$BasePath/.cache"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/AMRBART/fine-tune/train-AMRBART-large-AMRParsing.sh\n",
    "<<<BEFORE_EDIT>>>\n",
    "\n",
    "Dataset=LDC2020\n",
    "#Dataset=LDC2017\n",
    "\n",
    "BasePath=/mnt/nfs-storage/data        # change dir here\n",
    "<<<AFTER_EDIT>>>\n",
    "\n",
    "Dataset=LDC2020\n",
    "#Dataset=LDC2017\n",
    "Dataset=AMRBART\n",
    "\n",
    "BasePath=/mnt/nfs-storage/data        # change dir here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d34edf1",
   "metadata": {},
   "source": [
    "### Step 27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd77935f",
   "metadata": {},
   "source": [
    "Execution Start time: 22:41:58, End time: 22:41:58 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bd51df",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fd4d99",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74a679b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n    --model_name_or_path $MODEL \\\n    --overwrite_output_dir \\\n    --unified_input True \\\n    --per_device_train_batch_size 16 \\\n    --per_device_eval_batch_size 8 \\\n    --gradient_accumulation_steps 1 \\\n    --learning_rate $lr \\\n    --optim \"adamw_hf\" \\\n    --lr_scheduler_type \"polynomial\" \\\n    --warmup_steps 200 \\\n    --num_train_epochs 1 \\\n    --early_stopping 10 \\\n    --max_source_length 128 \\\n    --max_target_length 128 \\\n    --val_max_target_length 128 \\\n    --generation_max_length 128 \\\n    --generation_num_beams 1 \\\n    --label_smoothing_factor 0.1 \\\n    --evaluation_strategy \"epoch\" \\\n    --weight_decay 0.01 \\\n    --max_grad_norm 0 \\\n    --max_steps -1 \\\n    --predict_with_generate \\\n    --smart_init False \\\n    --use_fast_tokenizer False \\"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/AMRBART/fine-tune/train-AMRBART-large-AMRParsing.sh\n",
    "<<<BEFORE_EDIT>>>\n",
    "    --optim \"adamw_hf\" \\\n",
    "    --lr_scheduler_type \"polynomial\" \\\n",
    "    --warmup_steps 200 \\\n",
    "    --num_train_epochs 30 \\\n",
    "    --early_stopping 10 \\\n",
    "    --max_source_length 400 \\\n",
    "    --max_target_length 1024 \\\n",
    "    --val_max_target_length 1024 \\\n",
    "    --generation_max_length 1024 \\\n",
    "    --generation_num_beams 5 \\\n",
    "    --label_smoothing_factor 0.1 \\\n",
    "    --evaluation_strategy \"epoch\" \\\n",
    "<<<AFTER_EDIT>>>\n",
    "    --optim \"adamw_hf\" \\\n",
    "    --lr_scheduler_type \"polynomial\" \\\n",
    "    --warmup_steps 200 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --early_stopping 10 \\\n",
    "    --max_source_length 128 \\\n",
    "    --max_target_length 128 \\\n",
    "    --val_max_target_length 128 \\\n",
    "    --generation_max_length 128 \\\n",
    "    --generation_num_beams 1 \\\n",
    "    --label_smoothing_factor 0.1 \\\n",
    "    --evaluation_strategy \"epoch\" \\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0c23da",
   "metadata": {},
   "source": [
    "### Step 28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4752e220",
   "metadata": {},
   "source": [
    "Execution Start time: 22:41:58, End time: 22:42:15 (duration: 16 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709c8910",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319c1f49",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407d1541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/AMRBART/fine-tune\n08/20/2024 22:42:04 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n08/20/2024 22:42:04 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n_n_gpu=0,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=8,\ndataloader_pin_memory=True,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=False,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=True,\ndo_train=True,\nearly_stopping=10,\neval_accumulation_steps=None,\neval_dataloader_num_workers=2,\neval_delay=0,\neval_lenpen=1.0,\neval_steps=None,\nevaluation_strategy=epoch,\nfp16=False,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_min_num_params=0,\nfull_determinism=False,\ngeneration_max_length=128,\ngeneration_num_beams=1,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngreater_is_better=True,\ngroup_by_length=False,\nhalf_precision_backend=cuda_amp,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=True,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.1,\nlearning_rate=1e-05,\nlength_column_name=length,\nload_best_model_at_end=True,\nlocal_rank=-1,\nlog_level=-1,\nlog_level_replica=-1,\nlog_on_each_node=True,\nlogging_dir=/content/AMRBART/fine-tune/outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp/logs,\nlogging_first_step=True,\nlogging_nan_inf_filter=True,\nlogging_steps=20,\nlogging_strategy=steps,\nlr_scheduler_type=polynomial,\nmax_grad_norm=0.0,\nmax_steps=-1,\nmetric_for_best_model=eval_smatch,\nmp_parameters=,\nno_cuda=False,\nnum_train_epochs=1.0,\noptim=adamw_hf,\noutput_dir=/content/AMRBART/fine-tune/outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp,\noverwrite_output_dir=True,\npast_index=-1,\nper_device_eval_batch_size=8,\nper_device_train_batch_size=16,\npredict_with_generate=True,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=/content/AMRBART/fine-tune/outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp,\nsave_on_each_node=False,\nsave_steps=500,\nsave_strategy=epoch,\nsave_total_limit=1,\nseed=42,\nsharded_ddp=[],\nskip_memory_metrics=True,\nsmart_init=False,\nsortish_sampler=False,\ntask=text2amr,\ntf32=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nwarmup_ratio=0.0,\nwarmup_steps=200,\nweight_decay=0.01,\nxpu_backend=None,\n)\n[INFO|configuration_utils.py:681] 2024-08-20 22:42:04,827 >> loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /mnt/nfs-storage/data/.cache/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\n[INFO|configuration_utils.py:730] 2024-08-20 22:42:04,828 >> Model config BartConfig {\n  \"_name_or_path\": \"facebook/bart-base\",\n  \"activation_dropout\": 0.1,\n  \"activation_function\": \"gelu\",\n  \"add_bias_logits\": false,\n  \"add_final_layer_norm\": false,\n  \"architectures\": [\n    \"BartModel\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"bos_token_id\": 0,\n  \"classif_dropout\": 0.1,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 768,\n  \"decoder_attention_heads\": 12,\n  \"decoder_ffn_dim\": 3072,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 6,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"early_stopping\": true,\n  \"encoder_attention_heads\": 12,\n  \"encoder_ffn_dim\": 3072,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 6,\n  \"eos_token_id\": 2,\n  \"forced_bos_token_id\": 0,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\"\n  },\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2\n  },\n  \"max_position_embeddings\": 1024,\n  \"model_type\": \"bart\",\n  \"no_repeat_ngram_size\": 3,\n  \"normalize_before\": false,\n  \"normalize_embedding\": true,\n  \"num_beams\": 4,\n  \"num_hidden_layers\": 6,\n  \"pad_token_id\": 1,\n  \"scale_embedding\": false,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 128,\n      \"min_length\": 12,\n      \"num_beams\": 4\n    },\n    \"summarization_cnn\": {\n      \"length_penalty\": 2.0,\n      \"max_length\": 142,\n      \"min_length\": 56,\n      \"num_beams\": 4\n    },\n    \"summarization_xsum\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 62,\n      \"min_length\": 11,\n      \"num_beams\": 6\n    }\n  },\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.21.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\n[INFO|tokenization_utils_base.py:1803] 2024-08-20 22:42:05,070 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/vocab.json from cache at /mnt/nfs-storage/data/.cache/0d6fc8b2ef1860c1f8f0baff4b021e3426cc7d11b153f98e563b799603ee2f25.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n[INFO|tokenization_utils_base.py:1803] 2024-08-20 22:42:05,070 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/merges.txt from cache at /mnt/nfs-storage/data/.cache/6e75e35f0bdd15870c98387e13b93a8e100237eb33ad99c36277a0562bd6d850.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n[INFO|tokenization_utils_base.py:1803] 2024-08-20 22:42:05,070 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:1803] 2024-08-20 22:42:05,070 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:1803] 2024-08-20 22:42:05,070 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/tokenizer_config.json from cache at /mnt/nfs-storage/data/.cache/1abf196c889c24daca2909359ca2090e5fcbfa21a9ea36d763f70adbafb500d7.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8\n[INFO|configuration_utils.py:681] 2024-08-20 22:42:05,112 >> loading configuration file https://huggingface.co/facebook/bart-large/resolve/main/config.json from cache at /mnt/nfs-storage/data/.cache/3f12fb71b844fcb7d591fdd4e55027da90d7b5dd6aa5430ad00ec6d76585f26c.bc22f15dc7ba074ee0a60bdd34c5f2fe3b6d746f89e765303376c51aff04e260\n[INFO|configuration_utils.py:730] 2024-08-20 22:42:05,113 >> Model config BartConfig {\n  \"_name_or_path\": \"facebook/bart-large\",\n  \"activation_dropout\": 0.1,\n  \"activation_function\": \"gelu\",\n  \"add_bias_logits\": false,\n  \"add_final_layer_norm\": false,\n  \"architectures\": [\n    \"BartModel\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"bos_token_id\": 0,\n  \"classif_dropout\": 0.1,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 12,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"early_stopping\": true,\n  \"encoder_attention_heads\": 16,\n  \"encoder_ffn_dim\": 4096,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 12,\n  \"eos_token_id\": 2,\n  \"forced_bos_token_id\": 0,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\"\n  },\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2\n  },\n  \"max_position_embeddings\": 1024,\n  \"model_type\": \"bart\",\n  \"no_repeat_ngram_size\": 3,\n  \"normalize_before\": false,\n  \"num_beams\": 4,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"scale_embedding\": false,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 128,\n      \"min_length\": 12,\n      \"num_beams\": 4\n    },\n    \"summarization_cnn\": {\n      \"length_penalty\": 2.0,\n      \"max_length\": 142,\n      \"min_length\": 56,\n      \"num_beams\": 4\n    },\n    \"summarization_xsum\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 62,\n      \"min_length\": 11,\n      \"num_beams\": 6\n    }\n  },\n  \"transformers_version\": \"4.21.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\n[WARNING|tokenization_utils_base.py:1896] 2024-08-20 22:42:05,113 >> The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'BartTokenizer'. \nThe class this function is called from is 'AMRBartTokenizer'.\nAdded 2965 AMR tokens\n[INFO|modeling_utils.py:2041] 2024-08-20 22:42:05,282 >> loading weights file https://huggingface.co/facebook/bart-base/resolve/main/pytorch_model.bin from cache at /mnt/nfs-storage/data/.cache/486355ec722ef05fd480e999d4c763be56549ae930f6a3742ee721a5d2a05647.f2f355ad2775769afc60592b43a46d72ca548375e3a1d65f381a751e711cbadd\n[INFO|modeling_utils.py:2435] 2024-08-20 22:42:07,706 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n\n[INFO|modeling_utils.py:2443] 2024-08-20 22:42:07,706 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\nDataset cache dir: /mnt/nfs-storage/data/.cache\n/usr/local/lib/python3.10/site-packages/datasets/load.py:929: FutureWarning: The repository for data contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /content/AMRBART/fine-tune/data_interface/data.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\nUsing custom data configuration default-fe0b2204358d5c61\n08/20/2024 22:42:08 - INFO - datasets.builder - Using custom data configuration default-fe0b2204358d5c61\nLoading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/data/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5\n08/20/2024 22:42:08 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/data/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5\nGenerating dataset data (/content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-fe0b2204358d5c61/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5)\n08/20/2024 22:42:08 - INFO - datasets.builder - Generating dataset data (/content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-fe0b2204358d5c61/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5)\nDownloading and preparing dataset data/default to /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-fe0b2204358d5c61/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5...\n08/20/2024 22:42:08 - INFO - datasets.builder - Downloading and preparing dataset data/default to /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-fe0b2204358d5c61/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5...\nGenerating train split\n08/20/2024 22:42:08 - INFO - datasets.builder - Generating train split\n\nGenerating train split: 0 examples [00:00, ? examples/s]\nGenerating train split: 10 examples [00:00, 699.49 examples/s]\nGenerating validation split\n08/20/2024 22:42:08 - INFO - datasets.builder - Generating validation split\n\nGenerating validation split: 0 examples [00:00, ? examples/s]\nGenerating validation split: 10 examples [00:00, 4495.50 examples/s]\nGenerating test split\n08/20/2024 22:42:08 - INFO - datasets.builder - Generating test split\n\nGenerating test split: 0 examples [00:00, ? examples/s]\nGenerating test split: 10 examples [00:00, 4748.99 examples/s]\nUnable to verify splits sizes.\n08/20/2024 22:42:08 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\nDataset data downloaded and prepared to /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-fe0b2204358d5c61/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5. Subsequent calls will reuse this data.\n08/20/2024 22:42:08 - INFO - datasets.builder - Dataset data downloaded and prepared to /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-fe0b2204358d5c61/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5. Subsequent calls will reuse this data.\ndatasets: DatasetDict({\n    train: Dataset({\n        features: ['src', 'tgt'],\n        num_rows: 10\n    })\n    validation: Dataset({\n        features: ['src', 'tgt'],\n        num_rows: 10\n    })\n    test: Dataset({\n        features: ['src', 'tgt'],\n        num_rows: 10\n    })\n})\ncolums: ['src', 'tgt']\n\nRunning tokenizer on train dataset:   0%|          | 0/10 [00:00<?, ? examples/s]Caching processed dataset at /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-fe0b2204358d5c61/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5/cache-2b3c3e7dca723fe0.arrow\n08/20/2024 22:42:09 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-fe0b2204358d5c61/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5/cache-2b3c3e7dca723fe0.arrow\n\nRunning tokenizer on train dataset: 100%|██████████| 10/10 [00:00<00:00, 379.21 examples/s]\nSaving cached train data ...\n\nSaving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s]\nSaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 3409.73 examples/s]\nSaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 3246.87 examples/s]\n\nRunning tokenizer on validation dataset:   0%|          | 0/10 [00:00<?, ? examples/s]Caching processed dataset at /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-fe0b2204358d5c61/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5/cache-f82739632a1ea0aa.arrow\n08/20/2024 22:42:10 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-fe0b2204358d5c61/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5/cache-f82739632a1ea0aa.arrow\n\nRunning tokenizer on validation dataset: 100%|██████████| 10/10 [00:00<00:00, 406.41 examples/s]\nSaving cached validation data ...\n\nSaving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s]\nSaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 3325.65 examples/s]\nSaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 3167.19 examples/s]\n\nRunning tokenizer on prediction dataset:   0%|          | 0/10 [00:00<?, ? examples/s]Caching processed dataset at /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-fe0b2204358d5c61/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5/cache-4bf5ba09576d92b0.arrow\n08/20/2024 22:42:12 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-fe0b2204358d5c61/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5/cache-4bf5ba09576d92b0.arrow\n\nRunning tokenizer on prediction dataset: 100%|██████████| 10/10 [00:00<00:00, 529.90 examples/s]\nSaving cached test data ...\n\nSaving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s]\nSaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 3573.57 examples/s]\nSaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 3397.57 examples/s]\n/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 5, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/usr/local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nAll 1 steps, warm_up steps: 200\n\n  0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 5, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\nTraceback (most recent call last):\n  File \"/content/AMRBART/fine-tune/main.py\", line 506, in <module>\n    main()\n  File \"/content/AMRBART/fine-tune/main.py\", line 423, in main\n    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n  File \"/content/AMRBART/fine-tune/base_trainer.py\", line 1501, in train\n    return inner_training_loop(\n  File \"/content/AMRBART/fine-tune/base_trainer.py\", line 1717, in _inner_training_loop\n    for step, inputs in enumerate(epoch_iterator):\n  File \"/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 631, in __next__\n    data = self._next_data()\n  File \"/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1346, in _next_data\n    return self._process_data(data)\n  File \"/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1372, in _process_data\n    data.reraise()\n  File \"/usr/local/lib/python3.10/site-packages/torch/_utils.py\", line 722, in reraise\n    raise exception\nTypeError: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/content/AMRBART/fine-tune/data_interface/dataset.py\", line 134, in __call__\n    padding_func(\n  File \"/content/AMRBART/fine-tune/data_interface/dataset.py\", line 16, in padding_func\n    if pad_to_multiple_of > 1:\nTypeError: '>' not supported between instances of 'NoneType' and 'int'\n\n\n  0%|          | 0/1 [00:01<?, ?it/s]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/AMRBART/fine-tune\n",
    "!bash train-AMRBART-large-AMRParsing.sh \"facebook/bart-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36c21b1",
   "metadata": {},
   "source": [
    "### Step 29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc8f91e",
   "metadata": {},
   "source": [
    "Execution Start time: 22:42:15, End time: 22:42:15 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5355a0a1",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e922fa",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4d6847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There seems to be a bug in file `main.py` related to the attribute `pad_to_multple_of`. The script fails when it is\n",
    "# equal to `None`. Aparently, it is resolved by replacing `None` with `1`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750179f2",
   "metadata": {},
   "source": [
    "### Step 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbd15cc",
   "metadata": {},
   "source": [
    "Execution Start time: 22:42:15, End time: 22:42:15 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8012cbc4",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c41372",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281ac2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/AMRBART\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/AMRBART/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcde08b",
   "metadata": {},
   "source": [
    "### Step 31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4868da",
   "metadata": {},
   "source": [
    "Execution Start time: 22:42:15, End time: 22:42:16 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915a7ff1",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99edd23b",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0185cda2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        else:\n            predict_dataset = load_from_disk(data_args.data_cache_dir + \"/test\", keep_in_memory=True)\n\n    # label_pad_token_id = -100 if data_args.ignore_pad_token_for_loss else tokenizer.pad_token_id\n    label_pad_token_id = tokenizer.pad_token_id\n    \n    DataCollatorCate = DataCollatorForAMR2Text if training_args.task == \"amr2text\" else DataCollatorForAMRParsing\n    data_collator = DataCollatorCate(\n        tokenizer,\n        label_pad_token_id=label_pad_token_id,\n        pad_to_multiple_of=8 if training_args.fp16 else 1,\n    )\n    \n    metric = load_metric(path=\"metric/sacrebleu.py\") if training_args.task == \"amr2text\" else None\n\n    def compute_metrics_parsing(eval_preds, global_step=0, prefix=\"val\"):\n        prefix = \"test\" if prefix == \"predict\" else \"val\"\n        preds, labels, inputs = eval_preds\n        # print(\"inputs:\", inputs)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/AMRBART/fine-tune/main.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "    data_collator = DataCollatorCate(\n",
    "        tokenizer,\n",
    "        label_pad_token_id=label_pad_token_id,\n",
    "        pad_to_multiple_of=8 if training_args.fp16 else None,\n",
    "    )\n",
    "    \n",
    "<<<AFTER_EDIT>>>\n",
    "    data_collator = DataCollatorCate(\n",
    "        tokenizer,\n",
    "        label_pad_token_id=label_pad_token_id,\n",
    "        pad_to_multiple_of=8 if training_args.fp16 else 1,\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96138184",
   "metadata": {},
   "source": [
    "### Step 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fd40cc",
   "metadata": {},
   "source": [
    "Execution Start time: 22:42:16, End time: 22:42:16 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c76b5dc",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9279b37d",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1b2375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a80608",
   "metadata": {},
   "source": [
    "### Step 33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c44fb9f",
   "metadata": {},
   "source": [
    "Execution Start time: 22:42:18, End time: 22:43:51 (duration: 93 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27ca168",
   "metadata": {},
   "source": [
    "**Thought**: Now that the script has been patched to handle the `pad_to_multiple_of` issue, I will attempt to run the training process again. This time, it should proceed without errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58058580",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0beb701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/AMRBART/fine-tune\n08/20/2024 22:42:24 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n08/20/2024 22:42:24 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n_n_gpu=0,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=8,\ndataloader_pin_memory=True,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=False,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=True,\ndo_train=True,\nearly_stopping=10,\neval_accumulation_steps=None,\neval_dataloader_num_workers=2,\neval_delay=0,\neval_lenpen=1.0,\neval_steps=None,\nevaluation_strategy=epoch,\nfp16=False,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_min_num_params=0,\nfull_determinism=False,\ngeneration_max_length=128,\ngeneration_num_beams=1,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngreater_is_better=True,\ngroup_by_length=False,\nhalf_precision_backend=cuda_amp,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=True,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.1,\nlearning_rate=1e-05,\nlength_column_name=length,\nload_best_model_at_end=True,\nlocal_rank=-1,\nlog_level=-1,\nlog_level_replica=-1,\nlog_on_each_node=True,\nlogging_dir=/content/AMRBART/fine-tune/outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp/logs,\nlogging_first_step=True,\nlogging_nan_inf_filter=True,\nlogging_steps=20,\nlogging_strategy=steps,\nlr_scheduler_type=polynomial,\nmax_grad_norm=0.0,\nmax_steps=-1,\nmetric_for_best_model=eval_smatch,\nmp_parameters=,\nno_cuda=False,\nnum_train_epochs=1.0,\noptim=adamw_hf,\noutput_dir=/content/AMRBART/fine-tune/outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp,\noverwrite_output_dir=True,\npast_index=-1,\nper_device_eval_batch_size=8,\nper_device_train_batch_size=16,\npredict_with_generate=True,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=/content/AMRBART/fine-tune/outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp,\nsave_on_each_node=False,\nsave_steps=500,\nsave_strategy=epoch,\nsave_total_limit=1,\nseed=42,\nsharded_ddp=[],\nskip_memory_metrics=True,\nsmart_init=False,\nsortish_sampler=False,\ntask=text2amr,\ntf32=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nwarmup_ratio=0.0,\nwarmup_steps=200,\nweight_decay=0.01,\nxpu_backend=None,\n)\n[INFO|configuration_utils.py:681] 2024-08-20 22:42:24,538 >> loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /mnt/nfs-storage/data/.cache/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\n[INFO|configuration_utils.py:730] 2024-08-20 22:42:24,539 >> Model config BartConfig {\n  \"_name_or_path\": \"facebook/bart-base\",\n  \"activation_dropout\": 0.1,\n  \"activation_function\": \"gelu\",\n  \"add_bias_logits\": false,\n  \"add_final_layer_norm\": false,\n  \"architectures\": [\n    \"BartModel\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"bos_token_id\": 0,\n  \"classif_dropout\": 0.1,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 768,\n  \"decoder_attention_heads\": 12,\n  \"decoder_ffn_dim\": 3072,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 6,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"early_stopping\": true,\n  \"encoder_attention_heads\": 12,\n  \"encoder_ffn_dim\": 3072,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 6,\n  \"eos_token_id\": 2,\n  \"forced_bos_token_id\": 0,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\"\n  },\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2\n  },\n  \"max_position_embeddings\": 1024,\n  \"model_type\": \"bart\",\n  \"no_repeat_ngram_size\": 3,\n  \"normalize_before\": false,\n  \"normalize_embedding\": true,\n  \"num_beams\": 4,\n  \"num_hidden_layers\": 6,\n  \"pad_token_id\": 1,\n  \"scale_embedding\": false,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 128,\n      \"min_length\": 12,\n      \"num_beams\": 4\n    },\n    \"summarization_cnn\": {\n      \"length_penalty\": 2.0,\n      \"max_length\": 142,\n      \"min_length\": 56,\n      \"num_beams\": 4\n    },\n    \"summarization_xsum\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 62,\n      \"min_length\": 11,\n      \"num_beams\": 6\n    }\n  },\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.21.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\n[INFO|tokenization_utils_base.py:1803] 2024-08-20 22:42:24,741 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/vocab.json from cache at /mnt/nfs-storage/data/.cache/0d6fc8b2ef1860c1f8f0baff4b021e3426cc7d11b153f98e563b799603ee2f25.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n[INFO|tokenization_utils_base.py:1803] 2024-08-20 22:42:24,741 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/merges.txt from cache at /mnt/nfs-storage/data/.cache/6e75e35f0bdd15870c98387e13b93a8e100237eb33ad99c36277a0562bd6d850.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n[INFO|tokenization_utils_base.py:1803] 2024-08-20 22:42:24,742 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:1803] 2024-08-20 22:42:24,742 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:1803] 2024-08-20 22:42:24,742 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/tokenizer_config.json from cache at /mnt/nfs-storage/data/.cache/1abf196c889c24daca2909359ca2090e5fcbfa21a9ea36d763f70adbafb500d7.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8\n[INFO|configuration_utils.py:681] 2024-08-20 22:42:24,779 >> loading configuration file https://huggingface.co/facebook/bart-large/resolve/main/config.json from cache at /mnt/nfs-storage/data/.cache/3f12fb71b844fcb7d591fdd4e55027da90d7b5dd6aa5430ad00ec6d76585f26c.bc22f15dc7ba074ee0a60bdd34c5f2fe3b6d746f89e765303376c51aff04e260\n[INFO|configuration_utils.py:730] 2024-08-20 22:42:24,780 >> Model config BartConfig {\n  \"_name_or_path\": \"facebook/bart-large\",\n  \"activation_dropout\": 0.1,\n  \"activation_function\": \"gelu\",\n  \"add_bias_logits\": false,\n  \"add_final_layer_norm\": false,\n  \"architectures\": [\n    \"BartModel\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"bos_token_id\": 0,\n  \"classif_dropout\": 0.1,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 12,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"early_stopping\": true,\n  \"encoder_attention_heads\": 16,\n  \"encoder_ffn_dim\": 4096,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 12,\n  \"eos_token_id\": 2,\n  \"forced_bos_token_id\": 0,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\"\n  },\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2\n  },\n  \"max_position_embeddings\": 1024,\n  \"model_type\": \"bart\",\n  \"no_repeat_ngram_size\": 3,\n  \"normalize_before\": false,\n  \"num_beams\": 4,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"scale_embedding\": false,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 128,\n      \"min_length\": 12,\n      \"num_beams\": 4\n    },\n    \"summarization_cnn\": {\n      \"length_penalty\": 2.0,\n      \"max_length\": 142,\n      \"min_length\": 56,\n      \"num_beams\": 4\n    },\n    \"summarization_xsum\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 62,\n      \"min_length\": 11,\n      \"num_beams\": 6\n    }\n  },\n  \"transformers_version\": \"4.21.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\n[WARNING|tokenization_utils_base.py:1896] 2024-08-20 22:42:24,780 >> The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'BartTokenizer'. \nThe class this function is called from is 'AMRBartTokenizer'.\nAdded 2965 AMR tokens\n[INFO|modeling_utils.py:2041] 2024-08-20 22:42:24,947 >> loading weights file https://huggingface.co/facebook/bart-base/resolve/main/pytorch_model.bin from cache at /mnt/nfs-storage/data/.cache/486355ec722ef05fd480e999d4c763be56549ae930f6a3742ee721a5d2a05647.f2f355ad2775769afc60592b43a46d72ca548375e3a1d65f381a751e711cbadd\n[INFO|modeling_utils.py:2435] 2024-08-20 22:42:27,066 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n\n[INFO|modeling_utils.py:2443] 2024-08-20 22:42:27,067 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\nDataset cache dir: /mnt/nfs-storage/data/.cache\n/usr/local/lib/python3.10/site-packages/datasets/load.py:929: FutureWarning: The repository for data contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /content/AMRBART/fine-tune/data_interface/data.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\nUsing custom data configuration default-fe0b2204358d5c61\n08/20/2024 22:42:27 - INFO - datasets.builder - Using custom data configuration default-fe0b2204358d5c61\nLoading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/data/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5\n08/20/2024 22:42:27 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/data/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5\nOverwrite dataset info from restored data version if exists.\n08/20/2024 22:42:27 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\nLoading Dataset info from /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-fe0b2204358d5c61/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5\n08/20/2024 22:42:27 - INFO - datasets.info - Loading Dataset info from /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-fe0b2204358d5c61/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5\nFound cached dataset data (/content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-fe0b2204358d5c61/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5)\n08/20/2024 22:42:27 - INFO - datasets.builder - Found cached dataset data (/content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-fe0b2204358d5c61/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5)\nLoading Dataset info from /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-fe0b2204358d5c61/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5\n08/20/2024 22:42:27 - INFO - datasets.info - Loading Dataset info from /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-fe0b2204358d5c61/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5\ndatasets: DatasetDict({\n    train: Dataset({\n        features: ['src', 'tgt'],\n        num_rows: 10\n    })\n    validation: Dataset({\n        features: ['src', 'tgt'],\n        num_rows: 10\n    })\n    test: Dataset({\n        features: ['src', 'tgt'],\n        num_rows: 10\n    })\n})\ncolums: ['src', 'tgt']\n/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 5, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/usr/local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nAll 1 steps, warm_up steps: 200\n\n  0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 5, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n\n100%|██████████| 1/1 [00:10<00:00, 10.98s/it]\n                                             \n{'loss': 7.5741, 'learning_rate': 5.0000000000000004e-08, 'epoch': 1.0}\n\n100%|██████████| 1/1 [00:10<00:00, 10.98s/it]Generation Kwargs:\n{'max_length': 128, 'max_gen_length': 128, 'num_beams': 1}\nSaving dummy inputs...\n\n\n  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n\n100%|██████████| 2/2 [00:06<00:00,  3.38s/it]\u001b[A\n                                             \n\n\n                                             \n\u001b[A{'eval_loss': 8.709596633911133, 'eval_smatch': 0.0, 'eval_gen_len': 128.0, 'eval_runtime': 23.2624, 'eval_samples_per_second': 0.43, 'eval_steps_per_second': 0.086, 'epoch': 1.0}\n\n100%|██████████| 1/1 [00:34<00:00, 10.98s/it]\n\n100%|██████████| 2/2 [00:06<00:00,  3.38s/it]\u001b[A\n\n                                             \u001b[A[INFO|configuration_utils.py:451] 2024-08-20 22:43:03,267 >> Configuration saved in /content/AMRBART/fine-tune/outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp/checkpoint-1/config.json\n[INFO|modeling_utils.py:1566] 2024-08-20 22:43:03,758 >> Model weights saved in /content/AMRBART/fine-tune/outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp/checkpoint-1/pytorch_model.bin\n[INFO|tokenization_utils_base.py:2145] 2024-08-20 22:43:03,760 >> tokenizer config file saved in /content/AMRBART/fine-tune/outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp/checkpoint-1/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2152] 2024-08-20 22:43:03,760 >> Special tokens file saved in /content/AMRBART/fine-tune/outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp/checkpoint-1/special_tokens_map.json\n\n                                             \n{'train_runtime': 36.4806, 'train_samples_per_second': 0.274, 'train_steps_per_second': 0.027, 'train_loss': 7.574057102203369, 'epoch': 1.0}\n\n100%|██████████| 1/1 [00:36<00:00, 10.98s/it]\n100%|██████████| 1/1 [00:36<00:00, 36.47s/it]\n***** train metrics *****\n  epoch                    =        1.0\n  train_loss               =     7.5741\n  train_runtime            = 0:00:36.48\n  train_samples            =         10\n  train_samples_per_second =      0.274\n  train_steps_per_second   =      0.027\n08/20/2024 22:43:05 - INFO - __main__ - *** Evaluate ***\nGeneration Kwargs:\n{'max_length': 128, 'num_beams': 1, 'max_gen_length': 128}\n\n  0%|          | 0/2 [00:00<?, ?it/s]\n100%|██████████| 2/2 [00:05<00:00,  2.94s/it]\n100%|██████████| 2/2 [00:05<00:00,  2.97s/it]\n***** eval metrics *****\n  epoch                   =        1.0\n  eval_gen_len            =      128.0\n  eval_loss               =     8.7096\n  eval_runtime            = 0:00:22.58\n  eval_samples            =         10\n  eval_samples_per_second =      0.443\n  eval_smatch             =        0.0\n  eval_steps_per_second   =      0.089\n08/20/2024 22:43:28 - INFO - __main__ - *** Predict ***\n\n  0%|          | 0/2 [00:00<?, ?it/s]\n100%|██████████| 2/2 [00:05<00:00,  2.81s/it]\n100%|██████████| 2/2 [00:05<00:00,  2.83s/it]\n***** predict metrics *****\n  predict_gen_len            =      128.0\n  predict_loss               =     7.6844\n  predict_runtime            = 0:00:22.47\n  predict_samples            =         10\n  predict_samples_per_second =      0.445\n  predict_smatch             =        0.0\n  predict_steps_per_second   =      0.089\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/AMRBART/fine-tune\n",
    "!bash train-AMRBART-large-AMRParsing.sh \"facebook/bart-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c7ee52",
   "metadata": {},
   "source": [
    "### Step 34"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0351e3d0",
   "metadata": {},
   "source": [
    "Execution Start time: 22:43:52, End time: 22:43:52 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6b9f13",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf1f340",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3076ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10e07a47",
   "metadata": {},
   "source": [
    "### Step 35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf329946",
   "metadata": {},
   "source": [
    "Execution Start time: 22:43:54, End time: 22:43:54 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44846881",
   "metadata": {},
   "source": [
    "**Thought**: The training process has completed successfully, and the last training loss is reported as `predict_loss = 7.6844`. I will now submit this result in the required JSON format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31c5b56",
   "metadata": {},
   "source": [
    "**Action (submit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12713320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'last_loss': 7.6844}"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
