{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de8e0fa8",
   "metadata": {},
   "source": [
    "### System Prompt\n",
    "<details>\n",
    "SETTING: You are an autonomous programmer, and you're working directly in the command line with a special Jupyter notebook interface.\n",
    "\n",
    "The special Jupyter notebook interface consists of a file editor that shows you 100 lines of a file at a time.\n",
    "You can execute commands in the notebook using:\n",
    "\n",
    "1. Bash commands: Commands starting with !.\n",
    "2. Python commands: Standard Python code.\n",
    "3. Magic commands: Commands starting with %, e.g., %cd <path>.\n",
    "\n",
    "Additionally, you can also use the following commands to help you navigate and edit files.\n",
    "\n",
    "COMMANDS:\n",
    "\n",
    "open:\n",
    "  docstring: opens the file at the given path in the editor. If line_number is provided, the window will be move to include that line\n",
    "  signature: open <path> [<line_number>]\n",
    "  arguments:\n",
    "    - path (string) [required]: the path to the file to open\n",
    "    - line_number (integer) [optional]: the line number to move the window to (if not provided, the window will start at the top of the file)\n",
    "\n",
    "create:\n",
    "  docstring: creates and opens a new file with the given name\n",
    "  signature: create <filename>\n",
    "  arguments:\n",
    "    - filename (string) [required]: the name of the file to create\n",
    "\n",
    "goto:\n",
    "  docstring: moves the window to show <line_number>\n",
    "  signature: goto <line_number>\n",
    "  arguments:\n",
    "    - line_number (integer) [required]: the line number to move the window to\n",
    "\n",
    "scroll_down:\n",
    "  docstring: moves the window down {WINDOW} lines\n",
    "  signature: scroll_down\n",
    "\n",
    "scroll_up:\n",
    "  docstring: moves the window down {WINDOW} lines\n",
    "  signature: scroll_up\n",
    "\n",
    "submit:\n",
    "  docstring: submits your final answer\n",
    "  signature: submit <answer>\n",
    "  arguments:\n",
    "    - answer (string) [required]: the answer to be submitted\n",
    "\n",
    "search_dir:\n",
    "  docstring: searches for search_term in all files in dir_path. If dir_path is not provided, searches in the current directory\n",
    "  signature: search_dir <search_term> [<dir_path>]\n",
    "  arguments:\n",
    "    - search_term (string) [required]: the term to search for\n",
    "    - dir_path (string) [optional]: the directory to search in (if not provided, searches in the current directory)\n",
    "\n",
    "search_file:\n",
    "  docstring: searches for search_term in file. If file is not provided, searches in the current open file\n",
    "  signature: search_file <search_term> [<file>]\n",
    "  arguments:\n",
    "    - search_term (string) [required]: the term to search for\n",
    "    - file (string) [optional]: the file to search in (if not provided, searches in the current open file)\n",
    "\n",
    "find_file:\n",
    "  docstring: finds all files with the given name in dir_path. If dir_path is not provided, searches in the current directory\n",
    "  signature: find_file <file_name> [<dir_path>]\n",
    "  arguments:\n",
    "    - file_name (string) [required]: the name of the file to search for\n",
    "    - dir_path (string) [optional]: the directory to search in (if not provided, searches in the current directory)\n",
    "\n",
    "edit:\n",
    "  docstring: replaces lines <start_line> through <end_line> (inclusive) with the given text in the open file. The replacement text is terminated by a line with only end_of_edit on it. All of the <replacement text> will be entered, so make sure your indentation is formatted properly. Python files will be checked for syntax errors after the edit. If the system detects a syntax error, the edit will not be executed. Simply try to edit the file again, but make sure to read the error message and modify the edit command you issue accordingly. Issuing the same command a second time will just lead to the same error message again.\n",
    "  signature: edit <start_line>:<end_line>\n",
    "<replacement_text>\n",
    "end_of_edit\n",
    "  arguments:\n",
    "    - start_line (integer) [required]: the line number to start the edit at\n",
    "    - end_line (integer) [required]: the line number to end the edit at (inclusive)\n",
    "    - replacement_text (string) [required]: the text to replace the current selection with\n",
    "\n",
    "\n",
    "\n",
    "Please note that THE EDIT COMMAND REQUIRES PROPER INDENTATION. \n",
    "If you'd like to add the line '        print(x)' you must fully write that out, with all those spaces before the code! Indentation is important and code that is not indented correctly will fail and require fixing before it can be run.\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "Your shell prompt is formatted as follows:\n",
    "(Open file: <path>)\n",
    "(Current directory: <cwd>)\n",
    "In [ ]\n",
    "\n",
    "You need to format your output using two fields: discussion and command.\n",
    "Your output should always include _one_ discussion and _one_ command field EXACTLY as in the following example:\n",
    "DISCUSSION\n",
    "First I'll start by using ls to see what files are in the current directory. Then maybe we can look at some relevant files to see what they look like.\n",
    "```\n",
    "!ls -a\n",
    "```\n",
    "\n",
    "You should only include a *SINGLE* command in the command section and then wait for a response from the shell before continuing with more discussion and commands. Everything you include in the DISCUSSION section will be saved for future reference.\n",
    "If you'd like to issue two commands at once, PLEASE DO NOT DO THAT! Please instead first submit just the first command, and then after receiving a response you'll be able to issue the second command. \n",
    "You're free to use any other bash commands you want (e.g. find, grep, cat, ls, cd) in addition to the special commands listed above.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893b8f65",
   "metadata": {},
   "source": [
    "### Query\n",
    "Fine-tune the bert-base-uncased model on the RTE dataset from the GLUE benchmark using the HyPe method with uniform perturbation type and a perturbation epsilon of 1e-5. Report the results. Report accuracy and loss on the evaluation set as a json structured as follows: {\"accuracy\": 0.0, \"loss\": 0.0} (replace 0.0 with the actual values).\n",
    "\n",
    "Additional instructions:\n",
    "1. Only load the first 10 examples of each (training/validation/test) set in the dataset.\n",
    "2. Run for only 1 epoch.\n",
    "3. Set the following hyperparameters: per_device_train_batch_size = 32, learning_rate = 5e-4, seed = 24423. Other hyperparameters should be set as those in readme example.\n",
    "\n",
    "Git repository: https://github.com/yuanhy1997/HyPe\n",
    "\n",
    "I have already executed some of the required steps. Now, you should make the necessary changes to make sure the code runs on a CPU. Your goal is to successfully run\n",
    "```\n",
    "!python main.py \\\n",
    "  --adam_epsilon 1e-6 \\\n",
    "  --max_seq_length 128 \\\n",
    "  --eval_accumulation_steps 20 \\\n",
    "  --do_eval \\\n",
    "  --do_train \\\n",
    "  --warmup_ratio 0.1 \\\n",
    "  --weight_decay 0.01 \\\n",
    "  --save_total_limit 1 \\\n",
    "  --save_steps 30000 \\\n",
    "  --model_name_or_path bert-base-uncased \\\n",
    "  --task_name rte \\\n",
    "  --per_device_train_batch_size 32 \\\n",
    "  --learning_rate 5e-4 \\\n",
    "  --num_train_epochs 1 \\\n",
    "  --seed 24423 \\\n",
    "  --output_dir final_results \\\n",
    "  --low_resource \\\n",
    "  --dropout_rate 0.0 \\\n",
    "  --hype_type uniform \\\n",
    "  --hype_eps 1e-5 \\\n",
    "  --no_cuda True\n",
    "```\n",
    "and submit the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e006ff",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f050e0",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981cbd43",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed95944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cloning into 'HyPe'...\r\nremote: Enumerating objects: 27, done.\u001b[K\r\nremote: Counting objects:   3% (1/27)\u001b[K\rremote: Counting objects:   7% (2/27)\u001b[K\rremote: Counting objects:  11% (3/27)\u001b[K\rremote: Counting objects:  14% (4/27)\u001b[K\rremote: Counting objects:  18% (5/27)\u001b[K\rremote: Counting objects:  22% (6/27)\u001b[K\rremote: Counting objects:  25% (7/27)\u001b[K\rremote: Counting objects:  29% (8/27)\u001b[K\rremote: Counting objects:  33% (9/27)\u001b[K\rremote: Counting objects:  37% (10/27)\u001b[K\rremote: Counting objects:  40% (11/27)\u001b[K\rremote: Counting objects:  44% (12/27)\u001b[K\rremote: Counting objects:  48% (13/27)\u001b[K\rremote: Counting objects:  51% (14/27)\u001b[K\rremote: Counting objects:  55% (15/27)\u001b[K\rremote: Counting objects:  59% (16/27)\u001b[K\rremote: Counting objects:  62% (17/27)\u001b[K\rremote: Counting objects:  66% (18/27)\u001b[K\rremote: Counting objects:  70% (19/27)\u001b[K\rremote: Counting objects:  74% (20/27)\u001b[K\rremote: Counting objects:  77% (21/27)\u001b[K\rremote: Counting objects:  81% (22/27)\u001b[K\rremote: Counting objects:  85% (23/27)\u001b[K\rremote: Counting objects:  88% (24/27)\u001b[K\rremote: Counting objects:  92% (25/27)\u001b[K\rremote: Counting objects:  96% (26/27)\u001b[K\rremote: Counting objects: 100% (27/27)\u001b[K\rremote: Counting objects: 100% (27/27), done.\u001b[K\r\nremote: Compressing objects:   4% (1/24)\u001b[K\rremote: Compressing objects:   8% (2/24)\u001b[K\rremote: Compressing objects:  12% (3/24)\u001b[K\rremote: Compressing objects:  16% (4/24)\u001b[K\rremote: Compressing objects:  20% (5/24)\u001b[K\rremote: Compressing objects:  25% (6/24)\u001b[K\rremote: Compressing objects:  29% (7/24)\u001b[K\rremote: Compressing objects:  33% (8/24)\u001b[K\rremote: Compressing objects:  37% (9/24)\u001b[K\rremote: Compressing objects:  41% (10/24)\u001b[K\rremote: Compressing objects:  45% (11/24)\u001b[K\rremote: Compressing objects:  50% (12/24)\u001b[K\rremote: Compressing objects:  54% (13/24)\u001b[K\rremote: Compressing objects:  58% (14/24)\u001b[K\rremote: Compressing objects:  62% (15/24)\u001b[K\rremote: Compressing objects:  66% (16/24)\u001b[K\rremote: Compressing objects:  70% (17/24)\u001b[K\rremote: Compressing objects:  75% (18/24)\u001b[K\rremote: Compressing objects:  79% (19/24)\u001b[K\rremote: Compressing objects:  83% (20/24)\u001b[K\rremote: Compressing objects:  87% (21/24)\u001b[K\rremote: Compressing objects:  91% (22/24)\u001b[K\rremote: Compressing objects:  95% (23/24)\u001b[K\rremote: Compressing objects: 100% (24/24)\u001b[K\rremote: Compressing objects: 100% (24/24), done.\u001b[K\r\nReceiving objects:   3% (1/27)\rReceiving objects:   7% (2/27)\rReceiving objects:  11% (3/27)\rReceiving objects:  14% (4/27)\rReceiving objects:  18% (5/27)\rReceiving objects:  22% (6/27)\rReceiving objects:  25% (7/27)\rReceiving objects:  29% (8/27)\rReceiving objects:  33% (9/27)\rReceiving objects:  37% (10/27)\rReceiving objects:  40% (11/27)\rReceiving objects:  44% (12/27)\rReceiving objects:  48% (13/27)\rReceiving objects:  51% (14/27)\rReceiving objects:  55% (15/27)\rReceiving objects:  59% (16/27)\rremote: Total 27 (delta 9), reused 9 (delta 3), pack-reused 0 (from 0)\u001b[K\r\nReceiving objects:  62% (17/27)\rReceiving objects:  66% (18/27)\rReceiving objects:  70% (19/27)\rReceiving objects:  74% (20/27)\rReceiving objects:  77% (21/27)\rReceiving objects:  81% (22/27)\rReceiving objects:  85% (23/27)\rReceiving objects:  88% (24/27)\rReceiving objects:  92% (25/27)\rReceiving objects:  96% (26/27)\rReceiving objects: 100% (27/27)\rReceiving objects: 100% (27/27), 107.14 KiB | 1.79 MiB/s, done.\r\nResolving deltas:   0% (0/9)\rResolving deltas:  11% (1/9)\rResolving deltas:  22% (2/9)\rResolving deltas:  33% (3/9)\rResolving deltas:  44% (4/9)\rResolving deltas:  55% (5/9)\rResolving deltas:  66% (6/9)\rResolving deltas:  77% (7/9)\rResolving deltas:  88% (8/9)\rResolving deltas: 100% (9/9)\rResolving deltas: 100% (9/9), done.\r\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!git clone https://github.com/Yuanhy1997/HyPe.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bd1f7e",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ffb18b",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d589a03",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87d55c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/HyPe\n/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd ./HyPe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250a4f71",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f15922",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd04ed4b",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c859171e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting datasets==2.19.0\r\n  Downloading datasets-2.19.0-py3-none-any.whl.metadata (19 kB)\r\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (3.15.1)\r\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (1.25.2)\r\nCollecting pyarrow>=12.0.0 (from datasets==2.19.0)\r\n  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\r\nCollecting pyarrow-hotfix (from datasets==2.19.0)\r\n  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\r\nCollecting dill<0.3.9,>=0.3.0 (from datasets==2.19.0)\r\n  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\r\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (2.0.3)\r\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (2.31.0)\r\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (4.66.4)\r\nCollecting xxhash (from datasets==2.19.0)\r\n  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\r\nCollecting multiprocess (from datasets==2.19.0)\r\n  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\r\nRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets==2.19.0) (2023.6.0)\r\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (3.8.3)\r\nRequirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (0.23.3)\r\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (24.1)\r\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (6.0.1)\r\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0) (23.2.0)\r\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0) (2.1.1)\r\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0) (6.0.5)\r\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0) (4.0.3)\r\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0) (1.9.4)\r\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0) (1.4.1)\r\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0) (1.3.1)\r\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets==2.19.0) (4.12.2)\r\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.19.0) (3.7)\r\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.19.0) (2.0.7)\r\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.19.0) (2024.6.2)\r\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas->datasets==2.19.0) (2.9.0.post0)\r\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets==2.19.0) (2024.1)\r\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets==2.19.0) (2024.1)\r\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.19.0) (1.16.0)\r\nDownloading datasets-2.19.0-py3-none-any.whl (542 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/542.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m217.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m205.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/39.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/39.9 MB\u001b[0m \u001b[31m257.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/39.9 MB\u001b[0m \u001b[31m200.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/39.9 MB\u001b[0m \u001b[31m260.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m32.5/39.9 MB\u001b[0m \u001b[31m246.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m230.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/134.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m281.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\r\nDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/194.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m170.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hInstalling collected packages: xxhash, pyarrow-hotfix, pyarrow, dill, multiprocess, datasets\r\nSuccessfully installed datasets-2.19.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 pyarrow-hotfix-0.6 xxhash-3.5.0\r\n\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\nCollecting transformers==4.30.0\r\n  Downloading transformers-4.30.0-py3-none-any.whl.metadata (113 kB)\r\n\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/113.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from transformers==4.30.0) (3.15.1)\r\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/site-packages (from transformers==4.30.0) (0.23.3)\r\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from transformers==4.30.0) (1.25.2)\r\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from transformers==4.30.0) (24.1)\r\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers==4.30.0) (6.0.1)\r\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers==4.30.0) (2023.12.25)\r\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers==4.30.0) (2.31.0)\r\nCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.30.0)\r\n  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\r\nRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/site-packages (from transformers==4.30.0) (0.4.3)\r\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/site-packages (from transformers==4.30.0) (4.66.4)\r\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0) (2023.6.0)\r\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0) (4.12.2)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.30.0) (2.1.1)\r\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.30.0) (3.7)\r\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.30.0) (2.0.7)\r\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.30.0) (2024.6.2)\r\nDownloading transformers-4.30.0-py3-none-any.whl (7.2 MB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/7.2 MB\u001b[0m \u001b[31m106.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m145.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m318.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hInstalling collected packages: tokenizers, transformers\r\n  Attempting uninstall: tokenizers\r\n    Found existing installation: tokenizers 0.19.1\r\n    Uninstalling tokenizers-0.19.1:\r\n      Successfully uninstalled tokenizers-0.19.1\r\n  Attempting uninstall: transformers\r\n    Found existing installation: transformers 4.40.2\r\n    Uninstalling transformers-4.40.2:\r\n      Successfully uninstalled transformers-4.40.2\r\nSuccessfully installed tokenizers-0.13.3 transformers-4.30.0\r\n\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\nCollecting accelerate\r\n  Downloading accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\r\nRequirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/site-packages (from accelerate) (1.25.2)\r\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from accelerate) (24.1)\r\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/site-packages (from accelerate) (5.9.8)\r\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/site-packages (from accelerate) (6.0.1)\r\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/site-packages (from accelerate) (2.2.1+cpu)\r\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/site-packages (from accelerate) (0.23.3)\r\nRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/site-packages (from accelerate) (0.4.3)\r\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.1)\r\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2023.6.0)\r\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.31.0)\r\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\r\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\r\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\r\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\r\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\r\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.1.1)\r\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\r\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.7)\r\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.6.2)\r\nRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\r\nDownloading accelerate-0.33.0-py3-none-any.whl (315 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/315.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.1/315.1 kB\u001b[0m \u001b[31m143.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hInstalling collected packages: accelerate\r\nSuccessfully installed accelerate-0.33.0\r\n\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install datasets==2.19.0\n",
    "!pip install transformers==4.30.0\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1cbbd4",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f5c899",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30aef4e",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187497bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating file for store final results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09f30f4",
   "metadata": {},
   "source": [
    "### Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20174d48",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401e6e24",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dcb9c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%mkdir ./final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8ffeac",
   "metadata": {},
   "source": [
    "### Step 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed59e9d4",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd617c23",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2015cce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n    # or by passing the --help flag to this script.\n    # We now keep distinct sets of args, for a cleaner separation of concerns.\n\n    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n        # If we pass only one argument to the script and it's the path to a json file,\n        # let's parse it to get our arguments.\n        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n    else:\n        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n        training_args.no_cuda = True\n        print(f\"The training arguments are {training_args}\")\n\n    if 'xlnet' in model_args.model_name_or_path:\n        bsz, trainstep, warmstep = xlnet_train_args[data_args.task_name]\n        training_args.per_device_train_batch_size = bsz\n        training_args.max_steps = trainstep\n        training_args.warmup_steps = warmstep\n    if 'electra' in model_args.model_name_or_path:\n        bsz, epo = electra_train_args[data_args.task_name]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/HyPe/main.py. CHANGING:\n",
    "\n",
    "\n",
    "        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n",
    "    else:\n",
    "        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "\n",
    "    if 'xlnet' in model_args.model_name_or_path:\n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n",
    "    else:\n",
    "        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "        training_args.no_cuda = True\n",
    "        print(f\"The training arguments are {training_args}\")\n",
    "\n",
    "    if 'xlnet' in model_args.model_name_or_path:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ff2651",
   "metadata": {},
   "source": [
    "### Step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25a17fa",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e74aafe",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a1057d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n    #\n    # If the CSVs/JSONs contain only one non-label column, the script does single sentence classification on this\n    # single column. You can easily tweak this behavior (see below)\n    #\n    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n    # download the dataset.\n    if data_args.task_name is not None:\n        # Downloading and loading a dataset from the hub.\n        datasets = load_dataset(\"glue.py\", data_args.task_name)\n        if data_args.low_resource:\n            # with open(f'./1ksample_subset_{data_args.task_name}.pkl','rb') as f:\n            #     subsample_indices = pickle.load(f)\n            np.random.seed(13323)\n            subsample_indices = np.random.randint(low = 0 , high = 1000, size = (10,))\n            datasets['train'] = datasets['train'].select(subsample_indices)\n    else:\n        # Loading a dataset from your local files.\n        # CSV/JSON training and evaluation files are needed.\n        data_files = {\"train\": data_args.train_file, \"validation\": data_args.validation_file}\n\n        # Get the test dataset: you can provide your own CSV/JSON test file (see below)\n        # when you use `do_predict` without specifying a GLUE benchmark task."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/HyPe/main.py. CHANGING:\n",
    "\n",
    "\n",
    "        # Downloading and loading a dataset from the hub.\n",
    "        datasets = load_dataset(\"glue.py\", data_args.task_name)\n",
    "        if data_args.low_resource:\n",
    "            with open(f'./1ksample_subset_{data_args.task_name}.pkl','rb') as f:\n",
    "                subsample_indices = pickle.load(f)\n",
    "            datasets['train'] = datasets['train'].select(subsample_indices)\n",
    "    else:\n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "        # Downloading and loading a dataset from the hub.\n",
    "        datasets = load_dataset(\"glue.py\", data_args.task_name)\n",
    "        if data_args.low_resource:\n",
    "            # with open(f'./1ksample_subset_{data_args.task_name}.pkl','rb') as f:\n",
    "            #     subsample_indices = pickle.load(f)\n",
    "            np.random.seed(13323)\n",
    "            subsample_indices = np.random.randint(low = 0 , high = 1000, size = (10,))\n",
    "            datasets['train'] = datasets['train'].select(subsample_indices)\n",
    "    else:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c7f908",
   "metadata": {},
   "source": [
    "### Step 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8ebaa0",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cc44bb",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783c4105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In the following we are changing the key word argument in @add_code_sample_docstrings is changed from tokenizer_class to processr_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff382b4",
   "metadata": {},
   "source": [
    "### Step 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677f521b",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9573434",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc6df58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n    def _prune_heads(self, heads_to_prune):\n        \"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/HyPe/hype_modeling_bert.py. CHANGING:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6812ccfa",
   "metadata": {},
   "source": [
    "### Step 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2701f615",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba5e5da",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aff6d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        self.init_weights()\n\n    def get_output_embeddings(self):\n        return self.cls.predictions.decoder\n\n    def set_output_embeddings(self, new_embeddings):\n        self.cls.predictions.decoder = new_embeddings\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=MaskedLMOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/HyPe/hype_modeling_bert.py. CHANGING:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=MaskedLMOutput,\n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=MaskedLMOutput,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9adaeb",
   "metadata": {},
   "source": [
    "### Step 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcc3c21",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a2dc4b",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91d2af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        self.config = config\n\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=SequenceClassifierOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/HyPe/hype_modeling_bert.py. CHANGING:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=SequenceClassifierOutput,\n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=SequenceClassifierOutput,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d691c703",
   "metadata": {},
   "source": [
    "### Step 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cf562f",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e7c77f",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6dd252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        super().__init__(config)\n\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, 1)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=MultipleChoiceModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/HyPe/hype_modeling_bert.py. CHANGING:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=MultipleChoiceModelOutput,\n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=MultipleChoiceModelOutput,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1629c7a5",
   "metadata": {},
   "source": [
    "### Step 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746cfa3d",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc40d1e8",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f269487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config, add_pooling_layer=False)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class =_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=TokenClassifierOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/HyPe/hype_modeling_bert.py. CHANGING:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=TokenClassifierOutput,\n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class =_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=TokenClassifierOutput,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8a801b",
   "metadata": {},
   "source": [
    "### Step 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4130f72f",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b8dc6a",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eda63e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config, add_pooling_layer=False)\n        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class =_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=QuestionAnsweringModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/HyPe/hype_modeling_bert.py. CHANGING:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=QuestionAnsweringModelOutput,\n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class =_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=QuestionAnsweringModelOutput,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd0c69e",
   "metadata": {},
   "source": [
    "### Step 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7837b36e",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf526217",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a0f496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n    def _prune_heads(self, heads_to_prune):\n        \"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    @add_start_docstrings_to_model_forward(ELECTRA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=BaseModelOutputWithCrossAttentions,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/HyPe/hype_modeling_electra.py. CHANGING:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ELECTRA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=BaseModelOutputWithCrossAttentions,\n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ELECTRA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=BaseModelOutputWithCrossAttentions,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1e895e",
   "metadata": {},
   "source": [
    "### Step 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c279ef8",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7be688",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb303b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.config = config\n        self.electra = ElectraModel(config)\n        self.classifier = ElectraClassificationHead(config)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(ELECTRA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=SequenceClassifierOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/HyPe/hype_modeling_electra.py. CHANGING:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ELECTRA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=SequenceClassifierOutput,\n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ELECTRA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=SequenceClassifierOutput,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14641594",
   "metadata": {},
   "source": [
    "### Step 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdd380b",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e7bacb",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae5704f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        self.init_weights()\n\n    def get_output_embeddings(self):\n        return self.generator_lm_head\n\n    def set_output_embeddings(self, word_embeddings):\n        self.generator_lm_head = word_embeddings\n\n    @add_start_docstrings_to_model_forward(ELECTRA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=MaskedLMOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/HyPe/hype_modeling_electra.py. CHANGING:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ELECTRA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=MaskedLMOutput,\n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ELECTRA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=MaskedLMOutput,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ec62e4",
   "metadata": {},
   "source": [
    "### Step 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4109c7ea",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f4eea9",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf7e8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.electra = ElectraModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(ELECTRA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=TokenClassifierOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/HyPe/hype_modeling_electra.py. CHANGING:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ELECTRA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=TokenClassifierOutput,\n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ELECTRA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=TokenClassifierOutput,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa656146",
   "metadata": {},
   "source": [
    "### Step 19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97304e2d",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57773729",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6ec2e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.electra = ElectraModel(config)\n        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(ELECTRA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=QuestionAnsweringModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/HyPe/hype_modeling_electra.py. CHANGING:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ELECTRA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=QuestionAnsweringModelOutput,\n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ELECTRA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=QuestionAnsweringModelOutput,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e437c4",
   "metadata": {},
   "source": [
    "### Step 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb42889",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6334fc",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928d74db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        super().__init__(config)\n\n        self.electra = ElectraModel(config)\n        self.sequence_summary = SequenceSummary(config)\n        self.classifier = nn.Linear(config.hidden_size, 1)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(ELECTRA_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=MultipleChoiceModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/HyPe/hype_modeling_electra.py. CHANGING:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ELECTRA_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=MultipleChoiceModelOutput,\n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ELECTRA_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=MultipleChoiceModelOutput,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fca90f",
   "metadata": {},
   "source": [
    "### Step 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43e077b",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2e0358",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060ef29f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n    def _prune_heads(self, heads_to_prune):\n        \"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"(batch_size, sequence_length)\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    # Copied from transformers.models.bert.modeling_bert.BertModel.forward\n    def forward(\n        self,\n        input_ids=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/HyPe/hype_modeling_roberta.py. CHANGING:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"(batch_size, sequence_length)\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"(batch_size, sequence_length)\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0019c581",
   "metadata": {},
   "source": [
    "### Step 22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc15f54",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70046d4c",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d8f70e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        self.init_weights()\n\n    def get_output_embeddings(self):\n        return self.lm_head.decoder\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head.decoder = new_embeddings\n\n    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=MaskedLMOutput,\n        config_class=_CONFIG_FOR_DOC,\n        mask=\"<mask>\",\n    )\n    def forward(\n        self,\n        input_ids=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/HyPe/hype_modeling_roberta.py. CHANGING:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=MaskedLMOutput,\n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=MaskedLMOutput,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee4e2b7",
   "metadata": {},
   "source": [
    "### Step 23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864c32f2",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4ac51e",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee33cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        self.num_labels = config.num_labels\n        self.config = config\n\n        self.roberta = RobertaModel(config, add_pooling_layer=False)\n        self.classifier = RobertaClassificationHead(config)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=SequenceClassifierOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/HyPe/hype_modeling_roberta.py. CHANGING:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=SequenceClassifierOutput,\n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=SequenceClassifierOutput,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b137ce",
   "metadata": {},
   "source": [
    "### Step 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4878d47",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10056c1",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad1820d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        super().__init__(config)\n\n        self.roberta = RobertaModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, 1)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=MultipleChoiceModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        token_type_ids=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/HyPe/hype_modeling_roberta.py. CHANGING:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=MultipleChoiceModelOutput,\n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=MultipleChoiceModelOutput,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5fb541",
   "metadata": {},
   "source": [
    "### Step 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c133ae4",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d0d4ec",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8927a45c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        self.num_labels = config.num_labels\n\n        self.roberta = RobertaModel(config, add_pooling_layer=False)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=TokenClassifierOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/HyPe/hype_modeling_roberta.py. CHANGING:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=TokenClassifierOutput,\n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=TokenClassifierOutput,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a2f6b7",
   "metadata": {},
   "source": [
    "### Step 26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e10f86d",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93de06e",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ecdfb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.roberta = RobertaModel(config, add_pooling_layer=False)\n        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=QuestionAnsweringModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/HyPe/hype_modeling_roberta.py. CHANGING:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=QuestionAnsweringModelOutput,\n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=QuestionAnsweringModelOutput,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe92fcb",
   "metadata": {},
   "source": [
    "### Step 27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8bb3b8",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40195f96",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5899c089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n            self.noise_type = noise_type\n        elif noise_type == \"uniform\":\n            self.noise_eps = eps\n            self.noise_type = noise_type\n        else:\n            self.noise_type = None\n            IndexError(f'Invalid noise type input {noise_type}!!!')\n\n    @add_start_docstrings_to_model_forward(XLNET_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=XLNetModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/HyPe/hype_modeling_xlnet.py. CHANGING:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(XLNET_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=XLNetModelOutput,\n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(XLNET_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=XLNetModelOutput,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c908c0e7",
   "metadata": {},
   "source": [
    "### Step 28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e04522",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68b2075",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8f230f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        self.config = config\n\n        self.transformer = XLNetModel(config)\n        self.sequence_summary = SequenceSummary(config)\n        self.logits_proj = nn.Linear(config.d_model, config.num_labels)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(XLNET_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=XLNetForSequenceClassificationOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/HyPe/hype_modeling_xlnet.py. CHANGING:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(XLNET_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=XLNetForSequenceClassificationOutput,\n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(XLNET_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=XLNetForSequenceClassificationOutput,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9904b475",
   "metadata": {},
   "source": [
    "### Step 29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026f262f",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c046e9",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127d549f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.transformer = XLNetModel(config)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(XLNET_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=XLNetForTokenClassificationOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/HyPe/hype_modeling_xlnet.py. CHANGING:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(XLNET_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=XLNetForTokenClassificationOutput,\n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(XLNET_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=XLNetForTokenClassificationOutput,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24d1b9e",
   "metadata": {},
   "source": [
    "### Step 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5433a7af",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750b28bc",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783ddc88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        super().__init__(config)\n\n        self.transformer = XLNetModel(config)\n        self.sequence_summary = SequenceSummary(config)\n        self.logits_proj = nn.Linear(config.d_model, 1)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(XLNET_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=XLNetForMultipleChoiceOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        token_type_ids=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/HyPe/hype_modeling_xlnet.py. CHANGING:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(XLNET_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=XLNetForMultipleChoiceOutput,\n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(XLNET_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=XLNetForMultipleChoiceOutput,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d1e2e3",
   "metadata": {},
   "source": [
    "### Step 31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ac12e6",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3791b05",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fac61f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.transformer = XLNetModel(config)\n        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(XLNET_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=XLNetForQuestionAnsweringSimpleOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/HyPe/hype_modeling_xlnet.py. CHANGING:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(XLNET_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=XLNetForQuestionAnsweringSimpleOutput,\n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(XLNET_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=XLNetForQuestionAnsweringSimpleOutput,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec67b06e",
   "metadata": {},
   "source": [
    "### Step 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b3dad4",
   "metadata": {},
   "source": [
    "Execution Start time: 05:48:17, End time: 05:48:32 (duration: 15 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f99da69",
   "metadata": {},
   "source": [
    "**Thought**: Now that we have made the necessary changes to ensure the code runs on a CPU, let's execute the script with the specified parameters to fine-tune the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed731e5",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0013b831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The training arguments are TrainingArguments(\n_n_gpu=0,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-06,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_pin_memory=True,\nddp_backend=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=False,\ndo_train=True,\neval_accumulation_steps=20,\neval_delay=0,\neval_steps=None,\nevaluation_strategy=no,\nfp16=False,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngreater_is_better=None,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=0.0005,\nlength_column_name=length,\nload_best_model_at_end=False,\nlocal_rank=0,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=final_results/runs/Aug22_05-48-22_modal,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=500,\nlogging_strategy=steps,\nlr_scheduler_type=linear,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=None,\nmp_parameters=,\nno_cuda=True,\nnum_train_epochs=1.0,\noptim=adamw_hf,\noptim_args=None,\noutput_dir=final_results,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=8,\nper_device_train_batch_size=32,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=final_results,\nsave_on_each_node=False,\nsave_safetensors=False,\nsave_steps=30000,\nsave_strategy=steps,\nsave_total_limit=1,\nseed=24423,\nsharded_ddp=[],\nskip_memory_metrics=True,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_mps_device=False,\nwarmup_ratio=0.1,\nwarmup_steps=0,\nweight_decay=0.01,\nxpu_backend=None,\n)\n08/22/2024 05:48:22 - WARNING - __main__ -   Process rank: 0, device: cpu, n_gpu: 0distributed training: True, 16-bits training: False\n08/22/2024 05:48:22 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n_n_gpu=0,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-06,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_pin_memory=True,\nddp_backend=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=False,\ndo_train=True,\neval_accumulation_steps=20,\neval_delay=0,\neval_steps=None,\nevaluation_strategy=no,\nfp16=False,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngreater_is_better=None,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=0.0005,\nlength_column_name=length,\nload_best_model_at_end=False,\nlocal_rank=0,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=final_results/runs/Aug22_05-48-22_modal,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=500,\nlogging_strategy=steps,\nlr_scheduler_type=linear,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=None,\nmp_parameters=,\nno_cuda=True,\nnum_train_epochs=1.0,\noptim=adamw_hf,\noptim_args=None,\noutput_dir=final_results/task_rte_seed_24423_lr_0.0005_mode_None_hype_type_uniform/,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=8,\nper_device_train_batch_size=32,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=final_results,\nsave_on_each_node=False,\nsave_safetensors=False,\nsave_steps=30000,\nsave_strategy=steps,\nsave_total_limit=1,\nseed=24423,\nsharded_ddp=[],\nskip_memory_metrics=True,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_mps_device=False,\nwarmup_ratio=0.1,\nwarmup_steps=0,\nweight_decay=0.01,\nxpu_backend=None,\n)\n/usr/local/lib/python3.10/site-packages/datasets/load.py:929: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at glue.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\nRepo card metadata block was not found. Setting CardData to empty.\n08/22/2024 05:48:22 - WARNING - huggingface_hub.repocard -   Repo card metadata block was not found. Setting CardData to empty.\n\nDownloading data:   0%|                              | 0.00/697k [00:00<?, ?B/s]\n\n\nGenerating train split: 0 examples [00:00, ? examples/s]\nGenerating train split: 2490 examples [00:00, 27424.33 examples/s]\n\nGenerating validation split: 0 examples [00:00, ? examples/s]\nGenerating validation split: 277 examples [00:00, 29976.32 examples/s]\n\nGenerating test split: 0 examples [00:00, ? examples/s]\nGenerating test split: 3000 examples [00:00, 35742.75 examples/s]\n/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\nconfig.json:   0%|                                    | 0.00/570 [00:00<?, ?B/s]\n\n[INFO|configuration_utils.py:669] 2024-08-22 05:48:23,374 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n[INFO|configuration_utils.py:725] 2024-08-22 05:48:23,374 >> Model config BertConfig {\n  \"_name_or_path\": \"bert-base-uncased\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"finetuning_task\": \"rte\",\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.30.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\n\ntokenizer_config.json:   0%|                         | 0.00/48.0 [00:00<?, ?B/s]\ntokenizer_config.json: 100%|██████████████████| 48.0/48.0 [00:00<00:00, 558kB/s]\n[INFO|configuration_utils.py:669] 2024-08-22 05:48:23,560 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n[INFO|configuration_utils.py:725] 2024-08-22 05:48:23,560 >> Model config BertConfig {\n  \"_name_or_path\": \"bert-base-uncased\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.30.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\n\nvocab.txt:   0%|                                     | 0.00/232k [00:00<?, ?B/s]\n\n\ntokenizer.json:   0%|                                | 0.00/466k [00:00<?, ?B/s]\n\n\n[INFO|tokenization_utils_base.py:1823] 2024-08-22 05:48:24,403 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n[INFO|tokenization_utils_base.py:1823] 2024-08-22 05:48:24,403 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n[INFO|tokenization_utils_base.py:1823] 2024-08-22 05:48:24,403 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:1823] 2024-08-22 05:48:24,403 >> loading file special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:1823] 2024-08-22 05:48:24,403 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n[INFO|configuration_utils.py:669] 2024-08-22 05:48:24,404 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n[INFO|configuration_utils.py:725] 2024-08-22 05:48:24,405 >> Model config BertConfig {\n  \"_name_or_path\": \"bert-base-uncased\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.30.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\n\nmodel.safetensors:   0%|                             | 0.00/440M [00:00<?, ?B/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[INFO|modeling_utils.py:2578] 2024-08-22 05:48:27,646 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n- This IS expected if you are initializing BertForSequenceClassification_HyPe from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification_HyPe from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n[WARNING|modeling_utils.py:3297] 2024-08-22 05:48:30,370 >> Some weights of BertForSequenceClassification_HyPe were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\nMap:   0%|                                        | 0/10 [00:00<?, ? examples/s]\nMap: 100%|██████████████████████████████| 10/10 [00:00<00:00, 329.08 examples/s]\n\nMap:   0%|                                       | 0/277 [00:00<?, ? examples/s]\nMap: 100%|███████████████████████████| 277/277 [00:00<00:00, 8480.45 examples/s]\n\nMap:   0%|                                      | 0/3000 [00:00<?, ? examples/s]\nMap:  67%|████████████████        | 2000/3000 [00:00<00:00, 13051.69 examples/s]\nMap: 100%|████████████████████████| 3000/3000 [00:00<00:00, 11875.06 examples/s]\n08/22/2024 05:48:30 - INFO - __main__ -   Sample 4 of the training set: {'sentence1': 'Cyanide is also used by Muru-ami fishermen in the Philippines to drive fish from the coral reefs.', 'sentence2': 'Philippine fishermen use cyanide in fishing.', 'label': 0, 'idx': 813, 'input_ids': [101, 22330, 7088, 3207, 2003, 2036, 2109, 2011, 14163, 6820, 1011, 26445, 16532, 1999, 1996, 5137, 2000, 3298, 3869, 2013, 1996, 11034, 21484, 1012, 102, 7802, 16532, 2224, 22330, 7088, 3207, 1999, 5645, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n08/22/2024 05:48:30 - INFO - __main__ -   Sample 7 of the training set: {'sentence1': 'Ecuador has received short shrift in U.S. policy debates. Yet in recent years, significant public debate has taken place in Ecuador on the joint U.S./Colombian initiative and its effects on Ecuador and its citizens.', 'sentence2': 'Peru debates border region with Ecuador.', 'label': 1, 'idx': 658, 'input_ids': [101, 10378, 2038, 2363, 2460, 14880, 6199, 1999, 1057, 1012, 1055, 1012, 3343, 14379, 1012, 2664, 1999, 3522, 2086, 1010, 3278, 2270, 5981, 2038, 2579, 2173, 1999, 10378, 2006, 1996, 4101, 1057, 1012, 1055, 1012, 1013, 13598, 6349, 1998, 2049, 3896, 2006, 10378, 1998, 2049, 4480, 1012, 102, 7304, 14379, 3675, 2555, 2007, 10378, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n08/22/2024 05:48:30 - INFO - __main__ -   Sample 1 of the training set: {'sentence1': 'The 26-member International Energy Agency said, Friday, that member countries would release oil to help relieve the U.S. fuel crisis caused by Hurricane Katrina.', 'sentence2': 'Responding to a plea from the International Energy Agency for member countries to release reserves, Canada is prepared to help.', 'label': 1, 'idx': 146, 'input_ids': [101, 1996, 2656, 1011, 2266, 2248, 2943, 4034, 2056, 1010, 5958, 1010, 2008, 2266, 3032, 2052, 2713, 3514, 2000, 2393, 15804, 1996, 1057, 1012, 1055, 1012, 4762, 5325, 3303, 2011, 7064, 16864, 1012, 102, 14120, 2000, 1037, 14865, 2013, 1996, 2248, 2943, 4034, 2005, 2266, 3032, 2000, 2713, 8269, 1010, 2710, 2003, 4810, 2000, 2393, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n/content/HyPe/main.py:534: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n  metric = load_metric(\"metric.py\", data_args.task_name)\n/usr/local/lib/python3.10/site-packages/datasets/load.py:855: FutureWarning: The repository for metric contains custom code which must be executed to correctly load the metric. You can inspect the repository content at metric.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n  warnings.warn(\n08/22/2024 05:48:30 - INFO - __main__ -   using default trainer\n08/22/2024 05:48:30 - WARNING - accelerate.utils.other -   Detected kernel version 4.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n[INFO|trainer.py:776] 2024-08-22 05:48:31,028 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification_HyPe.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `BertForSequenceClassification_HyPe.forward`,  you can safely ignore this message.\n/usr/local/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n[INFO|trainer.py:1777] 2024-08-22 05:48:31,868 >> ***** Running training *****\n[INFO|trainer.py:1778] 2024-08-22 05:48:31,868 >>   Num examples = 10\n[INFO|trainer.py:1779] 2024-08-22 05:48:31,868 >>   Num Epochs = 1\n[INFO|trainer.py:1780] 2024-08-22 05:48:31,868 >>   Instantaneous batch size per device = 32\n[INFO|trainer.py:1781] 2024-08-22 05:48:31,868 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n[INFO|trainer.py:1782] 2024-08-22 05:48:31,868 >>   Gradient Accumulation steps = 1\n[INFO|trainer.py:1783] 2024-08-22 05:48:31,868 >>   Total optimization steps = 1\n[INFO|trainer.py:1784] 2024-08-22 05:48:31,869 >>   Number of trainable parameters = 109,483,778\n\n  0%|                                                     | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/site-packages/transformers/modeling_utils.py:884: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\nTraceback (most recent call last):\n  File \"/content/HyPe/main.py\", line 664, in <module>\n    main()\n  File \"/content/HyPe/main.py\", line 581, in main\n    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n  File \"/usr/local/lib/python3.10/site-packages/transformers/trainer.py\", line 1645, in train\n    return inner_training_loop(\n  File \"/usr/local/lib/python3.10/site-packages/transformers/trainer.py\", line 1929, in _inner_training_loop\n    tr_loss_step = self.training_step(model, inputs)\n  File \"/usr/local/lib/python3.10/site-packages/transformers/trainer.py\", line 2750, in training_step\n    loss = self.compute_loss(model, inputs)\n  File \"/usr/local/lib/python3.10/site-packages/transformers/trainer.py\", line 2775, in compute_loss\n    outputs = model(**inputs)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/content/HyPe/hype_modeling_bert.py\", line 1586, in forward\n    outputs = self.bert(\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/content/HyPe/hype_modeling_bert.py\", line 1055, in forward\n    encoder_outputs = self.encoder(\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/content/HyPe/hype_modeling_bert.py\", line 612, in forward\n    noise = self.noise_eps * torch.cuda.FloatTensor(hidden_states.shape).uniform_()\nTypeError: type torch.cuda.FloatTensor not available. Torch not compiled with CUDA enabled.\n\n  0%|                                                     | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python main.py \\\n",
    "  --adam_epsilon 1e-6 \\\n",
    "  --max_seq_length 128 \\\n",
    "  --eval_accumulation_steps 20 \\\n",
    "  --do_eval \\\n",
    "  --do_train \\\n",
    "  --warmup_ratio 0.1 \\\n",
    "  --weight_decay 0.01 \\\n",
    "  --save_total_limit 1 \\\n",
    "  --save_steps 30000 \\\n",
    "  --model_name_or_path bert-base-uncased \\\n",
    "  --task_name rte \\\n",
    "  --per_device_train_batch_size 32 \\\n",
    "  --learning_rate 5e-4 \\\n",
    "  --num_train_epochs 1 \\\n",
    "  --seed 24423 \\\n",
    "  --output_dir final_results \\\n",
    "  --low_resource \\\n",
    "  --dropout_rate 0.0 \\\n",
    "  --hype_type uniform \\\n",
    "  --hype_eps 1e-5 \\\n",
    "  --no_cuda True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814044b0",
   "metadata": {},
   "source": [
    "### Step 33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77fe932",
   "metadata": {},
   "source": [
    "Execution Start time: 05:48:35, End time: 05:48:35 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308378f2",
   "metadata": {},
   "source": [
    "**Thought**: The error message indicates that the code is attempting to use a CUDA tensor, which is not available since we are running on a CPU. We need to modify the code to use a CPU tensor instead.\n",
    "\n",
    "Let's locate the line in `hype_modeling_bert.py` where `torch.cuda.FloatTensor` is used and change it to `torch.FloatTensor`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693aef59",
   "metadata": {},
   "source": [
    "**Action (search_file)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9fc0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Found 6 matches for \"torch.cuda.FloatTensor\" in /content/HyPe/hype_modeling_bert.py:\nLine 520:                noise = self.noise_eps * torch.cuda.FloatTensor(hidden_states.shape).normal_()\nLine 522:                noise = self.noise_eps * torch.cuda.FloatTensor(hidden_states.shape).uniform_()\nLine 601:                            noise = self.noise_eps * torch.cuda.FloatTensor(hidden_states.shape).normal_()\nLine 603:                            noise = self.noise_eps * torch.cuda.FloatTensor(hidden_states.shape).uniform_()\nLine 610:                        noise = self.noise_eps * torch.cuda.FloatTensor(hidden_states.shape).normal_()\nLine 612:                        noise = self.noise_eps * torch.cuda.FloatTensor(hidden_states.shape).uniform_()\nEnd of matches for \"torch.cuda.FloatTensor\" in /content/HyPe/hype_modeling_bert.py\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_file \"torch.cuda.FloatTensor\" hype_modeling_bert.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b80962b",
   "metadata": {},
   "source": [
    "### Step 34"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bf968f",
   "metadata": {},
   "source": [
    "Execution Start time: 05:48:38, End time: 05:48:38 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eda566",
   "metadata": {},
   "source": [
    "**Thought**: We have found multiple instances of `torch.cuda.FloatTensor` in the `hype_modeling_bert.py` file. We need to replace all occurrences with `torch.FloatTensor` to ensure compatibility with CPU execution.\n",
    "\n",
    "Let's edit the file to make these changes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf0173a",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb38de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/HyPe/hype_modeling_bert.py (1924 lines total)]\n(504 more lines above)\n505:                head_mask,\n506:                encoder_hidden_states,\n507:                encoder_attention_mask,\n508:                cross_attn_past_key_value,\n509:                output_attentions,\n510:            )\n511:            attention_output = cross_attention_outputs[0]\n512:            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n513:\n514:            # add cross-attn cache to positions 3,4 of present_key_value tuple\n515:            cross_attn_present_key_value = cross_attention_outputs[-1]\n516:            present_key_value = present_key_value + cross_attn_present_key_value\n517:\n518:        if self.training and self.noise_type is not None:\n519:            if self.noise_type == 'normal':\n520:                noise = self.noise_eps * torch.cuda.FloatTensor(hidden_states.shape).normal_()\n521:            elif self.noise_type == 'uniform':\n522:                noise = self.noise_eps * torch.cuda.FloatTensor(hidden_states.shape).uniform_()\n523:            else:\n524:                RuntimeError\n525:            attention_output = attention_output + noise\n526:\n527:        layer_output = apply_chunking_to_forward(\n528:            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n529:        )\n530:        outputs = (layer_output,) + outputs\n531:\n532:        # if decoder, return the attn key/values as the last output\n533:        if self.is_decoder:\n534:            outputs = outputs + (present_key_value,)\n535:\n536:        return outputs\n537:\n538:    def feed_forward_chunk(self, attention_output):\n539:        intermediate_output = self.intermediate(attention_output)\n540:        layer_output = self.output(intermediate_output, attention_output)\n541:        return layer_output\n542:\n543:\n544:class BertEncoder(nn.Module):\n545:    def __init__(self, config):\n546:        super().__init__()\n547:        self.config = config\n548:        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n549:        self.noise_type = None\n550:    \n551:    def add_noise_sampler(self, eps, noise_type, add_intermediate = False, only_intermediate = False, layer_range = None):\n552:\n553:        if noise_type == \"normal\":\n554:            self.noise_eps = eps\n555:            self.noise_type = noise_type\n556:        elif noise_type == \"uniform\":\n557:            self.noise_eps = eps\n558:            self.noise_type = noise_type\n559:        else:\n560:            IndexError(f'Invalid noise type input {noise_type}!!!')\n561:        \n562:        if add_intermediate:\n563:            for layer in self.layer:\n564:                layer.add_noise(self.noise_type, self.noise_eps)\n565:        \n566:        if only_intermediate:\n567:            self.noise_type = None\n568:            self.noise_eps = None\n569:        \n570:        if layer_range:\n571:            self.layer_range = layer_range\n572:        else:\n573:            self.layer_range = None\n574:\n575:    def forward(\n576:        self,\n577:        hidden_states,\n578:        attention_mask=None,\n579:        head_mask=None,\n580:        encoder_hidden_states=None,\n581:        encoder_attention_mask=None,\n582:        past_key_values=None,\n583:        use_cache=None,\n584:        output_attentions=False,\n585:        output_hidden_states=False,\n586:        return_dict=True,\n587:    ):\n588:        all_hidden_states = () if output_hidden_states else None\n589:        all_self_attentions = () if output_attentions else None\n590:        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n591:\n592:        next_decoder_cache = () if use_cache else None\n593:        for i, layer_module in enumerate(self.layer):\n594:            if output_hidden_states:\n595:                all_hidden_states = all_hidden_states + (hidden_states,)\n596:       \n597:            if self.training and self.noise_type is not None:\n598:                if self.layer_range is not None:\n599:                    if i in self.layer_range:\n600:                        if self.noise_type == 'normal':\n601:                            noise = self.noise_eps * torch.cuda.FloatTensor(hidden_states.shape).normal_()\n602:                        elif self.noise_type == 'uniform':\n603:                            noise = self.noise_eps * torch.cuda.FloatTensor(hidden_states.shape).uniform_()\n604:                        else:\n(1320 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open hype_modeling_bert.py 520"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb7a08c",
   "metadata": {},
   "source": [
    "### Step 35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9b20b6",
   "metadata": {},
   "source": [
    "Execution Start time: 05:48:40, End time: 05:48:41 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5196a7",
   "metadata": {},
   "source": [
    "**Thought**: Let's replace all instances of `torch.cuda.FloatTensor` with `torch.FloatTensor` in the visible lines to ensure compatibility with CPU execution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acb4393",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6fc8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/HyPe/hype_modeling_bert.py (1924 lines total)]\n(469 more lines above)\n470:        head_mask=None,\n471:        encoder_hidden_states=None,\n472:        encoder_attention_mask=None,\n473:        past_key_value=None,\n474:        output_attentions=False,\n475:    ):\n476:        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n477:        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n478:        self_attention_outputs = self.attention(\n479:            hidden_states,\n480:            attention_mask,\n481:            head_mask,\n482:            output_attentions=output_attentions,\n483:            past_key_value=self_attn_past_key_value,\n484:        )\n485:        attention_output = self_attention_outputs[0]\n486:\n487:        # if decoder, the last output is tuple of self-attn cache\n488:        if self.is_decoder:\n489:            outputs = self_attention_outputs[1:-1]\n490:            present_key_value = self_attention_outputs[-1]\n491:        else:\n492:            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n493:\n494:        cross_attn_present_key_value = None\n495:        if self.is_decoder and encoder_hidden_states is not None:\n496:            assert hasattr(\n497:                self, \"crossattention\"\n498:            ), f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\"\n499:\n500:            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n501:            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n502:            cross_attention_outputs = self.crossattention(\n503:                attention_output,\n504:                attention_mask,\n505:                head_mask,\n506:                encoder_hidden_states,\n507:                encoder_attention_mask,\n508:                cross_attn_past_key_value,\n509:                output_attentions,\n510:            )\n511:            attention_output = cross_attention_outputs[0]\n512:            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n513:\n514:            # add cross-attn cache to positions 3,4 of present_key_value tuple\n515:            cross_attn_present_key_value = cross_attention_outputs[-1]\n516:            present_key_value = present_key_value + cross_attn_present_key_value\n517:\n518:        if self.training and self.noise_type is not None:\n519:            if self.noise_type == 'normal':\n520:                noise = self.noise_eps * torch.FloatTensor(hidden_states.shape).normal_()\n521:            elif self.noise_type == 'uniform':\n522:                noise = self.noise_eps * torch.FloatTensor(hidden_states.shape).uniform_()\n523:            else:\n524:                RuntimeError\n525:            attention_output = attention_output + noise\n526:\n527:        layer_output = apply_chunking_to_forward(\n528:            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n529:        )\n530:        outputs = (layer_output,) + outputs\n531:\n532:        # if decoder, return the attn key/values as the last output\n533:        if self.is_decoder:\n534:            outputs = outputs + (present_key_value,)\n535:\n536:        return outputs\n537:\n538:    def feed_forward_chunk(self, attention_output):\n539:        intermediate_output = self.intermediate(attention_output)\n540:        layer_output = self.output(intermediate_output, attention_output)\n541:        return layer_output\n542:\n543:\n544:class BertEncoder(nn.Module):\n545:    def __init__(self, config):\n546:        super().__init__()\n547:        self.config = config\n548:        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n549:        self.noise_type = None\n550:    \n551:    def add_noise_sampler(self, eps, noise_type, add_intermediate = False, only_intermediate = False, layer_range = None):\n552:\n553:        if noise_type == \"normal\":\n554:            self.noise_eps = eps\n555:            self.noise_type = noise_type\n556:        elif noise_type == \"uniform\":\n557:            self.noise_eps = eps\n558:            self.noise_type = noise_type\n559:        else:\n560:            IndexError(f'Invalid noise type input {noise_type}!!!')\n561:        \n562:        if add_intermediate:\n563:            for layer in self.layer:\n564:                layer.add_noise(self.noise_type, self.noise_eps)\n565:        \n566:        if only_intermediate:\n567:            self.noise_type = None\n568:            self.noise_eps = None\n569:        \n(1355 more lines below)\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 520:522\n",
    "                noise = self.noise_eps * torch.FloatTensor(hidden_states.shape).normal_()\n",
    "            elif self.noise_type == 'uniform':\n",
    "                noise = self.noise_eps * torch.FloatTensor(hidden_states.shape).uniform_()\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd36d683",
   "metadata": {},
   "source": [
    "### Step 36"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914ecb4c",
   "metadata": {},
   "source": [
    "Execution Start time: 05:48:44, End time: 05:48:51 (duration: 7 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f6cd07",
   "metadata": {},
   "source": [
    "**Thought**: The changes have been made successfully. Now, let's rerun the script to fine-tune the model with the updated code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9e9eb3",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053202e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The training arguments are TrainingArguments(\n_n_gpu=0,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-06,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_pin_memory=True,\nddp_backend=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=False,\ndo_train=True,\neval_accumulation_steps=20,\neval_delay=0,\neval_steps=None,\nevaluation_strategy=no,\nfp16=False,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngreater_is_better=None,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=0.0005,\nlength_column_name=length,\nload_best_model_at_end=False,\nlocal_rank=0,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=final_results/runs/Aug22_05-48-48_modal,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=500,\nlogging_strategy=steps,\nlr_scheduler_type=linear,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=None,\nmp_parameters=,\nno_cuda=True,\nnum_train_epochs=1.0,\noptim=adamw_hf,\noptim_args=None,\noutput_dir=final_results,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=8,\nper_device_train_batch_size=32,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=final_results,\nsave_on_each_node=False,\nsave_safetensors=False,\nsave_steps=30000,\nsave_strategy=steps,\nsave_total_limit=1,\nseed=24423,\nsharded_ddp=[],\nskip_memory_metrics=True,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_mps_device=False,\nwarmup_ratio=0.1,\nwarmup_steps=0,\nweight_decay=0.01,\nxpu_backend=None,\n)\n08/22/2024 05:48:48 - WARNING - __main__ -   Process rank: 0, device: cpu, n_gpu: 0distributed training: True, 16-bits training: False\n08/22/2024 05:48:48 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n_n_gpu=0,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-06,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_pin_memory=True,\nddp_backend=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=False,\ndo_train=True,\neval_accumulation_steps=20,\neval_delay=0,\neval_steps=None,\nevaluation_strategy=no,\nfp16=False,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngreater_is_better=None,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=0.0005,\nlength_column_name=length,\nload_best_model_at_end=False,\nlocal_rank=0,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=final_results/runs/Aug22_05-48-48_modal,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=500,\nlogging_strategy=steps,\nlr_scheduler_type=linear,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=None,\nmp_parameters=,\nno_cuda=True,\nnum_train_epochs=1.0,\noptim=adamw_hf,\noptim_args=None,\noutput_dir=final_results/task_rte_seed_24423_lr_0.0005_mode_None_hype_type_uniform/,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=8,\nper_device_train_batch_size=32,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=final_results,\nsave_on_each_node=False,\nsave_safetensors=False,\nsave_steps=30000,\nsave_strategy=steps,\nsave_total_limit=1,\nseed=24423,\nsharded_ddp=[],\nskip_memory_metrics=True,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_mps_device=False,\nwarmup_ratio=0.1,\nwarmup_steps=0,\nweight_decay=0.01,\nxpu_backend=None,\n)\n/usr/local/lib/python3.10/site-packages/datasets/load.py:929: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at glue.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\nRepo card metadata block was not found. Setting CardData to empty.\n08/22/2024 05:48:48 - WARNING - huggingface_hub.repocard -   Repo card metadata block was not found. Setting CardData to empty.\n/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n[INFO|configuration_utils.py:669] 2024-08-22 05:48:48,336 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n[INFO|configuration_utils.py:725] 2024-08-22 05:48:48,337 >> Model config BertConfig {\n  \"_name_or_path\": \"bert-base-uncased\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"finetuning_task\": \"rte\",\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.30.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\n[INFO|configuration_utils.py:669] 2024-08-22 05:48:48,429 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n[INFO|configuration_utils.py:725] 2024-08-22 05:48:48,430 >> Model config BertConfig {\n  \"_name_or_path\": \"bert-base-uncased\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.30.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\n[INFO|tokenization_utils_base.py:1823] 2024-08-22 05:48:48,431 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n[INFO|tokenization_utils_base.py:1823] 2024-08-22 05:48:48,431 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n[INFO|tokenization_utils_base.py:1823] 2024-08-22 05:48:48,431 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:1823] 2024-08-22 05:48:48,431 >> loading file special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:1823] 2024-08-22 05:48:48,431 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n[INFO|configuration_utils.py:669] 2024-08-22 05:48:48,432 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n[INFO|configuration_utils.py:725] 2024-08-22 05:48:48,432 >> Model config BertConfig {\n  \"_name_or_path\": \"bert-base-uncased\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.30.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\n[INFO|modeling_utils.py:2578] 2024-08-22 05:48:48,456 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n- This IS expected if you are initializing BertForSequenceClassification_HyPe from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification_HyPe from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n[WARNING|modeling_utils.py:3297] 2024-08-22 05:48:49,389 >> Some weights of BertForSequenceClassification_HyPe were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\nMap:   0%|                                       | 0/277 [00:00<?, ? examples/s]\nMap: 100%|███████████████████████████| 277/277 [00:00<00:00, 7847.29 examples/s]\n08/22/2024 05:48:49 - INFO - __main__ -   Sample 4 of the training set: {'sentence1': 'Cyanide is also used by Muru-ami fishermen in the Philippines to drive fish from the coral reefs.', 'sentence2': 'Philippine fishermen use cyanide in fishing.', 'label': 0, 'idx': 813, 'input_ids': [101, 22330, 7088, 3207, 2003, 2036, 2109, 2011, 14163, 6820, 1011, 26445, 16532, 1999, 1996, 5137, 2000, 3298, 3869, 2013, 1996, 11034, 21484, 1012, 102, 7802, 16532, 2224, 22330, 7088, 3207, 1999, 5645, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n08/22/2024 05:48:49 - INFO - __main__ -   Sample 7 of the training set: {'sentence1': 'Ecuador has received short shrift in U.S. policy debates. Yet in recent years, significant public debate has taken place in Ecuador on the joint U.S./Colombian initiative and its effects on Ecuador and its citizens.', 'sentence2': 'Peru debates border region with Ecuador.', 'label': 1, 'idx': 658, 'input_ids': [101, 10378, 2038, 2363, 2460, 14880, 6199, 1999, 1057, 1012, 1055, 1012, 3343, 14379, 1012, 2664, 1999, 3522, 2086, 1010, 3278, 2270, 5981, 2038, 2579, 2173, 1999, 10378, 2006, 1996, 4101, 1057, 1012, 1055, 1012, 1013, 13598, 6349, 1998, 2049, 3896, 2006, 10378, 1998, 2049, 4480, 1012, 102, 7304, 14379, 3675, 2555, 2007, 10378, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n08/22/2024 05:48:49 - INFO - __main__ -   Sample 1 of the training set: {'sentence1': 'The 26-member International Energy Agency said, Friday, that member countries would release oil to help relieve the U.S. fuel crisis caused by Hurricane Katrina.', 'sentence2': 'Responding to a plea from the International Energy Agency for member countries to release reserves, Canada is prepared to help.', 'label': 1, 'idx': 146, 'input_ids': [101, 1996, 2656, 1011, 2266, 2248, 2943, 4034, 2056, 1010, 5958, 1010, 2008, 2266, 3032, 2052, 2713, 3514, 2000, 2393, 15804, 1996, 1057, 1012, 1055, 1012, 4762, 5325, 3303, 2011, 7064, 16864, 1012, 102, 14120, 2000, 1037, 14865, 2013, 1996, 2248, 2943, 4034, 2005, 2266, 3032, 2000, 2713, 8269, 1010, 2710, 2003, 4810, 2000, 2393, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n/content/HyPe/main.py:534: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n  metric = load_metric(\"metric.py\", data_args.task_name)\n/usr/local/lib/python3.10/site-packages/datasets/load.py:855: FutureWarning: The repository for metric contains custom code which must be executed to correctly load the metric. You can inspect the repository content at metric.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n  warnings.warn(\n08/22/2024 05:48:49 - INFO - __main__ -   using default trainer\n08/22/2024 05:48:49 - WARNING - accelerate.utils.other -   Detected kernel version 4.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n[INFO|trainer.py:776] 2024-08-22 05:48:49,712 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification_HyPe.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `BertForSequenceClassification_HyPe.forward`,  you can safely ignore this message.\n/usr/local/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n[INFO|trainer.py:1777] 2024-08-22 05:48:50,404 >> ***** Running training *****\n[INFO|trainer.py:1778] 2024-08-22 05:48:50,405 >>   Num examples = 10\n[INFO|trainer.py:1779] 2024-08-22 05:48:50,405 >>   Num Epochs = 1\n[INFO|trainer.py:1780] 2024-08-22 05:48:50,405 >>   Instantaneous batch size per device = 32\n[INFO|trainer.py:1781] 2024-08-22 05:48:50,405 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n[INFO|trainer.py:1782] 2024-08-22 05:48:50,405 >>   Gradient Accumulation steps = 1\n[INFO|trainer.py:1783] 2024-08-22 05:48:50,405 >>   Total optimization steps = 1\n[INFO|trainer.py:1784] 2024-08-22 05:48:50,405 >>   Number of trainable parameters = 109,483,778\n\n  0%|                                                     | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/site-packages/transformers/modeling_utils.py:884: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\nTraceback (most recent call last):\n  File \"/content/HyPe/main.py\", line 664, in <module>\n    main()\n  File \"/content/HyPe/main.py\", line 581, in main\n    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n  File \"/usr/local/lib/python3.10/site-packages/transformers/trainer.py\", line 1645, in train\n    return inner_training_loop(\n  File \"/usr/local/lib/python3.10/site-packages/transformers/trainer.py\", line 1929, in _inner_training_loop\n    tr_loss_step = self.training_step(model, inputs)\n  File \"/usr/local/lib/python3.10/site-packages/transformers/trainer.py\", line 2750, in training_step\n    loss = self.compute_loss(model, inputs)\n  File \"/usr/local/lib/python3.10/site-packages/transformers/trainer.py\", line 2775, in compute_loss\n    outputs = model(**inputs)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/content/HyPe/hype_modeling_bert.py\", line 1586, in forward\n    outputs = self.bert(\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/content/HyPe/hype_modeling_bert.py\", line 1055, in forward\n    encoder_outputs = self.encoder(\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/content/HyPe/hype_modeling_bert.py\", line 612, in forward\n    noise = self.noise_eps * torch.cuda.FloatTensor(hidden_states.shape).uniform_()\nTypeError: type torch.cuda.FloatTensor not available. Torch not compiled with CUDA enabled.\n\n  0%|                                                     | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python main.py \\\n",
    "  --adam_epsilon 1e-6 \\\n",
    "  --max_seq_length 128 \\\n",
    "  --eval_accumulation_steps 20 \\\n",
    "  --do_eval \\\n",
    "  --do_train \\\n",
    "  --warmup_ratio 0.1 \\\n",
    "  --weight_decay 0.01 \\\n",
    "  --save_total_limit 1 \\\n",
    "  --save_steps 30000 \\\n",
    "  --model_name_or_path bert-base-uncased \\\n",
    "  --task_name rte \\\n",
    "  --per_device_train_batch_size 32 \\\n",
    "  --learning_rate 5e-4 \\\n",
    "  --num_train_epochs 1 \\\n",
    "  --seed 24423 \\\n",
    "  --output_dir final_results \\\n",
    "  --low_resource \\\n",
    "  --dropout_rate 0.0 \\\n",
    "  --hype_type uniform \\\n",
    "  --hype_eps 1e-5 \\\n",
    "  --no_cuda True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5970337",
   "metadata": {},
   "source": [
    "### Step 37"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367ecf02",
   "metadata": {},
   "source": [
    "Execution Start time: 05:48:53, End time: 05:48:53 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3edf9f",
   "metadata": {},
   "source": [
    "**Thought**: It seems like there was another instance of `torch.cuda.FloatTensor` that we missed in the `hype_modeling_bert.py` file. Let's locate this instance and replace it with `torch.FloatTensor`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e614dc8e",
   "metadata": {},
   "source": [
    "**Action (goto)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e62f151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/HyPe/hype_modeling_bert.py (1924 lines total)]\n(596 more lines above)\n597:            if self.training and self.noise_type is not None:\n598:                if self.layer_range is not None:\n599:                    if i in self.layer_range:\n600:                        if self.noise_type == 'normal':\n601:                            noise = self.noise_eps * torch.cuda.FloatTensor(hidden_states.shape).normal_()\n602:                        elif self.noise_type == 'uniform':\n603:                            noise = self.noise_eps * torch.cuda.FloatTensor(hidden_states.shape).uniform_()\n604:                        else:\n605:                            RuntimeError\n606:                        hidden_states = hidden_states + noise\n607:                    \n608:                else:\n609:                    if self.noise_type == 'normal':\n610:                        noise = self.noise_eps * torch.cuda.FloatTensor(hidden_states.shape).normal_()\n611:                    elif self.noise_type == 'uniform':\n612:                        noise = self.noise_eps * torch.cuda.FloatTensor(hidden_states.shape).uniform_()\n613:                    else:\n614:                        RuntimeError\n615:                    hidden_states = hidden_states + noise\n616:\n617:                        \n618:\n619:            layer_head_mask = head_mask[i] if head_mask is not None else None\n620:            past_key_value = past_key_values[i] if past_key_values is not None else None\n621:\n622:            if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n623:\n624:                if use_cache:\n625:                    logger.warning(\n626:                        \"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting \"\n627:                        \"`use_cache=False`...\"\n628:                    )\n629:                    use_cache = False\n630:\n631:                def create_custom_forward(module):\n632:                    def custom_forward(*inputs):\n633:                        return module(*inputs, past_key_value, output_attentions)\n634:\n635:                    return custom_forward\n636:\n637:                layer_outputs = torch.utils.checkpoint.checkpoint(\n638:                    create_custom_forward(layer_module),\n639:                    hidden_states,\n640:                    attention_mask,\n641:                    layer_head_mask,\n642:                    encoder_hidden_states,\n643:                    encoder_attention_mask,\n644:                )\n645:            else:\n646:                layer_outputs = layer_module(\n647:                    hidden_states,\n648:                    attention_mask,\n649:                    layer_head_mask,\n650:                    encoder_hidden_states,\n651:                    encoder_attention_mask,\n652:                    past_key_value,\n653:                    output_attentions,\n654:                )\n655:\n656:            hidden_states = layer_outputs[0]\n657:            if use_cache:\n658:                next_decoder_cache += (layer_outputs[-1],)\n659:            if output_attentions:\n660:                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n661:                if self.config.add_cross_attention:\n662:                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n663:\n664:        if output_hidden_states:\n665:            all_hidden_states = all_hidden_states + (hidden_states,)\n666:\n667:        if not return_dict:\n668:            return tuple(\n669:                v\n670:                for v in [\n671:                    hidden_states,\n672:                    next_decoder_cache,\n673:                    all_hidden_states,\n674:                    all_self_attentions,\n675:                    all_cross_attentions,\n676:                ]\n677:                if v is not None\n678:            )\n679:        return BaseModelOutputWithPastAndCrossAttentions(\n680:            last_hidden_state=hidden_states,\n681:            past_key_values=next_decoder_cache,\n682:            hidden_states=all_hidden_states,\n683:            attentions=all_self_attentions,\n684:            cross_attentions=all_cross_attentions,\n685:        )\n686:\n687:\n688:class BertPooler(nn.Module):\n689:    def __init__(self, config):\n690:        super().__init__()\n691:        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n692:        self.activation = nn.Tanh()\n693:\n694:    def forward(self, hidden_states):\n695:        # We \"pool\" the model by simply taking the hidden state corresponding\n696:        # to the first token.\n(1228 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goto 612"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1871b3",
   "metadata": {},
   "source": [
    "### Step 38"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65a843c",
   "metadata": {},
   "source": [
    "Execution Start time: 05:48:54, End time: 05:48:55 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241c579a",
   "metadata": {},
   "source": [
    "**Thought**: We have located the remaining instance of `torch.cuda.FloatTensor`. Let's replace it with `torch.FloatTensor` to ensure compatibility with CPU execution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bf4a02",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe3e3a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/HyPe/hype_modeling_bert.py (1924 lines total)]\n(561 more lines above)\n562:        if add_intermediate:\n563:            for layer in self.layer:\n564:                layer.add_noise(self.noise_type, self.noise_eps)\n565:        \n566:        if only_intermediate:\n567:            self.noise_type = None\n568:            self.noise_eps = None\n569:        \n570:        if layer_range:\n571:            self.layer_range = layer_range\n572:        else:\n573:            self.layer_range = None\n574:\n575:    def forward(\n576:        self,\n577:        hidden_states,\n578:        attention_mask=None,\n579:        head_mask=None,\n580:        encoder_hidden_states=None,\n581:        encoder_attention_mask=None,\n582:        past_key_values=None,\n583:        use_cache=None,\n584:        output_attentions=False,\n585:        output_hidden_states=False,\n586:        return_dict=True,\n587:    ):\n588:        all_hidden_states = () if output_hidden_states else None\n589:        all_self_attentions = () if output_attentions else None\n590:        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n591:\n592:        next_decoder_cache = () if use_cache else None\n593:        for i, layer_module in enumerate(self.layer):\n594:            if output_hidden_states:\n595:                all_hidden_states = all_hidden_states + (hidden_states,)\n596:       \n597:            if self.training and self.noise_type is not None:\n598:                if self.layer_range is not None:\n599:                    if i in self.layer_range:\n600:                        if self.noise_type == 'normal':\n601:                            noise = self.noise_eps * torch.cuda.FloatTensor(hidden_states.shape).normal_()\n602:                        elif self.noise_type == 'uniform':\n603:                            noise = self.noise_eps * torch.cuda.FloatTensor(hidden_states.shape).uniform_()\n604:                        else:\n605:                            RuntimeError\n606:                        hidden_states = hidden_states + noise\n607:                    \n608:                else:\n609:                    if self.noise_type == 'normal':\n610:                        noise = self.noise_eps * torch.cuda.FloatTensor(hidden_states.shape).normal_()\n611:                    elif self.noise_type == 'uniform':\n612:                        noise = self.noise_eps * torch.FloatTensor(hidden_states.shape).uniform_()\n613:                    else:\n614:                        RuntimeError\n615:                    hidden_states = hidden_states + noise\n616:\n617:                        \n618:\n619:            layer_head_mask = head_mask[i] if head_mask is not None else None\n620:            past_key_value = past_key_values[i] if past_key_values is not None else None\n621:\n622:            if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n623:\n624:                if use_cache:\n625:                    logger.warning(\n626:                        \"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting \"\n627:                        \"`use_cache=False`...\"\n628:                    )\n629:                    use_cache = False\n630:\n631:                def create_custom_forward(module):\n632:                    def custom_forward(*inputs):\n633:                        return module(*inputs, past_key_value, output_attentions)\n634:\n635:                    return custom_forward\n636:\n637:                layer_outputs = torch.utils.checkpoint.checkpoint(\n638:                    create_custom_forward(layer_module),\n639:                    hidden_states,\n640:                    attention_mask,\n641:                    layer_head_mask,\n642:                    encoder_hidden_states,\n643:                    encoder_attention_mask,\n644:                )\n645:            else:\n646:                layer_outputs = layer_module(\n647:                    hidden_states,\n648:                    attention_mask,\n649:                    layer_head_mask,\n650:                    encoder_hidden_states,\n651:                    encoder_attention_mask,\n652:                    past_key_value,\n653:                    output_attentions,\n654:                )\n655:\n656:            hidden_states = layer_outputs[0]\n657:            if use_cache:\n658:                next_decoder_cache += (layer_outputs[-1],)\n659:            if output_attentions:\n660:                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n661:                if self.config.add_cross_attention:\n(1263 more lines below)\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 612:612\n",
    "                        noise = self.noise_eps * torch.FloatTensor(hidden_states.shape).uniform_()\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b07a67",
   "metadata": {},
   "source": [
    "### Step 39"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6426086",
   "metadata": {},
   "source": [
    "Execution Start time: 05:48:59, End time: 05:49:53 (duration: 54 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17789ff5",
   "metadata": {},
   "source": [
    "**Thought**: The necessary changes have been made to replace `torch.cuda.FloatTensor` with `torch.FloatTensor`. Let's attempt to run the script again to fine-tune the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26ebc4f",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798a02f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The training arguments are TrainingArguments(\n_n_gpu=0,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-06,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_pin_memory=True,\nddp_backend=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=False,\ndo_train=True,\neval_accumulation_steps=20,\neval_delay=0,\neval_steps=None,\nevaluation_strategy=no,\nfp16=False,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngreater_is_better=None,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=0.0005,\nlength_column_name=length,\nload_best_model_at_end=False,\nlocal_rank=0,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=final_results/runs/Aug22_05-49-02_modal,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=500,\nlogging_strategy=steps,\nlr_scheduler_type=linear,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=None,\nmp_parameters=,\nno_cuda=True,\nnum_train_epochs=1.0,\noptim=adamw_hf,\noptim_args=None,\noutput_dir=final_results,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=8,\nper_device_train_batch_size=32,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=final_results,\nsave_on_each_node=False,\nsave_safetensors=False,\nsave_steps=30000,\nsave_strategy=steps,\nsave_total_limit=1,\nseed=24423,\nsharded_ddp=[],\nskip_memory_metrics=True,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_mps_device=False,\nwarmup_ratio=0.1,\nwarmup_steps=0,\nweight_decay=0.01,\nxpu_backend=None,\n)\n08/22/2024 05:49:02 - WARNING - __main__ -   Process rank: 0, device: cpu, n_gpu: 0distributed training: True, 16-bits training: False\n08/22/2024 05:49:02 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n_n_gpu=0,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-06,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_pin_memory=True,\nddp_backend=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=False,\ndo_train=True,\neval_accumulation_steps=20,\neval_delay=0,\neval_steps=None,\nevaluation_strategy=no,\nfp16=False,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngreater_is_better=None,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=0.0005,\nlength_column_name=length,\nload_best_model_at_end=False,\nlocal_rank=0,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=final_results/runs/Aug22_05-49-02_modal,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=500,\nlogging_strategy=steps,\nlr_scheduler_type=linear,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=None,\nmp_parameters=,\nno_cuda=True,\nnum_train_epochs=1.0,\noptim=adamw_hf,\noptim_args=None,\noutput_dir=final_results/task_rte_seed_24423_lr_0.0005_mode_None_hype_type_uniform/,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=8,\nper_device_train_batch_size=32,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=final_results,\nsave_on_each_node=False,\nsave_safetensors=False,\nsave_steps=30000,\nsave_strategy=steps,\nsave_total_limit=1,\nseed=24423,\nsharded_ddp=[],\nskip_memory_metrics=True,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_mps_device=False,\nwarmup_ratio=0.1,\nwarmup_steps=0,\nweight_decay=0.01,\nxpu_backend=None,\n)\n/usr/local/lib/python3.10/site-packages/datasets/load.py:929: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at glue.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\nRepo card metadata block was not found. Setting CardData to empty.\n08/22/2024 05:49:02 - WARNING - huggingface_hub.repocard -   Repo card metadata block was not found. Setting CardData to empty.\n/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n[INFO|configuration_utils.py:669] 2024-08-22 05:49:02,901 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n[INFO|configuration_utils.py:725] 2024-08-22 05:49:02,902 >> Model config BertConfig {\n  \"_name_or_path\": \"bert-base-uncased\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"finetuning_task\": \"rte\",\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.30.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\n[INFO|configuration_utils.py:669] 2024-08-22 05:49:03,000 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n[INFO|configuration_utils.py:725] 2024-08-22 05:49:03,000 >> Model config BertConfig {\n  \"_name_or_path\": \"bert-base-uncased\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.30.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\n[INFO|tokenization_utils_base.py:1823] 2024-08-22 05:49:03,002 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n[INFO|tokenization_utils_base.py:1823] 2024-08-22 05:49:03,002 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n[INFO|tokenization_utils_base.py:1823] 2024-08-22 05:49:03,002 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:1823] 2024-08-22 05:49:03,002 >> loading file special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:1823] 2024-08-22 05:49:03,002 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n[INFO|configuration_utils.py:669] 2024-08-22 05:49:03,003 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n[INFO|configuration_utils.py:725] 2024-08-22 05:49:03,003 >> Model config BertConfig {\n  \"_name_or_path\": \"bert-base-uncased\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.30.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\n[INFO|modeling_utils.py:2578] 2024-08-22 05:49:03,026 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n- This IS expected if you are initializing BertForSequenceClassification_HyPe from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification_HyPe from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n[WARNING|modeling_utils.py:3297] 2024-08-22 05:49:03,934 >> Some weights of BertForSequenceClassification_HyPe were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\nMap:   0%|                                      | 0/3000 [00:00<?, ? examples/s]\nMap:  33%|████████▎                | 1000/3000 [00:00<00:00, 5990.17 examples/s]\nMap: 100%|█████████████████████████| 3000/3000 [00:00<00:00, 9769.88 examples/s]\nMap: 100%|█████████████████████████| 3000/3000 [00:00<00:00, 9094.66 examples/s]\n08/22/2024 05:49:04 - INFO - __main__ -   Sample 4 of the training set: {'sentence1': 'Cyanide is also used by Muru-ami fishermen in the Philippines to drive fish from the coral reefs.', 'sentence2': 'Philippine fishermen use cyanide in fishing.', 'label': 0, 'idx': 813, 'input_ids': [101, 22330, 7088, 3207, 2003, 2036, 2109, 2011, 14163, 6820, 1011, 26445, 16532, 1999, 1996, 5137, 2000, 3298, 3869, 2013, 1996, 11034, 21484, 1012, 102, 7802, 16532, 2224, 22330, 7088, 3207, 1999, 5645, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n08/22/2024 05:49:04 - INFO - __main__ -   Sample 7 of the training set: {'sentence1': 'Ecuador has received short shrift in U.S. policy debates. Yet in recent years, significant public debate has taken place in Ecuador on the joint U.S./Colombian initiative and its effects on Ecuador and its citizens.', 'sentence2': 'Peru debates border region with Ecuador.', 'label': 1, 'idx': 658, 'input_ids': [101, 10378, 2038, 2363, 2460, 14880, 6199, 1999, 1057, 1012, 1055, 1012, 3343, 14379, 1012, 2664, 1999, 3522, 2086, 1010, 3278, 2270, 5981, 2038, 2579, 2173, 1999, 10378, 2006, 1996, 4101, 1057, 1012, 1055, 1012, 1013, 13598, 6349, 1998, 2049, 3896, 2006, 10378, 1998, 2049, 4480, 1012, 102, 7304, 14379, 3675, 2555, 2007, 10378, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n08/22/2024 05:49:04 - INFO - __main__ -   Sample 1 of the training set: {'sentence1': 'The 26-member International Energy Agency said, Friday, that member countries would release oil to help relieve the U.S. fuel crisis caused by Hurricane Katrina.', 'sentence2': 'Responding to a plea from the International Energy Agency for member countries to release reserves, Canada is prepared to help.', 'label': 1, 'idx': 146, 'input_ids': [101, 1996, 2656, 1011, 2266, 2248, 2943, 4034, 2056, 1010, 5958, 1010, 2008, 2266, 3032, 2052, 2713, 3514, 2000, 2393, 15804, 1996, 1057, 1012, 1055, 1012, 4762, 5325, 3303, 2011, 7064, 16864, 1012, 102, 14120, 2000, 1037, 14865, 2013, 1996, 2248, 2943, 4034, 2005, 2266, 3032, 2000, 2713, 8269, 1010, 2710, 2003, 4810, 2000, 2393, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n/content/HyPe/main.py:534: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n  metric = load_metric(\"metric.py\", data_args.task_name)\n/usr/local/lib/python3.10/site-packages/datasets/load.py:855: FutureWarning: The repository for metric contains custom code which must be executed to correctly load the metric. You can inspect the repository content at metric.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n  warnings.warn(\n08/22/2024 05:49:04 - INFO - __main__ -   using default trainer\n08/22/2024 05:49:04 - WARNING - accelerate.utils.other -   Detected kernel version 4.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n[INFO|trainer.py:776] 2024-08-22 05:49:04,481 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification_HyPe.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `BertForSequenceClassification_HyPe.forward`,  you can safely ignore this message.\n/usr/local/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n[INFO|trainer.py:1777] 2024-08-22 05:49:05,167 >> ***** Running training *****\n[INFO|trainer.py:1778] 2024-08-22 05:49:05,167 >>   Num examples = 10\n[INFO|trainer.py:1779] 2024-08-22 05:49:05,168 >>   Num Epochs = 1\n[INFO|trainer.py:1780] 2024-08-22 05:49:05,168 >>   Instantaneous batch size per device = 32\n[INFO|trainer.py:1781] 2024-08-22 05:49:05,168 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n[INFO|trainer.py:1782] 2024-08-22 05:49:05,168 >>   Gradient Accumulation steps = 1\n[INFO|trainer.py:1783] 2024-08-22 05:49:05,168 >>   Total optimization steps = 1\n[INFO|trainer.py:1784] 2024-08-22 05:49:05,168 >>   Number of trainable parameters = 109,483,778\n\n  0%|                                                     | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/site-packages/transformers/modeling_utils.py:884: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n\n100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.68s/it][INFO|trainer.py:2044] 2024-08-22 05:49:10,848 >> \n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n\n                                                                                \n{'train_runtime': 5.6799, 'train_samples_per_second': 1.761, 'train_steps_per_second': 0.176, 'train_loss': 0.7121345400810242, 'epoch': 1.0}\n\n100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.68s/it]\n100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.68s/it]\n[INFO|trainer.py:2916] 2024-08-22 05:49:10,851 >> Saving model checkpoint to final_results/task_rte_seed_24423_lr_0.0005_mode_None_hype_type_uniform/\n[INFO|configuration_utils.py:458] 2024-08-22 05:49:10,852 >> Configuration saved in final_results/task_rte_seed_24423_lr_0.0005_mode_None_hype_type_uniform/config.json\n[INFO|modeling_utils.py:1853] 2024-08-22 05:49:11,906 >> Model weights saved in final_results/task_rte_seed_24423_lr_0.0005_mode_None_hype_type_uniform/pytorch_model.bin\n[INFO|tokenization_utils_base.py:2194] 2024-08-22 05:49:11,907 >> tokenizer config file saved in final_results/task_rte_seed_24423_lr_0.0005_mode_None_hype_type_uniform/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2201] 2024-08-22 05:49:11,907 >> Special tokens file saved in final_results/task_rte_seed_24423_lr_0.0005_mode_None_hype_type_uniform/special_tokens_map.json\n08/22/2024 05:49:11 - INFO - __main__ -   ***** Train results *****\n08/22/2024 05:49:11 - INFO - __main__ -     epoch = 1.0\n08/22/2024 05:49:11 - INFO - __main__ -     train_loss = 0.7121345400810242\n08/22/2024 05:49:11 - INFO - __main__ -     train_runtime = 5.6799\n08/22/2024 05:49:11 - INFO - __main__ -     train_samples_per_second = 1.761\n08/22/2024 05:49:11 - INFO - __main__ -     train_steps_per_second = 0.176\n08/22/2024 05:49:11 - INFO - __main__ -   *** Evaluate ***\n[INFO|trainer.py:776] 2024-08-22 05:49:11,950 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification_HyPe.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `BertForSequenceClassification_HyPe.forward`,  you can safely ignore this message.\n[INFO|trainer.py:3190] 2024-08-22 05:49:11,957 >> ***** Running Evaluation *****\n[INFO|trainer.py:3192] 2024-08-22 05:49:11,957 >>   Num examples = 277\n[INFO|trainer.py:3195] 2024-08-22 05:49:11,957 >>   Batch size = 8\n\n  0%|                                                    | 0/35 [00:00<?, ?it/s]\n  6%|██▌                                         | 2/35 [00:01<00:18,  1.77it/s]\n  9%|███▊                                        | 3/35 [00:02<00:25,  1.26it/s]\n 11%|█████                                       | 4/35 [00:03<00:28,  1.08it/s]\n 14%|██████▎                                     | 5/35 [00:04<00:29,  1.00it/s]\n 17%|███████▌                                    | 6/35 [00:05<00:30,  1.04s/it]\n 20%|████████▊                                   | 7/35 [00:06<00:30,  1.07s/it]\n 23%|██████████                                  | 8/35 [00:07<00:29,  1.09s/it]\n 26%|███████████▎                                | 9/35 [00:09<00:28,  1.10s/it]\n 29%|████████████▎                              | 10/35 [00:10<00:27,  1.11s/it]\n 31%|█████████████▌                             | 11/35 [00:11<00:26,  1.12s/it]\n 34%|██████████████▋                            | 12/35 [00:12<00:25,  1.13s/it]\n 37%|███████████████▉                           | 13/35 [00:13<00:24,  1.13s/it]\n 40%|█████████████████▏                         | 14/35 [00:14<00:23,  1.13s/it]\n 43%|██████████████████▍                        | 15/35 [00:15<00:22,  1.15s/it]\n 46%|███████████████████▋                       | 16/35 [00:17<00:22,  1.19s/it]\n 49%|████████████████████▉                      | 17/35 [00:18<00:22,  1.23s/it]\n 51%|██████████████████████                     | 18/35 [00:19<00:20,  1.22s/it]\n 54%|███████████████████████▎                   | 19/35 [00:20<00:19,  1.21s/it]\n 57%|████████████████████████▌                  | 20/35 [00:22<00:18,  1.20s/it]\n 60%|█████████████████████████▊                 | 21/35 [00:23<00:16,  1.19s/it]\n 63%|███████████████████████████                | 22/35 [00:24<00:15,  1.19s/it]\n 66%|████████████████████████████▎              | 23/35 [00:25<00:14,  1.19s/it]\n 69%|█████████████████████████████▍             | 24/35 [00:26<00:13,  1.18s/it]\n 71%|██████████████████████████████▋            | 25/35 [00:28<00:11,  1.19s/it]\n 74%|███████████████████████████████▉           | 26/35 [00:29<00:10,  1.18s/it]\n 77%|█████████████████████████████████▏         | 27/35 [00:30<00:09,  1.18s/it]\n 80%|██████████████████████████████████▍        | 28/35 [00:31<00:08,  1.18s/it]\n 83%|███████████████████████████████████▋       | 29/35 [00:32<00:07,  1.18s/it]\n 86%|████████████████████████████████████▊      | 30/35 [00:33<00:05,  1.18s/it]\n 89%|██████████████████████████████████████     | 31/35 [00:35<00:04,  1.17s/it]\n 91%|███████████████████████████████████████▎   | 32/35 [00:36<00:03,  1.16s/it]\n 94%|████████████████████████████████████████▌  | 33/35 [00:37<00:02,  1.16s/it]\n 97%|█████████████████████████████████████████▊ | 34/35 [00:38<00:01,  1.15s/it]\n100%|███████████████████████████████████████████| 35/35 [00:39<00:00,  1.03s/it]\n100%|███████████████████████████████████████████| 35/35 [00:39<00:00,  1.12s/it]\n08/22/2024 05:49:52 - INFO - __main__ -   ***** Eval results rte *****\n08/22/2024 05:49:52 - INFO - __main__ -     epoch = 1.0\n08/22/2024 05:49:52 - INFO - __main__ -     eval_accuracy = 0.4729241877256318\n08/22/2024 05:49:52 - INFO - __main__ -     eval_loss = 0.7289056181907654\n08/22/2024 05:49:52 - INFO - __main__ -     eval_runtime = 40.4163\n08/22/2024 05:49:52 - INFO - __main__ -     eval_samples_per_second = 6.854\n08/22/2024 05:49:52 - INFO - __main__ -     eval_steps_per_second = 0.866\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python main.py \\\n",
    "  --adam_epsilon 1e-6 \\\n",
    "  --max_seq_length 128 \\\n",
    "  --eval_accumulation_steps 20 \\\n",
    "  --do_eval \\\n",
    "  --do_train \\\n",
    "  --warmup_ratio 0.1 \\\n",
    "  --weight_decay 0.01 \\\n",
    "  --save_total_limit 1 \\\n",
    "  --save_steps 30000 \\\n",
    "  --model_name_or_path bert-base-uncased \\\n",
    "  --task_name rte \\\n",
    "  --per_device_train_batch_size 32 \\\n",
    "  --learning_rate 5e-4 \\\n",
    "  --num_train_epochs 1 \\\n",
    "  --seed 24423 \\\n",
    "  --output_dir final_results \\\n",
    "  --low_resource \\\n",
    "  --dropout_rate 0.0 \\\n",
    "  --hype_type uniform \\\n",
    "  --hype_eps 1e-5 \\\n",
    "  --no_cuda True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da51b33e",
   "metadata": {},
   "source": [
    "### Step 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e757cb1",
   "metadata": {},
   "source": [
    "Execution Start time: 05:50:01, End time: 05:50:02 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d089564",
   "metadata": {},
   "source": [
    "**Thought**: The script has successfully run, and we have obtained the evaluation results. The accuracy and loss on the evaluation set are as follows:\n",
    "\n",
    "- Accuracy: 0.4729241877256318\n",
    "- Loss: 0.7289056181907654\n",
    "\n",
    "Let's submit these results in the required JSON format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad7f981",
   "metadata": {},
   "source": [
    "**Action (submit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0651f924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{accuracy: 0.4729, loss: 0.7289}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit {\"accuracy\": 0.4729, \"loss\": 0.7289}"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
