{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac92be02",
   "metadata": {},
   "source": [
    "### Query\n",
    "Pre-train the TransPolymer model on \"data/pretrain.csv\" using the data in the repository, the config.yaml configuration and the Pretrain.py script provided. Report the last training loss as a json structured as follows: {\"train_loss\": 0.0} (replace 0.0 with the actual value).\n",
    "\n",
    "Additional instructions:\n",
    "1. Load only the first 10 rows of in the training dataset\n",
    "2. Train only one epoch.\n",
    "3. Do not use distributed training, train on a single CPU with xpu_backend set to `gloo`.\n",
    "4. Set the batch size to 1.\n",
    "5. Set pytorch random seed to 0.\n",
    "\n",
    "Git repository: https://github.com/ChangwenXu98/TransPolymer\n",
    "\n",
    "I have already executed some of the required steps. Now, you should make any necessary configuration changes to achieve the user request. Your goal is to successfully make the required changes and successfully run\n",
    "```\n",
    "# Without the --nproc_per_node argument\n",
    "!CUDA_VISIBLE_DEVICES=\",\" python Pretrain.py\n",
    "```\n",
    "then submit the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3530fe2c",
   "metadata": {},
   "source": [
    "### Prompt\n",
    "<details>\n",
    "[{'role': 'system', 'content': 'Interact in a jupyter notebook to solve the request.\\nOutput a thought and an action.\\nThought can reason about the current situation, and action allows you to interact with the Jupyter notebook. There are three types of actions:\\n(1) execute, which executes a notebook cell with the command under the \"content\" field. This could be a bash command (beginning with !), a python command, or magic command (beginning with %).\\n(2) edit, which allows you to replace existing lines in a file with new lines. To replace lines, you will need to provide the lines as they appear before the replacement, and the lines to replace them with.\\nIn the \"content\" field, format should be ```{filename}\\n<<<BEFORE_EDIT>>>\\nlines before edit\\nlines before edit\\n<<<AFTER_EDIT>>>\\nlines after edit\\nlines after edit```\\n  - Include the entire lines WITH indentation. After editing, make sure indentation is correct.\\n(3) submit, which submits the final answer (or null if there\\'s no answer to submit), formatted as JSON (make sure to use double quotes for keys and values).\\nAlways output exactly one \"thought\" and one \"action\", and encapsulate the content with <content>...</content> tags.\\nI will provide the cell output (observation) for you.\\n\\nTips:\\n- When a github repository is given, you should start by cloning it. You can read the readme file and browse the repo for installation and usage instructions.\\n- Avoid using conda, instead use pip for package management.\\n- Each execution has a 5-minute timeout. After this period, you can decide whether to interrupt the kernel or continue waiting.\\n  You should interrupt if: (1) you are prompted with an interactive message (e.g., \"Do you want to continue? [Y/n]\"). In that case, you can bypass it by executing echo \"y\" | command, or (2) the execution is taking longer than expected because the wrong task is being executed, such as a different model or incorrect number of epochs.\\n  You should continue to wait if the current execution is correct and proceeding as expected.\\n\\nHere are some examples.\\nGit repository: EleutherAI/lm-evaluation-harness\\nQuery: Evaluate EleutherAI/gpt-j-6B on hellaswag. What\\'s the score?\\nThought: First, I need to clone the GitHub repository \\'EleutherAI/lm-evaluation-harness\\'.\\nAction: execute\\n<content>\\n!git clone https://github.com/EleutherAI/lm-evaluation-harness.git\\n</content>\\nObservation: ```\\nCloning into \\'lm-evaluation-harness\\'...\\n```\\nThought: The repository has been cloned. Next, I should check the README file for installation and usage instructions to proceed with evaluating EleutherAI/gpt-j-6B on hellaswag.\\nAction: execute\\n<content>\\n!cat lm-evaluation-harness/README.md\\n</content>\\nObservation: ```# Language Model Evaluation Harness ... truncated ...\\n```\\nThought: The README provides detailed instructions for installing the `lm-eval` package and evaluating models like GPT-J-6B on tasks like hellaswag. I need to install the package and its dependencies first.\\nAction: execute\\n<content>\\n!pip install -e lm-evaluation-harness/.\\n</content>\\nObservation: ```\\nObtaining file:///home/nora/lm-evaluation-harness\\n  Installing build dependencies ... truncated ...\\n```\\nThought: The `lm-eval` package and its dependencies have been installed successfully. Now, I can proceed with evaluating the EleutherAI/gpt-j-6B model on the hellaswag task as per the instructions in the README.\\nAction: execute\\n<content>\\n!lm_eval --model hf --model_args pretrained=EleutherAI/gpt-j-6B --tasks hellaswag --device cuda:0 --batch_size 8\\n</content>\\nObservation: ```\\n{\\n  \"hellaswag\": {\\n    \"acc\": 0.24603658536585363,\\n    \"acc_norm\": 0.34436059706446426,\\n  },  ... truncated ...\\n```\\nThought: The `lm-eval` was executed successfully, providing the metrics I needed. The `acc` metric is 0.2460. The `acc_norm` metric is 0.3444\\nAction: submit\\n<content>\\n{\"acc\": 0.2460, \"acc_norm\": 0.3444}\\n</content>\\n\\nQuery: use pandas to read the file \\'data.csv\\' and display the first 2 rows.\\nThought: First, I need to read the file \\'data.csv\\' using the pandas library.\\nAction: execute\\n<content>\\nimport pandas as pd\\npd.read_csv(\\'data.csv\\').head()\\n</content>\\nObservation: ```\\n    name\\n  0  Alice\\n   1    Bob\\n  2  Charlie\\n```\\nThought: The file \\'data.csv\\' has been read successfully and the first 2 rows have been displayed.\\nAction: submit\\n<content>\\n[\"Alice\", \"Bob\"]\\n</content>\\n\\nQuery: Change the model name in the \\'config.json\\' file from \\'roberta-base\\' to \\'roberta-large\\'.\\nThought: First, I need to read the \\'config.json\\' file to find the location where I can replace the model name.\\nAction: execute\\n<content>\\n!cat config.json\\n</content>\\nObservation: ```\\n{\\n    \"model\": \"roberta-base\",\\n    \"batch_size\": 8,\\n    \"epochs\": 10\\n}\\n```\\nThought: The file has been read successfully. Now, I can replace the model name in the \\'config.json\\' file.\\nAction: edit\\n<content>\\nconfig.json\\n<<<BEFORE_EDIT>>>\\n{\\n    \"model\": \"roberta-base\",\\n    \"batch_size\": 8,\\n<<<AFTER_EDIT>>>\\n{\\n    \"model\": \"roberta-large\",\\n    \"batch_size\": 8,\\n</content>\\nObservation: \"editing file config.json\"\\nThought: The model name has been replaced in the \\'config.json\\' file.\\nAction: submit\\n<content>\\nnull\\n</content>\\n'}, {'role': 'user', 'content': 'Query: Pre-train the TransPolymer model on \"data/pretrain.csv\" using the data in the repository, the config.yaml configuration and the Pretrain.py script provided. Report the last training loss as a json structured as follows: {\"train_loss\": 0.0} (replace 0.0 with the actual value).\\n\\nAdditional instructions:\\n1. Load only the first 10 rows of in the training dataset\\n2. Train only one epoch.\\n3. Do not use distributed training, train on a single CPU with xpu_backend set to `gloo`.\\n4. Set the batch size to 1.\\n5. Set pytorch random seed to 0.\\n\\nGit repository: https://github.com/ChangwenXu98/TransPolymer\\n\\nI have already executed some of the required steps. Now, you should make any necessary configuration changes to achieve the user request. Your goal is to successfully make the required changes and successfully run\\n```\\n# Without the --nproc_per_node argument\\n!CUDA_VISIBLE_DEVICES=\",\" python Pretrain.py\\n```\\nthen submit the answer.'}]\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e043d7f",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d6fb67",
   "metadata": {},
   "source": [
    "Execution Start time: 00:14:42, End time: 00:14:42 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc99bb9",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eab72c3",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dafd2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Repository Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec40a617",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d899e21c",
   "metadata": {},
   "source": [
    "Execution Start time: 00:14:42, End time: 00:14:43 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d7fd94",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352a9985",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38efe92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cloning into 'TransPolymer'...\nremote: Enumerating objects: 177, done.\u001b[K\nremote: Counting objects: 100% (26/26)\u001b[K\nremote: Counting objects: 100% (26/26), done.\u001b[K\nremote: Compressing objects: 100% (14/14)\u001b[K\nremote: Compressing objects: 100% (14/14), done.\u001b[K\nremote: Total 177 (delta 16), reused 12 (delta 12), pack-reused 151 (from 1)\u001b[K\nReceiving objects: 100% (177/177)\nReceiving objects: 100% (177/177), 1.68 MiB | 5.53 MiB/s, done.\nResolving deltas: 100% (70/70)\nResolving deltas: 100% (70/70), done.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!git clone https://github.com/ChangwenXu98/TransPolymer.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a086ce",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6676b4",
   "metadata": {},
   "source": [
    "Execution Start time: 00:14:43, End time: 00:14:43 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a9cdad",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f701548a",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5031a642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/TransPolymer\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd TransPolymer/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3e3b24",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8dc9af",
   "metadata": {},
   "source": [
    "Execution Start time: 00:14:43, End time: 00:14:44 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682e8068",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d1eb9e",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14eb44a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attention_vis.py              Pretrain.py  config_attention.yaml  dataset.py\nDownstream.py                 README.md    config_finetune.yaml   figs/\nLICENSE.md                    ckpt/        config_tSNE.yaml       tSNE.py\nPolymerSmilesTokenization.py  config.yaml  data/\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab2d008",
   "metadata": {},
   "source": [
    "### Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a94e00f",
   "metadata": {},
   "source": [
    "Execution Start time: 00:14:44, End time: 00:14:44 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a14b78",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e52e0ca",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9229e7e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5K\tckpt/pretrain.pt\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pretrained model\n",
    "!du -sh ckpt/pretrain.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07abd25",
   "metadata": {},
   "source": [
    "### Step 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae69d4ae",
   "metadata": {},
   "source": [
    "Execution Start time: 00:14:44, End time: 00:14:44 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60326181",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35375d75",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083c3c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Package Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dded52",
   "metadata": {},
   "source": [
    "### Step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fd2966",
   "metadata": {},
   "source": [
    "Execution Start time: 00:14:44, End time: 00:14:54 (duration: 10 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c679d4db",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da3bf7d",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907b29e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting rdkit\n  Downloading rdkit-2024.3.5-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.9 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from rdkit) (1.25.2)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/site-packages (from rdkit) (10.3.0)\nDownloading rdkit-2024.3.5-cp310-cp310-manylinux_2_28_x86_64.whl (33.1 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/33.1 MB ? eta -:--:--\n\n\n\n\n\n\n\n\u001b[?25hInstalling collected packages: rdkit\nSuccessfully installed rdkit-2024.3.5\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install rdkit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ec9312",
   "metadata": {},
   "source": [
    "### Step 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ad6ae9",
   "metadata": {},
   "source": [
    "Execution Start time: 00:14:54, End time: 00:15:19 (duration: 24 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcae9fb",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d01f49a",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb424a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting transformers==4.20.1\n  Downloading transformers-4.20.1-py3-none-any.whl.metadata (77 kB)\n\u001b[?25l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/77.3 kB ? eta -:--:--\n\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from transformers==4.20.1) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/site-packages (from transformers==4.20.1) (0.23.3)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from transformers==4.20.1) (1.25.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from transformers==4.20.1) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers==4.20.1) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers==4.20.1) (2023.12.25)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers==4.20.1) (2.31.0)\nCollecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers==4.20.1)\n  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.5 kB)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/site-packages (from transformers==4.20.1) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.20.1) (2023.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.20.1) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.20.1) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.20.1) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.20.1) (2.0.7)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.20.1) (2024.6.2)\nDownloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/4.4 MB ? eta -:--:--\n\n\u001b[?25hDownloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/6.6 MB ? eta -:--:--\n\n\n\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.1\n    Uninstalling tokenizers-0.19.1:\n      Successfully uninstalled tokenizers-0.19.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.40.2\n    Uninstalling transformers-4.40.2:\n      Successfully uninstalled transformers-4.40.2\nSuccessfully installed tokenizers-0.12.1 transformers-4.20.1\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: newer version breaks the code\n",
    "!pip install transformers==4.20.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371a72f1",
   "metadata": {},
   "source": [
    "### Step 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8580a25",
   "metadata": {},
   "source": [
    "Execution Start time: 00:15:19, End time: 00:15:19 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b117bdb",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a185bed",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64fff7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For distriubted NN training\n",
    "# !pip install fairscale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c389cea9",
   "metadata": {},
   "source": [
    "### Step 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7674cdfe",
   "metadata": {},
   "source": [
    "Execution Start time: 00:15:19, End time: 00:15:23 (duration: 4 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98be56a7",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887afba0",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012fbfa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch                     2.2.1+cpu\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip list | grep torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e505d8bb",
   "metadata": {},
   "source": [
    "### Step 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c21afa5",
   "metadata": {},
   "source": [
    "Execution Start time: 00:15:23, End time: 00:15:26 (duration: 3 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0df92ed",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f62f69c",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c29b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Looking in links: https://developer.intel.com/ipex-whl-stable-cpu\nERROR: Could not find a version that satisfies the requirement oneccl_bind_pt==2.3.0 (from versions: none)\nERROR: No matching distribution found for oneccl_bind_pt==2.3.0\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CCL for multi GPU training\n",
    "!pip install oneccl_bind_pt==2.3.0 -f https://developer.intel.com/ipex-whl-stable-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f39d53",
   "metadata": {},
   "source": [
    "### Step 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bdc4a8",
   "metadata": {},
   "source": [
    "Execution Start time: 00:15:26, End time: 00:15:26 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da504620",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b38b3f0",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a881c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbcd4a0",
   "metadata": {},
   "source": [
    "### Step 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b973cc",
   "metadata": {},
   "source": [
    "Execution Start time: 00:15:26, End time: 00:15:27 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e76f12",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d85541",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6094600d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " EPS.csv   Nc.csv        PE_II_aug.csv         pretrain_500k.csv   vocab/\n Eea.csv   OPV.csv       Xc.csv                pretrain_50k.csv\n Egb.csv   OPV_aug.csv  'original datasets'/   pretrain_5k.csv\n Egc.csv   PE_I.csv      pretrain.csv          test_PE_I.csv\n Ei.csv    PE_II.csv     pretrain_1M.csv       train_PE_I.csv\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ls data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d700cc",
   "metadata": {},
   "source": [
    "### Step 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c30bb8",
   "metadata": {},
   "source": [
    "Execution Start time: 00:15:27, End time: 00:15:28 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3617202c",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d980e2e",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c33891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3 data/pretrain.csv\nversion https://git-lfs.github.com/spec/v1\noid sha256:0ddfe57eee7301f7f166edc907f8bf58273a8d089ead34207c3c7e9d044bfaf1\nsize 245980630\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!wc -l data/pretrain.csv\n",
    "!head data/pretrain.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae1b889",
   "metadata": {},
   "source": [
    "### Step 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4525afcf",
   "metadata": {},
   "source": [
    "Execution Start time: 00:15:28, End time: 00:15:29 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd535695",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf62da8",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2345514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep only the first 10 lines\n",
    "!sed -i '51i\\    valid_data = valid_data[:10]' Pretrain.py\n",
    "!sed -i '51i\\    train_data = train_data[:10]' Pretrain.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e96445",
   "metadata": {},
   "source": [
    "### Step 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d56f122",
   "metadata": {},
   "source": [
    "Execution Start time: 00:15:29, End time: 00:15:30 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb8763d",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbf5271",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5473cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    \"\"\"Construct MLM model\"\"\"\n\n    model = RobertaForMaskedLM(config=config).to(device)\n\n\n\n    \"\"\"Load Data\"\"\"\n\n    train_data, valid_data = split(pretrain_config['file_path'])\n\n    train_data = train_data[:10]\n    valid_data = valid_data[:10]\n    data_train = LoadPretrainData(tokenizer=tokenizer, dataset=train_data, blocksize=pretrain_config['blocksize'])\n\n    data_valid = LoadPretrainData(tokenizer=tokenizer, dataset=valid_data, blocksize=pretrain_config['blocksize'])\n\n\n\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!head -55 Pretrain.py | tail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0d850f",
   "metadata": {},
   "source": [
    "### Step 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0180b32",
   "metadata": {},
   "source": [
    "Execution Start time: 00:15:30, End time: 00:15:31 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bc5c0d",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab8161e",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096f5434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        learning_rate=pretrain_config['lr_rate'],\n\n        lr_scheduler_type=pretrain_config['scheduler_type'],\n\n        weight_decay=pretrain_config['weight_decay'],\n\n        warmup_ratio=pretrain_config['warmup_ratio'],\n\n        report_to=pretrain_config['report_to'],\n\n        xpu_backend=\"gloo\", no_cuda=True,\n        dataloader_num_workers=pretrain_config['dataloader_num_workers'],\n\n        sharded_ddp=pretrain_config['sharded_ddp'],\n\n    )\n\n\n\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enable fsdp (fully sharded data parallel)\n",
    "# !sed -i '78i\\        fsdp=\"full_shard offload\"' Pretrain.py\n",
    "!sed -i '78i\\        xpu_backend=\"gloo\", no_cuda=True,' Pretrain.py\n",
    "# !sed -i '78i\\        no_cuda=True,' Pretrain.py\n",
    "# !sed -i '78d' Pretrain.py\n",
    "!head -82 Pretrain.py | tail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996ed476",
   "metadata": {},
   "source": [
    "### Step 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d4eaee",
   "metadata": {},
   "source": [
    "Execution Start time: 00:15:31, End time: 00:15:31 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a025b2a",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd981703",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7199f7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n\"\"\"Import PolymerSmilesTokenizer from PolymerSmilesTokenization.py\"\"\"\nfrom PolymerSmilesTokenization import PolymerSmilesTokenizer\n\n\"\"\"Import LoadPretrainData\"\"\"\nfrom dataset import LoadPretrainData\n\n\"\"\"Device\"\"\"\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntorch.cuda.is_available() #checking if CUDA + Colab GPU works\n\n# set seeds\ntorch.manual_seed(0)\nimport random\nrandom.seed(0)\nimport numpy as np\nnp.random.seed(0)\n\n\"\"\"train-validation split\"\"\"\ndef split(file_path):\n    dataset = pd.read_csv(file_path, header=None).values\n    train_data, valid_data = train_test_split(dataset, test_size=0.2, random_state=1)\n    return train_data, valid_data\n\ndef main(pretrain_config):\n    \"\"\"Use Roberta configuration\"\"\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/TransPolymer/Pretrain.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.is_available() #checking if CUDA + Colab GPU works\n",
    "\n",
    "\"\"\"train-validation split\"\"\"\n",
    "def split(file_path):\n",
    "<<<AFTER_EDIT>>>\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.is_available() #checking if CUDA + Colab GPU works\n",
    "\n",
    "# set seeds\n",
    "torch.manual_seed(0)\n",
    "import random\n",
    "random.seed(0)\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "\"\"\"train-validation split\"\"\"\n",
    "def split(file_path):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f006c43",
   "metadata": {},
   "source": [
    "### Step 19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5a23c4",
   "metadata": {},
   "source": [
    "Execution Start time: 00:15:31, End time: 00:15:31 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb7e3de",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba7475d",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5829f60c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Training \n",
    "# Tried to change the code inside Pretrain.py to use 'gloo' backend. Couldn't make it work! Looks like transformers Trainer only supports CCL & MPI libraries for mult-cpu training. Reference: https://huggingface.co/docs/transformers/en/perf_train_cpu_many#usage-in-trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79e0000",
   "metadata": {},
   "source": [
    "### Step 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0811d3b6",
   "metadata": {},
   "source": [
    "Execution Start time: 00:15:31, End time: 00:15:32 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e43e96",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70ef913",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d403d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Training with 1 CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1571f33d",
   "metadata": {},
   "source": [
    "### Step 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010ec3ac",
   "metadata": {},
   "source": [
    "Execution Start time: 00:15:33, End time: 00:15:56 (duration: 23 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceea57f6",
   "metadata": {},
   "source": [
    "**Thought**: Now that the necessary configurations have been made, I can proceed to run the pre-training script using the specified command. This will allow us to obtain the last training loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839dd88c",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03a8d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\nDownloading:   0%|                                   | 0.00/878k [00:00<?, ?B/s]\n\n\nDownloading:   0%|                                   | 0.00/446k [00:00<?, ?B/s]\n\n\nDownloading:   0%|                                   | 0.00/25.0 [00:00<?, ?B/s]\nDownloading: 100%|████████████████████████████| 25.0/25.0 [00:00<00:00, 228kB/s]\n\nDownloading:   0%|                                    | 0.00/481 [00:00<?, ?B/s]\n\nThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \nThe class this function is called from is 'PolymerSmilesTokenizer'.\nTraceback (most recent call last):\n  File \"/content/TransPolymer/Pretrain.py\", line 119, in <module>\n    main(pretrain_config)\n  File \"/content/TransPolymer/Pretrain.py\", line 69, in main\n    training_args = TrainingArguments(\n  File \"<string>\", line 101, in __init__\n  File \"/usr/local/lib/python3.10/site-packages/transformers/training_args.py\", line 1070, in __post_init__\n    raise ValueError(\nValueError: FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation (`--fp16_full_eval`) can only be used on CUDA devices.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=\",\" python Pretrain.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59ec714",
   "metadata": {},
   "source": [
    "### Step 22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c6940b",
   "metadata": {},
   "source": [
    "Execution Start time: 00:15:57, End time: 00:15:58 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4347e231",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the script is attempting to use FP16 mixed precision training, which is only supported on CUDA devices. Since we are running on a CPU, we need to disable FP16 training in the configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a81c21",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631ea376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Could not find the following lines to replace in the file content:\n```\n  fp16: true\n```"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.yaml\n",
    "<<<BEFORE_EDIT>>>\n",
    "  fp16: true\n",
    "<<<AFTER_EDIT>>>\n",
    "  fp16: false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f15597",
   "metadata": {},
   "source": [
    "### Step 23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98452e0f",
   "metadata": {},
   "source": [
    "Execution Start time: 00:15:59, End time: 00:16:00 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c749df97",
   "metadata": {},
   "source": [
    "**Thought**: It seems that the `fp16` setting is not present in the `config.yaml` file. I should check the `Pretrain.py` script to see if `fp16` is being set there and disable it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db135896",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469ef495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77:        fp16=pretrain_config['fp16'],\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!grep -n \"fp16\" Pretrain.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d635356",
   "metadata": {},
   "source": [
    "### Step 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac60e88",
   "metadata": {},
   "source": [
    "Execution Start time: 00:16:01, End time: 00:16:02 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce012f4",
   "metadata": {},
   "source": [
    "**Thought**: The `fp16` setting is being set in the `Pretrain.py` script using the `pretrain_config['fp16']` configuration. I need to edit the `config.yaml` file to add or modify the `fp16` setting to `false` to prevent the script from using FP16 mixed precision training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749938c1",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1f2cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Could not find the following lines to replace in the file content:\n```\n# Add or modify the fp16 setting to false\n```"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.yaml\n",
    "<<<BEFORE_EDIT>>>\n",
    "# Add or modify the fp16 setting to false\n",
    "<<<AFTER_EDIT>>>\n",
    "fp16: false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be43a014",
   "metadata": {},
   "source": [
    "### Step 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53349ed",
   "metadata": {},
   "source": [
    "Execution Start time: 00:16:03, End time: 00:16:04 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfa0438",
   "metadata": {},
   "source": [
    "**Thought**: It seems I made an error in the edit action. Instead of trying to replace a non-existent line, I should add the `fp16: false` setting to the `config.yaml` file. I will append this setting to the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157fb328",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc5d540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!echo \"fp16: false\" >> config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca1cf04",
   "metadata": {},
   "source": [
    "### Step 26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af670154",
   "metadata": {},
   "source": [
    "Execution Start time: 00:16:05, End time: 00:21:05 (duration: 300 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660c3b8e",
   "metadata": {},
   "source": [
    "**Thought**: The `fp16` setting has been successfully added to the `config.yaml` file. Now, I can rerun the pre-training script to obtain the last training loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4950ad5b",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2681e2ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \nThe class this function is called from is 'PolymerSmilesTokenizer'.\n/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 18 worker processes in total. Our suggested max number of worker in current system is 5, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/usr/local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n***** Running training *****\n  Num examples = 2\n  Num Epochs = 30\n  Instantaneous batch size per device = 100\n  Total train batch size (w. parallel, distributed & accumulation) = 100\n  Gradient Accumulation steps = 1\n  Total optimization steps = 30\n\n  0%|                                                    | 0/30 [00:00<?, ?it/s]/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 18 worker processes in total. Our suggested max number of worker in current system is 5, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n\n  3%|█▍                                          | 1/30 [00:06<03:05,  6.40s/it]\n                                                                                \n{'loss': 10.4246, 'learning_rate': 2.5e-05, 'epoch': 1.0}\n\n  3%|█▍                                          | 1/30 [00:06<03:05,  6.40s/it]***** Running Evaluation *****\n  Num examples = 1\n  Batch size = 100\n\n\n  0%|                                                     | 0/1 [00:00<?, ?it/s]\u001b[A\n                                                                                \n\n\n                                                                                \n\u001b[A{'eval_loss': 10.684150695800781, 'eval_runtime': 1.717, 'eval_samples_per_second': 0.582, 'eval_steps_per_second': 0.582, 'epoch': 1.0}\n\n  3%|█▍                                          | 1/30 [00:08<03:05,  6.40s/it]\n\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 15.68it/s]\u001b[A\n\n                                                                                \u001b[ASaving model checkpoint to ckpt/pretrain.pt/checkpoint-1\nConfiguration saved in ckpt/pretrain.pt/checkpoint-1/config.json\nModel weights saved in ckpt/pretrain.pt/checkpoint-1/pytorch_model.bin\n/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 18 worker processes in total. Our suggested max number of worker in current system is 5, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n\n  7%|██▉                                         | 2/30 [00:13<03:07,  6.68s/it]\n                                                                                \n{'loss': 11.1067, 'learning_rate': 5e-05, 'epoch': 2.0}\n\n  7%|██▉                                         | 2/30 [00:13<03:07,  6.68s/it]***** Running Evaluation *****\n  Num examples = 1\n  Batch size = 100\n\n\n  0%|                                                     | 0/1 [00:00<?, ?it/s]\u001b[A\n                                                                                \n\n\n                                                                                \n\u001b[A{'eval_loss': 10.60680866241455, 'eval_runtime': 2.538, 'eval_samples_per_second': 0.394, 'eval_steps_per_second': 0.394, 'epoch': 2.0}\n\n  7%|██▉                                         | 2/30 [00:15<03:07,  6.68s/it]\n\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 11.10it/s]\u001b[A\n\n                                                                                \u001b[ASaving model checkpoint to ckpt/pretrain.pt/checkpoint-2\nConfiguration saved in ckpt/pretrain.pt/checkpoint-2/config.json\nModel weights saved in ckpt/pretrain.pt/checkpoint-2/pytorch_model.bin\n/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 18 worker processes in total. Our suggested max number of worker in current system is 5, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n\n 10%|████▍                                       | 3/30 [00:21<03:22,  7.50s/it]\n                                                                                \n{'loss': 10.8201, 'learning_rate': 4.8214285714285716e-05, 'epoch': 3.0}\n\n 10%|████▍                                       | 3/30 [00:21<03:22,  7.50s/it]***** Running Evaluation *****\n  Num examples = 1\n  Batch size = 100\n\n\n  0%|                                                     | 0/1 [00:00<?, ?it/s]\u001b[A\n                                                                                \n\n\n                                                                                \n\u001b[A{'eval_loss': 10.417933464050293, 'eval_runtime': 2.5413, 'eval_samples_per_second': 0.393, 'eval_steps_per_second': 0.393, 'epoch': 3.0}\n\n 10%|████▍                                       | 3/30 [00:24<03:22,  7.50s/it]\n\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  4.74it/s]\u001b[A\n\n                                                                                \u001b[ASaving model checkpoint to ckpt/pretrain.pt/checkpoint-3\nConfiguration saved in ckpt/pretrain.pt/checkpoint-3/config.json\nModel weights saved in ckpt/pretrain.pt/checkpoint-3/pytorch_model.bin\n/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 18 worker processes in total. Our suggested max number of worker in current system is 5, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n\n 13%|█████▊                                      | 4/30 [00:32<03:47,  8.76s/it]\n                                                                                \n{'loss': 10.991, 'learning_rate': 4.642857142857143e-05, 'epoch': 4.0}\n\n 13%|█████▊                                      | 4/30 [00:32<03:47,  8.76s/it]***** Running Evaluation *****\n  Num examples = 1\n  Batch size = 100\n\n\n  0%|                                                     | 0/1 [00:00<?, ?it/s]\u001b[A\n                                                                                \n\n\n                                                                                \n\u001b[A{'eval_loss': 9.722207069396973, 'eval_runtime': 2.954, 'eval_samples_per_second': 0.339, 'eval_steps_per_second': 0.339, 'epoch': 4.0}\n\n 13%|█████▊                                      | 4/30 [00:35<03:47,  8.76s/it]\n\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  4.34it/s]\u001b[A\n\n                                                                                \u001b[ASaving model checkpoint to ckpt/pretrain.pt/checkpoint-4\nConfiguration saved in ckpt/pretrain.pt/checkpoint-4/config.json\nModel weights saved in ckpt/pretrain.pt/checkpoint-4/pytorch_model.bin\nDeleting older checkpoint [ckpt/pretrain.pt/checkpoint-1] due to args.save_total_limit\n/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 18 worker processes in total. Our suggested max number of worker in current system is 5, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n\n 17%|███████▎                                    | 5/30 [00:46<04:25, 10.61s/it]\n                                                                                \n{'loss': 9.2151, 'learning_rate': 4.464285714285715e-05, 'epoch': 5.0}\n\n 17%|███████▎                                    | 5/30 [00:46<04:25, 10.61s/it]***** Running Evaluation *****\n  Num examples = 1\n  Batch size = 100\n\n\n  0%|                                                     | 0/1 [00:00<?, ?it/s]\u001b[A\n                                                                                \n\n\n                                                                                \n\u001b[A{'eval_loss': 10.405168533325195, 'eval_runtime': 4.3352, 'eval_samples_per_second': 0.231, 'eval_steps_per_second': 0.231, 'epoch': 5.0}\n\n 17%|███████▎                                    | 5/30 [00:50<04:25, 10.61s/it]\n\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  3.33it/s]\u001b[A\n\n                                                                                \u001b[ASaving model checkpoint to ckpt/pretrain.pt/checkpoint-5\nConfiguration saved in ckpt/pretrain.pt/checkpoint-5/config.json\nModel weights saved in ckpt/pretrain.pt/checkpoint-5/pytorch_model.bin\nDeleting older checkpoint [ckpt/pretrain.pt/checkpoint-2] due to args.save_total_limit\n/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 18 worker processes in total. Our suggested max number of worker in current system is 5, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n\n 20%|████████▊                                   | 6/30 [01:02<04:57, 12.38s/it]\n                                                                                \n{'loss': 10.0937, 'learning_rate': 4.2857142857142856e-05, 'epoch': 6.0}\n\n 20%|████████▊                                   | 6/30 [01:02<04:57, 12.38s/it]***** Running Evaluation *****\n  Num examples = 1\n  Batch size = 100\n\n\n  0%|                                                     | 0/1 [00:00<?, ?it/s]\u001b[A\n                                                                                \n\n\n                                                                                \n\u001b[A{'eval_loss': 8.782649040222168, 'eval_runtime': 5.3473, 'eval_samples_per_second': 0.187, 'eval_steps_per_second': 0.187, 'epoch': 6.0}\n\n 20%|████████▊                                   | 6/30 [01:07<04:57, 12.38s/it]\n\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  2.60it/s]\u001b[A\n\n                                                                                \u001b[ASaving model checkpoint to ckpt/pretrain.pt/checkpoint-6\nConfiguration saved in ckpt/pretrain.pt/checkpoint-6/config.json\nModel weights saved in ckpt/pretrain.pt/checkpoint-6/pytorch_model.bin\nDeleting older checkpoint [ckpt/pretrain.pt/checkpoint-3] due to args.save_total_limit\n/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 18 worker processes in total. Our suggested max number of worker in current system is 5, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n\n 23%|██████████▎                                 | 7/30 [01:15<04:52, 12.71s/it]\n                                                                                \n{'loss': 8.7883, 'learning_rate': 4.107142857142857e-05, 'epoch': 7.0}\n\n 23%|██████████▎                                 | 7/30 [01:15<04:52, 12.71s/it]***** Running Evaluation *****\n  Num examples = 1\n  Batch size = 100\n\n\n  0%|                                                     | 0/1 [00:00<?, ?it/s]\u001b[A\n                                                                                \n\n\n                                                                                \n\u001b[A{'eval_loss': 9.518895149230957, 'eval_runtime': 3.0273, 'eval_samples_per_second': 0.33, 'eval_steps_per_second': 0.33, 'epoch': 7.0}\n\n 23%|██████████▎                                 | 7/30 [01:18<04:52, 12.71s/it]\n\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  4.04it/s]\u001b[A\n\n                                                                                \u001b[ASaving model checkpoint to ckpt/pretrain.pt/checkpoint-7\nConfiguration saved in ckpt/pretrain.pt/checkpoint-7/config.json\nModel weights saved in ckpt/pretrain.pt/checkpoint-7/pytorch_model.bin\nDeleting older checkpoint [ckpt/pretrain.pt/checkpoint-4] due to args.save_total_limit\n/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 18 worker processes in total. Our suggested max number of worker in current system is 5, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n\n 27%|███████████▋                                | 8/30 [01:31<05:01, 13.71s/it]\n                                                                                \n{'loss': 8.5657, 'learning_rate': 3.928571428571429e-05, 'epoch': 8.0}\n\n 27%|███████████▋                                | 8/30 [01:31<05:01, 13.71s/it]***** Running Evaluation *****\n  Num examples = 1\n  Batch size = 100\n\n\n  0%|                                                     | 0/1 [00:00<?, ?it/s]\u001b[A\n                                                                                \n\n\n                                                                                \n\u001b[A{'eval_loss': nan, 'eval_runtime': 5.7996, 'eval_samples_per_second': 0.172, 'eval_steps_per_second': 0.172, 'epoch': 8.0}\n\n 27%|███████████▋                                | 8/30 [01:37<05:01, 13.71s/it]\n\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  1.90it/s]\u001b[A\n\n                                                                                \u001b[ASaving model checkpoint to ckpt/pretrain.pt/checkpoint-8\nConfiguration saved in ckpt/pretrain.pt/checkpoint-8/config.json\nModel weights saved in ckpt/pretrain.pt/checkpoint-8/pytorch_model.bin\nDeleting older checkpoint [ckpt/pretrain.pt/checkpoint-5] due to args.save_total_limit\n/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 18 worker processes in total. Our suggested max number of worker in current system is 5, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n\n 30%|█████████████▏                              | 9/30 [02:02<06:43, 19.21s/it]\n                                                                                \n{'loss': 9.9222, 'learning_rate': 3.7500000000000003e-05, 'epoch': 9.0}\n\n 30%|█████████████▏                              | 9/30 [02:04<06:43, 19.21s/it]***** Running Evaluation *****\n  Num examples = 1\n  Batch size = 100\n\n\n  0%|                                                     | 0/1 [00:00<?, ?it/s]\u001b[A\n                                                                                \n\n\n                                                                                \n\u001b[A{'eval_loss': 10.166376113891602, 'eval_runtime': 17.3049, 'eval_samples_per_second': 0.058, 'eval_steps_per_second': 0.058, 'epoch': 9.0}\n\n 30%|█████████████▏                              | 9/30 [02:21<06:43, 19.21s/it]\n\n100%|█████████████████████████████████████████████| 1/1 [00:01<00:00,  1.62s/it]\u001b[A\n\n                                                                                \u001b[ASaving model checkpoint to ckpt/pretrain.pt/checkpoint-9\nConfiguration saved in ckpt/pretrain.pt/checkpoint-9/config.json\nModel weights saved in ckpt/pretrain.pt/checkpoint-9/pytorch_model.bin\nDeleting older checkpoint [ckpt/pretrain.pt/checkpoint-6] due to args.save_total_limit\n/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 18 worker processes in total. Our suggested max number of worker in current system is 5, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n\n 33%|██████████████▎                            | 10/30 [02:53<09:40, 29.02s/it]\n                                                                                \n{'loss': 8.8855, 'learning_rate': 3.571428571428572e-05, 'epoch': 10.0}\n\n 33%|██████████████▎                            | 10/30 [02:54<09:40, 29.02s/it]***** Running Evaluation *****\n  Num examples = 1\n  Batch size = 100\n\n\n  0%|                                                     | 0/1 [00:00<?, ?it/s]\u001b[A\n                                                                                \n\n\n                                                                                \n\u001b[A{'eval_loss': 10.277322769165039, 'eval_runtime': 14.9433, 'eval_samples_per_second': 0.067, 'eval_steps_per_second': 0.067, 'epoch': 10.0}\n\n 33%|██████████████▎                            | 10/30 [03:09<09:40, 29.02s/it]\n\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  1.16it/s]\u001b[A\n\n                                                                                \u001b[ASaving model checkpoint to ckpt/pretrain.pt/checkpoint-10\nConfiguration saved in ckpt/pretrain.pt/checkpoint-10/config.json\nModel weights saved in ckpt/pretrain.pt/checkpoint-10/pytorch_model.bin\nDeleting older checkpoint [ckpt/pretrain.pt/checkpoint-7] due to args.save_total_limit\n/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 18 worker processes in total. Our suggested max number of worker in current system is 5, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n\n 37%|███████████████▊                           | 11/30 [03:39<10:49, 34.18s/it]\n                                                                                \n{'loss': 9.3944, 'learning_rate': 3.392857142857143e-05, 'epoch': 11.0}\n\n 37%|███████████████▊                           | 11/30 [03:40<10:49, 34.18s/it]***** Running Evaluation *****\n  Num examples = 1\n  Batch size = 100\n\n\n  0%|                                                     | 0/1 [00:00<?, ?it/s]\u001b[A\n                                                                                \n\n\n                                                                                \n\u001b[A{'eval_loss': 9.311185836791992, 'eval_runtime': 15.6236, 'eval_samples_per_second': 0.064, 'eval_steps_per_second': 0.064, 'epoch': 11.0}\n\n 37%|███████████████▊                           | 11/30 [03:56<10:49, 34.18s/it]\n\n100%|█████████████████████████████████████████████| 1/1 [00:01<00:00,  1.88s/it]\u001b[A\n\n                                                                                \u001b[ASaving model checkpoint to ckpt/pretrain.pt/checkpoint-11\nConfiguration saved in ckpt/pretrain.pt/checkpoint-11/config.json\nModel weights saved in ckpt/pretrain.pt/checkpoint-11/pytorch_model.bin\nDeleting older checkpoint [ckpt/pretrain.pt/checkpoint-8] due to args.save_total_limit\n/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 18 worker processes in total. Our suggested max number of worker in current system is 5, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n\n 40%|█████████████████▏                         | 12/30 [04:23<11:06, 37.03s/it]\n                                                                                \n{'loss': 9.0369, 'learning_rate': 3.2142857142857144e-05, 'epoch': 12.0}\n\n 40%|█████████████████▏                         | 12/30 [04:24<11:06, 37.03s/it]***** Running Evaluation *****\n  Num examples = 1\n  Batch size = 100\n\n\n  0%|                                                     | 0/1 [00:00<?, ?it/s]\u001b[A\n                                                                                \n\n\n                                                                                \n\u001b[A{'eval_loss': 10.910731315612793, 'eval_runtime': 23.4253, 'eval_samples_per_second': 0.043, 'eval_steps_per_second': 0.043, 'epoch': 12.0}\n\n 40%|█████████████████▏                         | 12/30 [04:47<11:06, 37.03s/it]\n\n100%|█████████████████████████████████████████████| 1/1 [00:01<00:00,  1.46s/it]\u001b[A\n\n                                                                                \u001b[ASaving model checkpoint to ckpt/pretrain.pt/checkpoint-12\nConfiguration saved in ckpt/pretrain.pt/checkpoint-12/config.json\n\n\nYour command has already run for 300 seconds. It is still running. You can continue to wait or interrupt it with Thought: ... \nAction: interrupt\n```\n``` \nor:\nThought: ... \nAction: continue_wait\n```\n```"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=\",\" python Pretrain.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baa3236",
   "metadata": {},
   "source": [
    "### Step 27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66179edf",
   "metadata": {},
   "source": [
    "Execution Start time: 00:21:07, End time: 00:21:14 (duration: 6 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7854deb",
   "metadata": {},
   "source": [
    "**Thought**: The training script is running for more than 5 minutes, which is longer than expected. I need to ensure that the script is configured to run for only one epoch as per the user's request. I will check the `config.yaml` file to verify the number of epochs and adjust it if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2038091",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825d6d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3:epochs: 30                              # total number of epochs\n\n9:save_strategy: 'epoch'                  # save strategy of trainer\n\n13:logging_strategy: 'epoch'               # logging frequency\n\n14:evaluation_strategy: 'epoch'            # validation frequency\n\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!grep -n \"epoch\" config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e620b01a",
   "metadata": {},
   "source": [
    "### Step 28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbd1ab2",
   "metadata": {},
   "source": [
    "Execution Start time: 00:21:15, End time: 00:21:15 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96d744a",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed9023f",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6805711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db9b652f",
   "metadata": {},
   "source": [
    "### Step 29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2153246",
   "metadata": {},
   "source": [
    "Execution Start time: 00:21:17, End time: 00:21:20 (duration: 2 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc06a91",
   "metadata": {},
   "source": [
    "**Thought**: The `config.yaml` file specifies 30 epochs, which is likely causing the extended runtime. I need to modify the number of epochs to 1 to align with the user's request for training only one epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147d529a",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26559615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\nfile_path: 'data/pretrain.csv'          # pretrain data path\n\nepochs: 1                               # total number of epochs\nbatch_size: 100                         # batch size\nlr_rate: 0.00005                        # learning rate\nscheduler_type: 'linear'                # scheduler type\nweight_decay: 0.0                       # weight decay for AdamW\nwarmup_ratio: 0.05                      # warm-up ratio for scheduler\nsave_strategy: 'epoch'                  # save strategy of trainer"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.yaml\n",
    "<<<BEFORE_EDIT>>>\n",
    "epochs: 30                              # total number of epochs\n",
    "<<<AFTER_EDIT>>>\n",
    "epochs: 1                               # total number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d314f5b",
   "metadata": {},
   "source": [
    "### Step 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af0ae53",
   "metadata": {},
   "source": [
    "Execution Start time: 00:21:21, End time: 00:22:53 (duration: 91 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de28ed7f",
   "metadata": {},
   "source": [
    "**Thought**: The number of epochs has been successfully updated to 1 in the `config.yaml` file. Now, I can rerun the pre-training script to obtain the last training loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda6a939",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09316430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \nThe class this function is called from is 'PolymerSmilesTokenizer'.\n/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 18 worker processes in total. Our suggested max number of worker in current system is 5, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/usr/local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n***** Running training *****\n  Num examples = 2\n  Num Epochs = 1\n  Instantaneous batch size per device = 100\n  Total train batch size (w. parallel, distributed & accumulation) = 100\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1\n\n  0%|                                                     | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 18 worker processes in total. Our suggested max number of worker in current system is 5, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n\n100%|█████████████████████████████████████████████| 1/1 [00:20<00:00, 20.39s/it]\n                                                                                \n{'loss': 10.4246, 'learning_rate': 0.0, 'epoch': 1.0}\n\n100%|█████████████████████████████████████████████| 1/1 [00:21<00:00, 20.39s/it]***** Running Evaluation *****\n  Num examples = 1\n  Batch size = 100\n\n\n  0%|                                                     | 0/1 [00:00<?, ?it/s]\u001b[A\n                                                                                \n\n\n                                                                                \n\u001b[A{'eval_loss': 10.684150695800781, 'eval_runtime': 10.5276, 'eval_samples_per_second': 0.095, 'eval_steps_per_second': 0.095, 'epoch': 1.0}\n\n100%|█████████████████████████████████████████████| 1/1 [00:31<00:00, 20.39s/it]\n\n100%|█████████████████████████████████████████████| 1/1 [00:01<00:00,  1.03s/it]\u001b[A\n\n                                                                                \u001b[ASaving model checkpoint to ckpt/pretrain.pt/checkpoint-1\nConfiguration saved in ckpt/pretrain.pt/checkpoint-1/config.json\nModel weights saved in ckpt/pretrain.pt/checkpoint-1/pytorch_model.bin\nDeleting older checkpoint [ckpt/pretrain.pt/checkpoint-9] due to args.save_total_limit\nDeleting older checkpoint [ckpt/pretrain.pt/checkpoint-10] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n\n                                                                                \n{'train_runtime': 34.7127, 'train_samples_per_second': 0.058, 'train_steps_per_second': 0.029, 'train_loss': 10.424647331237793, 'epoch': 1.0}\n\n100%|█████████████████████████████████████████████| 1/1 [00:34<00:00, 20.39s/it]\n100%|█████████████████████████████████████████████| 1/1 [00:34<00:00, 34.58s/it]\nSaving model checkpoint to ckpt/pretrain.pt\nConfiguration saved in ckpt/pretrain.pt/config.json\nModel weights saved in ckpt/pretrain.pt/pytorch_model.bin\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=\",\" python Pretrain.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dc4b2e",
   "metadata": {},
   "source": [
    "### Step 31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3bb552",
   "metadata": {},
   "source": [
    "Execution Start time: 00:22:55, End time: 00:22:55 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38a3bb7",
   "metadata": {},
   "source": [
    "**Thought**: The pre-training script has completed successfully, and the last training loss is reported as 10.4246. I will now submit this result in the required JSON format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a44fc3",
   "metadata": {},
   "source": [
    "**Action (submit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87693f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'train_loss': 10.4246}"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
