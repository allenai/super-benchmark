{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d25a167",
   "metadata": {},
   "source": [
    "### System Prompt\n",
    "<details>\n",
    "SETTING: You are an autonomous programmer, and you're working directly in the command line with a special Jupyter notebook interface.\n",
    "\n",
    "The special Jupyter notebook interface consists of a file editor that shows you 100 lines of a file at a time.\n",
    "You can execute commands in the notebook using:\n",
    "\n",
    "1. Bash commands: Commands starting with !.\n",
    "2. Python commands: Standard Python code.\n",
    "3. Magic commands: Commands starting with %, e.g., %cd <path>.\n",
    "\n",
    "Additionally, you can also use the following commands to help you navigate and edit files.\n",
    "\n",
    "COMMANDS:\n",
    "\n",
    "open:\n",
    "  docstring: opens the file at the given path in the editor. If line_number is provided, the window will be move to include that line\n",
    "  signature: open <path> [<line_number>]\n",
    "  arguments:\n",
    "    - path (string) [required]: the path to the file to open\n",
    "    - line_number (integer) [optional]: the line number to move the window to (if not provided, the window will start at the top of the file)\n",
    "\n",
    "create:\n",
    "  docstring: creates and opens a new file with the given name\n",
    "  signature: create <filename>\n",
    "  arguments:\n",
    "    - filename (string) [required]: the name of the file to create\n",
    "\n",
    "goto:\n",
    "  docstring: moves the window to show <line_number>\n",
    "  signature: goto <line_number>\n",
    "  arguments:\n",
    "    - line_number (integer) [required]: the line number to move the window to\n",
    "\n",
    "scroll_down:\n",
    "  docstring: moves the window down {WINDOW} lines\n",
    "  signature: scroll_down\n",
    "\n",
    "scroll_up:\n",
    "  docstring: moves the window down {WINDOW} lines\n",
    "  signature: scroll_up\n",
    "\n",
    "submit:\n",
    "  docstring: submits your final answer\n",
    "  signature: submit <answer>\n",
    "  arguments:\n",
    "    - answer (string) [required]: the answer to be submitted\n",
    "\n",
    "search_dir:\n",
    "  docstring: searches for search_term in all files in dir_path. If dir_path is not provided, searches in the current directory\n",
    "  signature: search_dir <search_term> [<dir_path>]\n",
    "  arguments:\n",
    "    - search_term (string) [required]: the term to search for\n",
    "    - dir_path (string) [optional]: the directory to search in (if not provided, searches in the current directory)\n",
    "\n",
    "search_file:\n",
    "  docstring: searches for search_term in file. If file is not provided, searches in the current open file\n",
    "  signature: search_file <search_term> [<file>]\n",
    "  arguments:\n",
    "    - search_term (string) [required]: the term to search for\n",
    "    - file (string) [optional]: the file to search in (if not provided, searches in the current open file)\n",
    "\n",
    "find_file:\n",
    "  docstring: finds all files with the given name in dir_path. If dir_path is not provided, searches in the current directory\n",
    "  signature: find_file <file_name> [<dir_path>]\n",
    "  arguments:\n",
    "    - file_name (string) [required]: the name of the file to search for\n",
    "    - dir_path (string) [optional]: the directory to search in (if not provided, searches in the current directory)\n",
    "\n",
    "edit:\n",
    "  docstring: replaces lines <start_line> through <end_line> (inclusive) with the given text in the open file. The replacement text is terminated by a line with only end_of_edit on it. All of the <replacement text> will be entered, so make sure your indentation is formatted properly. Python files will be checked for syntax errors after the edit. If the system detects a syntax error, the edit will not be executed. Simply try to edit the file again, but make sure to read the error message and modify the edit command you issue accordingly. Issuing the same command a second time will just lead to the same error message again.\n",
    "  signature: edit <start_line>:<end_line>\n",
    "<replacement_text>\n",
    "end_of_edit\n",
    "  arguments:\n",
    "    - start_line (integer) [required]: the line number to start the edit at\n",
    "    - end_line (integer) [required]: the line number to end the edit at (inclusive)\n",
    "    - replacement_text (string) [required]: the text to replace the current selection with\n",
    "\n",
    "\n",
    "\n",
    "Please note that THE EDIT COMMAND REQUIRES PROPER INDENTATION. \n",
    "If you'd like to add the line '        print(x)' you must fully write that out, with all those spaces before the code! Indentation is important and code that is not indented correctly will fail and require fixing before it can be run.\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "Your shell prompt is formatted as follows:\n",
    "(Open file: <path>)\n",
    "(Current directory: <cwd>)\n",
    "In [ ]\n",
    "\n",
    "You need to format your output using two fields: discussion and command.\n",
    "Your output should always include _one_ discussion and _one_ command field EXACTLY as in the following example:\n",
    "DISCUSSION\n",
    "First I'll start by using ls to see what files are in the current directory. Then maybe we can look at some relevant files to see what they look like.\n",
    "```\n",
    "!ls -a\n",
    "```\n",
    "\n",
    "You should only include a *SINGLE* command in the command section and then wait for a response from the shell before continuing with more discussion and commands. Everything you include in the DISCUSSION section will be saved for future reference.\n",
    "If you'd like to issue two commands at once, PLEASE DO NOT DO THAT! Please instead first submit just the first command, and then after receiving a response you'll be able to issue the second command. \n",
    "You're free to use any other bash commands you want (e.g. find, grep, cat, ls, cd) in addition to the special commands listed above.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5b7ab6",
   "metadata": {},
   "source": [
    "### Query\n",
    "Train BERT-WN on the AG-News dataset with 20% single-flip label noise using the `main.py` script. Evaluate the model and report the resulting metrics.\n",
    "\n",
    "The repository does not provide the dataset. Use huggingface datasets to get it (ag_news) Report the accuracy and weighted average precision, recall, f1-score, and support, as a json structured as follows: {\"accuracy\": 0.0, \"weighted_avg\": {\"precision\": 0.0, \"recall\": 0.0, \"f1-score\": 0.0, \"support\": 0}} (replace 0.0 with the actual values).\n",
    "\n",
    "Additional instructions:\n",
    "1. Run for only 5 training steps. All other hyper-parameters should be taken from the readme example.\n",
    "2. Load these specific instances indices: from the train split [1,2,3,4,5,6,100,101,102,103,104,105,451,452,453,454,455,456,501,502,503,504,505,506], test split: [1,2,3,101,103,452,455,501,502,503,504,505], validation split: [0,1,2,6,7,8,12,13,14,18,19,20]\n",
    "\n",
    "Git repository: https://github.com/uds-lsv/BERT-LNL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2b7cb1",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e5e48b",
   "metadata": {},
   "source": [
    "Execution Start time: 18:09:39, End time: 18:09:40 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a51564",
   "metadata": {},
   "source": [
    "**Thought**: First, I need to clone the GitHub repository 'uds-lsv/BERT-LNL' to access the necessary scripts and files for training and evaluating the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49659477",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e96077e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cloning into 'BERT-LNL'...\nremote: Enumerating objects: 24, done.\u001b[K\nremote: Counting objects: 100% (24/24)\u001b[K\nremote: Counting objects: 100% (24/24), done.\u001b[K\nremote: Compressing objects: 100% (20/20)\u001b[K\nremote: Compressing objects: 100% (20/20), done.\u001b[K\nremote: Total 24 (delta 4), reused 20 (delta 4), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (24/24)\nReceiving objects: 100% (24/24), 22.55 KiB | 11.28 MiB/s, done.\nResolving deltas: 100% (4/4)\nResolving deltas: 100% (4/4), done.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!git clone https://github.com/uds-lsv/BERT-LNL.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c982e191",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e03ce6",
   "metadata": {},
   "source": [
    "Execution Start time: 18:09:41, End time: 18:09:41 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5d85f6",
   "metadata": {},
   "source": [
    "**Thought**: The repository has been cloned successfully. Next, I should check the README file for installation and usage instructions to proceed with training and evaluating the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e424d6",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa269539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/BERT-LNL/README.md (48 lines total)]\n1:# BERT_LNL\n2:Code for paper [\"Is BERT Robust to Label Noise? A Study on Learning with Noisy Labels in\n3:Text Classification\"](https://aclanthology.org/2022.insights-1.8.pdf).\n4:\n5:## Data Preparation\n6:### Datasets\n7:We run our experiments on the following four datasets: AG-News, IMDB, Yorùbá, and Hausa.\n8:\n9:| Name | # train | # validation | # test | data source                                            |\n10:|:--------|:--------|:-------------|:-------|:-------------------------------------------------------|\n11:| AGNews | 108000  | 12000        | 7600   | [LINK](https://arxiv.org/abs/1509.01626)               | \n12:| IMDB | 21246   | 3754         | 2500   | [LINK](https://dl.acm.org/doi/10.5555/2002472.2002491) | \n13:| Yorùbá | 1340    | 189         | 379   | [LINK](https://github.com/uds-lsv/transfer-distant-transformer-african) | \n14:| Hausa | 2045    | 290         | 582   | [LINK](https://github.com/uds-lsv/transfer-distant-transformer-african) | \n15:\n16:### Preprocessing\n17:Create a `data` directory as the home directory for all datasets, then create a folder for each dataset in the `data` directory.\n18:\n19:For each dataset, create the following files:\n20:- `[train/validation/test].txt`: one document per line.\n21:- `[train/validation/test]_labels.pickle` files: list of labels (we assume that labels are already encoded in label ids, e.g., [0,0,1,2,3,...,4]).\n22:- If there is no validation data for a given dataset, then there is no need to create the corresponding `validation` files.\n23:- Save files in a `txt_data` directory inside the dataset folder. For example, the full path to the `train.txt` file of the AGNews dataset is `data/AGNews/txt_data/train.txt`.\n24:\n25:## Examples\n26:Run `BERT-WN` on AG-News, with 20% single-flip label noise:\n27:```\n28:CUDA_VISIBLE_DEVICES=[CUDA_ID] python3 ../main.py \\\n29:--dataset AG-NEWs \\\n30:--log_root [LOG_ROOT] \\\n31:--data_root [DATA_ROOT] \\\n32:--trainer_name bert_wn \\\n33:--model_name bert-base-uncased \\\n34:--gen_val \\\n35:--nl_batch_size 32 \\\n36:--eval_batch_size 32 \\\n37:--gradient_accumulation_steps 1 \\\n38:--max_sen_len 64 \\\n39:--lr 0.00002 \\\n40:--num_training_steps 3000 \\\n41:--patience 25 \\\n42:--eval_freq 50 \\\n43:--store_model $STORE_MODEL \\\n44:--noise_level 0.2 \\\n45:--noise_type sflip \\\n46:--manualSeed 1234\n47:```\n48:\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open BERT-LNL/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2a52a7",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe8b965",
   "metadata": {},
   "source": [
    "Execution Start time: 18:09:43, End time: 18:09:44 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9eb6ef",
   "metadata": {},
   "source": [
    "**Thought**: The README file provides instructions for running BERT-WN on the AG-News dataset with 20% single-flip label noise. I need to set up the environment and install the necessary packages before proceeding with the training. Let's start by navigating into the cloned repository directory to prepare for the next steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28959511",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba8fc82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/BERT-LNL\n/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd BERT-LNL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9281e116",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b133937",
   "metadata": {},
   "source": [
    "Execution Start time: 18:09:45, End time: 18:09:46 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b202344b",
   "metadata": {},
   "source": [
    "**Thought**: Now that we are in the correct directory, the next step is to install the necessary Python packages. This will likely include packages for handling datasets, training models, and evaluating results. Let's install the packages listed in the `requirements.txt` file if it exists.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0936133",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e4b66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faab95b",
   "metadata": {},
   "source": [
    "### Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8090f8a9",
   "metadata": {},
   "source": [
    "Execution Start time: 18:09:48, End time: 18:09:56 (duration: 7 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431bafaf",
   "metadata": {},
   "source": [
    "**Thought**: It seems there is no `requirements.txt` file in the repository. We will need to manually install the necessary packages. Based on the task requirements and typical dependencies for working with BERT and datasets, we should install `transformers`, `datasets`, and `scikit-learn`. Let's proceed with installing these packages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665d6100",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e0517a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Requirement already satisfied: transformers in /usr/local/lib/python3.10/site-packages (4.40.2)\nCollecting datasets\n  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/site-packages (1.5.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/site-packages (from transformers) (0.23.3)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from transformers) (1.25.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/site-packages (from transformers) (4.66.4)\nCollecting pyarrow>=15.0.0 (from datasets)\n  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\nCollecting dill<0.3.9,>=0.3.0 (from datasets)\n  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from datasets) (2.0.3)\nCollecting requests (from transformers)\n  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\nCollecting xxhash (from datasets)\n  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nCollecting multiprocess (from datasets)\n  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2023.6.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/site-packages (from datasets) (3.8.3)\nRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn) (1.11.4)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (2.1.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.12.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (2.0.7)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDownloading datasets-2.21.0-py3-none-any.whl (527 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/527.3 kB ? eta -:--:--\n\n\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/116.3 kB ? eta -:--:--\n\n\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/39.9 MB ? eta -:--:--\n\n\n\n\n\n\n\n\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/64.9 kB ? eta -:--:--\n\n\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/134.8 kB ? eta -:--:--\n\n\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/194.1 kB ? eta -:--:--\n\n\u001b[?25hInstalling collected packages: xxhash, requests, pyarrow, dill, multiprocess, datasets\n  Attempting uninstall: requests\n    Found existing installation: requests 2.31.0\n    Uninstalling requests-2.31.0:\n      Successfully uninstalled requests-2.31.0\nSuccessfully installed datasets-2.21.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 requests-2.32.3 xxhash-3.5.0\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install transformers datasets scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0dbc01",
   "metadata": {},
   "source": [
    "### Step 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed55fbc2",
   "metadata": {},
   "source": [
    "Execution Start time: 18:09:58, End time: 18:09:58 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be52fe96",
   "metadata": {},
   "source": [
    "**Thought**: The necessary packages have been installed successfully. Now, I need to modify the `main.py` script to load specific instances from the AG-News dataset and run the training for only 5 steps as specified in the task. Let's open the `main.py` file to make these modifications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028996f1",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce713850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/BERT-LNL/main.py (163 lines total)]\n1:import argparse\n2:from loading_utils import prepare_data, prepare_af_data\n3:from utils import create_logger, save_args, create_trainer, load_config, save_config\n4:import numpy as np\n5:import torch\n6:import random\n7:\n8:\n9:def main():\n10:    parser = argparse.ArgumentParser()\n11:    parser.add_argument('--dataset', type=str, default='IMDB', choices=['SST-2', 'AG_News',\n12:                                                                        'TREC', 'IMDB', 'Yelp-5',\n13:                                                                        'Yoruba', 'Hausa'])\n14:    parser.add_argument('--data_root', type=str, default=\"\")\n15:    parser.add_argument('--log_root', type=str, default=\"\",\n16:                        help='output directory to save logs in training/testing')\n17:\n18:    parser.add_argument('--trainer_name', type=str, default='bert_wn',\n19:                        choices=['bert_wn', 'bert_ct', 'bert_cm', 'bert_cmgt', 'bert_smoothing'],\n20:                        help='trainer selection: '\n21:                             'bert_wn: without noise-handling,'\n22:                             'bert_ct: co-teaching, '\n23:                             'bert_cm: noise matrix, '\n24:                             'bert_cmgt: ground truth noise matrix,'\n25:                             'bert_smoothing: label smoothing')\n26:    parser.add_argument('--model_name', type=str, default='bert-base-uncased',\n27:                        choices=['bert-base-uncased', 'bert-base-cased',\n28:                                 'bert-large-uncased', 'bert-base-multilingual-cased'],\n29:                        help='backbone selection')\n30:    parser.add_argument('--exp_name', type=str, default='')\n31:\n32:\n33:    # Preprocessing Related\n34:    parser.add_argument('--max_sen_len', type=int, default=512,\n35:                        help='max sentence length, longer sentences will be truncated')\n36:    parser.add_argument('--special_token_offsets', type=int, default=2,\n37:                        help='number of special tokens used in bert tokenizer for text classification')\n38:    parser.add_argument('--truncate_mode', type=str, default='last',\n39:                        choices=['hybrid, last'], help='last: last 510 tokens, hybrid: first 128 + last 382')\n40:    parser.add_argument('--freeze_bert', action='store_true',\n41:                        help='freeze the bert backbone, i.e. use bert as feature extractor')\n42:\n43:    # BERT settings Related\n44:    parser.add_argument('--bert_dropout_rate', type=float, default=0.1)\n45:    parser.add_argument('--noise_level', type=float, default=0.0,\n46:                        help='noise level for injected noise')\n47:    parser.add_argument('--noise_type', default='uniform_m',\n48:                        choices=['uniform_m', 'sflip'],\n49:                        help='noise types: uniform_m: uniform noise, sflip: single-flip noise')\n50:    parser.add_argument('--val_fraction', type=float, default=0.1,\n51:                        help='if no validation set is provided, use this fraction of training set as validation set')\n52:\n53:    # training related\n54:    parser.add_argument('--num_epochs', type=int, default=1, help='set either num_epochs or num_training_steps')\n55:    parser.add_argument('--num_training_steps', type=int, default=-1, help='set it to -1 if num_epochs is set')\n56:    parser.add_argument('--train_eval_freq', type=int, default=10,\n57:                        help='evaluate the model on training set after every [train_eval_freq] training steps')\n58:    parser.add_argument('--eval_freq', type=int, default=50,\n59:                        help='evaluate the model on the validation and test sets'\n60:                             'after every [eval_freq] training steps')\n61:    parser.add_argument('--fast_eval', action='store_true',\n62:                        help='use 10% of the test set for evaluation, to speed up the evaluation prcoess')\n63:\n64:    parser.add_argument('--nl_batch_size', type=int, default=16,\n65:                        help='noisy labeled samples per batch, can be understood as the training batch size')\n66:    parser.add_argument('--eval_batch_size', type=int, default=50,\n67:                        help='evaluation batch size during testing')\n68:    parser.add_argument('--warmup_steps', type=int, default=0,\n69:                        help='warmup steps for learning rate scheduler')\n70:    parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n71:                        help=\"number of updates steps to accumulate before performing a backward/update pass.\")\n72:\n73:    parser.add_argument('--gen_val', action='store_true',\n74:                        help='generate validation set, enable it if there is no validation set')\n75:    parser.add_argument('--store_model', type=int, default=0, help='save models after training')\n76:\n77:\n78:    # co-teaching related\n79:    parser.add_argument('--forget_factor', type=float, default=1.0)\n80:    parser.add_argument('--T_k', type=int, default=10)\n81:    parser.add_argument('--c', type=float, default=1.0)\n82:\n83:\n84:    # smoothing trainer related\n85:    parser.add_argument('--smoothing_factor', type=float, default=0.2,\n86:                        help='label smoothing levels, 0.0 means no smoothing')\n87:\n88:\n89:    # cm trainer related\n90:    parser.add_argument('--cm_mse_weight', type=float, default=0.01,\n91:                        help='regularization factor for the mse loss, check https://aclanthology.org/N19-1328/')\n92:\n93:\n94:    # optimizer related\n95:    parser.add_argument('--lr', type=float, default=2e-5)\n96:    parser.add_argument(\"--weight_decay\", default=0.0, type=float,\n97:                        help=\"Weight deay if we apply some.\")\n98:    parser.add_argument('--exp_decay_rate', type=float, default=0.9998)\n99:    parser.add_argument('--max_grad_norm', type=float, default=1.0)\n100:    parser.add_argument('--patience', type=float, default=20, help='patience for early stopping')\n(63 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81301943",
   "metadata": {},
   "source": [
    "### Step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e2311b",
   "metadata": {},
   "source": [
    "Execution Start time: 18:10:00, End time: 18:10:00 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166fb391",
   "metadata": {},
   "source": [
    "**Thought**: To modify the `main.py` script, I need to adjust the script to load specific instance indices from the AG-News dataset and set the number of training steps to 5. I'll search for the relevant sections in the script where the dataset is loaded and the training steps are defined. Let's start by searching for the dataset loading section in the file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ceb349",
   "metadata": {},
   "source": [
    "**Action (search_file)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eb15e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bash: line 1: syntax error near unexpected token `\"prepare_data\"'\nbash: line 1: `source /content/commands/_env_vars.sh && source /content/commands/defaults.sh && source /content/commands/search.sh && source /content/commands/edit_linting.sh && search_file(\"prepare_data\")'\nError: CalledProcessError: Command 'b'source /content/commands/_env_vars.sh && source /content/commands/defaults.sh && source /content/commands/search.sh && source /content/commands/edit_linting.sh && search_file(\"prepare_data\")\\n'' returned non-zero exit status 2.---------------------------------------------------------------------------CalledProcessError                        Traceback (most recent call last)Cell In[27], line 1\n----> 1 get_ipython().run_cell_magic('bash', '', 'source /content/commands/_env_vars.sh && source /content/commands/defaults.sh && source /content/commands/search.sh && source /content/commands/edit_linting.sh && search_file(\"prepare_data\")\\n')\nFile /usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2541, in InteractiveShell.run_cell_magic(self, magic_name, line, cell)\n   2539 with self.builtin_trap:\n   2540     args = (magic_arg_s, cell)\n-> 2541     result = fn(*args, **kwargs)\n   2543 # The code below prevents the output from being displayed\n   2544 # when using magics with decorator @output_can_be_silenced\n   2545 # when the last Python token in the expression is a ';'.\n   2546 if getattr(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, False):\nFile /usr/local/lib/python3.10/site-packages/IPython/core/magics/script.py:155, in ScriptMagics._make_script_magic.<locals>.named_script_magic(line, cell)\n    153 else:\n    154     line = script\n--> 155 return self.shebang(line, cell)\nFile /usr/local/lib/python3.10/site-packages/IPython/core/magics/script.py:315, in ScriptMagics.shebang(self, line, cell)\n    310 if args.raise_error and p.returncode != 0:\n    311     # If we get here and p.returncode is still None, we must have\n    312     # killed it but not yet seen its return code. We don't wait for it,\n    313     # in case it's stuck in uninterruptible sleep. -9 = SIGKILL\n    314     rc = p.returncode or -9\n--> 315     raise CalledProcessError(rc, cell)\nCalledProcessError: Command 'b'source /content/commands/_env_vars.sh && source /content/commands/defaults.sh && source /content/commands/search.sh && source /content/commands/edit_linting.sh && search_file(\"prepare_data\")\\n'' returned non-zero exit status 2."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_file(\"prepare_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40c16a3",
   "metadata": {},
   "source": [
    "### Step 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6e0877",
   "metadata": {},
   "source": [
    "Execution Start time: 18:10:02, End time: 18:10:02 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a60f9af",
   "metadata": {},
   "source": [
    "**Thought**: It seems there was an error with the search command. Let's try searching for the term \"dataset\" in the `main.py` file to locate where the dataset is being loaded.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff35a32",
   "metadata": {},
   "source": [
    "**Action (search_file)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba01d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bash: line 1: syntax error near unexpected token `\"dataset\"'\nbash: line 1: `source /content/commands/_env_vars.sh && source /content/commands/defaults.sh && source /content/commands/search.sh && source /content/commands/edit_linting.sh && search_file(\"dataset\")'\nError: CalledProcessError: Command 'b'source /content/commands/_env_vars.sh && source /content/commands/defaults.sh && source /content/commands/search.sh && source /content/commands/edit_linting.sh && search_file(\"dataset\")\\n'' returned non-zero exit status 2.---------------------------------------------------------------------------CalledProcessError                        Traceback (most recent call last)Cell In[29], line 1\n----> 1 get_ipython().run_cell_magic('bash', '', 'source /content/commands/_env_vars.sh && source /content/commands/defaults.sh && source /content/commands/search.sh && source /content/commands/edit_linting.sh && search_file(\"dataset\")\\n')\nFile /usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2541, in InteractiveShell.run_cell_magic(self, magic_name, line, cell)\n   2539 with self.builtin_trap:\n   2540     args = (magic_arg_s, cell)\n-> 2541     result = fn(*args, **kwargs)\n   2543 # The code below prevents the output from being displayed\n   2544 # when using magics with decorator @output_can_be_silenced\n   2545 # when the last Python token in the expression is a ';'.\n   2546 if getattr(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, False):\nFile /usr/local/lib/python3.10/site-packages/IPython/core/magics/script.py:155, in ScriptMagics._make_script_magic.<locals>.named_script_magic(line, cell)\n    153 else:\n    154     line = script\n--> 155 return self.shebang(line, cell)\nFile /usr/local/lib/python3.10/site-packages/IPython/core/magics/script.py:315, in ScriptMagics.shebang(self, line, cell)\n    310 if args.raise_error and p.returncode != 0:\n    311     # If we get here and p.returncode is still None, we must have\n    312     # killed it but not yet seen its return code. We don't wait for it,\n    313     # in case it's stuck in uninterruptible sleep. -9 = SIGKILL\n    314     rc = p.returncode or -9\n--> 315     raise CalledProcessError(rc, cell)\nCalledProcessError: Command 'b'source /content/commands/_env_vars.sh && source /content/commands/defaults.sh && source /content/commands/search.sh && source /content/commands/edit_linting.sh && search_file(\"dataset\")\\n'' returned non-zero exit status 2."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_file(\"dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a6a399",
   "metadata": {},
   "source": [
    "### Step 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f150af07",
   "metadata": {},
   "source": [
    "Execution Start time: 18:10:03, End time: 18:10:04 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8eb15e",
   "metadata": {},
   "source": [
    "**Thought**: It seems there is an issue with the search command. Let's manually scroll through the `main.py` file to locate the section where the dataset is being loaded and processed. I'll start by scrolling down to see more of the file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69bef5f",
   "metadata": {},
   "source": [
    "**Action (scroll_down)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8788c769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/BERT-LNL/main.py (163 lines total)]\n(63 more lines above)\n64:    parser.add_argument('--nl_batch_size', type=int, default=16,\n65:                        help='noisy labeled samples per batch, can be understood as the training batch size')\n66:    parser.add_argument('--eval_batch_size', type=int, default=50,\n67:                        help='evaluation batch size during testing')\n68:    parser.add_argument('--warmup_steps', type=int, default=0,\n69:                        help='warmup steps for learning rate scheduler')\n70:    parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n71:                        help=\"number of updates steps to accumulate before performing a backward/update pass.\")\n72:\n73:    parser.add_argument('--gen_val', action='store_true',\n74:                        help='generate validation set, enable it if there is no validation set')\n75:    parser.add_argument('--store_model', type=int, default=0, help='save models after training')\n76:\n77:\n78:    # co-teaching related\n79:    parser.add_argument('--forget_factor', type=float, default=1.0)\n80:    parser.add_argument('--T_k', type=int, default=10)\n81:    parser.add_argument('--c', type=float, default=1.0)\n82:\n83:\n84:    # smoothing trainer related\n85:    parser.add_argument('--smoothing_factor', type=float, default=0.2,\n86:                        help='label smoothing levels, 0.0 means no smoothing')\n87:\n88:\n89:    # cm trainer related\n90:    parser.add_argument('--cm_mse_weight', type=float, default=0.01,\n91:                        help='regularization factor for the mse loss, check https://aclanthology.org/N19-1328/')\n92:\n93:\n94:    # optimizer related\n95:    parser.add_argument('--lr', type=float, default=2e-5)\n96:    parser.add_argument(\"--weight_decay\", default=0.0, type=float,\n97:                        help=\"Weight deay if we apply some.\")\n98:    parser.add_argument('--exp_decay_rate', type=float, default=0.9998)\n99:    parser.add_argument('--max_grad_norm', type=float, default=1.0)\n100:    parser.add_argument('--patience', type=float, default=20, help='patience for early stopping')\n101:\n102:    # hardware related\n103:    parser.add_argument('--use_cuda', type=bool, default=True)\n104:    parser.add_argument('--num_workers', type=int, default=0)\n105:    parser.add_argument('--cuda_device', type=str, default=\"0\")\n106:    parser.add_argument('--manualSeed', type=int, default=1234, help='random seed for reproducibility')\n107:    parser.add_argument('--noisy_label_seed', type=int, default=1234, help='random seed for reproducibility')\n108:\n109:    args = parser.parse_args()\n110:\n111:    if args.manualSeed is None:\n112:        args.manualSeed = random.randint(1, 10000)\n113:    random.seed(args.manualSeed)\n114:    np.random.seed(args.manualSeed)\n115:    torch.manual_seed(args.manualSeed)\n116:    torch.backends.cudnn.benchmark = False\n117:    if args.use_cuda:\n118:        torch.cuda.manual_seed_all(args.manualSeed)\n119:        torch.backends.cudnn.deterministic = True\n120:\n121:    # Create the Handler for logging records/messages to a file\n122:    logger, log_dir = create_logger(args.log_root, args)\n123:    save_args(log_dir, args)\n124:    logger.info(\"Training started\")\n125:    num_classes_map = {'AG_News': 4, 'TREC': 6, 'SST-2': 2, 'IMDB': 2, 'Yelp-5': 5,\n126:                       'Yoruba':7, 'Hausa':5}\n127:\n128:    logger.info(f'log dir: {log_dir}')\n129:    num_classes = num_classes_map[args.dataset]\n130:    r_state = np.random.RandomState(args.noisy_label_seed)\n131:\n132:    if args.dataset in ['SST-2', 'AG_News', 'TREC', 'IMDB', 'Yelp-5']:\n133:        logger.info(f'loading {args.dataset}...')\n134:\n135:\n136:        if args.dataset in ['SST-2', 'AG_News', 'TREC', 'Yelp-5']:\n137:            has_val = False\n138:            has_ul = False\n139:        elif args.dataset in ['IMDB']:\n140:            has_val = True\n141:            has_ul = True\n142:        else:\n143:            raise ValueError('need to set has_val and has_ul')\n144:\n145:        nl_set, ul_set, v_set, t_set, l2id, id2l = prepare_data(args, logger, r_state, num_classes, has_val, has_ul)\n146:    elif args.dataset in ['Yoruba', 'Hausa']:\n147:        has_ul=False\n148:        nl_set, ul_set, v_set, t_set, l2id, id2l = prepare_af_data(args, logger,\n149:                                                                   num_classes, has_ul)\n150:    else:\n151:        raise NotImplementedError(f\"dataset {args.dataset} not supported\")\n152:\n153:\n154:    model_config = load_config(args)\n155:    model_config['num_classes'] = num_classes\n156:\n157:    trainer = create_trainer(args, logger, log_dir, model_config, (nl_set, ul_set, v_set, t_set, l2id, id2l), r_state)\n158:    trainer.train(args, logger, (nl_set, ul_set, v_set, t_set, l2id, id2l))\n159:    save_config(log_dir, 'model_config', trainer.model_config)  # model_config could be updated during model creation\n160:\n161:\n162:if __name__=='__main__':\n163:    main()\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scroll_down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c818d0",
   "metadata": {},
   "source": [
    "### Step 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b20a1c2",
   "metadata": {},
   "source": [
    "Execution Start time: 18:10:05, End time: 18:10:06 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cc909c",
   "metadata": {},
   "source": [
    "**Thought**: I've found the section where the dataset is being loaded using the `prepare_data` function. To proceed, I need to modify this function to load specific instance indices from the AG-News dataset and set the number of training steps to 5. Let's search for the definition of the `prepare_data` function to make these changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fe8908",
   "metadata": {},
   "source": [
    "**Action (search_file)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcdb5ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bash: line 1: syntax error near unexpected token `\"def prepare_data\"'\nbash: line 1: `source /content/commands/_env_vars.sh && source /content/commands/defaults.sh && source /content/commands/search.sh && source /content/commands/edit_linting.sh && search_file(\"def prepare_data\")'\nError: CalledProcessError: Command 'b'source /content/commands/_env_vars.sh && source /content/commands/defaults.sh && source /content/commands/search.sh && source /content/commands/edit_linting.sh && search_file(\"def prepare_data\")\\n'' returned non-zero exit status 2.---------------------------------------------------------------------------CalledProcessError                        Traceback (most recent call last)Cell In[33], line 1\n----> 1 get_ipython().run_cell_magic('bash', '', 'source /content/commands/_env_vars.sh && source /content/commands/defaults.sh && source /content/commands/search.sh && source /content/commands/edit_linting.sh && search_file(\"def prepare_data\")\\n')\nFile /usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2541, in InteractiveShell.run_cell_magic(self, magic_name, line, cell)\n   2539 with self.builtin_trap:\n   2540     args = (magic_arg_s, cell)\n-> 2541     result = fn(*args, **kwargs)\n   2543 # The code below prevents the output from being displayed\n   2544 # when using magics with decorator @output_can_be_silenced\n   2545 # when the last Python token in the expression is a ';'.\n   2546 if getattr(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, False):\nFile /usr/local/lib/python3.10/site-packages/IPython/core/magics/script.py:155, in ScriptMagics._make_script_magic.<locals>.named_script_magic(line, cell)\n    153 else:\n    154     line = script\n--> 155 return self.shebang(line, cell)\nFile /usr/local/lib/python3.10/site-packages/IPython/core/magics/script.py:315, in ScriptMagics.shebang(self, line, cell)\n    310 if args.raise_error and p.returncode != 0:\n    311     # If we get here and p.returncode is still None, we must have\n    312     # killed it but not yet seen its return code. We don't wait for it,\n    313     # in case it's stuck in uninterruptible sleep. -9 = SIGKILL\n    314     rc = p.returncode or -9\n--> 315     raise CalledProcessError(rc, cell)\nCalledProcessError: Command 'b'source /content/commands/_env_vars.sh && source /content/commands/defaults.sh && source /content/commands/search.sh && source /content/commands/edit_linting.sh && search_file(\"def prepare_data\")\\n'' returned non-zero exit status 2."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_file(\"def prepare_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c4252f",
   "metadata": {},
   "source": [
    "### Step 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4467396e",
   "metadata": {},
   "source": [
    "Execution Start time: 18:10:07, End time: 18:10:08 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512345b3",
   "metadata": {},
   "source": [
    "**Thought**: It seems there is a persistent issue with the search command. Let's manually scroll through the file to locate the `prepare_data` function definition. I'll continue scrolling down to find this section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3853dda1",
   "metadata": {},
   "source": [
    "**Action (scroll_down)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6c3d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/BERT-LNL/main.py (163 lines total)]\n(63 more lines above)\n64:    parser.add_argument('--nl_batch_size', type=int, default=16,\n65:                        help='noisy labeled samples per batch, can be understood as the training batch size')\n66:    parser.add_argument('--eval_batch_size', type=int, default=50,\n67:                        help='evaluation batch size during testing')\n68:    parser.add_argument('--warmup_steps', type=int, default=0,\n69:                        help='warmup steps for learning rate scheduler')\n70:    parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n71:                        help=\"number of updates steps to accumulate before performing a backward/update pass.\")\n72:\n73:    parser.add_argument('--gen_val', action='store_true',\n74:                        help='generate validation set, enable it if there is no validation set')\n75:    parser.add_argument('--store_model', type=int, default=0, help='save models after training')\n76:\n77:\n78:    # co-teaching related\n79:    parser.add_argument('--forget_factor', type=float, default=1.0)\n80:    parser.add_argument('--T_k', type=int, default=10)\n81:    parser.add_argument('--c', type=float, default=1.0)\n82:\n83:\n84:    # smoothing trainer related\n85:    parser.add_argument('--smoothing_factor', type=float, default=0.2,\n86:                        help='label smoothing levels, 0.0 means no smoothing')\n87:\n88:\n89:    # cm trainer related\n90:    parser.add_argument('--cm_mse_weight', type=float, default=0.01,\n91:                        help='regularization factor for the mse loss, check https://aclanthology.org/N19-1328/')\n92:\n93:\n94:    # optimizer related\n95:    parser.add_argument('--lr', type=float, default=2e-5)\n96:    parser.add_argument(\"--weight_decay\", default=0.0, type=float,\n97:                        help=\"Weight deay if we apply some.\")\n98:    parser.add_argument('--exp_decay_rate', type=float, default=0.9998)\n99:    parser.add_argument('--max_grad_norm', type=float, default=1.0)\n100:    parser.add_argument('--patience', type=float, default=20, help='patience for early stopping')\n101:\n102:    # hardware related\n103:    parser.add_argument('--use_cuda', type=bool, default=True)\n104:    parser.add_argument('--num_workers', type=int, default=0)\n105:    parser.add_argument('--cuda_device', type=str, default=\"0\")\n106:    parser.add_argument('--manualSeed', type=int, default=1234, help='random seed for reproducibility')\n107:    parser.add_argument('--noisy_label_seed', type=int, default=1234, help='random seed for reproducibility')\n108:\n109:    args = parser.parse_args()\n110:\n111:    if args.manualSeed is None:\n112:        args.manualSeed = random.randint(1, 10000)\n113:    random.seed(args.manualSeed)\n114:    np.random.seed(args.manualSeed)\n115:    torch.manual_seed(args.manualSeed)\n116:    torch.backends.cudnn.benchmark = False\n117:    if args.use_cuda:\n118:        torch.cuda.manual_seed_all(args.manualSeed)\n119:        torch.backends.cudnn.deterministic = True\n120:\n121:    # Create the Handler for logging records/messages to a file\n122:    logger, log_dir = create_logger(args.log_root, args)\n123:    save_args(log_dir, args)\n124:    logger.info(\"Training started\")\n125:    num_classes_map = {'AG_News': 4, 'TREC': 6, 'SST-2': 2, 'IMDB': 2, 'Yelp-5': 5,\n126:                       'Yoruba':7, 'Hausa':5}\n127:\n128:    logger.info(f'log dir: {log_dir}')\n129:    num_classes = num_classes_map[args.dataset]\n130:    r_state = np.random.RandomState(args.noisy_label_seed)\n131:\n132:    if args.dataset in ['SST-2', 'AG_News', 'TREC', 'IMDB', 'Yelp-5']:\n133:        logger.info(f'loading {args.dataset}...')\n134:\n135:\n136:        if args.dataset in ['SST-2', 'AG_News', 'TREC', 'Yelp-5']:\n137:            has_val = False\n138:            has_ul = False\n139:        elif args.dataset in ['IMDB']:\n140:            has_val = True\n141:            has_ul = True\n142:        else:\n143:            raise ValueError('need to set has_val and has_ul')\n144:\n145:        nl_set, ul_set, v_set, t_set, l2id, id2l = prepare_data(args, logger, r_state, num_classes, has_val, has_ul)\n146:    elif args.dataset in ['Yoruba', 'Hausa']:\n147:        has_ul=False\n148:        nl_set, ul_set, v_set, t_set, l2id, id2l = prepare_af_data(args, logger,\n149:                                                                   num_classes, has_ul)\n150:    else:\n151:        raise NotImplementedError(f\"dataset {args.dataset} not supported\")\n152:\n153:\n154:    model_config = load_config(args)\n155:    model_config['num_classes'] = num_classes\n156:\n157:    trainer = create_trainer(args, logger, log_dir, model_config, (nl_set, ul_set, v_set, t_set, l2id, id2l), r_state)\n158:    trainer.train(args, logger, (nl_set, ul_set, v_set, t_set, l2id, id2l))\n159:    save_config(log_dir, 'model_config', trainer.model_config)  # model_config could be updated during model creation\n160:\n161:\n162:if __name__=='__main__':\n163:    main()\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scroll_down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31485046",
   "metadata": {},
   "source": [
    "### Step 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a880a0d",
   "metadata": {},
   "source": [
    "Execution Start time: 18:10:09, End time: 18:10:10 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cccbda",
   "metadata": {},
   "source": [
    "**Thought**: I've found the section where the `prepare_data` function is called to load the dataset. To proceed, I need to locate the definition of the `prepare_data` function to modify it for loading specific instance indices. Let's scroll up to find the function definition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd01db67",
   "metadata": {},
   "source": [
    "**Action (scroll_up)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00d6cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/BERT-LNL/main.py (163 lines total)]\n1:import argparse\n2:from loading_utils import prepare_data, prepare_af_data\n3:from utils import create_logger, save_args, create_trainer, load_config, save_config\n4:import numpy as np\n5:import torch\n6:import random\n7:\n8:\n9:def main():\n10:    parser = argparse.ArgumentParser()\n11:    parser.add_argument('--dataset', type=str, default='IMDB', choices=['SST-2', 'AG_News',\n12:                                                                        'TREC', 'IMDB', 'Yelp-5',\n13:                                                                        'Yoruba', 'Hausa'])\n14:    parser.add_argument('--data_root', type=str, default=\"\")\n15:    parser.add_argument('--log_root', type=str, default=\"\",\n16:                        help='output directory to save logs in training/testing')\n17:\n18:    parser.add_argument('--trainer_name', type=str, default='bert_wn',\n19:                        choices=['bert_wn', 'bert_ct', 'bert_cm', 'bert_cmgt', 'bert_smoothing'],\n20:                        help='trainer selection: '\n21:                             'bert_wn: without noise-handling,'\n22:                             'bert_ct: co-teaching, '\n23:                             'bert_cm: noise matrix, '\n24:                             'bert_cmgt: ground truth noise matrix,'\n25:                             'bert_smoothing: label smoothing')\n26:    parser.add_argument('--model_name', type=str, default='bert-base-uncased',\n27:                        choices=['bert-base-uncased', 'bert-base-cased',\n28:                                 'bert-large-uncased', 'bert-base-multilingual-cased'],\n29:                        help='backbone selection')\n30:    parser.add_argument('--exp_name', type=str, default='')\n31:\n32:\n33:    # Preprocessing Related\n34:    parser.add_argument('--max_sen_len', type=int, default=512,\n35:                        help='max sentence length, longer sentences will be truncated')\n36:    parser.add_argument('--special_token_offsets', type=int, default=2,\n37:                        help='number of special tokens used in bert tokenizer for text classification')\n38:    parser.add_argument('--truncate_mode', type=str, default='last',\n39:                        choices=['hybrid, last'], help='last: last 510 tokens, hybrid: first 128 + last 382')\n40:    parser.add_argument('--freeze_bert', action='store_true',\n41:                        help='freeze the bert backbone, i.e. use bert as feature extractor')\n42:\n43:    # BERT settings Related\n44:    parser.add_argument('--bert_dropout_rate', type=float, default=0.1)\n45:    parser.add_argument('--noise_level', type=float, default=0.0,\n46:                        help='noise level for injected noise')\n47:    parser.add_argument('--noise_type', default='uniform_m',\n48:                        choices=['uniform_m', 'sflip'],\n49:                        help='noise types: uniform_m: uniform noise, sflip: single-flip noise')\n50:    parser.add_argument('--val_fraction', type=float, default=0.1,\n51:                        help='if no validation set is provided, use this fraction of training set as validation set')\n52:\n53:    # training related\n54:    parser.add_argument('--num_epochs', type=int, default=1, help='set either num_epochs or num_training_steps')\n55:    parser.add_argument('--num_training_steps', type=int, default=-1, help='set it to -1 if num_epochs is set')\n56:    parser.add_argument('--train_eval_freq', type=int, default=10,\n57:                        help='evaluate the model on training set after every [train_eval_freq] training steps')\n58:    parser.add_argument('--eval_freq', type=int, default=50,\n59:                        help='evaluate the model on the validation and test sets'\n60:                             'after every [eval_freq] training steps')\n61:    parser.add_argument('--fast_eval', action='store_true',\n62:                        help='use 10% of the test set for evaluation, to speed up the evaluation prcoess')\n63:\n64:    parser.add_argument('--nl_batch_size', type=int, default=16,\n65:                        help='noisy labeled samples per batch, can be understood as the training batch size')\n66:    parser.add_argument('--eval_batch_size', type=int, default=50,\n67:                        help='evaluation batch size during testing')\n68:    parser.add_argument('--warmup_steps', type=int, default=0,\n69:                        help='warmup steps for learning rate scheduler')\n70:    parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n71:                        help=\"number of updates steps to accumulate before performing a backward/update pass.\")\n72:\n73:    parser.add_argument('--gen_val', action='store_true',\n74:                        help='generate validation set, enable it if there is no validation set')\n75:    parser.add_argument('--store_model', type=int, default=0, help='save models after training')\n76:\n77:\n78:    # co-teaching related\n79:    parser.add_argument('--forget_factor', type=float, default=1.0)\n80:    parser.add_argument('--T_k', type=int, default=10)\n81:    parser.add_argument('--c', type=float, default=1.0)\n82:\n83:\n84:    # smoothing trainer related\n85:    parser.add_argument('--smoothing_factor', type=float, default=0.2,\n86:                        help='label smoothing levels, 0.0 means no smoothing')\n87:\n88:\n89:    # cm trainer related\n90:    parser.add_argument('--cm_mse_weight', type=float, default=0.01,\n91:                        help='regularization factor for the mse loss, check https://aclanthology.org/N19-1328/')\n92:\n93:\n94:    # optimizer related\n95:    parser.add_argument('--lr', type=float, default=2e-5)\n96:    parser.add_argument(\"--weight_decay\", default=0.0, type=float,\n97:                        help=\"Weight deay if we apply some.\")\n98:    parser.add_argument('--exp_decay_rate', type=float, default=0.9998)\n99:    parser.add_argument('--max_grad_norm', type=float, default=1.0)\n100:    parser.add_argument('--patience', type=float, default=20, help='patience for early stopping')\n(63 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scroll_up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc658b8",
   "metadata": {},
   "source": [
    "### Step 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13f788d",
   "metadata": {},
   "source": [
    "Execution Start time: 18:10:12, End time: 18:10:12 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8441e5f4",
   "metadata": {},
   "source": [
    "**Thought**: The `prepare_data` function is imported from `loading_utils`. To modify the dataset loading process, I need to open the `loading_utils.py` file where the `prepare_data` function is defined. Let's open this file to make the necessary changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db05298",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a65e3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/BERT-LNL/loading_utils.py (196 lines total)]\n1:import os\n2:import copy\n3:import numpy as np\n4:import pickle\n5:import torch\n6:import wandb\n7:from tqdm import tqdm\n8:from text_dataset import TextBertDataset, TextBertRealDataset\n9:from transformers import AutoTokenizer\n10:from transformers import BertTokenizerFast, BertTokenizer\n11:import utils\n12:\n13:\n14:def prepare_data(args, logger, r_state, num_classes, has_val, has_ul):\n15:    # used for experiments with injected noise\n16:\n17:    tokenizer = load_tokenizer(args)\n18:    tr_data, val_data = get_training_validation_set(args, logger, tokenizer, r_state, has_val, num_classes)\n19:    test_data = load_and_cache_text(args, tokenizer, logger, tag='test')\n20:\n21:    n_set = TextBertDataset(args, tr_data, tokenizer, r_state, num_classes, make_noisy=True)\n22:    v_set = TextBertDataset(args, val_data, tokenizer, r_state, num_classes, make_noisy=True)\n23:    t_set = TextBertDataset(args, test_data, tokenizer, r_state, num_classes, make_noisy=False)\n24:\n25:    n_set_noisy_labels = copy.deepcopy(n_set.noisy_labels)\n26:    v_set_noisy_labels = copy.deepcopy(v_set.noisy_labels)\n27:    n_set_noisy_labels_hash = hash(tuple(n_set_noisy_labels))\n28:    v_set_noisy_labels_hash = hash(tuple(v_set_noisy_labels))\n29:    # wandb.run.summary[\"train_n_hash\"] = n_set_noisy_labels_hash\n30:    # wandb.run.summary[\"val_n_hash\"] = v_set_noisy_labels_hash\n31:\n32:    u_set = None\n33:    l2id = None\n34:    id2l = None\n35:\n36:    return n_set, u_set, v_set, t_set, l2id, id2l\n37:\n38:\n39:def prepare_af_data(args, logger, num_classes, has_ul):\n40:    tokenizer = load_tokenizer(args)\n41:    n_set = get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag='train')\n42:    v_set = get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag='validation')\n43:    t_set = get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag='test')\n44:\n45:    assert not has_ul  # we do not have unlabeled data in Yoruba and Hausa dataset\n46:    u_set = None\n47:\n48:    label_mapping_data_dir = os.path.join(args.data_root, args.dataset, 'txt_data')\n49:    l2id = utils.pickle_load(os.path.join(label_mapping_data_dir, 'l2idx.pickle'))\n50:    id2l = utils.pickle_load(os.path.join(label_mapping_data_dir, 'idx2l.pickle'))\n51:\n52:    return n_set, u_set, v_set, t_set, l2id, id2l\n53:\n54:\n55:def get_training_validation_set(args, logger, tokenizer, r_state, has_val, num_classes):\n56:    # sanity check: args.gen_val is used when there is no validation set\n57:    if has_val:\n58:        assert not args.gen_val\n59:\n60:    tr_data = load_and_cache_text(args, tokenizer, logger, tag='train')\n61:\n62:    if has_val:  # original validation set available\n63:        val_data = load_and_cache_text(args, tokenizer, logger, tag='validation')\n64:    elif args.gen_val:  # create validation set using the training set\n65:        val_indices_path = os.path.join(args.data_root, args.dataset, 'val_indices', f'{args.dataset}_val_indices.pickle')\n66:        with open(val_indices_path, 'rb') as handle:\n67:            val_indices = pickle.load(handle)\n68:\n69:        val_mask = np.zeros(len(tr_data['labels']), dtype=bool)\n70:        val_mask[val_indices] = True\n71:\n72:        val_features = {k: v[val_mask] for k,v in tr_data['features'].items()}\n73:        val_labels = tr_data['labels'][val_mask]\n74:        val_text  = np.array(tr_data['text'])[val_mask]\n75:\n76:        train_features = {k: v[~val_mask] for k,v in tr_data['features'].items()}\n77:        train_labels = tr_data['labels'][~val_mask]\n78:        train_text  = np.array(tr_data['text'])[~val_mask]\n79:\n80:        val_data = {\"features\": val_features, \"labels\": val_labels, \"text\": val_text}\n81:        tr_data = {\"features\": train_features, \"labels\": train_labels, \"text\": train_text}\n82:\n83:    else:\n84:        raise ValueError(\"we need a validation set, set gen_val to True to extract\"\n85:                         \"a subset from the training data as validation data\")\n86:\n87:    return tr_data, val_data\n88:\n89:def get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag):\n90:    noisy_data_tag = f'{tag}_clean'\n91:\n92:    #get text data with noisy labels\n93:    clean_noisy_data = load_and_cache_text(args, tokenizer, logger, tag=noisy_data_tag)\n94:\n95:    #get the clean training and the clean validation sets\n96:    txt_data_dir = os.path.join(args.data_root, args.dataset, 'txt_data')\n97:    input_path = os.path.join(txt_data_dir, f'{tag}_clean_noisy_labels.pickle')\n98:    noisy_labels = load_pickle_data(input_path)\n99:\n100:    td = TextBertRealDataset(args, clean_noisy_data, noisy_labels, tokenizer, num_classes)\n(96 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open loading_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c08887",
   "metadata": {},
   "source": [
    "### Step 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ebbff2",
   "metadata": {},
   "source": [
    "Execution Start time: 18:10:24, End time: 18:10:25 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52632a7",
   "metadata": {},
   "source": [
    "**Thought**: The `prepare_data` function is responsible for loading the dataset. To modify it for loading specific instance indices, I need to adjust the dataset loading logic to filter the data based on the provided indices for train, test, and validation splits. Let's edit the `prepare_data` function to include these changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61939f28",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ab18c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/BERT-LNL/loading_utils.py (205 lines total)]\n1:import os\n2:import copy\n3:import numpy as np\n4:import pickle\n5:import torch\n6:import wandb\n7:from tqdm import tqdm\n8:from text_dataset import TextBertDataset, TextBertRealDataset\n9:from transformers import AutoTokenizer\n10:from transformers import BertTokenizerFast, BertTokenizer\n11:import utils\n12:\n13:\n14:def prepare_data(args, logger, r_state, num_classes, has_val, has_ul):\n15:    # used for experiments with injected noise\n16:\n17:    tokenizer = load_tokenizer(args)\n18:    tr_data, val_data = get_training_validation_set(args, logger, tokenizer, r_state, has_val, num_classes)\n19:    test_data = load_and_cache_text(args, tokenizer, logger, tag='test')\n20:\n21:    # Filter specific instance indices\n22:    train_indices = [1,2,3,4,5,6,100,101,102,103,104,105,451,452,453,454,455,456,501,502,503,504,505,506]\n23:    test_indices = [1,2,3,101,103,452,455,501,502,503,504,505]\n24:    val_indices = [0,1,2,6,7,8,12,13,14,18,19,20]\n25:\n26:    tr_data = {k: [v[i] for i in train_indices] for k, v in tr_data.items()}\n27:    val_data = {k: [v[i] for i in val_indices] for k, v in val_data.items()}\n28:    test_data = {k: [v[i] for i in test_indices] for k, v in test_data.items()}\n29:\n30:    n_set = TextBertDataset(args, tr_data, tokenizer, r_state, num_classes, make_noisy=True)\n31:    v_set = TextBertDataset(args, val_data, tokenizer, r_state, num_classes, make_noisy=True)\n32:    t_set = TextBertDataset(args, test_data, tokenizer, r_state, num_classes, make_noisy=False)\n33:\n34:    n_set_noisy_labels = copy.deepcopy(n_set.noisy_labels)\n35:    v_set_noisy_labels = copy.deepcopy(v_set.noisy_labels)\n36:    n_set_noisy_labels_hash = hash(tuple(n_set_noisy_labels))\n37:    v_set_noisy_labels_hash = hash(tuple(v_set_noisy_labels))\n38:    # wandb.run.summary[\"train_n_hash\"] = n_set_noisy_labels_hash\n39:    # wandb.run.summary[\"val_n_hash\"] = v_set_noisy_labels_hash\n40:\n41:    u_set = None\n42:    l2id = None\n43:    id2l = None\n44:\n45:    return n_set, u_set, v_set, t_set, l2id, id2l\n46:\n47:\n48:def prepare_af_data(args, logger, num_classes, has_ul):\n49:    tokenizer = load_tokenizer(args)\n50:    n_set = get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag='train')\n51:    v_set = get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag='validation')\n52:    t_set = get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag='test')\n53:\n54:    assert not has_ul  # we do not have unlabeled data in Yoruba and Hausa dataset\n55:    u_set = None\n56:\n57:    label_mapping_data_dir = os.path.join(args.data_root, args.dataset, 'txt_data')\n58:    l2id = utils.pickle_load(os.path.join(label_mapping_data_dir, 'l2idx.pickle'))\n59:    id2l = utils.pickle_load(os.path.join(label_mapping_data_dir, 'idx2l.pickle'))\n60:\n61:    return n_set, u_set, v_set, t_set, l2id, id2l\n62:\n63:\n64:def get_training_validation_set(args, logger, tokenizer, r_state, has_val, num_classes):\n65:    # sanity check: args.gen_val is used when there is no validation set\n66:    if has_val:\n67:        assert not args.gen_val\n68:\n69:    tr_data = load_and_cache_text(args, tokenizer, logger, tag='train')\n70:\n71:    if has_val:  # original validation set available\n72:        val_data = load_and_cache_text(args, tokenizer, logger, tag='validation')\n73:    elif args.gen_val:  # create validation set using the training set\n74:        val_indices_path = os.path.join(args.data_root, args.dataset, 'val_indices', f'{args.dataset}_val_indices.pickle')\n75:        with open(val_indices_path, 'rb') as handle:\n76:            val_indices = pickle.load(handle)\n77:\n78:        val_mask = np.zeros(len(tr_data['labels']), dtype=bool)\n79:        val_mask[val_indices] = True\n80:\n81:        val_features = {k: v[val_mask] for k,v in tr_data['features'].items()}\n82:        val_labels = tr_data['labels'][val_mask]\n83:        val_text  = np.array(tr_data['text'])[val_mask]\n84:\n85:        train_features = {k: v[~val_mask] for k,v in tr_data['features'].items()}\n86:        train_labels = tr_data['labels'][~val_mask]\n87:        train_text  = np.array(tr_data['text'])[~val_mask]\n88:\n89:        val_data = {\"features\": val_features, \"labels\": val_labels, \"text\": val_text}\n90:        tr_data = {\"features\": train_features, \"labels\": train_labels, \"text\": train_text}\n91:\n92:    else:\n93:        raise ValueError(\"we need a validation set, set gen_val to True to extract\"\n94:                         \"a subset from the training data as validation data\")\n95:\n96:    return tr_data, val_data\n97:\n98:def get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag):\n99:    noisy_data_tag = f'{tag}_clean'\n100:\n(105 more lines below)\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 14:36\n",
    "def prepare_data(args, logger, r_state, num_classes, has_val, has_ul):\n",
    "    # used for experiments with injected noise\n",
    "\n",
    "    tokenizer = load_tokenizer(args)\n",
    "    tr_data, val_data = get_training_validation_set(args, logger, tokenizer, r_state, has_val, num_classes)\n",
    "    test_data = load_and_cache_text(args, tokenizer, logger, tag='test')\n",
    "\n",
    "    # Filter specific instance indices\n",
    "    train_indices = [1,2,3,4,5,6,100,101,102,103,104,105,451,452,453,454,455,456,501,502,503,504,505,506]\n",
    "    test_indices = [1,2,3,101,103,452,455,501,502,503,504,505]\n",
    "    val_indices = [0,1,2,6,7,8,12,13,14,18,19,20]\n",
    "\n",
    "    tr_data = {k: [v[i] for i in train_indices] for k, v in tr_data.items()}\n",
    "    val_data = {k: [v[i] for i in val_indices] for k, v in val_data.items()}\n",
    "    test_data = {k: [v[i] for i in test_indices] for k, v in test_data.items()}\n",
    "\n",
    "    n_set = TextBertDataset(args, tr_data, tokenizer, r_state, num_classes, make_noisy=True)\n",
    "    v_set = TextBertDataset(args, val_data, tokenizer, r_state, num_classes, make_noisy=True)\n",
    "    t_set = TextBertDataset(args, test_data, tokenizer, r_state, num_classes, make_noisy=False)\n",
    "\n",
    "    n_set_noisy_labels = copy.deepcopy(n_set.noisy_labels)\n",
    "    v_set_noisy_labels = copy.deepcopy(v_set.noisy_labels)\n",
    "    n_set_noisy_labels_hash = hash(tuple(n_set_noisy_labels))\n",
    "    v_set_noisy_labels_hash = hash(tuple(v_set_noisy_labels))\n",
    "    # wandb.run.summary[\"train_n_hash\"] = n_set_noisy_labels_hash\n",
    "    # wandb.run.summary[\"val_n_hash\"] = v_set_noisy_labels_hash\n",
    "\n",
    "    u_set = None\n",
    "    l2id = None\n",
    "    id2l = None\n",
    "\n",
    "    return n_set, u_set, v_set, t_set, l2id, id2l\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdde84a4",
   "metadata": {},
   "source": [
    "### Step 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e628657",
   "metadata": {},
   "source": [
    "Execution Start time: 18:10:27, End time: 18:10:27 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553c1cdc",
   "metadata": {},
   "source": [
    "**Thought**: The `prepare_data` function has been successfully modified to load specific instance indices for the train, test, and validation splits. Now, I need to adjust the number of training steps in the `main.py` script to 5. Let's open the `main.py` file again to make this change.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f694f44a",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0ea98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/BERT-LNL/main.py (163 lines total)]\n1:import argparse\n2:from loading_utils import prepare_data, prepare_af_data\n3:from utils import create_logger, save_args, create_trainer, load_config, save_config\n4:import numpy as np\n5:import torch\n6:import random\n7:\n8:\n9:def main():\n10:    parser = argparse.ArgumentParser()\n11:    parser.add_argument('--dataset', type=str, default='IMDB', choices=['SST-2', 'AG_News',\n12:                                                                        'TREC', 'IMDB', 'Yelp-5',\n13:                                                                        'Yoruba', 'Hausa'])\n14:    parser.add_argument('--data_root', type=str, default=\"\")\n15:    parser.add_argument('--log_root', type=str, default=\"\",\n16:                        help='output directory to save logs in training/testing')\n17:\n18:    parser.add_argument('--trainer_name', type=str, default='bert_wn',\n19:                        choices=['bert_wn', 'bert_ct', 'bert_cm', 'bert_cmgt', 'bert_smoothing'],\n20:                        help='trainer selection: '\n21:                             'bert_wn: without noise-handling,'\n22:                             'bert_ct: co-teaching, '\n23:                             'bert_cm: noise matrix, '\n24:                             'bert_cmgt: ground truth noise matrix,'\n25:                             'bert_smoothing: label smoothing')\n26:    parser.add_argument('--model_name', type=str, default='bert-base-uncased',\n27:                        choices=['bert-base-uncased', 'bert-base-cased',\n28:                                 'bert-large-uncased', 'bert-base-multilingual-cased'],\n29:                        help='backbone selection')\n30:    parser.add_argument('--exp_name', type=str, default='')\n31:\n32:\n33:    # Preprocessing Related\n34:    parser.add_argument('--max_sen_len', type=int, default=512,\n35:                        help='max sentence length, longer sentences will be truncated')\n36:    parser.add_argument('--special_token_offsets', type=int, default=2,\n37:                        help='number of special tokens used in bert tokenizer for text classification')\n38:    parser.add_argument('--truncate_mode', type=str, default='last',\n39:                        choices=['hybrid, last'], help='last: last 510 tokens, hybrid: first 128 + last 382')\n40:    parser.add_argument('--freeze_bert', action='store_true',\n41:                        help='freeze the bert backbone, i.e. use bert as feature extractor')\n42:\n43:    # BERT settings Related\n44:    parser.add_argument('--bert_dropout_rate', type=float, default=0.1)\n45:    parser.add_argument('--noise_level', type=float, default=0.0,\n46:                        help='noise level for injected noise')\n47:    parser.add_argument('--noise_type', default='uniform_m',\n48:                        choices=['uniform_m', 'sflip'],\n49:                        help='noise types: uniform_m: uniform noise, sflip: single-flip noise')\n50:    parser.add_argument('--val_fraction', type=float, default=0.1,\n51:                        help='if no validation set is provided, use this fraction of training set as validation set')\n52:\n53:    # training related\n54:    parser.add_argument('--num_epochs', type=int, default=1, help='set either num_epochs or num_training_steps')\n55:    parser.add_argument('--num_training_steps', type=int, default=-1, help='set it to -1 if num_epochs is set')\n56:    parser.add_argument('--train_eval_freq', type=int, default=10,\n57:                        help='evaluate the model on training set after every [train_eval_freq] training steps')\n58:    parser.add_argument('--eval_freq', type=int, default=50,\n59:                        help='evaluate the model on the validation and test sets'\n60:                             'after every [eval_freq] training steps')\n61:    parser.add_argument('--fast_eval', action='store_true',\n62:                        help='use 10% of the test set for evaluation, to speed up the evaluation prcoess')\n63:\n64:    parser.add_argument('--nl_batch_size', type=int, default=16,\n65:                        help='noisy labeled samples per batch, can be understood as the training batch size')\n66:    parser.add_argument('--eval_batch_size', type=int, default=50,\n67:                        help='evaluation batch size during testing')\n68:    parser.add_argument('--warmup_steps', type=int, default=0,\n69:                        help='warmup steps for learning rate scheduler')\n70:    parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n71:                        help=\"number of updates steps to accumulate before performing a backward/update pass.\")\n72:\n73:    parser.add_argument('--gen_val', action='store_true',\n74:                        help='generate validation set, enable it if there is no validation set')\n75:    parser.add_argument('--store_model', type=int, default=0, help='save models after training')\n76:\n77:\n78:    # co-teaching related\n79:    parser.add_argument('--forget_factor', type=float, default=1.0)\n80:    parser.add_argument('--T_k', type=int, default=10)\n81:    parser.add_argument('--c', type=float, default=1.0)\n82:\n83:\n84:    # smoothing trainer related\n85:    parser.add_argument('--smoothing_factor', type=float, default=0.2,\n86:                        help='label smoothing levels, 0.0 means no smoothing')\n87:\n88:\n89:    # cm trainer related\n90:    parser.add_argument('--cm_mse_weight', type=float, default=0.01,\n91:                        help='regularization factor for the mse loss, check https://aclanthology.org/N19-1328/')\n92:\n93:\n94:    # optimizer related\n95:    parser.add_argument('--lr', type=float, default=2e-5)\n96:    parser.add_argument(\"--weight_decay\", default=0.0, type=float,\n97:                        help=\"Weight deay if we apply some.\")\n98:    parser.add_argument('--exp_decay_rate', type=float, default=0.9998)\n99:    parser.add_argument('--max_grad_norm', type=float, default=1.0)\n100:    parser.add_argument('--patience', type=float, default=20, help='patience for early stopping')\n(63 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36b1100",
   "metadata": {},
   "source": [
    "### Step 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fac079a",
   "metadata": {},
   "source": [
    "Execution Start time: 18:10:29, End time: 18:10:30 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5d13d3",
   "metadata": {},
   "source": [
    "**Thought**: To set the number of training steps to 5, I need to modify the `num_training_steps` argument in the `main.py` script. Let's edit the file to make this change.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfd1cda",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36dabaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\n\nERRORS:\n- E999 IndentationError: unexpected indent\n\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /content/BERT-LNL/main.py (163 lines total)]\n(49 more lines above)\n49:                        help='noise types: uniform_m: uniform noise, sflip: single-flip noise')\n50:    parser.add_argument('--val_fraction', type=float, default=0.1,\n51:                        help='if no validation set is provided, use this fraction of training set as validation set')\n52:\n53:    # training related\n54:    parser.add_argument('--num_epochs', type=int, default=1, help='set either num_epochs or num_training_steps')\n55:parser.add_argument('--num_training_steps', type=int, default=5, help='set it to -1 if num_epochs is set')\n56:    parser.add_argument('--train_eval_freq', type=int, default=10,\n57:                        help='evaluate the model on training set after every [train_eval_freq] training steps')\n58:    parser.add_argument('--eval_freq', type=int, default=50,\n59:                        help='evaluate the model on the validation and test sets'\n(104 more lines below)\n-------------------------------------------------\n\nThis is the original code before your edit\n-------------------------------------------------\n[File: /content/BERT-LNL/main.py (163 lines total)]\n(50 more lines above)\n50:    parser.add_argument('--val_fraction', type=float, default=0.1,\n51:                        help='if no validation set is provided, use this fraction of training set as validation set')\n52:\n53:    # training related\n54:    parser.add_argument('--num_epochs', type=int, default=1, help='set either num_epochs or num_training_steps')\n55:    parser.add_argument('--num_training_steps', type=int, default=-1, help='set it to -1 if num_epochs is set')\n56:    parser.add_argument('--train_eval_freq', type=int, default=10,\n57:                        help='evaluate the model on training set after every [train_eval_freq] training steps')\n58:    parser.add_argument('--eval_freq', type=int, default=50,\n59:                        help='evaluate the model on the validation and test sets'\n60:                             'after every [eval_freq] training steps')\n(103 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 55:55\n",
    "parser.add_argument('--num_training_steps', type=int, default=5, help='set it to -1 if num_epochs is set')\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd818e0e",
   "metadata": {},
   "source": [
    "### Step 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d22848e",
   "metadata": {},
   "source": [
    "Execution Start time: 18:10:33, End time: 18:10:34 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb55fb34",
   "metadata": {},
   "source": [
    "**Thought**: The error occurred due to incorrect indentation. Let's correct the indentation for the `num_training_steps` argument in the `main.py` script. I'll ensure the line is properly aligned with the surrounding code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec56c5ae",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933c20be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/BERT-LNL/main.py (163 lines total)]\n(4 more lines above)\n5:import torch\n6:import random\n7:\n8:\n9:def main():\n10:    parser = argparse.ArgumentParser()\n11:    parser.add_argument('--dataset', type=str, default='IMDB', choices=['SST-2', 'AG_News',\n12:                                                                        'TREC', 'IMDB', 'Yelp-5',\n13:                                                                        'Yoruba', 'Hausa'])\n14:    parser.add_argument('--data_root', type=str, default=\"\")\n15:    parser.add_argument('--log_root', type=str, default=\"\",\n16:                        help='output directory to save logs in training/testing')\n17:\n18:    parser.add_argument('--trainer_name', type=str, default='bert_wn',\n19:                        choices=['bert_wn', 'bert_ct', 'bert_cm', 'bert_cmgt', 'bert_smoothing'],\n20:                        help='trainer selection: '\n21:                             'bert_wn: without noise-handling,'\n22:                             'bert_ct: co-teaching, '\n23:                             'bert_cm: noise matrix, '\n24:                             'bert_cmgt: ground truth noise matrix,'\n25:                             'bert_smoothing: label smoothing')\n26:    parser.add_argument('--model_name', type=str, default='bert-base-uncased',\n27:                        choices=['bert-base-uncased', 'bert-base-cased',\n28:                                 'bert-large-uncased', 'bert-base-multilingual-cased'],\n29:                        help='backbone selection')\n30:    parser.add_argument('--exp_name', type=str, default='')\n31:\n32:\n33:    # Preprocessing Related\n34:    parser.add_argument('--max_sen_len', type=int, default=512,\n35:                        help='max sentence length, longer sentences will be truncated')\n36:    parser.add_argument('--special_token_offsets', type=int, default=2,\n37:                        help='number of special tokens used in bert tokenizer for text classification')\n38:    parser.add_argument('--truncate_mode', type=str, default='last',\n39:                        choices=['hybrid, last'], help='last: last 510 tokens, hybrid: first 128 + last 382')\n40:    parser.add_argument('--freeze_bert', action='store_true',\n41:                        help='freeze the bert backbone, i.e. use bert as feature extractor')\n42:\n43:    # BERT settings Related\n44:    parser.add_argument('--bert_dropout_rate', type=float, default=0.1)\n45:    parser.add_argument('--noise_level', type=float, default=0.0,\n46:                        help='noise level for injected noise')\n47:    parser.add_argument('--noise_type', default='uniform_m',\n48:                        choices=['uniform_m', 'sflip'],\n49:                        help='noise types: uniform_m: uniform noise, sflip: single-flip noise')\n50:    parser.add_argument('--val_fraction', type=float, default=0.1,\n51:                        help='if no validation set is provided, use this fraction of training set as validation set')\n52:\n53:    # training related\n54:    parser.add_argument('--num_epochs', type=int, default=1, help='set either num_epochs or num_training_steps')\n55:    parser.add_argument('--num_training_steps', type=int, default=5, help='set it to -1 if num_epochs is set')\n56:    parser.add_argument('--train_eval_freq', type=int, default=10,\n57:                        help='evaluate the model on training set after every [train_eval_freq] training steps')\n58:    parser.add_argument('--eval_freq', type=int, default=50,\n59:                        help='evaluate the model on the validation and test sets'\n60:                             'after every [eval_freq] training steps')\n61:    parser.add_argument('--fast_eval', action='store_true',\n62:                        help='use 10% of the test set for evaluation, to speed up the evaluation prcoess')\n63:\n64:    parser.add_argument('--nl_batch_size', type=int, default=16,\n65:                        help='noisy labeled samples per batch, can be understood as the training batch size')\n66:    parser.add_argument('--eval_batch_size', type=int, default=50,\n67:                        help='evaluation batch size during testing')\n68:    parser.add_argument('--warmup_steps', type=int, default=0,\n69:                        help='warmup steps for learning rate scheduler')\n70:    parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n71:                        help=\"number of updates steps to accumulate before performing a backward/update pass.\")\n72:\n73:    parser.add_argument('--gen_val', action='store_true',\n74:                        help='generate validation set, enable it if there is no validation set')\n75:    parser.add_argument('--store_model', type=int, default=0, help='save models after training')\n76:\n77:\n78:    # co-teaching related\n79:    parser.add_argument('--forget_factor', type=float, default=1.0)\n80:    parser.add_argument('--T_k', type=int, default=10)\n81:    parser.add_argument('--c', type=float, default=1.0)\n82:\n83:\n84:    # smoothing trainer related\n85:    parser.add_argument('--smoothing_factor', type=float, default=0.2,\n86:                        help='label smoothing levels, 0.0 means no smoothing')\n87:\n88:\n89:    # cm trainer related\n90:    parser.add_argument('--cm_mse_weight', type=float, default=0.01,\n91:                        help='regularization factor for the mse loss, check https://aclanthology.org/N19-1328/')\n92:\n93:\n94:    # optimizer related\n95:    parser.add_argument('--lr', type=float, default=2e-5)\n96:    parser.add_argument(\"--weight_decay\", default=0.0, type=float,\n97:                        help=\"Weight deay if we apply some.\")\n98:    parser.add_argument('--exp_decay_rate', type=float, default=0.9998)\n99:    parser.add_argument('--max_grad_norm', type=float, default=1.0)\n100:    parser.add_argument('--patience', type=float, default=20, help='patience for early stopping')\n101:\n102:    # hardware related\n103:    parser.add_argument('--use_cuda', type=bool, default=True)\n104:    parser.add_argument('--num_workers', type=int, default=0)\n(59 more lines below)\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 55:55\n",
    "    parser.add_argument('--num_training_steps', type=int, default=5, help='set it to -1 if num_epochs is set')\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17b136f",
   "metadata": {},
   "source": [
    "### Step 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacda3f8",
   "metadata": {},
   "source": [
    "Execution Start time: 18:10:38, End time: 18:10:43 (duration: 5 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c4d092",
   "metadata": {},
   "source": [
    "**Thought**: The `num_training_steps` has been successfully set to 5 in the `main.py` script. Now, I need to run the `main.py` script to train the BERT-WN model on the AG-News dataset with the specified settings. Let's execute the script and evaluate the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00671a25",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186b017a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Traceback (most recent call last):\n  File \"/content/BERT-LNL/main.py\", line 2, in <module>\n    from loading_utils import prepare_data, prepare_af_data\n  File \"/content/BERT-LNL/loading_utils.py\", line 6, in <module>\n    import wandb\nModuleNotFoundError: No module named 'wandb'\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python main.py --dataset AG_News --noise_level 0.2 --noise_type sflip --trainer_name bert_wn --model_name bert-base-uncased --num_training_steps 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5b7ce8",
   "metadata": {},
   "source": [
    "### Step 19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e72118e",
   "metadata": {},
   "source": [
    "Execution Start time: 18:10:45, End time: 18:10:52 (duration: 7 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1442c709",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the `wandb` module is not installed. This module is used for experiment tracking and logging. Let's install the `wandb` package to resolve this issue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cbc7d4",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a3799a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting wandb\n  Downloading wandb-0.17.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/site-packages (from wandb) (8.1.7)\nCollecting docker-pycreds>=0.4.0 (from wandb)\n  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\nCollecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/site-packages (from wandb) (4.2.2)\nRequirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/site-packages (from wandb) (5.27.1)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/site-packages (from wandb) (5.9.8)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/site-packages (from wandb) (2.32.3)\nCollecting sentry-sdk>=1.0.0 (from wandb)\n  Downloading sentry_sdk-2.13.0-py2.py3-none-any.whl.metadata (9.7 kB)\nCollecting setproctitle (from wandb)\n  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from wandb) (68.1.2)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nCollecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.6.2)\nCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\nDownloading wandb-0.17.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.3 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/9.3 MB ? eta -:--:--\n\n\n\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\nDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/207.3 kB ? eta -:--:--\n\n\u001b[?25hDownloading sentry_sdk-2.13.0-py2.py3-none-any.whl (309 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/309.1 kB ? eta -:--:--\n\n\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\nDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/62.7 kB ? eta -:--:--\n\n\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\nInstalling collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\nSuccessfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.13.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.7\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b32c4ef",
   "metadata": {},
   "source": [
    "### Step 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1002d2b5",
   "metadata": {},
   "source": [
    "Execution Start time: 18:10:54, End time: 18:11:01 (duration: 6 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2d3251",
   "metadata": {},
   "source": [
    "**Thought**: The `wandb` package has been installed successfully. Now, let's rerun the `main.py` script to train the BERT-WN model on the AG-News dataset with the specified settings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedf16ec",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50796fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INFO Training started\nINFO log dir: 08_20_18_10_59_sflip_nle0.2_nlb16\nINFO loading AG_News...\n/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\ntokenizer_config.json:   0%|                         | 0.00/48.0 [00:00<?, ?B/s]\ntokenizer_config.json: 100%|██████████████████| 48.0/48.0 [00:00<00:00, 388kB/s]\n\nconfig.json:   0%|                                    | 0.00/570 [00:00<?, ?B/s]\n\n\nvocab.txt:   0%|                                     | 0.00/232k [00:00<?, ?B/s]\n\n\ntokenizer.json:   0%|                                | 0.00/466k [00:00<?, ?B/s]\n\nTraceback (most recent call last):\n  File \"/content/BERT-LNL/main.py\", line 163, in <module>\n    main()\n  File \"/content/BERT-LNL/main.py\", line 145, in main\n    nl_set, ul_set, v_set, t_set, l2id, id2l = prepare_data(args, logger, r_state, num_classes, has_val, has_ul)\n  File \"/content/BERT-LNL/loading_utils.py\", line 18, in prepare_data\n    tr_data, val_data = get_training_validation_set(args, logger, tokenizer, r_state, has_val, num_classes)\n  File \"/content/BERT-LNL/loading_utils.py\", line 69, in get_training_validation_set\n    tr_data = load_and_cache_text(args, tokenizer, logger, tag='train')\n  File \"/content/BERT-LNL/loading_utils.py\", line 125, in load_and_cache_text\n    docs = read_txt(input_path)\n  File \"/content/BERT-LNL/loading_utils.py\", line 162, in read_txt\n    with open(file_path, 'r', encoding='utf-8') as f:\nFileNotFoundError: [Errno 2] No such file or directory: 'AG_News/txt_data/train.txt'\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python main.py --dataset AG_News --noise_level 0.2 --noise_type sflip --trainer_name bert_wn --model_name bert-base-uncased --num_training_steps 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d9d810",
   "metadata": {},
   "source": [
    "### Step 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f10bf6",
   "metadata": {},
   "source": [
    "Execution Start time: 18:11:04, End time: 18:11:04 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7528648",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the script is trying to read a file `train.txt` from a directory `AG_News/txt_data`, which does not exist. Since we are using the Hugging Face datasets library, we need to adjust the script to load the dataset directly from the library instead of expecting local files. Let's modify the `loading_utils.py` to load the AG-News dataset using the `datasets` library.\n",
    "\n",
    "I'll open the `loading_utils.py` file to make these changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303b781a",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eeec82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/BERT-LNL/loading_utils.py (205 lines total)]\n1:import os\n2:import copy\n3:import numpy as np\n4:import pickle\n5:import torch\n6:import wandb\n7:from tqdm import tqdm\n8:from text_dataset import TextBertDataset, TextBertRealDataset\n9:from transformers import AutoTokenizer\n10:from transformers import BertTokenizerFast, BertTokenizer\n11:import utils\n12:\n13:\n14:def prepare_data(args, logger, r_state, num_classes, has_val, has_ul):\n15:    # used for experiments with injected noise\n16:\n17:    tokenizer = load_tokenizer(args)\n18:    tr_data, val_data = get_training_validation_set(args, logger, tokenizer, r_state, has_val, num_classes)\n19:    test_data = load_and_cache_text(args, tokenizer, logger, tag='test')\n20:\n21:    # Filter specific instance indices\n22:    train_indices = [1,2,3,4,5,6,100,101,102,103,104,105,451,452,453,454,455,456,501,502,503,504,505,506]\n23:    test_indices = [1,2,3,101,103,452,455,501,502,503,504,505]\n24:    val_indices = [0,1,2,6,7,8,12,13,14,18,19,20]\n25:\n26:    tr_data = {k: [v[i] for i in train_indices] for k, v in tr_data.items()}\n27:    val_data = {k: [v[i] for i in val_indices] for k, v in val_data.items()}\n28:    test_data = {k: [v[i] for i in test_indices] for k, v in test_data.items()}\n29:\n30:    n_set = TextBertDataset(args, tr_data, tokenizer, r_state, num_classes, make_noisy=True)\n31:    v_set = TextBertDataset(args, val_data, tokenizer, r_state, num_classes, make_noisy=True)\n32:    t_set = TextBertDataset(args, test_data, tokenizer, r_state, num_classes, make_noisy=False)\n33:\n34:    n_set_noisy_labels = copy.deepcopy(n_set.noisy_labels)\n35:    v_set_noisy_labels = copy.deepcopy(v_set.noisy_labels)\n36:    n_set_noisy_labels_hash = hash(tuple(n_set_noisy_labels))\n37:    v_set_noisy_labels_hash = hash(tuple(v_set_noisy_labels))\n38:    # wandb.run.summary[\"train_n_hash\"] = n_set_noisy_labels_hash\n39:    # wandb.run.summary[\"val_n_hash\"] = v_set_noisy_labels_hash\n40:\n41:    u_set = None\n42:    l2id = None\n43:    id2l = None\n44:\n45:    return n_set, u_set, v_set, t_set, l2id, id2l\n46:\n47:\n48:def prepare_af_data(args, logger, num_classes, has_ul):\n49:    tokenizer = load_tokenizer(args)\n50:    n_set = get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag='train')\n51:    v_set = get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag='validation')\n52:    t_set = get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag='test')\n53:\n54:    assert not has_ul  # we do not have unlabeled data in Yoruba and Hausa dataset\n55:    u_set = None\n56:\n57:    label_mapping_data_dir = os.path.join(args.data_root, args.dataset, 'txt_data')\n58:    l2id = utils.pickle_load(os.path.join(label_mapping_data_dir, 'l2idx.pickle'))\n59:    id2l = utils.pickle_load(os.path.join(label_mapping_data_dir, 'idx2l.pickle'))\n60:\n61:    return n_set, u_set, v_set, t_set, l2id, id2l\n62:\n63:\n64:def get_training_validation_set(args, logger, tokenizer, r_state, has_val, num_classes):\n65:    # sanity check: args.gen_val is used when there is no validation set\n66:    if has_val:\n67:        assert not args.gen_val\n68:\n69:    tr_data = load_and_cache_text(args, tokenizer, logger, tag='train')\n70:\n71:    if has_val:  # original validation set available\n72:        val_data = load_and_cache_text(args, tokenizer, logger, tag='validation')\n73:    elif args.gen_val:  # create validation set using the training set\n74:        val_indices_path = os.path.join(args.data_root, args.dataset, 'val_indices', f'{args.dataset}_val_indices.pickle')\n75:        with open(val_indices_path, 'rb') as handle:\n76:            val_indices = pickle.load(handle)\n77:\n78:        val_mask = np.zeros(len(tr_data['labels']), dtype=bool)\n79:        val_mask[val_indices] = True\n80:\n81:        val_features = {k: v[val_mask] for k,v in tr_data['features'].items()}\n82:        val_labels = tr_data['labels'][val_mask]\n83:        val_text  = np.array(tr_data['text'])[val_mask]\n84:\n85:        train_features = {k: v[~val_mask] for k,v in tr_data['features'].items()}\n86:        train_labels = tr_data['labels'][~val_mask]\n87:        train_text  = np.array(tr_data['text'])[~val_mask]\n88:\n89:        val_data = {\"features\": val_features, \"labels\": val_labels, \"text\": val_text}\n90:        tr_data = {\"features\": train_features, \"labels\": train_labels, \"text\": train_text}\n91:\n92:    else:\n93:        raise ValueError(\"we need a validation set, set gen_val to True to extract\"\n94:                         \"a subset from the training data as validation data\")\n95:\n96:    return tr_data, val_data\n97:\n98:def get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag):\n99:    noisy_data_tag = f'{tag}_clean'\n100:\n(105 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open loading_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1696a1",
   "metadata": {},
   "source": [
    "### Step 22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f83414",
   "metadata": {},
   "source": [
    "Execution Start time: 18:11:14, End time: 18:11:15 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f210226b",
   "metadata": {},
   "source": [
    "**Thought**: To resolve the issue, I need to modify the `get_training_validation_set` and `load_and_cache_text` functions to load the AG-News dataset using the `datasets` library. I'll replace the file loading logic with the dataset loading logic from the `datasets` library. Let's edit the `loading_utils.py` file to make these changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83ed328",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4419e01a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/BERT-LNL/loading_utils.py (186 lines total)]\n(13 more lines above)\n14:def prepare_data(args, logger, r_state, num_classes, has_val, has_ul):\n15:    # used for experiments with injected noise\n16:\n17:    tokenizer = load_tokenizer(args)\n18:    tr_data, val_data = get_training_validation_set(args, logger, tokenizer, r_state, has_val, num_classes)\n19:    test_data = load_and_cache_text(args, tokenizer, logger, tag='test')\n20:\n21:    # Filter specific instance indices\n22:    train_indices = [1,2,3,4,5,6,100,101,102,103,104,105,451,452,453,454,455,456,501,502,503,504,505,506]\n23:    test_indices = [1,2,3,101,103,452,455,501,502,503,504,505]\n24:    val_indices = [0,1,2,6,7,8,12,13,14,18,19,20]\n25:\n26:    tr_data = {k: [v[i] for i in train_indices] for k, v in tr_data.items()}\n27:    val_data = {k: [v[i] for i in val_indices] for k, v in val_data.items()}\n28:    test_data = {k: [v[i] for i in test_indices] for k, v in test_data.items()}\n29:\n30:    n_set = TextBertDataset(args, tr_data, tokenizer, r_state, num_classes, make_noisy=True)\n31:    v_set = TextBertDataset(args, val_data, tokenizer, r_state, num_classes, make_noisy=True)\n32:    t_set = TextBertDataset(args, test_data, tokenizer, r_state, num_classes, make_noisy=False)\n33:\n34:    n_set_noisy_labels = copy.deepcopy(n_set.noisy_labels)\n35:    v_set_noisy_labels = copy.deepcopy(v_set.noisy_labels)\n36:    n_set_noisy_labels_hash = hash(tuple(n_set_noisy_labels))\n37:    v_set_noisy_labels_hash = hash(tuple(v_set_noisy_labels))\n38:    # wandb.run.summary[\"train_n_hash\"] = n_set_noisy_labels_hash\n39:    # wandb.run.summary[\"val_n_hash\"] = v_set_noisy_labels_hash\n40:\n41:    u_set = None\n42:    l2id = None\n43:    id2l = None\n44:\n45:    return n_set, u_set, v_set, t_set, l2id, id2l\n46:\n47:\n48:def prepare_af_data(args, logger, num_classes, has_ul):\n49:    tokenizer = load_tokenizer(args)\n50:    n_set = get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag='train')\n51:    v_set = get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag='validation')\n52:    t_set = get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag='test')\n53:\n54:    assert not has_ul  # we do not have unlabeled data in Yoruba and Hausa dataset\n55:    u_set = None\n56:\n57:    label_mapping_data_dir = os.path.join(args.data_root, args.dataset, 'txt_data')\n58:    l2id = utils.pickle_load(os.path.join(label_mapping_data_dir, 'l2idx.pickle'))\n59:    id2l = utils.pickle_load(os.path.join(label_mapping_data_dir, 'idx2l.pickle'))\n60:\n61:    return n_set, u_set, v_set, t_set, l2id, id2l\n62:\n63:\n64:from datasets import load_dataset\n65:\n66:def get_training_validation_set(args, logger, tokenizer, r_state, has_val, num_classes):\n67:    # Load the AG-News dataset using the datasets library\n68:    dataset = load_dataset('ag_news')\n69:\n70:    # Filter specific instance indices\n71:    train_indices = [1,2,3,4,5,6,100,101,102,103,104,105,451,452,453,454,455,456,501,502,503,504,505,506]\n72:    val_indices = [0,1,2,6,7,8,12,13,14,18,19,20]\n73:\n74:    tr_data = dataset['train'].select(train_indices)\n75:    val_data = dataset['validation'].select(val_indices)\n76:\n77:    return tr_data, val_data\n78:\n79:def get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag):\n80:    noisy_data_tag = f'{tag}_clean'\n81:\n82:    #get text data with noisy labels\n83:    clean_noisy_data = load_and_cache_text(args, tokenizer, logger, tag=noisy_data_tag)\n84:\n85:    #get the clean training and the clean validation sets\n86:    txt_data_dir = os.path.join(args.data_root, args.dataset, 'txt_data')\n87:    input_path = os.path.join(txt_data_dir, f'{tag}_clean_noisy_labels.pickle')\n88:    noisy_labels = load_pickle_data(input_path)\n89:\n90:    td = TextBertRealDataset(args, clean_noisy_data, noisy_labels, tokenizer, num_classes)\n91:    return td\n92:\n93:\n94:\n95:def load_and_cache_text(args, tokenizer, logger, tag):\n96:    cached_features_dir = os.path.join(args.data_root, args.dataset, 'bert_preprocessed') # cache dir (output dir)\n97:    txt_data_dir = os.path.join(args.data_root, args.dataset, 'txt_data') # input file dir\n98:\n99:    if not os.path.exists(cached_features_dir):\n100:        os.makedirs(cached_features_dir)\n101:\n102:    cached_features_path = os.path.join(cached_features_dir,\n103:                                        f'{tag}_trun_{args.truncate_mode}_maxl_{args.max_sen_len}')\n104:    input_path = os.path.join(txt_data_dir, f'{tag}.txt')\n105:\n106:    docs = read_txt(input_path)\n107:\n108:    if os.path.exists(cached_features_path):\n109:        logger.info(f'[Loading and Caching] loading from cache...')\n110:        features = torch.load(cached_features_path)\n111:    else:\n112:        logger.info(f'[Loading and Caching] number of documents = {len(docs)}')\n113:        logger.info(f'[Loading and Caching] convert text to features...')\n(73 more lines below)\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 64:96\n",
    "from datasets import load_dataset\n",
    "\n",
    "def get_training_validation_set(args, logger, tokenizer, r_state, has_val, num_classes):\n",
    "    # Load the AG-News dataset using the datasets library\n",
    "    dataset = load_dataset('ag_news')\n",
    "\n",
    "    # Filter specific instance indices\n",
    "    train_indices = [1,2,3,4,5,6,100,101,102,103,104,105,451,452,453,454,455,456,501,502,503,504,505,506]\n",
    "    val_indices = [0,1,2,6,7,8,12,13,14,18,19,20]\n",
    "\n",
    "    tr_data = dataset['train'].select(train_indices)\n",
    "    val_data = dataset['validation'].select(val_indices)\n",
    "\n",
    "    return tr_data, val_data\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dceeba",
   "metadata": {},
   "source": [
    "### Step 23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d68012",
   "metadata": {},
   "source": [
    "Execution Start time: 18:11:19, End time: 18:11:21 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0475ab83",
   "metadata": {},
   "source": [
    "**Thought**: The `get_training_validation_set` function has been successfully updated to load the AG-News dataset using the `datasets` library. Now, I need to modify the `load_and_cache_text` function to load the test split using the same library. Let's proceed with editing the `load_and_cache_text` function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a12435",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7781816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\n\nERRORS:\n- F821 undefined name 'cached_features_path'\n- F821 undefined name 'cached_features_path'\n- F821 undefined name 'docs'\n- F821 undefined name 'docs'\n- F821 undefined name 'cached_features_path'\n- F821 undefined name 'txt_data_dir'\n- F821 undefined name 'docs'\n\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /content/BERT-LNL/loading_utils.py (184 lines total)]\n(89 more lines above)\n90:    td = TextBertRealDataset(args, clean_noisy_data, noisy_labels, tokenizer, num_classes)\n91:    return td\n92:\n93:\n94:\n95:def load_and_cache_text(args, tokenizer, logger, tag):\n96:    # Load the AG-News dataset using the datasets library\n97:    dataset = load_dataset('ag_news')\n98:\n99:    # Filter specific instance indices\n100:    test_indices = [1,2,3,101,103,452,455,501,502,503,504,505]\n101:\n102:    test_data = dataset['test'].select(test_indices)\n103:\n104:    return test_data\n105:\n106:    if os.path.exists(cached_features_path):\n107:        logger.info(f'[Loading and Caching] loading from cache...')\n108:        features = torch.load(cached_features_path)\n109:    else:\n(75 more lines below)\n-------------------------------------------------\n\nThis is the original code before your edit\n-------------------------------------------------\n[File: /content/BERT-LNL/loading_utils.py (186 lines total)]\n(89 more lines above)\n90:    td = TextBertRealDataset(args, clean_noisy_data, noisy_labels, tokenizer, num_classes)\n91:    return td\n92:\n93:\n94:\n95:def load_and_cache_text(args, tokenizer, logger, tag):\n96:    cached_features_dir = os.path.join(args.data_root, args.dataset, 'bert_preprocessed') # cache dir (output dir)\n97:    txt_data_dir = os.path.join(args.data_root, args.dataset, 'txt_data') # input file dir\n98:\n99:    if not os.path.exists(cached_features_dir):\n100:        os.makedirs(cached_features_dir)\n101:\n102:    cached_features_path = os.path.join(cached_features_dir,\n103:                                        f'{tag}_trun_{args.truncate_mode}_maxl_{args.max_sen_len}')\n104:    input_path = os.path.join(txt_data_dir, f'{tag}.txt')\n105:\n106:    docs = read_txt(input_path)\n107:\n108:    if os.path.exists(cached_features_path):\n109:        logger.info(f'[Loading and Caching] loading from cache...')\n110:        features = torch.load(cached_features_path)\n111:    else:\n(75 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 95:106\n",
    "def load_and_cache_text(args, tokenizer, logger, tag):\n",
    "    # Load the AG-News dataset using the datasets library\n",
    "    dataset = load_dataset('ag_news')\n",
    "\n",
    "    # Filter specific instance indices\n",
    "    test_indices = [1,2,3,101,103,452,455,501,502,503,504,505]\n",
    "\n",
    "    test_data = dataset['test'].select(test_indices)\n",
    "\n",
    "    return test_data\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59116b3",
   "metadata": {},
   "source": [
    "### Step 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563c6c85",
   "metadata": {},
   "source": [
    "Execution Start time: 18:11:27, End time: 18:11:28 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aafb73f",
   "metadata": {},
   "source": [
    "**Thought**: The error occurred because I didn't remove the existing code that references undefined variables. Let's correct this by replacing the entire function with the new logic for loading the test data using the `datasets` library. I'll ensure the previous logic is completely replaced.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdc3ab8",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c16ac29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\n\nERRORS:\n- F821 undefined name 'cached_features_path'\n- F821 undefined name 'cached_features_path'\n- F821 undefined name 'docs'\n- F821 undefined name 'docs'\n- F821 undefined name 'cached_features_path'\n- F821 undefined name 'txt_data_dir'\n- F821 undefined name 'docs'\n\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /content/BERT-LNL/loading_utils.py (184 lines total)]\n(89 more lines above)\n90:    td = TextBertRealDataset(args, clean_noisy_data, noisy_labels, tokenizer, num_classes)\n91:    return td\n92:\n93:\n94:\n95:def load_and_cache_text(args, tokenizer, logger, tag):\n96:    # Load the AG-News dataset using the datasets library\n97:    dataset = load_dataset('ag_news')\n98:\n99:    # Filter specific instance indices\n100:    test_indices = [1,2,3,101,103,452,455,501,502,503,504,505]\n101:\n102:    test_data = dataset['test'].select(test_indices)\n103:\n104:    return test_data\n105:\n106:    if os.path.exists(cached_features_path):\n107:        logger.info(f'[Loading and Caching] loading from cache...')\n108:        features = torch.load(cached_features_path)\n109:    else:\n(75 more lines below)\n-------------------------------------------------\n\nThis is the original code before your edit\n-------------------------------------------------\n[File: /content/BERT-LNL/loading_utils.py (186 lines total)]\n(89 more lines above)\n90:    td = TextBertRealDataset(args, clean_noisy_data, noisy_labels, tokenizer, num_classes)\n91:    return td\n92:\n93:\n94:\n95:def load_and_cache_text(args, tokenizer, logger, tag):\n96:    cached_features_dir = os.path.join(args.data_root, args.dataset, 'bert_preprocessed') # cache dir (output dir)\n97:    txt_data_dir = os.path.join(args.data_root, args.dataset, 'txt_data') # input file dir\n98:\n99:    if not os.path.exists(cached_features_dir):\n100:        os.makedirs(cached_features_dir)\n101:\n102:    cached_features_path = os.path.join(cached_features_dir,\n103:                                        f'{tag}_trun_{args.truncate_mode}_maxl_{args.max_sen_len}')\n104:    input_path = os.path.join(txt_data_dir, f'{tag}.txt')\n105:\n106:    docs = read_txt(input_path)\n107:\n108:    if os.path.exists(cached_features_path):\n109:        logger.info(f'[Loading and Caching] loading from cache...')\n110:        features = torch.load(cached_features_path)\n111:    else:\n(75 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 95:106\n",
    "def load_and_cache_text(args, tokenizer, logger, tag):\n",
    "    # Load the AG-News dataset using the datasets library\n",
    "    dataset = load_dataset('ag_news')\n",
    "\n",
    "    # Filter specific instance indices\n",
    "    test_indices = [1,2,3,101,103,452,455,501,502,503,504,505]\n",
    "\n",
    "    test_data = dataset['test'].select(test_indices)\n",
    "\n",
    "    return test_data\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dc742e",
   "metadata": {},
   "source": [
    "### Step 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cb5fe6",
   "metadata": {},
   "source": [
    "Execution Start time: 18:11:31, End time: 18:11:32 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c434ba",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that there are undefined variables because I didn't remove the existing code properly. Let's ensure the previous logic is completely replaced with the new logic for loading the test data using the `datasets` library. I'll correct this by removing the unnecessary lines.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e512f2db",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d94cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\n\nERRORS:\n- E999 IndentationError: unexpected indent\n\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /content/BERT-LNL/loading_utils.py (179 lines total)]\n(89 more lines above)\n90:    td = TextBertRealDataset(args, clean_noisy_data, noisy_labels, tokenizer, num_classes)\n91:    return td\n92:\n93:\n94:\n95:def load_and_cache_text(args, tokenizer, logger, tag):\n96:    # Load the AG-News dataset using the datasets library\n97:    dataset = load_dataset('ag_news')\n98:\n99:    # Filter specific instance indices\n100:    test_indices = [1,2,3,101,103,452,455,501,502,503,504,505]\n101:\n102:    test_data = dataset['test'].select(test_indices)\n103:\n104:    return test_data\n105:        logger.info(f'[Loading and Caching] number of documents = {len(docs)}')\n106:        logger.info(f'[Loading and Caching] convert text to features...')\n107:        features = get_input_features(docs, tokenizer, args)\n108:        logger.info(\"[Loading and Caching] saving/caching the features...\")\n109:        torch.save(features, cached_features_path)\n(70 more lines below)\n-------------------------------------------------\n\nThis is the original code before your edit\n-------------------------------------------------\n[File: /content/BERT-LNL/loading_utils.py (186 lines total)]\n(90 more lines above)\n90:    td = TextBertRealDataset(args, clean_noisy_data, noisy_labels, tokenizer, num_classes)\n91:    return td\n92:\n93:\n94:\n95:def load_and_cache_text(args, tokenizer, logger, tag):\n96:    cached_features_dir = os.path.join(args.data_root, args.dataset, 'bert_preprocessed') # cache dir (output dir)\n97:    txt_data_dir = os.path.join(args.data_root, args.dataset, 'txt_data') # input file dir\n98:\n99:    if not os.path.exists(cached_features_dir):\n100:        os.makedirs(cached_features_dir)\n101:\n102:    cached_features_path = os.path.join(cached_features_dir,\n103:                                        f'{tag}_trun_{args.truncate_mode}_maxl_{args.max_sen_len}')\n104:    input_path = os.path.join(txt_data_dir, f'{tag}.txt')\n105:\n106:    docs = read_txt(input_path)\n107:\n108:    if os.path.exists(cached_features_path):\n109:        logger.info(f'[Loading and Caching] loading from cache...')\n110:        features = torch.load(cached_features_path)\n111:    else:\n112:        logger.info(f'[Loading and Caching] number of documents = {len(docs)}')\n113:        logger.info(f'[Loading and Caching] convert text to features...')\n114:        features = get_input_features(docs, tokenizer, args)\n115:        logger.info(\"[Loading and Caching] saving/caching the features...\")\n116:        torch.save(features, cached_features_path)\n(70 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 95:111\n",
    "def load_and_cache_text(args, tokenizer, logger, tag):\n",
    "    # Load the AG-News dataset using the datasets library\n",
    "    dataset = load_dataset('ag_news')\n",
    "\n",
    "    # Filter specific instance indices\n",
    "    test_indices = [1,2,3,101,103,452,455,501,502,503,504,505]\n",
    "\n",
    "    test_data = dataset['test'].select(test_indices)\n",
    "\n",
    "    return test_data\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eee849",
   "metadata": {},
   "source": [
    "### Step 26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7648a2f",
   "metadata": {},
   "source": [
    "Execution Start time: 18:11:37, End time: 18:11:38 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf8fe0e",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates an unexpected indent. Let's correct the indentation and ensure the previous logic is completely replaced with the new logic for loading the test data using the `datasets` library. I'll make sure the indentation is consistent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4459c545",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da75a14c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\n\nERRORS:\n- E999 IndentationError: unexpected indent\n\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /content/BERT-LNL/loading_utils.py (179 lines total)]\n(89 more lines above)\n90:    td = TextBertRealDataset(args, clean_noisy_data, noisy_labels, tokenizer, num_classes)\n91:    return td\n92:\n93:\n94:\n95:def load_and_cache_text(args, tokenizer, logger, tag):\n96:    # Load the AG-News dataset using the datasets library\n97:    dataset = load_dataset('ag_news')\n98:\n99:    # Filter specific instance indices\n100:    test_indices = [1,2,3,101,103,452,455,501,502,503,504,505]\n101:\n102:    test_data = dataset['test'].select(test_indices)\n103:\n104:    return test_data\n105:        logger.info(f'[Loading and Caching] number of documents = {len(docs)}')\n106:        logger.info(f'[Loading and Caching] convert text to features...')\n107:        features = get_input_features(docs, tokenizer, args)\n108:        logger.info(\"[Loading and Caching] saving/caching the features...\")\n109:        torch.save(features, cached_features_path)\n(70 more lines below)\n-------------------------------------------------\n\nThis is the original code before your edit\n-------------------------------------------------\n[File: /content/BERT-LNL/loading_utils.py (186 lines total)]\n(90 more lines above)\n90:    td = TextBertRealDataset(args, clean_noisy_data, noisy_labels, tokenizer, num_classes)\n91:    return td\n92:\n93:\n94:\n95:def load_and_cache_text(args, tokenizer, logger, tag):\n96:    cached_features_dir = os.path.join(args.data_root, args.dataset, 'bert_preprocessed') # cache dir (output dir)\n97:    txt_data_dir = os.path.join(args.data_root, args.dataset, 'txt_data') # input file dir\n98:\n99:    if not os.path.exists(cached_features_dir):\n100:        os.makedirs(cached_features_dir)\n101:\n102:    cached_features_path = os.path.join(cached_features_dir,\n103:                                        f'{tag}_trun_{args.truncate_mode}_maxl_{args.max_sen_len}')\n104:    input_path = os.path.join(txt_data_dir, f'{tag}.txt')\n105:\n106:    docs = read_txt(input_path)\n107:\n108:    if os.path.exists(cached_features_path):\n109:        logger.info(f'[Loading and Caching] loading from cache...')\n110:        features = torch.load(cached_features_path)\n111:    else:\n112:        logger.info(f'[Loading and Caching] number of documents = {len(docs)}')\n113:        logger.info(f'[Loading and Caching] convert text to features...')\n114:        features = get_input_features(docs, tokenizer, args)\n115:        logger.info(\"[Loading and Caching] saving/caching the features...\")\n116:        torch.save(features, cached_features_path)\n(70 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 95:111\n",
    "def load_and_cache_text(args, tokenizer, logger, tag):\n",
    "    # Load the AG-News dataset using the datasets library\n",
    "    dataset = load_dataset('ag_news')\n",
    "\n",
    "    # Filter specific instance indices\n",
    "    test_indices = [1,2,3,101,103,452,455,501,502,503,504,505]\n",
    "\n",
    "    test_data = dataset['test'].select(test_indices)\n",
    "\n",
    "    return test_data\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28937d5c",
   "metadata": {},
   "source": [
    "### Step 27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7664513",
   "metadata": {},
   "source": [
    "Execution Start time: 18:11:41, End time: 18:11:42 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5ba6f1",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that there is an unexpected indent. It seems I didn't remove the old code properly. Let's ensure the previous logic is completely replaced with the new logic for loading the test data using the `datasets` library. I'll make sure to remove all the unnecessary lines and correct the indentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993c90d1",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9088e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\n\nERRORS:\n- E999 IndentationError: unexpected indent\n\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /content/BERT-LNL/loading_utils.py (174 lines total)]\n(89 more lines above)\n90:    td = TextBertRealDataset(args, clean_noisy_data, noisy_labels, tokenizer, num_classes)\n91:    return td\n92:\n93:\n94:\n95:def load_and_cache_text(args, tokenizer, logger, tag):\n96:    # Load the AG-News dataset using the datasets library\n97:    dataset = load_dataset('ag_news')\n98:\n99:    # Filter specific instance indices\n100:    test_indices = [1,2,3,101,103,452,455,501,502,503,504,505]\n101:\n102:    test_data = dataset['test'].select(test_indices)\n103:\n104:    return test_data\n105:        logger.info(\"[Loading and Caching] saved\")\n106:\n107:    logger.info(f'[Loading and Caching] loading labels...')\n108:    input_path = os.path.join(txt_data_dir, f'{tag}_labels.pickle')\n109:    with open(input_path, 'rb') as handle:\n(65 more lines below)\n-------------------------------------------------\n\nThis is the original code before your edit\n-------------------------------------------------\n[File: /content/BERT-LNL/loading_utils.py (186 lines total)]\n(89 more lines above)\n90:    td = TextBertRealDataset(args, clean_noisy_data, noisy_labels, tokenizer, num_classes)\n91:    return td\n92:\n93:\n94:\n95:def load_and_cache_text(args, tokenizer, logger, tag):\n96:    cached_features_dir = os.path.join(args.data_root, args.dataset, 'bert_preprocessed') # cache dir (output dir)\n97:    txt_data_dir = os.path.join(args.data_root, args.dataset, 'txt_data') # input file dir\n98:\n99:    if not os.path.exists(cached_features_dir):\n100:        os.makedirs(cached_features_dir)\n101:\n102:    cached_features_path = os.path.join(cached_features_dir,\n103:                                        f'{tag}_trun_{args.truncate_mode}_maxl_{args.max_sen_len}')\n104:    input_path = os.path.join(txt_data_dir, f'{tag}.txt')\n105:\n106:    docs = read_txt(input_path)\n107:\n108:    if os.path.exists(cached_features_path):\n109:        logger.info(f'[Loading and Caching] loading from cache...')\n110:        features = torch.load(cached_features_path)\n111:    else:\n112:        logger.info(f'[Loading and Caching] number of documents = {len(docs)}')\n113:        logger.info(f'[Loading and Caching] convert text to features...')\n114:        features = get_input_features(docs, tokenizer, args)\n115:        logger.info(\"[Loading and Caching] saving/caching the features...\")\n116:        torch.save(features, cached_features_path)\n117:        logger.info(\"[Loading and Caching] saved\")\n118:\n119:    logger.info(f'[Loading and Caching] loading labels...')\n120:    input_path = os.path.join(txt_data_dir, f'{tag}_labels.pickle')\n121:    with open(input_path, 'rb') as handle:\n(65 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 95:116\n",
    "def load_and_cache_text(args, tokenizer, logger, tag):\n",
    "    # Load the AG-News dataset using the datasets library\n",
    "    dataset = load_dataset('ag_news')\n",
    "\n",
    "    # Filter specific instance indices\n",
    "    test_indices = [1,2,3,101,103,452,455,501,502,503,504,505]\n",
    "\n",
    "    test_data = dataset['test'].select(test_indices)\n",
    "\n",
    "    return test_data\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b751480",
   "metadata": {},
   "source": [
    "### Step 28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42fe771",
   "metadata": {},
   "source": [
    "Execution Start time: 18:11:45, End time: 18:11:46 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c956bcc",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that there is an unexpected indent because the previous code was not completely removed. Let's ensure the old code is fully replaced with the new logic for loading the test data using the `datasets` library. I'll make sure to remove all the unnecessary lines and correct the indentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3964c77a",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928408d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\n\nERRORS:\n- F821 undefined name 'txt_data_dir'\n- F821 undefined name 'features'\n- F821 undefined name 'docs'\n\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /content/BERT-LNL/loading_utils.py (171 lines total)]\n(89 more lines above)\n90:    td = TextBertRealDataset(args, clean_noisy_data, noisy_labels, tokenizer, num_classes)\n91:    return td\n92:\n93:\n94:\n95:def load_and_cache_text(args, tokenizer, logger, tag):\n96:    # Load the AG-News dataset using the datasets library\n97:    dataset = load_dataset('ag_news')\n98:\n99:    # Filter specific instance indices\n100:    test_indices = [1,2,3,101,103,452,455,501,502,503,504,505]\n101:\n102:    test_data = dataset['test'].select(test_indices)\n103:\n104:    return test_data\n105:    input_path = os.path.join(txt_data_dir, f'{tag}_labels.pickle')\n106:    with open(input_path, 'rb') as handle:\n107:        labels = np.array(pickle.load(handle))\n108:\n109:    return {\"features\": features, \"labels\": labels, \"text\": docs}\n(62 more lines below)\n-------------------------------------------------\n\nThis is the original code before your edit\n-------------------------------------------------\n[File: /content/BERT-LNL/loading_utils.py (186 lines total)]\n(90 more lines above)\n90:    td = TextBertRealDataset(args, clean_noisy_data, noisy_labels, tokenizer, num_classes)\n91:    return td\n92:\n93:\n94:\n95:def load_and_cache_text(args, tokenizer, logger, tag):\n96:    cached_features_dir = os.path.join(args.data_root, args.dataset, 'bert_preprocessed') # cache dir (output dir)\n97:    txt_data_dir = os.path.join(args.data_root, args.dataset, 'txt_data') # input file dir\n98:\n99:    if not os.path.exists(cached_features_dir):\n100:        os.makedirs(cached_features_dir)\n101:\n102:    cached_features_path = os.path.join(cached_features_dir,\n103:                                        f'{tag}_trun_{args.truncate_mode}_maxl_{args.max_sen_len}')\n104:    input_path = os.path.join(txt_data_dir, f'{tag}.txt')\n105:\n106:    docs = read_txt(input_path)\n107:\n108:    if os.path.exists(cached_features_path):\n109:        logger.info(f'[Loading and Caching] loading from cache...')\n110:        features = torch.load(cached_features_path)\n111:    else:\n112:        logger.info(f'[Loading and Caching] number of documents = {len(docs)}')\n113:        logger.info(f'[Loading and Caching] convert text to features...')\n114:        features = get_input_features(docs, tokenizer, args)\n115:        logger.info(\"[Loading and Caching] saving/caching the features...\")\n116:        torch.save(features, cached_features_path)\n117:        logger.info(\"[Loading and Caching] saved\")\n118:\n119:    logger.info(f'[Loading and Caching] loading labels...')\n120:    input_path = os.path.join(txt_data_dir, f'{tag}_labels.pickle')\n121:    with open(input_path, 'rb') as handle:\n122:        labels = np.array(pickle.load(handle))\n123:\n124:    return {\"features\": features, \"labels\": labels, \"text\": docs}\n(62 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 95:119\n",
    "def load_and_cache_text(args, tokenizer, logger, tag):\n",
    "    # Load the AG-News dataset using the datasets library\n",
    "    dataset = load_dataset('ag_news')\n",
    "\n",
    "    # Filter specific instance indices\n",
    "    test_indices = [1,2,3,101,103,452,455,501,502,503,504,505]\n",
    "\n",
    "    test_data = dataset['test'].select(test_indices)\n",
    "\n",
    "    return test_data\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0beea5",
   "metadata": {},
   "source": [
    "### Step 29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a5a4eb",
   "metadata": {},
   "source": [
    "Execution Start time: 18:11:50, End time: 18:11:51 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759d18b9",
   "metadata": {},
   "source": [
    "**Thought**: The error occurred because I didn't remove the old code properly, which resulted in undefined variables. Let's ensure the previous logic is completely replaced with the new logic for loading the test data using the `datasets` library. I'll make sure to remove all the unnecessary lines and correct the indentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dc9515",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54f0d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/BERT-LNL/loading_utils.py (166 lines total)]\n(44 more lines above)\n45:    return n_set, u_set, v_set, t_set, l2id, id2l\n46:\n47:\n48:def prepare_af_data(args, logger, num_classes, has_ul):\n49:    tokenizer = load_tokenizer(args)\n50:    n_set = get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag='train')\n51:    v_set = get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag='validation')\n52:    t_set = get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag='test')\n53:\n54:    assert not has_ul  # we do not have unlabeled data in Yoruba and Hausa dataset\n55:    u_set = None\n56:\n57:    label_mapping_data_dir = os.path.join(args.data_root, args.dataset, 'txt_data')\n58:    l2id = utils.pickle_load(os.path.join(label_mapping_data_dir, 'l2idx.pickle'))\n59:    id2l = utils.pickle_load(os.path.join(label_mapping_data_dir, 'idx2l.pickle'))\n60:\n61:    return n_set, u_set, v_set, t_set, l2id, id2l\n62:\n63:\n64:from datasets import load_dataset\n65:\n66:def get_training_validation_set(args, logger, tokenizer, r_state, has_val, num_classes):\n67:    # Load the AG-News dataset using the datasets library\n68:    dataset = load_dataset('ag_news')\n69:\n70:    # Filter specific instance indices\n71:    train_indices = [1,2,3,4,5,6,100,101,102,103,104,105,451,452,453,454,455,456,501,502,503,504,505,506]\n72:    val_indices = [0,1,2,6,7,8,12,13,14,18,19,20]\n73:\n74:    tr_data = dataset['train'].select(train_indices)\n75:    val_data = dataset['validation'].select(val_indices)\n76:\n77:    return tr_data, val_data\n78:\n79:def get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag):\n80:    noisy_data_tag = f'{tag}_clean'\n81:\n82:    #get text data with noisy labels\n83:    clean_noisy_data = load_and_cache_text(args, tokenizer, logger, tag=noisy_data_tag)\n84:\n85:    #get the clean training and the clean validation sets\n86:    txt_data_dir = os.path.join(args.data_root, args.dataset, 'txt_data')\n87:    input_path = os.path.join(txt_data_dir, f'{tag}_clean_noisy_labels.pickle')\n88:    noisy_labels = load_pickle_data(input_path)\n89:\n90:    td = TextBertRealDataset(args, clean_noisy_data, noisy_labels, tokenizer, num_classes)\n91:    return td\n92:\n93:\n94:\n95:def load_and_cache_text(args, tokenizer, logger, tag):\n96:    # Load the AG-News dataset using the datasets library\n97:    dataset = load_dataset('ag_news')\n98:\n99:    # Filter specific instance indices\n100:    test_indices = [1,2,3,101,103,452,455,501,502,503,504,505]\n101:\n102:    test_data = dataset['test'].select(test_indices)\n103:\n104:    return test_data\n105:\n106:\n107:\n108:\n109:def load_tokenizer(args):\n110:    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n111:    return tokenizer\n112:\n113:\n114:def load_pickle_data(path):\n115:    with open(path, 'rb') as handle:\n116:        data = pickle.load(handle)\n117:    return data\n118:\n119:\n120:\n121:def read_txt(file_path):\n122:    text_list = []\n123:    with open(file_path, 'r', encoding='utf-8') as f:\n124:        text_list = [line.rstrip('\\n') for line in f]\n125:    return text_list\n126:\n127:\n128:def truncate_token_ids(token_ids, args, limit):\n129:    if args.truncate_mode == 'last':\n130:        return token_ids[-limit:]\n131:    elif args.truncate_mode == 'hybrid':\n132:        return token_ids[:128] + token_ids[-382:]\n133:    else:\n134:        raise ValueError('truncate model not supported')\n135:\n136:\n137:def get_input_features(docs, tokenizer, args):\n138:    limit = args.max_sen_len - args.special_token_offsets\n139:    # sanity check\n140:    if args.truncate_mode == 'hybrid':\n141:        assert args.max_sen_len == 512\n142:    assert limit > 0\n143:    num_docs = len(docs)\n144:\n(22 more lines below)\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 95:124\n",
    "def load_and_cache_text(args, tokenizer, logger, tag):\n",
    "    # Load the AG-News dataset using the datasets library\n",
    "    dataset = load_dataset('ag_news')\n",
    "\n",
    "    # Filter specific instance indices\n",
    "    test_indices = [1,2,3,101,103,452,455,501,502,503,504,505]\n",
    "\n",
    "    test_data = dataset['test'].select(test_indices)\n",
    "\n",
    "    return test_data\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a70f833",
   "metadata": {},
   "source": [
    "### Step 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe41b65",
   "metadata": {},
   "source": [
    "Execution Start time: 18:11:54, End time: 18:12:04 (duration: 10 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8397a368",
   "metadata": {},
   "source": [
    "**Thought**: The `load_and_cache_text` function has been successfully updated to load the test data using the `datasets` library. Now, let's rerun the `main.py` script to train the BERT-WN model on the AG-News dataset with the specified settings and evaluate the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31e17cb",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06483445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INFO Training started\nINFO log dir: 08_20_18_12_01_sflip_nle0.2_nlb16\nINFO loading AG_News...\n/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\nDownloading readme:   0%|                           | 0.00/8.07k [00:00<?, ?B/s]\nDownloading readme: 100%|██████████████████| 8.07k/8.07k [00:00<00:00, 51.4kB/s]\nDownloading readme: 100%|██████████████████| 8.07k/8.07k [00:00<00:00, 51.2kB/s]\n\nDownloading data:   0%|                             | 0.00/18.6M [00:00<?, ?B/s]\n\n\n\nDownloading data:   0%|                             | 0.00/1.23M [00:00<?, ?B/s]\n\n\nGenerating train split:   0%|                 | 0/120000 [00:00<?, ? examples/s]\nGenerating train split:  25%|▎| 30000/120000 [00:00<00:00, 247637.11 examples/s]\nGenerating train split:  72%|▋| 87000/120000 [00:00<00:00, 414895.64 examples/s]\nGenerating train split: 100%|█| 120000/120000 [00:00<00:00, 426274.59 examples/s\n\nGenerating test split:   0%|                    | 0/7600 [00:00<?, ? examples/s]\nGenerating test split: 100%|█████| 7600/7600 [00:00<00:00, 540750.66 examples/s]\nTraceback (most recent call last):\n  File \"/content/BERT-LNL/main.py\", line 163, in <module>\n    main()\n  File \"/content/BERT-LNL/main.py\", line 145, in main\n    nl_set, ul_set, v_set, t_set, l2id, id2l = prepare_data(args, logger, r_state, num_classes, has_val, has_ul)\n  File \"/content/BERT-LNL/loading_utils.py\", line 18, in prepare_data\n    tr_data, val_data = get_training_validation_set(args, logger, tokenizer, r_state, has_val, num_classes)\n  File \"/content/BERT-LNL/loading_utils.py\", line 75, in get_training_validation_set\n    val_data = dataset['validation'].select(val_indices)\n  File \"/usr/local/lib/python3.10/site-packages/datasets/dataset_dict.py\", line 75, in __getitem__\n    return super().__getitem__(k)\nKeyError: 'validation'\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python main.py --dataset AG_News --noise_level 0.2 --noise_type sflip --trainer_name bert_wn --model_name bert-base-uncased --num_training_steps 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb21c760",
   "metadata": {},
   "source": [
    "### Step 31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f629e8",
   "metadata": {},
   "source": [
    "Execution Start time: 18:12:11, End time: 18:12:12 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec3a29a",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that there is no 'validation' split in the AG-News dataset. The AG-News dataset typically has 'train' and 'test' splits. We should use a portion of the 'train' split as the validation set. Let's modify the `get_training_validation_set` function to handle this correctly.\n",
    "\n",
    "I'll edit the `loading_utils.py` file to adjust the dataset loading logic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c43b87",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01aec1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\n\nERRORS:\n- F821 undefined name 'load_and_cache_text'\n- F821 undefined name 'get_clean_and_noisy_data_by_tag'\n- F821 undefined name 'get_clean_and_noisy_data_by_tag'\n- F821 undefined name 'get_clean_and_noisy_data_by_tag'\n- F821 undefined name 'load_dataset'\n- F821 undefined name 'load_dataset'\n\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /content/BERT-LNL/loading_utils.py (145 lines total)]\n(58 more lines above)\n59:    id2l = utils.pickle_load(os.path.join(label_mapping_data_dir, 'idx2l.pickle'))\n60:\n61:    return n_set, u_set, v_set, t_set, l2id, id2l\n62:\n63:\n64:def get_training_validation_set(args, logger, tokenizer, r_state, has_val, num_classes):\n65:    # Load the AG-News dataset using the datasets library\n66:    dataset = load_dataset('ag_news')\n67:\n68:    # Filter specific instance indices\n69:    train_indices = [1,2,3,4,5,6,100,101,102,103,104,105,451,452,453,454,455,456,501,502,503,504,505,506]\n70:    val_indices = [0,1,2,6,7,8,12,13,14,18,19,20]\n71:\n72:    tr_data = dataset['train'].select(train_indices)\n73:    val_data = dataset['train'].select(val_indices)  # Use a portion of the train split as validation\n74:\n75:    return tr_data, val_data\n76:    dataset = load_dataset('ag_news')\n77:\n78:    # Filter specific instance indices\n79:    test_indices = [1,2,3,101,103,452,455,501,502,503,504,505]\n80:\n(65 more lines below)\n-------------------------------------------------\n\nThis is the original code before your edit\n-------------------------------------------------\n[File: /content/BERT-LNL/loading_utils.py (166 lines total)]\n(59 more lines above)\n59:    id2l = utils.pickle_load(os.path.join(label_mapping_data_dir, 'idx2l.pickle'))\n60:\n61:    return n_set, u_set, v_set, t_set, l2id, id2l\n62:\n63:\n64:from datasets import load_dataset\n65:\n66:def get_training_validation_set(args, logger, tokenizer, r_state, has_val, num_classes):\n67:    # Load the AG-News dataset using the datasets library\n68:    dataset = load_dataset('ag_news')\n69:\n70:    # Filter specific instance indices\n71:    train_indices = [1,2,3,4,5,6,100,101,102,103,104,105,451,452,453,454,455,456,501,502,503,504,505,506]\n72:    val_indices = [0,1,2,6,7,8,12,13,14,18,19,20]\n73:\n74:    tr_data = dataset['train'].select(train_indices)\n75:    val_data = dataset['validation'].select(val_indices)\n76:\n77:    return tr_data, val_data\n78:\n79:def get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag):\n80:    noisy_data_tag = f'{tag}_clean'\n81:\n82:    #get text data with noisy labels\n83:    clean_noisy_data = load_and_cache_text(args, tokenizer, logger, tag=noisy_data_tag)\n84:\n85:    #get the clean training and the clean validation sets\n86:    txt_data_dir = os.path.join(args.data_root, args.dataset, 'txt_data')\n87:    input_path = os.path.join(txt_data_dir, f'{tag}_clean_noisy_labels.pickle')\n88:    noisy_labels = load_pickle_data(input_path)\n89:\n90:    td = TextBertRealDataset(args, clean_noisy_data, noisy_labels, tokenizer, num_classes)\n91:    return td\n92:\n93:\n94:\n95:def load_and_cache_text(args, tokenizer, logger, tag):\n96:    # Load the AG-News dataset using the datasets library\n97:    dataset = load_dataset('ag_news')\n98:\n99:    # Filter specific instance indices\n100:    test_indices = [1,2,3,101,103,452,455,501,502,503,504,505]\n101:\n(65 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 64:96\n",
    "def get_training_validation_set(args, logger, tokenizer, r_state, has_val, num_classes):\n",
    "    # Load the AG-News dataset using the datasets library\n",
    "    dataset = load_dataset('ag_news')\n",
    "\n",
    "    # Filter specific instance indices\n",
    "    train_indices = [1,2,3,4,5,6,100,101,102,103,104,105,451,452,453,454,455,456,501,502,503,504,505,506]\n",
    "    val_indices = [0,1,2,6,7,8,12,13,14,18,19,20]\n",
    "\n",
    "    tr_data = dataset['train'].select(train_indices)\n",
    "    val_data = dataset['train'].select(val_indices)  # Use a portion of the train split as validation\n",
    "\n",
    "    return tr_data, val_data\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a72e3b",
   "metadata": {},
   "source": [
    "### Step 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3648534d",
   "metadata": {},
   "source": [
    "Execution Start time: 18:12:17, End time: 18:12:18 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa88187",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the `load_dataset` function is not recognized because I forgot to include the import statement in the edit. Let's ensure that the `datasets` library is imported at the top of the file. I'll correct this by adding the import statement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27728f2e",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea9e706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\n\nERRORS:\n- F821 undefined name 'load_and_cache_text'\n- F821 undefined name 'get_clean_and_noisy_data_by_tag'\n- F821 undefined name 'get_clean_and_noisy_data_by_tag'\n- F821 undefined name 'get_clean_and_noisy_data_by_tag'\n\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /content/BERT-LNL/loading_utils.py (147 lines total)]\n(58 more lines above)\n59:    id2l = utils.pickle_load(os.path.join(label_mapping_data_dir, 'idx2l.pickle'))\n60:\n61:    return n_set, u_set, v_set, t_set, l2id, id2l\n62:\n63:\n64:from datasets import load_dataset\n65:\n66:def get_training_validation_set(args, logger, tokenizer, r_state, has_val, num_classes):\n67:    # Load the AG-News dataset using the datasets library\n68:    dataset = load_dataset('ag_news')\n69:\n70:    # Filter specific instance indices\n71:    train_indices = [1,2,3,4,5,6,100,101,102,103,104,105,451,452,453,454,455,456,501,502,503,504,505,506]\n72:    val_indices = [0,1,2,6,7,8,12,13,14,18,19,20]\n73:\n74:    tr_data = dataset['train'].select(train_indices)\n75:    val_data = dataset['train'].select(val_indices)  # Use a portion of the train split as validation\n76:\n77:    return tr_data, val_data\n78:    dataset = load_dataset('ag_news')\n79:\n80:    # Filter specific instance indices\n81:    test_indices = [1,2,3,101,103,452,455,501,502,503,504,505]\n82:\n(65 more lines below)\n-------------------------------------------------\n\nThis is the original code before your edit\n-------------------------------------------------\n[File: /content/BERT-LNL/loading_utils.py (166 lines total)]\n(59 more lines above)\n59:    id2l = utils.pickle_load(os.path.join(label_mapping_data_dir, 'idx2l.pickle'))\n60:\n61:    return n_set, u_set, v_set, t_set, l2id, id2l\n62:\n63:\n64:from datasets import load_dataset\n65:\n66:def get_training_validation_set(args, logger, tokenizer, r_state, has_val, num_classes):\n67:    # Load the AG-News dataset using the datasets library\n68:    dataset = load_dataset('ag_news')\n69:\n70:    # Filter specific instance indices\n71:    train_indices = [1,2,3,4,5,6,100,101,102,103,104,105,451,452,453,454,455,456,501,502,503,504,505,506]\n72:    val_indices = [0,1,2,6,7,8,12,13,14,18,19,20]\n73:\n74:    tr_data = dataset['train'].select(train_indices)\n75:    val_data = dataset['validation'].select(val_indices)\n76:\n77:    return tr_data, val_data\n78:\n79:def get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag):\n80:    noisy_data_tag = f'{tag}_clean'\n81:\n82:    #get text data with noisy labels\n83:    clean_noisy_data = load_and_cache_text(args, tokenizer, logger, tag=noisy_data_tag)\n84:\n85:    #get the clean training and the clean validation sets\n86:    txt_data_dir = os.path.join(args.data_root, args.dataset, 'txt_data')\n87:    input_path = os.path.join(txt_data_dir, f'{tag}_clean_noisy_labels.pickle')\n88:    noisy_labels = load_pickle_data(input_path)\n89:\n90:    td = TextBertRealDataset(args, clean_noisy_data, noisy_labels, tokenizer, num_classes)\n91:    return td\n92:\n93:\n94:\n95:def load_and_cache_text(args, tokenizer, logger, tag):\n96:    # Load the AG-News dataset using the datasets library\n97:    dataset = load_dataset('ag_news')\n98:\n99:    # Filter specific instance indices\n100:    test_indices = [1,2,3,101,103,452,455,501,502,503,504,505]\n101:\n(65 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 64:96\n",
    "from datasets import load_dataset\n",
    "\n",
    "def get_training_validation_set(args, logger, tokenizer, r_state, has_val, num_classes):\n",
    "    # Load the AG-News dataset using the datasets library\n",
    "    dataset = load_dataset('ag_news')\n",
    "\n",
    "    # Filter specific instance indices\n",
    "    train_indices = [1,2,3,4,5,6,100,101,102,103,104,105,451,452,453,454,455,456,501,502,503,504,505,506]\n",
    "    val_indices = [0,1,2,6,7,8,12,13,14,18,19,20]\n",
    "\n",
    "    tr_data = dataset['train'].select(train_indices)\n",
    "    val_data = dataset['train'].select(val_indices)  # Use a portion of the train split as validation\n",
    "\n",
    "    return tr_data, val_data\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffe6c86",
   "metadata": {},
   "source": [
    "### Step 33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5651ff",
   "metadata": {},
   "source": [
    "Execution Start time: 18:12:23, End time: 18:12:24 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba239103",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that there are undefined names due to the incorrect placement of the import statement. Let's ensure the import statement is placed at the top of the file and that the logic for loading the dataset is correctly implemented. I'll fix this by editing the file again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05fcac0",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d898e061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\n\nERRORS:\n- F821 undefined name 'load_and_cache_text'\n- F821 undefined name 'get_clean_and_noisy_data_by_tag'\n- F821 undefined name 'get_clean_and_noisy_data_by_tag'\n- F821 undefined name 'get_clean_and_noisy_data_by_tag'\n\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /content/BERT-LNL/loading_utils.py (147 lines total)]\n(58 more lines above)\n59:    id2l = utils.pickle_load(os.path.join(label_mapping_data_dir, 'idx2l.pickle'))\n60:\n61:    return n_set, u_set, v_set, t_set, l2id, id2l\n62:\n63:\n64:from datasets import load_dataset\n65:\n66:def get_training_validation_set(args, logger, tokenizer, r_state, has_val, num_classes):\n67:    # Load the AG-News dataset using the datasets library\n68:    dataset = load_dataset('ag_news')\n69:\n70:    # Filter specific instance indices\n71:    train_indices = [1,2,3,4,5,6,100,101,102,103,104,105,451,452,453,454,455,456,501,502,503,504,505,506]\n72:    val_indices = [0,1,2,6,7,8,12,13,14,18,19,20]\n73:\n74:    tr_data = dataset['train'].select(train_indices)\n75:    val_data = dataset['train'].select(val_indices)  # Use a portion of the train split as validation\n76:\n77:    return tr_data, val_data\n78:    dataset = load_dataset('ag_news')\n79:\n80:    # Filter specific instance indices\n81:    test_indices = [1,2,3,101,103,452,455,501,502,503,504,505]\n82:\n(65 more lines below)\n-------------------------------------------------\n\nThis is the original code before your edit\n-------------------------------------------------\n[File: /content/BERT-LNL/loading_utils.py (166 lines total)]\n(59 more lines above)\n59:    id2l = utils.pickle_load(os.path.join(label_mapping_data_dir, 'idx2l.pickle'))\n60:\n61:    return n_set, u_set, v_set, t_set, l2id, id2l\n62:\n63:\n64:from datasets import load_dataset\n65:\n66:def get_training_validation_set(args, logger, tokenizer, r_state, has_val, num_classes):\n67:    # Load the AG-News dataset using the datasets library\n68:    dataset = load_dataset('ag_news')\n69:\n70:    # Filter specific instance indices\n71:    train_indices = [1,2,3,4,5,6,100,101,102,103,104,105,451,452,453,454,455,456,501,502,503,504,505,506]\n72:    val_indices = [0,1,2,6,7,8,12,13,14,18,19,20]\n73:\n74:    tr_data = dataset['train'].select(train_indices)\n75:    val_data = dataset['validation'].select(val_indices)\n76:\n77:    return tr_data, val_data\n78:\n79:def get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag):\n80:    noisy_data_tag = f'{tag}_clean'\n81:\n82:    #get text data with noisy labels\n83:    clean_noisy_data = load_and_cache_text(args, tokenizer, logger, tag=noisy_data_tag)\n84:\n85:    #get the clean training and the clean validation sets\n86:    txt_data_dir = os.path.join(args.data_root, args.dataset, 'txt_data')\n87:    input_path = os.path.join(txt_data_dir, f'{tag}_clean_noisy_labels.pickle')\n88:    noisy_labels = load_pickle_data(input_path)\n89:\n90:    td = TextBertRealDataset(args, clean_noisy_data, noisy_labels, tokenizer, num_classes)\n91:    return td\n92:\n93:\n94:\n95:def load_and_cache_text(args, tokenizer, logger, tag):\n96:    # Load the AG-News dataset using the datasets library\n97:    dataset = load_dataset('ag_news')\n98:\n99:    # Filter specific instance indices\n100:    test_indices = [1,2,3,101,103,452,455,501,502,503,504,505]\n101:\n(65 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 64:96\n",
    "from datasets import load_dataset\n",
    "\n",
    "def get_training_validation_set(args, logger, tokenizer, r_state, has_val, num_classes):\n",
    "    # Load the AG-News dataset using the datasets library\n",
    "    dataset = load_dataset('ag_news')\n",
    "\n",
    "    # Filter specific instance indices\n",
    "    train_indices = [1,2,3,4,5,6,100,101,102,103,104,105,451,452,453,454,455,456,501,502,503,504,505,506]\n",
    "    val_indices = [0,1,2,6,7,8,12,13,14,18,19,20]\n",
    "\n",
    "    tr_data = dataset['train'].select(train_indices)\n",
    "    val_data = dataset['train'].select(val_indices)  # Use a portion of the train split as validation\n",
    "\n",
    "    return tr_data, val_data\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e60bfaf",
   "metadata": {},
   "source": [
    "### Step 34"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c728a0b8",
   "metadata": {},
   "source": [
    "Execution Start time: 18:12:29, End time: 18:12:30 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13ee25a",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that there are undefined names due to the incorrect placement of the import statement and incomplete edits. Let's ensure the import statement is placed at the top of the file and that the logic for loading the dataset is correctly implemented. I'll fix this by editing the file again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f133a498",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3499c0af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/BERT-LNL/loading_utils.py (166 lines total)]\n(13 more lines above)\n14:def prepare_data(args, logger, r_state, num_classes, has_val, has_ul):\n15:    # used for experiments with injected noise\n16:\n17:    tokenizer = load_tokenizer(args)\n18:    tr_data, val_data = get_training_validation_set(args, logger, tokenizer, r_state, has_val, num_classes)\n19:    test_data = load_and_cache_text(args, tokenizer, logger, tag='test')\n20:\n21:    # Filter specific instance indices\n22:    train_indices = [1,2,3,4,5,6,100,101,102,103,104,105,451,452,453,454,455,456,501,502,503,504,505,506]\n23:    test_indices = [1,2,3,101,103,452,455,501,502,503,504,505]\n24:    val_indices = [0,1,2,6,7,8,12,13,14,18,19,20]\n25:\n26:    tr_data = {k: [v[i] for i in train_indices] for k, v in tr_data.items()}\n27:    val_data = {k: [v[i] for i in val_indices] for k, v in val_data.items()}\n28:    test_data = {k: [v[i] for i in test_indices] for k, v in test_data.items()}\n29:\n30:    n_set = TextBertDataset(args, tr_data, tokenizer, r_state, num_classes, make_noisy=True)\n31:    v_set = TextBertDataset(args, val_data, tokenizer, r_state, num_classes, make_noisy=True)\n32:    t_set = TextBertDataset(args, test_data, tokenizer, r_state, num_classes, make_noisy=False)\n33:\n34:    n_set_noisy_labels = copy.deepcopy(n_set.noisy_labels)\n35:    v_set_noisy_labels = copy.deepcopy(v_set.noisy_labels)\n36:    n_set_noisy_labels_hash = hash(tuple(n_set_noisy_labels))\n37:    v_set_noisy_labels_hash = hash(tuple(v_set_noisy_labels))\n38:    # wandb.run.summary[\"train_n_hash\"] = n_set_noisy_labels_hash\n39:    # wandb.run.summary[\"val_n_hash\"] = v_set_noisy_labels_hash\n40:\n41:    u_set = None\n42:    l2id = None\n43:    id2l = None\n44:\n45:    return n_set, u_set, v_set, t_set, l2id, id2l\n46:\n47:\n48:def prepare_af_data(args, logger, num_classes, has_ul):\n49:    tokenizer = load_tokenizer(args)\n50:    n_set = get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag='train')\n51:    v_set = get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag='validation')\n52:    t_set = get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag='test')\n53:\n54:    assert not has_ul  # we do not have unlabeled data in Yoruba and Hausa dataset\n55:    u_set = None\n56:\n57:    label_mapping_data_dir = os.path.join(args.data_root, args.dataset, 'txt_data')\n58:    l2id = utils.pickle_load(os.path.join(label_mapping_data_dir, 'l2idx.pickle'))\n59:    id2l = utils.pickle_load(os.path.join(label_mapping_data_dir, 'idx2l.pickle'))\n60:\n61:    return n_set, u_set, v_set, t_set, l2id, id2l\n62:\n63:\n64:from datasets import load_dataset\n65:\n66:def get_training_validation_set(args, logger, tokenizer, r_state, has_val, num_classes):\n67:    # Load the AG-News dataset using the datasets library\n68:    dataset = load_dataset('ag_news')\n69:\n70:    # Filter specific instance indices\n71:    train_indices = [1,2,3,4,5,6,100,101,102,103,104,105,451,452,453,454,455,456,501,502,503,504,505,506]\n72:    val_indices = [0,1,2,6,7,8,12,13,14,18,19,20]\n73:\n74:    tr_data = dataset['train'].select(train_indices)\n75:    val_data = dataset['train'].select(val_indices)  # Use a portion of the train split as validation\n76:\n77:    return tr_data, val_data\n78:\n79:def get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag):\n80:    noisy_data_tag = f'{tag}_clean'\n81:\n82:    #get text data with noisy labels\n83:    clean_noisy_data = load_and_cache_text(args, tokenizer, logger, tag=noisy_data_tag)\n84:\n85:    #get the clean training and the clean validation sets\n86:    txt_data_dir = os.path.join(args.data_root, args.dataset, 'txt_data')\n87:    input_path = os.path.join(txt_data_dir, f'{tag}_clean_noisy_labels.pickle')\n88:    noisy_labels = load_pickle_data(input_path)\n89:\n90:    td = TextBertRealDataset(args, clean_noisy_data, noisy_labels, tokenizer, num_classes)\n91:    return td\n92:\n93:\n94:\n95:def load_and_cache_text(args, tokenizer, logger, tag):\n96:    # Load the AG-News dataset using the datasets library\n97:    dataset = load_dataset('ag_news')\n98:\n99:    # Filter specific instance indices\n100:    test_indices = [1,2,3,101,103,452,455,501,502,503,504,505]\n101:\n102:    test_data = dataset['test'].select(test_indices)\n103:\n104:    return test_data\n105:\n106:\n107:\n108:\n109:def load_tokenizer(args):\n110:    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n111:    return tokenizer\n112:\n113:\n(53 more lines below)\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 64:77\n",
    "from datasets import load_dataset\n",
    "\n",
    "def get_training_validation_set(args, logger, tokenizer, r_state, has_val, num_classes):\n",
    "    # Load the AG-News dataset using the datasets library\n",
    "    dataset = load_dataset('ag_news')\n",
    "\n",
    "    # Filter specific instance indices\n",
    "    train_indices = [1,2,3,4,5,6,100,101,102,103,104,105,451,452,453,454,455,456,501,502,503,504,505,506]\n",
    "    val_indices = [0,1,2,6,7,8,12,13,14,18,19,20]\n",
    "\n",
    "    tr_data = dataset['train'].select(train_indices)\n",
    "    val_data = dataset['train'].select(val_indices)  # Use a portion of the train split as validation\n",
    "\n",
    "    return tr_data, val_data\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49691172",
   "metadata": {},
   "source": [
    "### Step 35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57f4922",
   "metadata": {},
   "source": [
    "Execution Start time: 18:12:32, End time: 18:12:43 (duration: 10 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb1ee38",
   "metadata": {},
   "source": [
    "**Thought**: The `get_training_validation_set` function has been successfully updated to use a portion of the train split as the validation set. Now, let's rerun the `main.py` script to train the BERT-WN model on the AG-News dataset with the specified settings and evaluate the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaf8881",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8282cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INFO Training started\nINFO log dir: 08_20_18_12_39_sflip_nle0.2_nlb16\nINFO loading AG_News...\n/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nTraceback (most recent call last):\n  File \"/content/BERT-LNL/main.py\", line 163, in <module>\n    main()\n  File \"/content/BERT-LNL/main.py\", line 145, in main\n    nl_set, ul_set, v_set, t_set, l2id, id2l = prepare_data(args, logger, r_state, num_classes, has_val, has_ul)\n  File \"/content/BERT-LNL/loading_utils.py\", line 26, in prepare_data\n    tr_data = {k: [v[i] for i in train_indices] for k, v in tr_data.items()}\nAttributeError: 'Dataset' object has no attribute 'items'. Did you mean: 'iter'?\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python main.py --dataset AG_News --noise_level 0.2 --noise_type sflip --trainer_name bert_wn --model_name bert-base-uncased --num_training_steps 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c52b078",
   "metadata": {},
   "source": [
    "### Step 36"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1ab74d",
   "metadata": {},
   "source": [
    "Execution Start time: 18:12:51, End time: 18:12:52 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e660e77",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the `Dataset` object does not have an `items` method. Instead, we should access the columns directly. Let's modify the `prepare_data` function to correctly access the dataset columns and filter the specific instance indices.\n",
    "\n",
    "I'll edit the `loading_utils.py` file to fix this issue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e487a0fc",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83845a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/BERT-LNL/loading_utils.py (175 lines total)]\n1:import os\n2:import copy\n3:import numpy as np\n4:import pickle\n5:import torch\n6:import wandb\n7:from tqdm import tqdm\n8:from text_dataset import TextBertDataset, TextBertRealDataset\n9:from transformers import AutoTokenizer\n10:from transformers import BertTokenizerFast, BertTokenizer\n11:import utils\n12:\n13:\n14:def prepare_data(args, logger, r_state, num_classes, has_val, has_ul):\n15:    # used for experiments with injected noise\n16:\n17:    tokenizer = load_tokenizer(args)\n18:    tr_data, val_data = get_training_validation_set(args, logger, tokenizer, r_state, has_val, num_classes)\n19:    test_data = load_and_cache_text(args, tokenizer, logger, tag='test')\n20:\n21:    # Filter specific instance indices\n22:    train_indices = [1,2,3,4,5,6,100,101,102,103,104,105,451,452,453,454,455,456,501,502,503,504,505,506]\n23:    test_indices = [1,2,3,101,103,452,455,501,502,503,504,505]\n24:    val_indices = [0,1,2,6,7,8,12,13,14,18,19,20]\n25:\n26:    tr_data = tr_data.select(train_indices)\n27:    val_data = val_data.select(val_indices)\n28:    test_data = test_data.select(test_indices)\n29:\n30:    n_set = TextBertDataset(args, tr_data, tokenizer, r_state, num_classes, make_noisy=True)\n31:    v_set = TextBertDataset(args, val_data, tokenizer, r_state, num_classes, make_noisy=True)\n32:    t_set = TextBertDataset(args, test_data, tokenizer, r_state, num_classes, make_noisy=False)\n33:\n34:    n_set_noisy_labels = copy.deepcopy(n_set.noisy_labels)\n35:    v_set_noisy_labels = copy.deepcopy(v_set.noisy_labels)\n36:    n_set_noisy_labels_hash = hash(tuple(n_set_noisy_labels))\n37:    v_set_noisy_labels_hash = hash(tuple(v_set_noisy_labels))\n38:    # wandb.run.summary[\"train_n_hash\"] = n_set_noisy_labels_hash\n39:    # wandb.run.summary[\"val_n_hash\"] = v_set_noisy_labels_hash\n40:\n41:    u_set = None\n42:    l2id = None\n43:    id2l = None\n44:\n45:    return n_set, u_set, v_set, t_set, l2id, id2l\n46:    v_set_noisy_labels_hash = hash(tuple(v_set_noisy_labels))\n47:    # wandb.run.summary[\"train_n_hash\"] = n_set_noisy_labels_hash\n48:    # wandb.run.summary[\"val_n_hash\"] = v_set_noisy_labels_hash\n49:\n50:    u_set = None\n51:    l2id = None\n52:    id2l = None\n53:\n54:    return n_set, u_set, v_set, t_set, l2id, id2l\n55:\n56:\n57:def prepare_af_data(args, logger, num_classes, has_ul):\n58:    tokenizer = load_tokenizer(args)\n59:    n_set = get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag='train')\n60:    v_set = get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag='validation')\n61:    t_set = get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag='test')\n62:\n63:    assert not has_ul  # we do not have unlabeled data in Yoruba and Hausa dataset\n64:    u_set = None\n65:\n66:    label_mapping_data_dir = os.path.join(args.data_root, args.dataset, 'txt_data')\n67:    l2id = utils.pickle_load(os.path.join(label_mapping_data_dir, 'l2idx.pickle'))\n68:    id2l = utils.pickle_load(os.path.join(label_mapping_data_dir, 'idx2l.pickle'))\n69:\n70:    return n_set, u_set, v_set, t_set, l2id, id2l\n71:\n72:\n73:from datasets import load_dataset\n74:\n75:def get_training_validation_set(args, logger, tokenizer, r_state, has_val, num_classes):\n76:    # Load the AG-News dataset using the datasets library\n77:    dataset = load_dataset('ag_news')\n78:\n79:    # Filter specific instance indices\n80:    train_indices = [1,2,3,4,5,6,100,101,102,103,104,105,451,452,453,454,455,456,501,502,503,504,505,506]\n81:    val_indices = [0,1,2,6,7,8,12,13,14,18,19,20]\n82:\n83:    tr_data = dataset['train'].select(train_indices)\n84:    val_data = dataset['train'].select(val_indices)  # Use a portion of the train split as validation\n85:\n86:    return tr_data, val_data\n87:\n88:def get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag):\n89:    noisy_data_tag = f'{tag}_clean'\n90:\n91:    #get text data with noisy labels\n92:    clean_noisy_data = load_and_cache_text(args, tokenizer, logger, tag=noisy_data_tag)\n93:\n94:    #get the clean training and the clean validation sets\n95:    txt_data_dir = os.path.join(args.data_root, args.dataset, 'txt_data')\n96:    input_path = os.path.join(txt_data_dir, f'{tag}_clean_noisy_labels.pickle')\n97:    noisy_labels = load_pickle_data(input_path)\n98:\n99:    td = TextBertRealDataset(args, clean_noisy_data, noisy_labels, tokenizer, num_classes)\n100:    return td\n(75 more lines below)\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 14:36\n",
    "def prepare_data(args, logger, r_state, num_classes, has_val, has_ul):\n",
    "    # used for experiments with injected noise\n",
    "\n",
    "    tokenizer = load_tokenizer(args)\n",
    "    tr_data, val_data = get_training_validation_set(args, logger, tokenizer, r_state, has_val, num_classes)\n",
    "    test_data = load_and_cache_text(args, tokenizer, logger, tag='test')\n",
    "\n",
    "    # Filter specific instance indices\n",
    "    train_indices = [1,2,3,4,5,6,100,101,102,103,104,105,451,452,453,454,455,456,501,502,503,504,505,506]\n",
    "    test_indices = [1,2,3,101,103,452,455,501,502,503,504,505]\n",
    "    val_indices = [0,1,2,6,7,8,12,13,14,18,19,20]\n",
    "\n",
    "    tr_data = tr_data.select(train_indices)\n",
    "    val_data = val_data.select(val_indices)\n",
    "    test_data = test_data.select(test_indices)\n",
    "\n",
    "    n_set = TextBertDataset(args, tr_data, tokenizer, r_state, num_classes, make_noisy=True)\n",
    "    v_set = TextBertDataset(args, val_data, tokenizer, r_state, num_classes, make_noisy=True)\n",
    "    t_set = TextBertDataset(args, test_data, tokenizer, r_state, num_classes, make_noisy=False)\n",
    "\n",
    "    n_set_noisy_labels = copy.deepcopy(n_set.noisy_labels)\n",
    "    v_set_noisy_labels = copy.deepcopy(v_set.noisy_labels)\n",
    "    n_set_noisy_labels_hash = hash(tuple(n_set_noisy_labels))\n",
    "    v_set_noisy_labels_hash = hash(tuple(v_set_noisy_labels))\n",
    "    # wandb.run.summary[\"train_n_hash\"] = n_set_noisy_labels_hash\n",
    "    # wandb.run.summary[\"val_n_hash\"] = v_set_noisy_labels_hash\n",
    "\n",
    "    u_set = None\n",
    "    l2id = None\n",
    "    id2l = None\n",
    "\n",
    "    return n_set, u_set, v_set, t_set, l2id, id2l\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ae7fed",
   "metadata": {},
   "source": [
    "### Step 37"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707d2594",
   "metadata": {},
   "source": [
    "Execution Start time: 18:12:55, End time: 18:13:05 (duration: 9 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08ed535",
   "metadata": {},
   "source": [
    "**Thought**: The `prepare_data` function has been successfully updated to correctly access the dataset columns and filter the specific instance indices. Now, let's rerun the `main.py` script to train the BERT-WN model on the AG-News dataset with the specified settings and evaluate the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d39b4f",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4959598a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INFO Training started\nINFO log dir: 08_20_18_13_02_sflip_nle0.2_nlb16\nINFO loading AG_News...\n/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nTraceback (most recent call last):\n  File \"/content/BERT-LNL/main.py\", line 163, in <module>\n    main()\n  File \"/content/BERT-LNL/main.py\", line 145, in main\n    nl_set, ul_set, v_set, t_set, l2id, id2l = prepare_data(args, logger, r_state, num_classes, has_val, has_ul)\n  File \"/content/BERT-LNL/loading_utils.py\", line 26, in prepare_data\n    tr_data = tr_data.select(train_indices)\n  File \"/usr/local/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 567, in wrapper\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/datasets/fingerprint.py\", line 482, in wrapper\n    out = func(dataset, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 3963, in select\n    return self._select_with_indices_mapping(\n  File \"/usr/local/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 567, in wrapper\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/datasets/fingerprint.py\", line 482, in wrapper\n    out = func(dataset, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 4094, in _select_with_indices_mapping\n    _check_valid_indices_value(int(max(indices)), size=size)\n  File \"/usr/local/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 659, in _check_valid_indices_value\n    raise IndexError(f\"Index {index} out of range for dataset of size {size}.\")\nIndexError: Index 506 out of range for dataset of size 24.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python main.py --dataset AG_News --noise_level 0.2 --noise_type sflip --trainer_name bert_wn --model_name bert-base-uncased --num_training_steps 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128eb54d",
   "metadata": {},
   "source": [
    "### Step 38"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e82d22",
   "metadata": {},
   "source": [
    "Execution Start time: 18:13:13, End time: 18:13:15 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42153fa8",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the index 506 is out of range for the dataset, which only has 24 instances. This suggests that the indices provided for the train split are incorrect for the AG-News dataset. Let's adjust the indices to ensure they are within the valid range for the dataset.\n",
    "\n",
    "I'll edit the `loading_utils.py` file to correct the indices for the train, test, and validation splits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a38a672",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d868a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/BERT-LNL/loading_utils.py (184 lines total)]\n1:import os\n2:import copy\n3:import numpy as np\n4:import pickle\n5:import torch\n6:import wandb\n7:from tqdm import tqdm\n8:from text_dataset import TextBertDataset, TextBertRealDataset\n9:from transformers import AutoTokenizer\n10:from transformers import BertTokenizerFast, BertTokenizer\n11:import utils\n12:\n13:\n14:def prepare_data(args, logger, r_state, num_classes, has_val, has_ul):\n15:    # used for experiments with injected noise\n16:\n17:    tokenizer = load_tokenizer(args)\n18:    tr_data, val_data = get_training_validation_set(args, logger, tokenizer, r_state, has_val, num_classes)\n19:    test_data = load_and_cache_text(args, tokenizer, logger, tag='test')\n20:\n21:    # Adjusted indices to fit within the dataset size\n22:    train_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n23:    test_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n24:    val_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n25:\n26:    tr_data = tr_data.select(train_indices)\n27:    val_data = val_data.select(val_indices)\n28:    test_data = test_data.select(test_indices)\n29:\n30:    n_set = TextBertDataset(args, tr_data, tokenizer, r_state, num_classes, make_noisy=True)\n31:    v_set = TextBertDataset(args, val_data, tokenizer, r_state, num_classes, make_noisy=True)\n32:    t_set = TextBertDataset(args, test_data, tokenizer, r_state, num_classes, make_noisy=False)\n33:\n34:    n_set_noisy_labels = copy.deepcopy(n_set.noisy_labels)\n35:    v_set_noisy_labels = copy.deepcopy(v_set.noisy_labels)\n36:    n_set_noisy_labels_hash = hash(tuple(n_set_noisy_labels))\n37:    v_set_noisy_labels_hash = hash(tuple(v_set_noisy_labels))\n38:    # wandb.run.summary[\"train_n_hash\"] = n_set_noisy_labels_hash\n39:    # wandb.run.summary[\"val_n_hash\"] = v_set_noisy_labels_hash\n40:\n41:    u_set = None\n42:    l2id = None\n43:    id2l = None\n44:\n45:    return n_set, u_set, v_set, t_set, l2id, id2l\n46:    v_set_noisy_labels_hash = hash(tuple(v_set_noisy_labels))\n47:    # wandb.run.summary[\"train_n_hash\"] = n_set_noisy_labels_hash\n48:    # wandb.run.summary[\"val_n_hash\"] = v_set_noisy_labels_hash\n49:\n50:    u_set = None\n51:    l2id = None\n52:    id2l = None\n53:\n54:    return n_set, u_set, v_set, t_set, l2id, id2l\n55:    v_set_noisy_labels_hash = hash(tuple(v_set_noisy_labels))\n56:    # wandb.run.summary[\"train_n_hash\"] = n_set_noisy_labels_hash\n57:    # wandb.run.summary[\"val_n_hash\"] = v_set_noisy_labels_hash\n58:\n59:    u_set = None\n60:    l2id = None\n61:    id2l = None\n62:\n63:    return n_set, u_set, v_set, t_set, l2id, id2l\n64:\n65:\n66:def prepare_af_data(args, logger, num_classes, has_ul):\n67:    tokenizer = load_tokenizer(args)\n68:    n_set = get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag='train')\n69:    v_set = get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag='validation')\n70:    t_set = get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag='test')\n71:\n72:    assert not has_ul  # we do not have unlabeled data in Yoruba and Hausa dataset\n73:    u_set = None\n74:\n75:    label_mapping_data_dir = os.path.join(args.data_root, args.dataset, 'txt_data')\n76:    l2id = utils.pickle_load(os.path.join(label_mapping_data_dir, 'l2idx.pickle'))\n77:    id2l = utils.pickle_load(os.path.join(label_mapping_data_dir, 'idx2l.pickle'))\n78:\n79:    return n_set, u_set, v_set, t_set, l2id, id2l\n80:\n81:\n82:from datasets import load_dataset\n83:\n84:def get_training_validation_set(args, logger, tokenizer, r_state, has_val, num_classes):\n85:    # Load the AG-News dataset using the datasets library\n86:    dataset = load_dataset('ag_news')\n87:\n88:    # Filter specific instance indices\n89:    train_indices = [1,2,3,4,5,6,100,101,102,103,104,105,451,452,453,454,455,456,501,502,503,504,505,506]\n90:    val_indices = [0,1,2,6,7,8,12,13,14,18,19,20]\n91:\n92:    tr_data = dataset['train'].select(train_indices)\n93:    val_data = dataset['train'].select(val_indices)  # Use a portion of the train split as validation\n94:\n95:    return tr_data, val_data\n96:\n97:def get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag):\n98:    noisy_data_tag = f'{tag}_clean'\n99:\n100:    #get text data with noisy labels\n(84 more lines below)\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 14:36\n",
    "def prepare_data(args, logger, r_state, num_classes, has_val, has_ul):\n",
    "    # used for experiments with injected noise\n",
    "\n",
    "    tokenizer = load_tokenizer(args)\n",
    "    tr_data, val_data = get_training_validation_set(args, logger, tokenizer, r_state, has_val, num_classes)\n",
    "    test_data = load_and_cache_text(args, tokenizer, logger, tag='test')\n",
    "\n",
    "    # Adjusted indices to fit within the dataset size\n",
    "    train_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
    "    test_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "    val_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "\n",
    "    tr_data = tr_data.select(train_indices)\n",
    "    val_data = val_data.select(val_indices)\n",
    "    test_data = test_data.select(test_indices)\n",
    "\n",
    "    n_set = TextBertDataset(args, tr_data, tokenizer, r_state, num_classes, make_noisy=True)\n",
    "    v_set = TextBertDataset(args, val_data, tokenizer, r_state, num_classes, make_noisy=True)\n",
    "    t_set = TextBertDataset(args, test_data, tokenizer, r_state, num_classes, make_noisy=False)\n",
    "\n",
    "    n_set_noisy_labels = copy.deepcopy(n_set.noisy_labels)\n",
    "    v_set_noisy_labels = copy.deepcopy(v_set.noisy_labels)\n",
    "    n_set_noisy_labels_hash = hash(tuple(n_set_noisy_labels))\n",
    "    v_set_noisy_labels_hash = hash(tuple(v_set_noisy_labels))\n",
    "    # wandb.run.summary[\"train_n_hash\"] = n_set_noisy_labels_hash\n",
    "    # wandb.run.summary[\"val_n_hash\"] = v_set_noisy_labels_hash\n",
    "\n",
    "    u_set = None\n",
    "    l2id = None\n",
    "    id2l = None\n",
    "\n",
    "    return n_set, u_set, v_set, t_set, l2id, id2l\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8317b4b2",
   "metadata": {},
   "source": [
    "### Step 39"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ea9237",
   "metadata": {},
   "source": [
    "Execution Start time: 18:13:18, End time: 18:13:27 (duration: 9 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d23b59",
   "metadata": {},
   "source": [
    "**Thought**: The indices have been adjusted to fit within the dataset size. Now, let's rerun the `main.py` script to train the BERT-WN model on the AG-News dataset with the specified settings and evaluate the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ad7c1e",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53e9c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INFO Training started\nINFO log dir: 08_20_18_13_24_sflip_nle0.2_nlb16\nINFO loading AG_News...\n/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nTraceback (most recent call last):\n  File \"/content/BERT-LNL/main.py\", line 163, in <module>\n    main()\n  File \"/content/BERT-LNL/main.py\", line 145, in main\n    nl_set, ul_set, v_set, t_set, l2id, id2l = prepare_data(args, logger, r_state, num_classes, has_val, has_ul)\n  File \"/content/BERT-LNL/loading_utils.py\", line 30, in prepare_data\n    n_set = TextBertDataset(args, tr_data, tokenizer, r_state, num_classes, make_noisy=True)\n  File \"/content/BERT-LNL/text_dataset.py\", line 14, in __init__\n    self.encodings = input_data['features']\n  File \"/usr/local/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 2872, in __getitem__\n    return self._getitem(key)\n  File \"/usr/local/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 2856, in _getitem\n    pa_subtable = query_table(self._data, key, indices=self._indices)\n  File \"/usr/local/lib/python3.10/site-packages/datasets/formatting/formatting.py\", line 590, in query_table\n    _check_valid_column_key(key, table.column_names)\n  File \"/usr/local/lib/python3.10/site-packages/datasets/formatting/formatting.py\", line 527, in _check_valid_column_key\n    raise KeyError(f\"Column {key} not in the dataset. Current columns in the dataset: {columns}\")\nKeyError: \"Column features not in the dataset. Current columns in the dataset: ['text', 'label']\"\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python main.py --dataset AG_News --noise_level 0.2 --noise_type sflip --trainer_name bert_wn --model_name bert-base-uncased --num_training_steps 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9564c448",
   "metadata": {},
   "source": [
    "### Step 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b658271",
   "metadata": {},
   "source": [
    "Execution Start time: 18:13:29, End time: 18:13:29 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cac979",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the dataset does not contain a column named 'features'. Instead, it has 'text' and 'label' columns. We need to adjust the `TextBertDataset` initialization to use the correct columns. Let's open the `text_dataset.py` file and modify the `TextBertDataset` class to handle this correctly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7927840d",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b2d115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/BERT-LNL/text_dataset.py (105 lines total)]\n1:import copy\n2:import torch\n3:from torch.utils import data\n4:import numpy as np\n5:import torch.nn.functional as F\n6:from noise_functions import make_data_noisy\n7:\n8:\n9:class TextBertDataset(data.Dataset):\n10:    def __init__(self, args, input_data, tokenizer, r_state, num_classes, make_noisy=False):\n11:        # https://huggingface.co/transformers/custom_datasets.html\n12:        self.args=args\n13:        self.tokenizer = tokenizer\n14:        self.encodings = input_data['features']\n15:        self.clean_labels = input_data['labels']\n16:        self.text = input_data['text']\n17:        self.make_noisy = make_noisy\n18:        self.num_classes = num_classes\n19:        self.noisy_labels = None\n20:        self.purity_list = None\n21:\n22:\n23:        if make_noisy:\n24:            clean_labels_copy = copy.deepcopy(input_data['labels'])\n25:            nl_y = make_data_noisy(clean_labels_copy, args.noise_level, noise_type=args.noise_type, r_state=r_state,\n26:                                   num_classes=self.num_classes)\n27:            self.noisy_labels = nl_y\n28:            self.purity_list = torch.tensor((np.array(nl_y) == np.array(clean_labels_copy))).long()\n29:        else:\n30:            self.noisy_labels = -1 * torch.ones(len(input_data['labels'])).long()\n31:            self.purity_list = -1 * torch.ones(len(input_data['labels'])).long()\n32:\n33:    def get_subset_by_indices(self, indices):\n34:        sub_encodings = {key: val[indices] for key, val in self.encodings.items()}\n35:        sub_text = self.text[indices]\n36:        sub_labels = self.clean_labels[indices]\n37:        input_data = {'data':sub_encodings, 'labels':sub_labels, 'text': sub_text}\n38:\n39:        # set make_noisy to False here, because we later manually add the noisy labels to the subset\n40:        subdataset = TextBertDataset(self.args, input_data, self.tokenizer, r_state=None,\n41:                                     num_classes=self.num_classes, make_noisy=False)\n42:\n43:        subdataset.noisy_labels = self.noisy_labels[indices]\n44:        subdataset.purity_list = self.purity_list[indices]\n45:\n46:        return subdataset\n47:\n48:    def __len__(self):\n49:        return len(self.clean_labels)\n50:\n51:    def __getitem__(self, index):\n52:        item = {key: val[index] for key, val in self.encodings.items()}\n53:        item['c_labels'] = self.clean_labels[index]\n54:        item['n_labels'] = self.noisy_labels[index]\n55:        item['purity'] = self.purity_list[index]\n56:        item['index'] = index\n57:        return item\n58:\n59:\n60:\n61:class TextBertRealDataset(data.Dataset):\n62:    def __init__(self, args, input_data, noisy_labels, tokenizer, num_classes):\n63:        # The difference to TextBertDataset is that the noisy labels are provided directly in RealDataset\n64:        self.args = args\n65:        self.num_classes = num_classes\n66:        self.tokenizer = tokenizer\n67:        self.text = input_data['text']\n68:        self.encodings = input_data['features']\n69:        self.clean_labels = input_data['labels']\n70:        self.noisy_labels = noisy_labels\n71:        assert len(self.noisy_labels) == len(self.clean_labels)\n72:        self.purity_list = torch.tensor((np.array(self.clean_labels) == np.array(self.noisy_labels))).long()\n73:\n74:        self.noisy_labels_one_hot = F.one_hot(torch.tensor(self.noisy_labels))\n75:\n76:    def get_noise_mat(self):\n77:        assert self.clean_labels is not None, \"noise matrix unavailable as no clean data is available\"\n78:        noise_mat = np.zeros((self.num_classes, self.num_classes))\n79:        for i, j in zip(self.clean_labels, self.noisy_labels):\n80:            noise_mat[i, j] += 1\n81:        noise_mat = noise_mat/noise_mat.sum(axis=1)[:, None]\n82:        return noise_mat\n83:\n84:    def get_subset_by_indices(self, indices):\n85:        sub_encodings = {key: val[indices] for key, val in self.encodings.items()}\n86:\n87:        sub_text = self.text[indices]\n88:        sub_labels = self.clean_labels[indices]\n89:        input_data = {'data': sub_encodings, 'labels': sub_labels, 'text': sub_text}\n90:        sub_noisy_labels = self.noisy_labels[indices]\n91:        return TextBertRealDataset(self.args, input_data, sub_noisy_labels, self.tokenizer, self.num_classes)\n92:\n93:\n94:    def __len__(self):\n95:        return len(self.clean_labels)\n96:\n97:    def __getitem__(self, index):\n98:        item = {key: val[index] for key, val in self.encodings.items()}\n99:        item['c_labels'] = self.clean_labels[index]\n100:        item['n_labels'] = self.noisy_labels[index]\n(5 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open text_dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82f2046",
   "metadata": {},
   "source": [
    "### Step 41"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c43842a",
   "metadata": {},
   "source": [
    "Execution Start time: 18:13:33, End time: 18:13:35 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f753c568",
   "metadata": {},
   "source": [
    "**Thought**: The `TextBertDataset` class is currently trying to access a 'features' column, which doesn't exist in the dataset. Instead, we should tokenize the 'text' column to create the encodings. Let's modify the `TextBertDataset` class to tokenize the 'text' column and use it as the encodings.\n",
    "\n",
    "I'll edit the `text_dataset.py` file to make these changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad5b164",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9c8b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\n\nERRORS:\n- E999 IndentationError: expected an indented block after class definition on line 9\n\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /content/BERT-LNL/text_dataset.py (109 lines total)]\n(4 more lines above)\n4:import numpy as np\n5:import torch.nn.functional as F\n6:from noise_functions import make_data_noisy\n7:\n8:\n9:class TextBertDataset(data.Dataset):\n10:def __init__(self, args, input_data, tokenizer, r_state, num_classes, make_noisy=False):\n11:    # https://huggingface.co/transformers/custom_datasets.html\n12:    self.args=args\n13:    self.tokenizer = tokenizer\n14:    self.encodings = tokenizer(input_data['text'], truncation=True, padding=True, max_length=args.max_sen_len)\n15:    self.clean_labels = input_data['labels']\n16:    self.text = input_data['text']\n17:    self.make_noisy = make_noisy\n18:    self.num_classes = num_classes\n19:    self.noisy_labels = None\n20:    self.purity_list = None\n21:        self.make_noisy = make_noisy\n22:        self.num_classes = num_classes\n23:        self.noisy_labels = None\n24:        self.purity_list = None\n(85 more lines below)\n-------------------------------------------------\n\nThis is the original code before your edit\n-------------------------------------------------\n[File: /content/BERT-LNL/text_dataset.py (105 lines total)]\n(5 more lines above)\n5:import torch.nn.functional as F\n6:from noise_functions import make_data_noisy\n7:\n8:\n9:class TextBertDataset(data.Dataset):\n10:    def __init__(self, args, input_data, tokenizer, r_state, num_classes, make_noisy=False):\n11:        # https://huggingface.co/transformers/custom_datasets.html\n12:        self.args=args\n13:        self.tokenizer = tokenizer\n14:        self.encodings = input_data['features']\n15:        self.clean_labels = input_data['labels']\n16:        self.text = input_data['text']\n17:        self.make_noisy = make_noisy\n18:        self.num_classes = num_classes\n19:        self.noisy_labels = None\n20:        self.purity_list = None\n21:\n(84 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 10:16\n",
    "def __init__(self, args, input_data, tokenizer, r_state, num_classes, make_noisy=False):\n",
    "    # https://huggingface.co/transformers/custom_datasets.html\n",
    "    self.args=args\n",
    "    self.tokenizer = tokenizer\n",
    "    self.encodings = tokenizer(input_data['text'], truncation=True, padding=True, max_length=args.max_sen_len)\n",
    "    self.clean_labels = input_data['labels']\n",
    "    self.text = input_data['text']\n",
    "    self.make_noisy = make_noisy\n",
    "    self.num_classes = num_classes\n",
    "    self.noisy_labels = None\n",
    "    self.purity_list = None\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef878cb",
   "metadata": {},
   "source": [
    "### Step 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960f4232",
   "metadata": {},
   "source": [
    "Execution Start time: 18:13:38, End time: 18:13:39 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e16b789",
   "metadata": {},
   "source": [
    "**Thought**: The error occurred due to incorrect indentation. Let's correct the indentation for the `__init__` method in the `TextBertDataset` class. I'll ensure the method is properly aligned with the class definition.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7d54b8",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fa1ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/BERT-LNL/text_dataset.py (105 lines total)]\n1:import copy\n2:import torch\n3:from torch.utils import data\n4:import numpy as np\n5:import torch.nn.functional as F\n6:from noise_functions import make_data_noisy\n7:\n8:\n9:class TextBertDataset(data.Dataset):\n10:    def __init__(self, args, input_data, tokenizer, r_state, num_classes, make_noisy=False):\n11:        # https://huggingface.co/transformers/custom_datasets.html\n12:        self.args=args\n13:        self.tokenizer = tokenizer\n14:        self.encodings = tokenizer(input_data['text'], truncation=True, padding=True, max_length=args.max_sen_len)\n15:        self.clean_labels = input_data['labels']\n16:        self.text = input_data['text']\n17:        self.make_noisy = make_noisy\n18:        self.num_classes = num_classes\n19:        self.noisy_labels = None\n20:        self.purity_list = None\n21:\n22:\n23:        if make_noisy:\n24:            clean_labels_copy = copy.deepcopy(input_data['labels'])\n25:            nl_y = make_data_noisy(clean_labels_copy, args.noise_level, noise_type=args.noise_type, r_state=r_state,\n26:                                   num_classes=self.num_classes)\n27:            self.noisy_labels = nl_y\n28:            self.purity_list = torch.tensor((np.array(nl_y) == np.array(clean_labels_copy))).long()\n29:        else:\n30:            self.noisy_labels = -1 * torch.ones(len(input_data['labels'])).long()\n31:            self.purity_list = -1 * torch.ones(len(input_data['labels'])).long()\n32:\n33:    def get_subset_by_indices(self, indices):\n34:        sub_encodings = {key: val[indices] for key, val in self.encodings.items()}\n35:        sub_text = self.text[indices]\n36:        sub_labels = self.clean_labels[indices]\n37:        input_data = {'data':sub_encodings, 'labels':sub_labels, 'text': sub_text}\n38:\n39:        # set make_noisy to False here, because we later manually add the noisy labels to the subset\n40:        subdataset = TextBertDataset(self.args, input_data, self.tokenizer, r_state=None,\n41:                                     num_classes=self.num_classes, make_noisy=False)\n42:\n43:        subdataset.noisy_labels = self.noisy_labels[indices]\n44:        subdataset.purity_list = self.purity_list[indices]\n45:\n46:        return subdataset\n47:\n48:    def __len__(self):\n49:        return len(self.clean_labels)\n50:\n51:    def __getitem__(self, index):\n52:        item = {key: val[index] for key, val in self.encodings.items()}\n53:        item['c_labels'] = self.clean_labels[index]\n54:        item['n_labels'] = self.noisy_labels[index]\n55:        item['purity'] = self.purity_list[index]\n56:        item['index'] = index\n57:        return item\n58:\n59:\n60:\n61:class TextBertRealDataset(data.Dataset):\n62:    def __init__(self, args, input_data, noisy_labels, tokenizer, num_classes):\n63:        # The difference to TextBertDataset is that the noisy labels are provided directly in RealDataset\n64:        self.args = args\n65:        self.num_classes = num_classes\n66:        self.tokenizer = tokenizer\n67:        self.text = input_data['text']\n68:        self.encodings = input_data['features']\n69:        self.clean_labels = input_data['labels']\n70:        self.noisy_labels = noisy_labels\n71:        assert len(self.noisy_labels) == len(self.clean_labels)\n72:        self.purity_list = torch.tensor((np.array(self.clean_labels) == np.array(self.noisy_labels))).long()\n73:\n74:        self.noisy_labels_one_hot = F.one_hot(torch.tensor(self.noisy_labels))\n75:\n76:    def get_noise_mat(self):\n77:        assert self.clean_labels is not None, \"noise matrix unavailable as no clean data is available\"\n78:        noise_mat = np.zeros((self.num_classes, self.num_classes))\n79:        for i, j in zip(self.clean_labels, self.noisy_labels):\n80:            noise_mat[i, j] += 1\n81:        noise_mat = noise_mat/noise_mat.sum(axis=1)[:, None]\n82:        return noise_mat\n83:\n84:    def get_subset_by_indices(self, indices):\n85:        sub_encodings = {key: val[indices] for key, val in self.encodings.items()}\n86:\n87:        sub_text = self.text[indices]\n88:        sub_labels = self.clean_labels[indices]\n89:        input_data = {'data': sub_encodings, 'labels': sub_labels, 'text': sub_text}\n90:        sub_noisy_labels = self.noisy_labels[indices]\n91:        return TextBertRealDataset(self.args, input_data, sub_noisy_labels, self.tokenizer, self.num_classes)\n92:\n93:\n94:    def __len__(self):\n95:        return len(self.clean_labels)\n96:\n97:    def __getitem__(self, index):\n98:        item = {key: val[index] for key, val in self.encodings.items()}\n99:        item['c_labels'] = self.clean_labels[index]\n100:        item['n_labels'] = self.noisy_labels[index]\n(5 more lines below)\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 10:20\n",
    "    def __init__(self, args, input_data, tokenizer, r_state, num_classes, make_noisy=False):\n",
    "        # https://huggingface.co/transformers/custom_datasets.html\n",
    "        self.args=args\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encodings = tokenizer(input_data['text'], truncation=True, padding=True, max_length=args.max_sen_len)\n",
    "        self.clean_labels = input_data['labels']\n",
    "        self.text = input_data['text']\n",
    "        self.make_noisy = make_noisy\n",
    "        self.num_classes = num_classes\n",
    "        self.noisy_labels = None\n",
    "        self.purity_list = None\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50dd638",
   "metadata": {},
   "source": [
    "### Step 43"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84097290",
   "metadata": {},
   "source": [
    "Execution Start time: 18:13:42, End time: 18:13:51 (duration: 8 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25962aa",
   "metadata": {},
   "source": [
    "**Thought**: The `TextBertDataset` class has been successfully updated to tokenize the 'text' column and use it as the encodings. Now, let's rerun the `main.py` script to train the BERT-WN model on the AG-News dataset with the specified settings and evaluate the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52698929",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dabdb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INFO Training started\nINFO log dir: 08_20_18_13_48_sflip_nle0.2_nlb16\nINFO loading AG_News...\n/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nTraceback (most recent call last):\n  File \"/content/BERT-LNL/main.py\", line 163, in <module>\n    main()\n  File \"/content/BERT-LNL/main.py\", line 145, in main\n    nl_set, ul_set, v_set, t_set, l2id, id2l = prepare_data(args, logger, r_state, num_classes, has_val, has_ul)\n  File \"/content/BERT-LNL/loading_utils.py\", line 30, in prepare_data\n    n_set = TextBertDataset(args, tr_data, tokenizer, r_state, num_classes, make_noisy=True)\n  File \"/content/BERT-LNL/text_dataset.py\", line 15, in __init__\n    self.clean_labels = input_data['labels']\n  File \"/usr/local/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 2872, in __getitem__\n    return self._getitem(key)\n  File \"/usr/local/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 2856, in _getitem\n    pa_subtable = query_table(self._data, key, indices=self._indices)\n  File \"/usr/local/lib/python3.10/site-packages/datasets/formatting/formatting.py\", line 590, in query_table\n    _check_valid_column_key(key, table.column_names)\n  File \"/usr/local/lib/python3.10/site-packages/datasets/formatting/formatting.py\", line 527, in _check_valid_column_key\n    raise KeyError(f\"Column {key} not in the dataset. Current columns in the dataset: {columns}\")\nKeyError: \"Column labels not in the dataset. Current columns in the dataset: ['text', 'label']\"\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python main.py --dataset AG_News --noise_level 0.2 --noise_type sflip --trainer_name bert_wn --model_name bert-base-uncased --num_training_steps 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f80bde",
   "metadata": {},
   "source": [
    "### Step 44"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03d90a0",
   "metadata": {},
   "source": [
    "Execution Start time: 18:13:53, End time: 18:13:54 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0fe4d3",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the dataset column is named 'label' instead of 'labels'. Let's update the `TextBertDataset` class to use the correct column name 'label'. I'll edit the `text_dataset.py` file to fix this issue.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1424d36",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc55fe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/BERT-LNL/text_dataset.py (105 lines total)]\n1:import copy\n2:import torch\n3:from torch.utils import data\n4:import numpy as np\n5:import torch.nn.functional as F\n6:from noise_functions import make_data_noisy\n7:\n8:\n9:class TextBertDataset(data.Dataset):\n10:    def __init__(self, args, input_data, tokenizer, r_state, num_classes, make_noisy=False):\n11:        # https://huggingface.co/transformers/custom_datasets.html\n12:        self.args=args\n13:        self.tokenizer = tokenizer\n14:        self.encodings = tokenizer(input_data['text'], truncation=True, padding=True, max_length=args.max_sen_len)\n15:        self.clean_labels = input_data['label']\n16:        self.text = input_data['text']\n17:        self.make_noisy = make_noisy\n18:        self.num_classes = num_classes\n19:        self.noisy_labels = None\n20:        self.purity_list = None\n21:\n22:\n23:        if make_noisy:\n24:            clean_labels_copy = copy.deepcopy(input_data['labels'])\n25:            nl_y = make_data_noisy(clean_labels_copy, args.noise_level, noise_type=args.noise_type, r_state=r_state,\n26:                                   num_classes=self.num_classes)\n27:            self.noisy_labels = nl_y\n28:            self.purity_list = torch.tensor((np.array(nl_y) == np.array(clean_labels_copy))).long()\n29:        else:\n30:            self.noisy_labels = -1 * torch.ones(len(input_data['labels'])).long()\n31:            self.purity_list = -1 * torch.ones(len(input_data['labels'])).long()\n32:\n33:    def get_subset_by_indices(self, indices):\n34:        sub_encodings = {key: val[indices] for key, val in self.encodings.items()}\n35:        sub_text = self.text[indices]\n36:        sub_labels = self.clean_labels[indices]\n37:        input_data = {'data':sub_encodings, 'labels':sub_labels, 'text': sub_text}\n38:\n39:        # set make_noisy to False here, because we later manually add the noisy labels to the subset\n40:        subdataset = TextBertDataset(self.args, input_data, self.tokenizer, r_state=None,\n41:                                     num_classes=self.num_classes, make_noisy=False)\n42:\n43:        subdataset.noisy_labels = self.noisy_labels[indices]\n44:        subdataset.purity_list = self.purity_list[indices]\n45:\n46:        return subdataset\n47:\n48:    def __len__(self):\n49:        return len(self.clean_labels)\n50:\n51:    def __getitem__(self, index):\n52:        item = {key: val[index] for key, val in self.encodings.items()}\n53:        item['c_labels'] = self.clean_labels[index]\n54:        item['n_labels'] = self.noisy_labels[index]\n55:        item['purity'] = self.purity_list[index]\n56:        item['index'] = index\n57:        return item\n58:\n59:\n60:\n61:class TextBertRealDataset(data.Dataset):\n62:    def __init__(self, args, input_data, noisy_labels, tokenizer, num_classes):\n63:        # The difference to TextBertDataset is that the noisy labels are provided directly in RealDataset\n64:        self.args = args\n65:        self.num_classes = num_classes\n66:        self.tokenizer = tokenizer\n67:        self.text = input_data['text']\n68:        self.encodings = input_data['features']\n69:        self.clean_labels = input_data['labels']\n70:        self.noisy_labels = noisy_labels\n71:        assert len(self.noisy_labels) == len(self.clean_labels)\n72:        self.purity_list = torch.tensor((np.array(self.clean_labels) == np.array(self.noisy_labels))).long()\n73:\n74:        self.noisy_labels_one_hot = F.one_hot(torch.tensor(self.noisy_labels))\n75:\n76:    def get_noise_mat(self):\n77:        assert self.clean_labels is not None, \"noise matrix unavailable as no clean data is available\"\n78:        noise_mat = np.zeros((self.num_classes, self.num_classes))\n79:        for i, j in zip(self.clean_labels, self.noisy_labels):\n80:            noise_mat[i, j] += 1\n81:        noise_mat = noise_mat/noise_mat.sum(axis=1)[:, None]\n82:        return noise_mat\n83:\n84:    def get_subset_by_indices(self, indices):\n85:        sub_encodings = {key: val[indices] for key, val in self.encodings.items()}\n86:\n87:        sub_text = self.text[indices]\n88:        sub_labels = self.clean_labels[indices]\n89:        input_data = {'data': sub_encodings, 'labels': sub_labels, 'text': sub_text}\n90:        sub_noisy_labels = self.noisy_labels[indices]\n91:        return TextBertRealDataset(self.args, input_data, sub_noisy_labels, self.tokenizer, self.num_classes)\n92:\n93:\n94:    def __len__(self):\n95:        return len(self.clean_labels)\n96:\n97:    def __getitem__(self, index):\n98:        item = {key: val[index] for key, val in self.encodings.items()}\n99:        item['c_labels'] = self.clean_labels[index]\n100:        item['n_labels'] = self.noisy_labels[index]\n(5 more lines below)\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 15:15\n",
    "        self.clean_labels = input_data['label']\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762bfda9",
   "metadata": {},
   "source": [
    "### Step 45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eace69",
   "metadata": {},
   "source": [
    "Execution Start time: 18:13:56, End time: 18:14:04 (duration: 7 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aab30a3",
   "metadata": {},
   "source": [
    "**Thought**: The `TextBertDataset` class has been successfully updated to use the correct column name 'label'. Now, let's rerun the `main.py` script to train the BERT-WN model on the AG-News dataset with the specified settings and evaluate the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6190083",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f34acc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INFO Training started\nINFO log dir: 08_20_18_14_02_sflip_nle0.2_nlb16\nINFO loading AG_News...\n/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nTraceback (most recent call last):\n  File \"/content/BERT-LNL/main.py\", line 163, in <module>\n    main()\n  File \"/content/BERT-LNL/main.py\", line 145, in main\n    nl_set, ul_set, v_set, t_set, l2id, id2l = prepare_data(args, logger, r_state, num_classes, has_val, has_ul)\n  File \"/content/BERT-LNL/loading_utils.py\", line 30, in prepare_data\n    n_set = TextBertDataset(args, tr_data, tokenizer, r_state, num_classes, make_noisy=True)\n  File \"/content/BERT-LNL/text_dataset.py\", line 24, in __init__\n    clean_labels_copy = copy.deepcopy(input_data['labels'])\n  File \"/usr/local/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 2872, in __getitem__\n    return self._getitem(key)\n  File \"/usr/local/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 2856, in _getitem\n    pa_subtable = query_table(self._data, key, indices=self._indices)\n  File \"/usr/local/lib/python3.10/site-packages/datasets/formatting/formatting.py\", line 590, in query_table\n    _check_valid_column_key(key, table.column_names)\n  File \"/usr/local/lib/python3.10/site-packages/datasets/formatting/formatting.py\", line 527, in _check_valid_column_key\n    raise KeyError(f\"Column {key} not in the dataset. Current columns in the dataset: {columns}\")\nKeyError: \"Column labels not in the dataset. Current columns in the dataset: ['text', 'label']\"\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python main.py --dataset AG_News --noise_level 0.2 --noise_type sflip --trainer_name bert_wn --model_name bert-base-uncased --num_training_steps 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9c5c39",
   "metadata": {},
   "source": [
    "### Step 46"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff286bec",
   "metadata": {},
   "source": [
    "Execution Start time: 18:14:06, End time: 18:14:07 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3431e1c4",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that there is still a reference to 'labels' instead of 'label' in the `TextBertDataset` class. Let's update any remaining references to use the correct column name 'label'. I'll edit the `text_dataset.py` file to fix this issue.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead272a8",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4c044c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\n\nERRORS:\n- E999 IndentationError: expected an indented block after 'if' statement on line 23\n\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /content/BERT-LNL/text_dataset.py (105 lines total)]\n(18 more lines above)\n18:        self.num_classes = num_classes\n19:        self.noisy_labels = None\n20:        self.purity_list = None\n21:\n22:\n23:        if make_noisy:\n24:        clean_labels_copy = copy.deepcopy(input_data['label'])\n25:            nl_y = make_data_noisy(clean_labels_copy, args.noise_level, noise_type=args.noise_type, r_state=r_state,\n26:                                   num_classes=self.num_classes)\n27:            self.noisy_labels = nl_y\n28:            self.purity_list = torch.tensor((np.array(nl_y) == np.array(clean_labels_copy))).long()\n(77 more lines below)\n-------------------------------------------------\n\nThis is the original code before your edit\n-------------------------------------------------\n[File: /content/BERT-LNL/text_dataset.py (105 lines total)]\n(19 more lines above)\n19:        self.noisy_labels = None\n20:        self.purity_list = None\n21:\n22:\n23:        if make_noisy:\n24:            clean_labels_copy = copy.deepcopy(input_data['labels'])\n25:            nl_y = make_data_noisy(clean_labels_copy, args.noise_level, noise_type=args.noise_type, r_state=r_state,\n26:                                   num_classes=self.num_classes)\n27:            self.noisy_labels = nl_y\n28:            self.purity_list = torch.tensor((np.array(nl_y) == np.array(clean_labels_copy))).long()\n29:        else:\n(76 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 24:24\n",
    "        clean_labels_copy = copy.deepcopy(input_data['label'])\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfff5a8",
   "metadata": {},
   "source": [
    "### Step 47"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87656965",
   "metadata": {},
   "source": [
    "Execution Start time: 18:14:10, End time: 18:14:11 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504cf193",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that there was an indentation issue with the `if` statement. Let's correct the indentation for the block under the `if make_noisy:` condition. I'll ensure the code is properly aligned.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d74ae10",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269ee4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/BERT-LNL/text_dataset.py (105 lines total)]\n1:import copy\n2:import torch\n3:from torch.utils import data\n4:import numpy as np\n5:import torch.nn.functional as F\n6:from noise_functions import make_data_noisy\n7:\n8:\n9:class TextBertDataset(data.Dataset):\n10:    def __init__(self, args, input_data, tokenizer, r_state, num_classes, make_noisy=False):\n11:        # https://huggingface.co/transformers/custom_datasets.html\n12:        self.args=args\n13:        self.tokenizer = tokenizer\n14:        self.encodings = tokenizer(input_data['text'], truncation=True, padding=True, max_length=args.max_sen_len)\n15:        self.clean_labels = input_data['label']\n16:        self.text = input_data['text']\n17:        self.make_noisy = make_noisy\n18:        self.num_classes = num_classes\n19:        self.noisy_labels = None\n20:        self.purity_list = None\n21:\n22:\n23:        if make_noisy:\n24:            clean_labels_copy = copy.deepcopy(input_data['label'])\n25:            nl_y = make_data_noisy(clean_labels_copy, args.noise_level, noise_type=args.noise_type, r_state=r_state,\n26:                                   num_classes=self.num_classes)\n27:            self.noisy_labels = nl_y\n28:            self.purity_list = torch.tensor((np.array(nl_y) == np.array(clean_labels_copy))).long()\n29:        else:\n30:            self.noisy_labels = -1 * torch.ones(len(input_data['labels'])).long()\n31:            self.purity_list = -1 * torch.ones(len(input_data['labels'])).long()\n32:\n33:    def get_subset_by_indices(self, indices):\n34:        sub_encodings = {key: val[indices] for key, val in self.encodings.items()}\n35:        sub_text = self.text[indices]\n36:        sub_labels = self.clean_labels[indices]\n37:        input_data = {'data':sub_encodings, 'labels':sub_labels, 'text': sub_text}\n38:\n39:        # set make_noisy to False here, because we later manually add the noisy labels to the subset\n40:        subdataset = TextBertDataset(self.args, input_data, self.tokenizer, r_state=None,\n41:                                     num_classes=self.num_classes, make_noisy=False)\n42:\n43:        subdataset.noisy_labels = self.noisy_labels[indices]\n44:        subdataset.purity_list = self.purity_list[indices]\n45:\n46:        return subdataset\n47:\n48:    def __len__(self):\n49:        return len(self.clean_labels)\n50:\n51:    def __getitem__(self, index):\n52:        item = {key: val[index] for key, val in self.encodings.items()}\n53:        item['c_labels'] = self.clean_labels[index]\n54:        item['n_labels'] = self.noisy_labels[index]\n55:        item['purity'] = self.purity_list[index]\n56:        item['index'] = index\n57:        return item\n58:\n59:\n60:\n61:class TextBertRealDataset(data.Dataset):\n62:    def __init__(self, args, input_data, noisy_labels, tokenizer, num_classes):\n63:        # The difference to TextBertDataset is that the noisy labels are provided directly in RealDataset\n64:        self.args = args\n65:        self.num_classes = num_classes\n66:        self.tokenizer = tokenizer\n67:        self.text = input_data['text']\n68:        self.encodings = input_data['features']\n69:        self.clean_labels = input_data['labels']\n70:        self.noisy_labels = noisy_labels\n71:        assert len(self.noisy_labels) == len(self.clean_labels)\n72:        self.purity_list = torch.tensor((np.array(self.clean_labels) == np.array(self.noisy_labels))).long()\n73:\n74:        self.noisy_labels_one_hot = F.one_hot(torch.tensor(self.noisy_labels))\n75:\n76:    def get_noise_mat(self):\n77:        assert self.clean_labels is not None, \"noise matrix unavailable as no clean data is available\"\n78:        noise_mat = np.zeros((self.num_classes, self.num_classes))\n79:        for i, j in zip(self.clean_labels, self.noisy_labels):\n80:            noise_mat[i, j] += 1\n81:        noise_mat = noise_mat/noise_mat.sum(axis=1)[:, None]\n82:        return noise_mat\n83:\n84:    def get_subset_by_indices(self, indices):\n85:        sub_encodings = {key: val[indices] for key, val in self.encodings.items()}\n86:\n87:        sub_text = self.text[indices]\n88:        sub_labels = self.clean_labels[indices]\n89:        input_data = {'data': sub_encodings, 'labels': sub_labels, 'text': sub_text}\n90:        sub_noisy_labels = self.noisy_labels[indices]\n91:        return TextBertRealDataset(self.args, input_data, sub_noisy_labels, self.tokenizer, self.num_classes)\n92:\n93:\n94:    def __len__(self):\n95:        return len(self.clean_labels)\n96:\n97:    def __getitem__(self, index):\n98:        item = {key: val[index] for key, val in self.encodings.items()}\n99:        item['c_labels'] = self.clean_labels[index]\n100:        item['n_labels'] = self.noisy_labels[index]\n(5 more lines below)\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 23:28\n",
    "        if make_noisy:\n",
    "            clean_labels_copy = copy.deepcopy(input_data['label'])\n",
    "            nl_y = make_data_noisy(clean_labels_copy, args.noise_level, noise_type=args.noise_type, r_state=r_state,\n",
    "                                   num_classes=self.num_classes)\n",
    "            self.noisy_labels = nl_y\n",
    "            self.purity_list = torch.tensor((np.array(nl_y) == np.array(clean_labels_copy))).long()\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab9b428",
   "metadata": {},
   "source": [
    "### Step 48"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7413f76",
   "metadata": {},
   "source": [
    "Execution Start time: 18:14:14, End time: 18:14:21 (duration: 6 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff134059",
   "metadata": {},
   "source": [
    "**Thought**: The `TextBertDataset` class has been successfully updated to use the correct column name 'label'. Now, let's rerun the `main.py` script to train the BERT-WN model on the AG-News dataset with the specified settings and evaluate the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852a552d",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5fc828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INFO Training started\nINFO log dir: 08_20_18_14_19_sflip_nle0.2_nlb16\nINFO loading AG_News...\n/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nTraceback (most recent call last):\n  File \"/content/BERT-LNL/main.py\", line 163, in <module>\n    main()\n  File \"/content/BERT-LNL/main.py\", line 145, in main\n    nl_set, ul_set, v_set, t_set, l2id, id2l = prepare_data(args, logger, r_state, num_classes, has_val, has_ul)\n  File \"/content/BERT-LNL/loading_utils.py\", line 31, in prepare_data\n    v_set = TextBertDataset(args, val_data, tokenizer, r_state, num_classes, make_noisy=True)\n  File \"/content/BERT-LNL/text_dataset.py\", line 25, in __init__\n    nl_y = make_data_noisy(clean_labels_copy, args.noise_level, noise_type=args.noise_type, r_state=r_state,\n  File \"/content/BERT-LNL/noise_functions.py\", line 33, in make_data_noisy\n    _, noisy_data = make_noisy_single_flip(y, noise_level, r_state, num_classes)\n  File \"/content/BERT-LNL/noise_functions.py\", line 95, in make_noisy_single_flip\n    assert num_classes == len(set(y))\nAssertionError\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python main.py --dataset AG_News --noise_level 0.2 --noise_type sflip --trainer_name bert_wn --model_name bert-base-uncased --num_training_steps 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df83021b",
   "metadata": {},
   "source": [
    "### Step 49"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868bd8c4",
   "metadata": {},
   "source": [
    "Execution Start time: 18:14:25, End time: 18:14:25 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7411f47",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates an assertion failure because the number of classes does not match the number of unique labels in the dataset. This suggests that the `num_classes` parameter might be incorrectly set. Let's verify and adjust the `num_classes` parameter to match the AG-News dataset, which typically has 4 classes.\n",
    "\n",
    "I'll edit the `main.py` file to ensure the `num_classes` parameter is set correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a182beba",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f7f9a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/BERT-LNL/main.py (163 lines total)]\n1:import argparse\n2:from loading_utils import prepare_data, prepare_af_data\n3:from utils import create_logger, save_args, create_trainer, load_config, save_config\n4:import numpy as np\n5:import torch\n6:import random\n7:\n8:\n9:def main():\n10:    parser = argparse.ArgumentParser()\n11:    parser.add_argument('--dataset', type=str, default='IMDB', choices=['SST-2', 'AG_News',\n12:                                                                        'TREC', 'IMDB', 'Yelp-5',\n13:                                                                        'Yoruba', 'Hausa'])\n14:    parser.add_argument('--data_root', type=str, default=\"\")\n15:    parser.add_argument('--log_root', type=str, default=\"\",\n16:                        help='output directory to save logs in training/testing')\n17:\n18:    parser.add_argument('--trainer_name', type=str, default='bert_wn',\n19:                        choices=['bert_wn', 'bert_ct', 'bert_cm', 'bert_cmgt', 'bert_smoothing'],\n20:                        help='trainer selection: '\n21:                             'bert_wn: without noise-handling,'\n22:                             'bert_ct: co-teaching, '\n23:                             'bert_cm: noise matrix, '\n24:                             'bert_cmgt: ground truth noise matrix,'\n25:                             'bert_smoothing: label smoothing')\n26:    parser.add_argument('--model_name', type=str, default='bert-base-uncased',\n27:                        choices=['bert-base-uncased', 'bert-base-cased',\n28:                                 'bert-large-uncased', 'bert-base-multilingual-cased'],\n29:                        help='backbone selection')\n30:    parser.add_argument('--exp_name', type=str, default='')\n31:\n32:\n33:    # Preprocessing Related\n34:    parser.add_argument('--max_sen_len', type=int, default=512,\n35:                        help='max sentence length, longer sentences will be truncated')\n36:    parser.add_argument('--special_token_offsets', type=int, default=2,\n37:                        help='number of special tokens used in bert tokenizer for text classification')\n38:    parser.add_argument('--truncate_mode', type=str, default='last',\n39:                        choices=['hybrid, last'], help='last: last 510 tokens, hybrid: first 128 + last 382')\n40:    parser.add_argument('--freeze_bert', action='store_true',\n41:                        help='freeze the bert backbone, i.e. use bert as feature extractor')\n42:\n43:    # BERT settings Related\n44:    parser.add_argument('--bert_dropout_rate', type=float, default=0.1)\n45:    parser.add_argument('--noise_level', type=float, default=0.0,\n46:                        help='noise level for injected noise')\n47:    parser.add_argument('--noise_type', default='uniform_m',\n48:                        choices=['uniform_m', 'sflip'],\n49:                        help='noise types: uniform_m: uniform noise, sflip: single-flip noise')\n50:    parser.add_argument('--val_fraction', type=float, default=0.1,\n51:                        help='if no validation set is provided, use this fraction of training set as validation set')\n52:\n53:    # training related\n54:    parser.add_argument('--num_epochs', type=int, default=1, help='set either num_epochs or num_training_steps')\n55:    parser.add_argument('--num_training_steps', type=int, default=5, help='set it to -1 if num_epochs is set')\n56:    parser.add_argument('--train_eval_freq', type=int, default=10,\n57:                        help='evaluate the model on training set after every [train_eval_freq] training steps')\n58:    parser.add_argument('--eval_freq', type=int, default=50,\n59:                        help='evaluate the model on the validation and test sets'\n60:                             'after every [eval_freq] training steps')\n61:    parser.add_argument('--fast_eval', action='store_true',\n62:                        help='use 10% of the test set for evaluation, to speed up the evaluation prcoess')\n63:\n64:    parser.add_argument('--nl_batch_size', type=int, default=16,\n65:                        help='noisy labeled samples per batch, can be understood as the training batch size')\n66:    parser.add_argument('--eval_batch_size', type=int, default=50,\n67:                        help='evaluation batch size during testing')\n68:    parser.add_argument('--warmup_steps', type=int, default=0,\n69:                        help='warmup steps for learning rate scheduler')\n70:    parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n71:                        help=\"number of updates steps to accumulate before performing a backward/update pass.\")\n72:\n73:    parser.add_argument('--gen_val', action='store_true',\n74:                        help='generate validation set, enable it if there is no validation set')\n75:    parser.add_argument('--store_model', type=int, default=0, help='save models after training')\n76:\n77:\n78:    # co-teaching related\n79:    parser.add_argument('--forget_factor', type=float, default=1.0)\n80:    parser.add_argument('--T_k', type=int, default=10)\n81:    parser.add_argument('--c', type=float, default=1.0)\n82:\n83:\n84:    # smoothing trainer related\n85:    parser.add_argument('--smoothing_factor', type=float, default=0.2,\n86:                        help='label smoothing levels, 0.0 means no smoothing')\n87:\n88:\n89:    # cm trainer related\n90:    parser.add_argument('--cm_mse_weight', type=float, default=0.01,\n91:                        help='regularization factor for the mse loss, check https://aclanthology.org/N19-1328/')\n92:\n93:\n94:    # optimizer related\n95:    parser.add_argument('--lr', type=float, default=2e-5)\n96:    parser.add_argument(\"--weight_decay\", default=0.0, type=float,\n97:                        help=\"Weight deay if we apply some.\")\n98:    parser.add_argument('--exp_decay_rate', type=float, default=0.9998)\n99:    parser.add_argument('--max_grad_norm', type=float, default=1.0)\n100:    parser.add_argument('--patience', type=float, default=20, help='patience for early stopping')\n(63 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53fce64",
   "metadata": {},
   "source": [
    "### Step 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6f03be",
   "metadata": {},
   "source": [
    "Execution Start time: 18:14:28, End time: 18:14:28 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a75278e",
   "metadata": {},
   "source": [
    "**Thought**: To resolve the issue, I need to ensure that the `num_classes` parameter is set to 4 for the AG-News dataset. Let's search for where `num_classes` is set or used in the `main.py` file and adjust it accordingly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33916294",
   "metadata": {},
   "source": [
    "**Action (search_file)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc8eeae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bash: line 1: syntax error near unexpected token `\"num_classes\"'\nbash: line 1: `source /content/commands/_env_vars.sh && source /content/commands/defaults.sh && source /content/commands/search.sh && source /content/commands/edit_linting.sh && search_file(\"num_classes\")'\nError: CalledProcessError: Command 'b'source /content/commands/_env_vars.sh && source /content/commands/defaults.sh && source /content/commands/search.sh && source /content/commands/edit_linting.sh && search_file(\"num_classes\")\\n'' returned non-zero exit status 2.---------------------------------------------------------------------------CalledProcessError                        Traceback (most recent call last)Cell In[113], line 1\n----> 1 get_ipython().run_cell_magic('bash', '', 'source /content/commands/_env_vars.sh && source /content/commands/defaults.sh && source /content/commands/search.sh && source /content/commands/edit_linting.sh && search_file(\"num_classes\")\\n')\nFile /usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2541, in InteractiveShell.run_cell_magic(self, magic_name, line, cell)\n   2539 with self.builtin_trap:\n   2540     args = (magic_arg_s, cell)\n-> 2541     result = fn(*args, **kwargs)\n   2543 # The code below prevents the output from being displayed\n   2544 # when using magics with decorator @output_can_be_silenced\n   2545 # when the last Python token in the expression is a ';'.\n   2546 if getattr(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, False):\nFile /usr/local/lib/python3.10/site-packages/IPython/core/magics/script.py:155, in ScriptMagics._make_script_magic.<locals>.named_script_magic(line, cell)\n    153 else:\n    154     line = script\n--> 155 return self.shebang(line, cell)\nFile /usr/local/lib/python3.10/site-packages/IPython/core/magics/script.py:315, in ScriptMagics.shebang(self, line, cell)\n    310 if args.raise_error and p.returncode != 0:\n    311     # If we get here and p.returncode is still None, we must have\n    312     # killed it but not yet seen its return code. We don't wait for it,\n    313     # in case it's stuck in uninterruptible sleep. -9 = SIGKILL\n    314     rc = p.returncode or -9\n--> 315     raise CalledProcessError(rc, cell)\nCalledProcessError: Command 'b'source /content/commands/_env_vars.sh && source /content/commands/defaults.sh && source /content/commands/search.sh && source /content/commands/edit_linting.sh && search_file(\"num_classes\")\\n'' returned non-zero exit status 2."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_file(\"num_classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6361b099",
   "metadata": {},
   "source": [
    "### Step 51"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7974839e",
   "metadata": {},
   "source": [
    "Execution Start time: 18:14:30, End time: 18:14:30 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0928a1",
   "metadata": {},
   "source": [
    "**Thought**: It seems there was an error with the search command. Let's manually scroll through the `main.py` file to locate where the `num_classes` parameter is set or used. I'll start by scrolling down to see more of the file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89d7782",
   "metadata": {},
   "source": [
    "**Action (scroll_down)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dba34eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/BERT-LNL/main.py (163 lines total)]\n(63 more lines above)\n64:    parser.add_argument('--nl_batch_size', type=int, default=16,\n65:                        help='noisy labeled samples per batch, can be understood as the training batch size')\n66:    parser.add_argument('--eval_batch_size', type=int, default=50,\n67:                        help='evaluation batch size during testing')\n68:    parser.add_argument('--warmup_steps', type=int, default=0,\n69:                        help='warmup steps for learning rate scheduler')\n70:    parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n71:                        help=\"number of updates steps to accumulate before performing a backward/update pass.\")\n72:\n73:    parser.add_argument('--gen_val', action='store_true',\n74:                        help='generate validation set, enable it if there is no validation set')\n75:    parser.add_argument('--store_model', type=int, default=0, help='save models after training')\n76:\n77:\n78:    # co-teaching related\n79:    parser.add_argument('--forget_factor', type=float, default=1.0)\n80:    parser.add_argument('--T_k', type=int, default=10)\n81:    parser.add_argument('--c', type=float, default=1.0)\n82:\n83:\n84:    # smoothing trainer related\n85:    parser.add_argument('--smoothing_factor', type=float, default=0.2,\n86:                        help='label smoothing levels, 0.0 means no smoothing')\n87:\n88:\n89:    # cm trainer related\n90:    parser.add_argument('--cm_mse_weight', type=float, default=0.01,\n91:                        help='regularization factor for the mse loss, check https://aclanthology.org/N19-1328/')\n92:\n93:\n94:    # optimizer related\n95:    parser.add_argument('--lr', type=float, default=2e-5)\n96:    parser.add_argument(\"--weight_decay\", default=0.0, type=float,\n97:                        help=\"Weight deay if we apply some.\")\n98:    parser.add_argument('--exp_decay_rate', type=float, default=0.9998)\n99:    parser.add_argument('--max_grad_norm', type=float, default=1.0)\n100:    parser.add_argument('--patience', type=float, default=20, help='patience for early stopping')\n101:\n102:    # hardware related\n103:    parser.add_argument('--use_cuda', type=bool, default=True)\n104:    parser.add_argument('--num_workers', type=int, default=0)\n105:    parser.add_argument('--cuda_device', type=str, default=\"0\")\n106:    parser.add_argument('--manualSeed', type=int, default=1234, help='random seed for reproducibility')\n107:    parser.add_argument('--noisy_label_seed', type=int, default=1234, help='random seed for reproducibility')\n108:\n109:    args = parser.parse_args()\n110:\n111:    if args.manualSeed is None:\n112:        args.manualSeed = random.randint(1, 10000)\n113:    random.seed(args.manualSeed)\n114:    np.random.seed(args.manualSeed)\n115:    torch.manual_seed(args.manualSeed)\n116:    torch.backends.cudnn.benchmark = False\n117:    if args.use_cuda:\n118:        torch.cuda.manual_seed_all(args.manualSeed)\n119:        torch.backends.cudnn.deterministic = True\n120:\n121:    # Create the Handler for logging records/messages to a file\n122:    logger, log_dir = create_logger(args.log_root, args)\n123:    save_args(log_dir, args)\n124:    logger.info(\"Training started\")\n125:    num_classes_map = {'AG_News': 4, 'TREC': 6, 'SST-2': 2, 'IMDB': 2, 'Yelp-5': 5,\n126:                       'Yoruba':7, 'Hausa':5}\n127:\n128:    logger.info(f'log dir: {log_dir}')\n129:    num_classes = num_classes_map[args.dataset]\n130:    r_state = np.random.RandomState(args.noisy_label_seed)\n131:\n132:    if args.dataset in ['SST-2', 'AG_News', 'TREC', 'IMDB', 'Yelp-5']:\n133:        logger.info(f'loading {args.dataset}...')\n134:\n135:\n136:        if args.dataset in ['SST-2', 'AG_News', 'TREC', 'Yelp-5']:\n137:            has_val = False\n138:            has_ul = False\n139:        elif args.dataset in ['IMDB']:\n140:            has_val = True\n141:            has_ul = True\n142:        else:\n143:            raise ValueError('need to set has_val and has_ul')\n144:\n145:        nl_set, ul_set, v_set, t_set, l2id, id2l = prepare_data(args, logger, r_state, num_classes, has_val, has_ul)\n146:    elif args.dataset in ['Yoruba', 'Hausa']:\n147:        has_ul=False\n148:        nl_set, ul_set, v_set, t_set, l2id, id2l = prepare_af_data(args, logger,\n149:                                                                   num_classes, has_ul)\n150:    else:\n151:        raise NotImplementedError(f\"dataset {args.dataset} not supported\")\n152:\n153:\n154:    model_config = load_config(args)\n155:    model_config['num_classes'] = num_classes\n156:\n157:    trainer = create_trainer(args, logger, log_dir, model_config, (nl_set, ul_set, v_set, t_set, l2id, id2l), r_state)\n158:    trainer.train(args, logger, (nl_set, ul_set, v_set, t_set, l2id, id2l))\n159:    save_config(log_dir, 'model_config', trainer.model_config)  # model_config could be updated during model creation\n160:\n161:\n162:if __name__=='__main__':\n163:    main()\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scroll_down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45b2ad5",
   "metadata": {},
   "source": [
    "### Step 52"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47eac45",
   "metadata": {},
   "source": [
    "Execution Start time: 18:14:33, End time: 18:14:34 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d10dae",
   "metadata": {},
   "source": [
    "**Thought**: The `num_classes` is set using a mapping dictionary `num_classes_map` which correctly assigns 4 classes for the 'AG_News' dataset. The issue might be elsewhere, possibly in how the labels are being processed. Let's ensure that the labels are being correctly handled in the dataset preparation. I'll review the `prepare_data` function to ensure the labels are correctly processed.\n",
    "\n",
    "Let's open the `loading_utils.py` file to verify the label processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8724d98",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdd722f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/BERT-LNL/loading_utils.py (184 lines total)]\n1:import os\n2:import copy\n3:import numpy as np\n4:import pickle\n5:import torch\n6:import wandb\n7:from tqdm import tqdm\n8:from text_dataset import TextBertDataset, TextBertRealDataset\n9:from transformers import AutoTokenizer\n10:from transformers import BertTokenizerFast, BertTokenizer\n11:import utils\n12:\n13:\n14:def prepare_data(args, logger, r_state, num_classes, has_val, has_ul):\n15:    # used for experiments with injected noise\n16:\n17:    tokenizer = load_tokenizer(args)\n18:    tr_data, val_data = get_training_validation_set(args, logger, tokenizer, r_state, has_val, num_classes)\n19:    test_data = load_and_cache_text(args, tokenizer, logger, tag='test')\n20:\n21:    # Adjusted indices to fit within the dataset size\n22:    train_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n23:    test_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n24:    val_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n25:\n26:    tr_data = tr_data.select(train_indices)\n27:    val_data = val_data.select(val_indices)\n28:    test_data = test_data.select(test_indices)\n29:\n30:    n_set = TextBertDataset(args, tr_data, tokenizer, r_state, num_classes, make_noisy=True)\n31:    v_set = TextBertDataset(args, val_data, tokenizer, r_state, num_classes, make_noisy=True)\n32:    t_set = TextBertDataset(args, test_data, tokenizer, r_state, num_classes, make_noisy=False)\n33:\n34:    n_set_noisy_labels = copy.deepcopy(n_set.noisy_labels)\n35:    v_set_noisy_labels = copy.deepcopy(v_set.noisy_labels)\n36:    n_set_noisy_labels_hash = hash(tuple(n_set_noisy_labels))\n37:    v_set_noisy_labels_hash = hash(tuple(v_set_noisy_labels))\n38:    # wandb.run.summary[\"train_n_hash\"] = n_set_noisy_labels_hash\n39:    # wandb.run.summary[\"val_n_hash\"] = v_set_noisy_labels_hash\n40:\n41:    u_set = None\n42:    l2id = None\n43:    id2l = None\n44:\n45:    return n_set, u_set, v_set, t_set, l2id, id2l\n46:    v_set_noisy_labels_hash = hash(tuple(v_set_noisy_labels))\n47:    # wandb.run.summary[\"train_n_hash\"] = n_set_noisy_labels_hash\n48:    # wandb.run.summary[\"val_n_hash\"] = v_set_noisy_labels_hash\n49:\n50:    u_set = None\n51:    l2id = None\n52:    id2l = None\n53:\n54:    return n_set, u_set, v_set, t_set, l2id, id2l\n55:    v_set_noisy_labels_hash = hash(tuple(v_set_noisy_labels))\n56:    # wandb.run.summary[\"train_n_hash\"] = n_set_noisy_labels_hash\n57:    # wandb.run.summary[\"val_n_hash\"] = v_set_noisy_labels_hash\n58:\n59:    u_set = None\n60:    l2id = None\n61:    id2l = None\n62:\n63:    return n_set, u_set, v_set, t_set, l2id, id2l\n64:\n65:\n66:def prepare_af_data(args, logger, num_classes, has_ul):\n67:    tokenizer = load_tokenizer(args)\n68:    n_set = get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag='train')\n69:    v_set = get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag='validation')\n70:    t_set = get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag='test')\n71:\n72:    assert not has_ul  # we do not have unlabeled data in Yoruba and Hausa dataset\n73:    u_set = None\n74:\n75:    label_mapping_data_dir = os.path.join(args.data_root, args.dataset, 'txt_data')\n76:    l2id = utils.pickle_load(os.path.join(label_mapping_data_dir, 'l2idx.pickle'))\n77:    id2l = utils.pickle_load(os.path.join(label_mapping_data_dir, 'idx2l.pickle'))\n78:\n79:    return n_set, u_set, v_set, t_set, l2id, id2l\n80:\n81:\n82:from datasets import load_dataset\n83:\n84:def get_training_validation_set(args, logger, tokenizer, r_state, has_val, num_classes):\n85:    # Load the AG-News dataset using the datasets library\n86:    dataset = load_dataset('ag_news')\n87:\n88:    # Filter specific instance indices\n89:    train_indices = [1,2,3,4,5,6,100,101,102,103,104,105,451,452,453,454,455,456,501,502,503,504,505,506]\n90:    val_indices = [0,1,2,6,7,8,12,13,14,18,19,20]\n91:\n92:    tr_data = dataset['train'].select(train_indices)\n93:    val_data = dataset['train'].select(val_indices)  # Use a portion of the train split as validation\n94:\n95:    return tr_data, val_data\n96:\n97:def get_clean_and_noisy_data_by_tag(args, logger, tokenizer, num_classes, tag):\n98:    noisy_data_tag = f'{tag}_clean'\n99:\n100:    #get text data with noisy labels\n(84 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open loading_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8410aa3e",
   "metadata": {},
   "source": [
    "### Step 53"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0385f2",
   "metadata": {},
   "source": [
    "Execution Start time: 18:14:37, End time: 18:14:37 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f35695",
   "metadata": {},
   "source": [
    "**Thought**: The `prepare_data` function seems to be correctly setting up the datasets. However, the error might be due to the `make_data_noisy` function expecting a different number of classes than what is present in the dataset. Let's verify the `make_data_noisy` function in the `noise_functions.py` file to ensure it handles the number of classes correctly.\n",
    "\n",
    "I'll open the `noise_functions.py` file to investigate further.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38f199a",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1807cc81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/BERT-LNL/noise_functions.py (100 lines total)]\n1:import numpy as np\n2:import copy\n3:\n4:\n5:\n6:def make_noisy_general(clean_data, noise_matrix, random_state, num_classes):\n7:    \"\"\" Perturbs the MNIST labels based on the probabilities of the given noise matrix\n8:\n9:    Args:\n10:        clean_data: list of instances\n11:        noise_matrix: defines the noise process\n12:        random_state: for reproducibility\n13:\n14:    Returns:\n15:        A perturbed copy of clean_data (the noisy_data)\n16:    \"\"\"\n17:    for row in noise_matrix:\n18:        assert np.isclose(np.sum(row), 1)\n19:\n20:    assert len(noise_matrix) == num_classes\n21:\n22:    noisy_data = copy.deepcopy(clean_data)\n23:    for i in range(len(noisy_data)):\n24:        probability_row = noise_matrix[noisy_data[i]]\n25:        noisy_data[i] = random_state.choice(num_classes, p=probability_row)\n26:    return noisy_data\n27:\n28:\n29:def make_data_noisy(y, noise_level, noise_type, r_state, num_classes):\n30:    assert noise_type in ['sflip', 'uniform', 'uniform_m']\n31:\n32:    if noise_type == 'sflip':\n33:        _, noisy_data = make_noisy_single_flip(y, noise_level, r_state, num_classes)\n34:    elif noise_type == 'uniform':\n35:        _, noisy_data = make_noisy_uniform(y, noise_level, r_state, num_classes)\n36:    elif noise_type == 'uniform_m':\n37:        _, noisy_data = make_noisy_uniform_m(y, noise_level, r_state, num_classes)\n38:    else:\n39:        raise NotImplementedError('noise type not supported')\n40:\n41:\n42:    return noisy_data\n43:\n44:def make_noisy_uniform(y, noise_level, r_state, num_classes):\n45:    assert num_classes == len(set(y))\n46:    clean_label_probability = 1 - noise_level\n47:    uniform_noise_probability = noise_level / num_classes  # distribute noise_level across all other labels\n48:    clean_label_probability += uniform_noise_probability\n49:\n50:    true_noise_matrix = np.empty((num_classes, num_classes))\n51:    true_noise_matrix.fill(uniform_noise_probability)\n52:    for true_label in range(num_classes):\n53:        true_noise_matrix[true_label][true_label] = clean_label_probability\n54:\n55:    noisy_data = make_noisy_general(y, true_noise_matrix, r_state, num_classes)\n56:\n57:    return true_noise_matrix, noisy_data\n58:\n59:def get_uniform_m_flip_mat(noise_level, num_classes):\n60:    clean_label_probability = 1 - noise_level\n61:    uniform_noise_probability = noise_level / (num_classes - 1)  # distribute noise_level across all other labels\n62:\n63:    true_noise_matrix = np.empty((num_classes, num_classes))\n64:    true_noise_matrix.fill(uniform_noise_probability)\n65:    for true_label in range(num_classes):\n66:        true_noise_matrix[true_label][true_label] = clean_label_probability\n67:\n68:    return true_noise_matrix\n69:\n70:\n71:def make_noisy_uniform_m(y, noise_level, r_state, num_classes):\n72:    assert num_classes == len(set(y))\n73:\n74:    true_noise_matrix = get_uniform_m_flip_mat(noise_level, num_classes)\n75:\n76:    noisy_data = make_noisy_general(y, true_noise_matrix, r_state, num_classes)\n77:\n78:    return true_noise_matrix, noisy_data\n79:\n80:\n81:\n82:def get_single_flip_mat(noise_level, num_classes):\n83:    flips = np.arange(num_classes)\n84:    flips = np.roll(flips, 1)\n85:\n86:    true_noise_matrix = np.zeros((num_classes, num_classes))\n87:    for true_label in range(num_classes):\n88:        true_noise_matrix[true_label][true_label] = 1 - noise_level\n89:        true_noise_matrix[true_label][flips[true_label]] = noise_level\n90:    return true_noise_matrix\n91:\n92:\n93:\n94:def make_noisy_single_flip(y, noise_level, r_state, num_classes):\n95:    assert num_classes == len(set(y))\n96:    true_noise_matrix = get_single_flip_mat(noise_level, num_classes)\n97:\n98:    noisy_data = make_noisy_general(y, true_noise_matrix, r_state, num_classes)\n99:\n100:    return true_noise_matrix, noisy_data\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open noise_functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e76e8d",
   "metadata": {},
   "source": [
    "### Step 54"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ade6b4",
   "metadata": {},
   "source": [
    "Execution Start time: 18:14:40, End time: 18:14:41 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0e41f3",
   "metadata": {},
   "source": [
    "**Thought**: The `make_noisy_single_flip` function asserts that the number of classes matches the number of unique labels in `y`. This assertion is failing, which suggests that the `num_classes` parameter might not be set correctly or the labels are not being processed as expected.\n",
    "\n",
    "Let's ensure that the `num_classes` parameter is set to 4 for the AG-News dataset and verify that the labels are correctly processed. I'll edit the `main.py` file to ensure the `num_classes` parameter is set correctly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0885d29",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744224d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/BERT-LNL/main.py (163 lines total)]\n1:import argparse\n2:from loading_utils import prepare_data, prepare_af_data\n3:from utils import create_logger, save_args, create_trainer, load_config, save_config\n4:import numpy as np\n5:import torch\n6:import random\n7:\n8:\n9:def main():\n10:    parser = argparse.ArgumentParser()\n11:    parser.add_argument('--dataset', type=str, default='IMDB', choices=['SST-2', 'AG_News',\n12:                                                                        'TREC', 'IMDB', 'Yelp-5',\n13:                                                                        'Yoruba', 'Hausa'])\n14:    parser.add_argument('--data_root', type=str, default=\"\")\n15:    parser.add_argument('--log_root', type=str, default=\"\",\n16:                        help='output directory to save logs in training/testing')\n17:\n18:    parser.add_argument('--trainer_name', type=str, default='bert_wn',\n19:                        choices=['bert_wn', 'bert_ct', 'bert_cm', 'bert_cmgt', 'bert_smoothing'],\n20:                        help='trainer selection: '\n21:                             'bert_wn: without noise-handling,'\n22:                             'bert_ct: co-teaching, '\n23:                             'bert_cm: noise matrix, '\n24:                             'bert_cmgt: ground truth noise matrix,'\n25:                             'bert_smoothing: label smoothing')\n26:    parser.add_argument('--model_name', type=str, default='bert-base-uncased',\n27:                        choices=['bert-base-uncased', 'bert-base-cased',\n28:                                 'bert-large-uncased', 'bert-base-multilingual-cased'],\n29:                        help='backbone selection')\n30:    parser.add_argument('--exp_name', type=str, default='')\n31:\n32:\n33:    # Preprocessing Related\n34:    parser.add_argument('--max_sen_len', type=int, default=512,\n35:                        help='max sentence length, longer sentences will be truncated')\n36:    parser.add_argument('--special_token_offsets', type=int, default=2,\n37:                        help='number of special tokens used in bert tokenizer for text classification')\n38:    parser.add_argument('--truncate_mode', type=str, default='last',\n39:                        choices=['hybrid, last'], help='last: last 510 tokens, hybrid: first 128 + last 382')\n40:    parser.add_argument('--freeze_bert', action='store_true',\n41:                        help='freeze the bert backbone, i.e. use bert as feature extractor')\n42:\n43:    # BERT settings Related\n44:    parser.add_argument('--bert_dropout_rate', type=float, default=0.1)\n45:    parser.add_argument('--noise_level', type=float, default=0.0,\n46:                        help='noise level for injected noise')\n47:    parser.add_argument('--noise_type', default='uniform_m',\n48:                        choices=['uniform_m', 'sflip'],\n49:                        help='noise types: uniform_m: uniform noise, sflip: single-flip noise')\n50:    parser.add_argument('--val_fraction', type=float, default=0.1,\n51:                        help='if no validation set is provided, use this fraction of training set as validation set')\n52:\n53:    # training related\n54:    parser.add_argument('--num_epochs', type=int, default=1, help='set either num_epochs or num_training_steps')\n55:    parser.add_argument('--num_training_steps', type=int, default=5, help='set it to -1 if num_epochs is set')\n56:    parser.add_argument('--train_eval_freq', type=int, default=10,\n57:                        help='evaluate the model on training set after every [train_eval_freq] training steps')\n58:    parser.add_argument('--eval_freq', type=int, default=50,\n59:                        help='evaluate the model on the validation and test sets'\n60:                             'after every [eval_freq] training steps')\n61:    parser.add_argument('--fast_eval', action='store_true',\n62:                        help='use 10% of the test set for evaluation, to speed up the evaluation prcoess')\n63:\n64:    parser.add_argument('--nl_batch_size', type=int, default=16,\n65:                        help='noisy labeled samples per batch, can be understood as the training batch size')\n66:    parser.add_argument('--eval_batch_size', type=int, default=50,\n67:                        help='evaluation batch size during testing')\n68:    parser.add_argument('--warmup_steps', type=int, default=0,\n69:                        help='warmup steps for learning rate scheduler')\n70:    parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n71:                        help=\"number of updates steps to accumulate before performing a backward/update pass.\")\n72:\n73:    parser.add_argument('--gen_val', action='store_true',\n74:                        help='generate validation set, enable it if there is no validation set')\n75:    parser.add_argument('--store_model', type=int, default=0, help='save models after training')\n76:\n77:\n78:    # co-teaching related\n79:    parser.add_argument('--forget_factor', type=float, default=1.0)\n80:    parser.add_argument('--T_k', type=int, default=10)\n81:    parser.add_argument('--c', type=float, default=1.0)\n82:\n83:\n84:    # smoothing trainer related\n85:    parser.add_argument('--smoothing_factor', type=float, default=0.2,\n86:                        help='label smoothing levels, 0.0 means no smoothing')\n87:\n88:\n89:    # cm trainer related\n90:    parser.add_argument('--cm_mse_weight', type=float, default=0.01,\n91:                        help='regularization factor for the mse loss, check https://aclanthology.org/N19-1328/')\n92:\n93:\n94:    # optimizer related\n95:    parser.add_argument('--lr', type=float, default=2e-5)\n96:    parser.add_argument(\"--weight_decay\", default=0.0, type=float,\n97:                        help=\"Weight deay if we apply some.\")\n98:    parser.add_argument('--exp_decay_rate', type=float, default=0.9998)\n99:    parser.add_argument('--max_grad_norm', type=float, default=1.0)\n100:    parser.add_argument('--patience', type=float, default=20, help='patience for early stopping')\n(63 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0615b430",
   "metadata": {},
   "source": [
    "### Step 55"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9641b49e",
   "metadata": {},
   "source": [
    "Execution Start time: 18:14:52, End time: 18:14:53 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d06deb",
   "metadata": {},
   "source": [
    "**Thought**: Exit due to context window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e285484a",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa5bf05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Exited"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exit_context"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
