{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9c2d9af",
   "metadata": {},
   "source": [
    "### Query\n",
    "Use the `search/sentiment.py` script to generate model outputs for sentiment classification on the imdb dataset with the flan-t5-base model. For the first prompt (prompt 0), for each instance id, report the predicted label (label with highest probability) as a json structured as follows: {\"10345\": 0, ...} (replace 10345 with the actual instance id and 0 with the actual predicted label for each instance id).\n",
    "\n",
    "Additional instructions:\n",
    "1. Only load the first 10 examples of each (training/validation/test) set in the dataset.\n",
    "\n",
    "Git repository: https://github.com/adianliusie/robust-prompt-classifier\n",
    "\n",
    "I have already executed some of the required steps. Now, you should fix any remaining issues. Your goal is to successfully run\n",
    "```\n",
    "!python search/sentiment.py --transformer flan-t5-base --path outputs/flan-t5-large/ --lim 10 --epochs 1\n",
    "ls outputs/flan-t5-large/imdb/prompt_0/\n",
    "import pickle\n",
    "with open(r\"outputs/flan-t5-large/imdb/prompt_0/probs.pk\", \"rb\") as input_file:\n",
    "   probs = pickle.load(input_file)\n",
    "import numpy as np\n",
    "for pid, prob in probs.items():\n",
    "  print(pid, \"::\", np.argmax(prob))\n",
    "```\n",
    "and submit the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15842df2",
   "metadata": {},
   "source": [
    "### Prompt\n",
    "<details>\n",
    "[{'role': 'system', 'content': 'Interact in a jupyter notebook to solve the request.\\nOutput a thought and an action.\\nThought can reason about the current situation, and action allows you to interact with the Jupyter notebook. There are three types of actions:\\n(1) execute, which executes a notebook cell with the command under the \"content\" field. This could be a bash command (beginning with !), a python command, or magic command (beginning with %).\\n(2) edit, which allows you to replace existing lines in a file with new lines. To replace lines, you will need to provide the lines as they appear before the replacement, and the lines to replace them with.\\nIn the \"content\" field, format should be ```{filename}\\n<<<BEFORE_EDIT>>>\\nlines before edit\\nlines before edit\\n<<<AFTER_EDIT>>>\\nlines after edit\\nlines after edit```\\n  - Include the entire lines WITH indentation. After editing, make sure indentation is correct.\\n(3) submit, which submits the final answer (or null if there\\'s no answer to submit), formatted as JSON (make sure to use double quotes for keys and values).\\nAlways output exactly one \"thought\" and one \"action\", and encapsulate the content with <content>...</content> tags.\\nI will provide the cell output (observation) for you.\\n\\nTips:\\n- When a github repository is given, you should start by cloning it. You can read the readme file and browse the repo for installation and usage instructions.\\n- Avoid using conda, instead use pip for package management.\\n- Each execution has a 5-minute timeout. After this period, you can decide whether to interrupt the kernel or continue waiting.\\n  You should interrupt if: (1) you are prompted with an interactive message (e.g., \"Do you want to continue? [Y/n]\"). In that case, you can bypass it by executing echo \"y\" | command, or (2) the execution is taking longer than expected because the wrong task is being executed, such as a different model or incorrect number of epochs.\\n  You should continue to wait if the current execution is correct and proceeding as expected.\\n\\nHere are some examples.\\nGit repository: EleutherAI/lm-evaluation-harness\\nQuery: Evaluate EleutherAI/gpt-j-6B on hellaswag. What\\'s the score?\\nThought: First, I need to clone the GitHub repository \\'EleutherAI/lm-evaluation-harness\\'.\\nAction: execute\\n<content>\\n!git clone https://github.com/EleutherAI/lm-evaluation-harness.git\\n</content>\\nObservation: ```\\nCloning into \\'lm-evaluation-harness\\'...\\n```\\nThought: The repository has been cloned. Next, I should check the README file for installation and usage instructions to proceed with evaluating EleutherAI/gpt-j-6B on hellaswag.\\nAction: execute\\n<content>\\n!cat lm-evaluation-harness/README.md\\n</content>\\nObservation: ```# Language Model Evaluation Harness ... truncated ...\\n```\\nThought: The README provides detailed instructions for installing the `lm-eval` package and evaluating models like GPT-J-6B on tasks like hellaswag. I need to install the package and its dependencies first.\\nAction: execute\\n<content>\\n!pip install -e lm-evaluation-harness/.\\n</content>\\nObservation: ```\\nObtaining file:///home/nora/lm-evaluation-harness\\n  Installing build dependencies ... truncated ...\\n```\\nThought: The `lm-eval` package and its dependencies have been installed successfully. Now, I can proceed with evaluating the EleutherAI/gpt-j-6B model on the hellaswag task as per the instructions in the README.\\nAction: execute\\n<content>\\n!lm_eval --model hf --model_args pretrained=EleutherAI/gpt-j-6B --tasks hellaswag --device cuda:0 --batch_size 8\\n</content>\\nObservation: ```\\n{\\n  \"hellaswag\": {\\n    \"acc\": 0.24603658536585363,\\n    \"acc_norm\": 0.34436059706446426,\\n  },  ... truncated ...\\n```\\nThought: The `lm-eval` was executed successfully, providing the metrics I needed. The `acc` metric is 0.2460. The `acc_norm` metric is 0.3444\\nAction: submit\\n<content>\\n{\"acc\": 0.2460, \"acc_norm\": 0.3444}\\n</content>\\n\\nQuery: use pandas to read the file \\'data.csv\\' and display the first 2 rows.\\nThought: First, I need to read the file \\'data.csv\\' using the pandas library.\\nAction: execute\\n<content>\\nimport pandas as pd\\npd.read_csv(\\'data.csv\\').head()\\n</content>\\nObservation: ```\\n    name\\n  0  Alice\\n   1    Bob\\n  2  Charlie\\n```\\nThought: The file \\'data.csv\\' has been read successfully and the first 2 rows have been displayed.\\nAction: submit\\n<content>\\n[\"Alice\", \"Bob\"]\\n</content>\\n\\nQuery: Change the model name in the \\'config.json\\' file from \\'roberta-base\\' to \\'roberta-large\\'.\\nThought: First, I need to read the \\'config.json\\' file to find the location where I can replace the model name.\\nAction: execute\\n<content>\\n!cat config.json\\n</content>\\nObservation: ```\\n{\\n    \"model\": \"roberta-base\",\\n    \"batch_size\": 8,\\n    \"epochs\": 10\\n}\\n```\\nThought: The file has been read successfully. Now, I can replace the model name in the \\'config.json\\' file.\\nAction: edit\\n<content>\\nconfig.json\\n<<<BEFORE_EDIT>>>\\n{\\n    \"model\": \"roberta-base\",\\n    \"batch_size\": 8,\\n<<<AFTER_EDIT>>>\\n{\\n    \"model\": \"roberta-large\",\\n    \"batch_size\": 8,\\n</content>\\nObservation: \"editing file config.json\"\\nThought: The model name has been replaced in the \\'config.json\\' file.\\nAction: submit\\n<content>\\nnull\\n</content>\\n'}, {'role': 'user', 'content': 'Query: Use the `search/sentiment.py` script to generate model outputs for sentiment classification on the imdb dataset with the flan-t5-base model. For the first prompt (prompt 0), for each instance id, report the predicted label (label with highest probability) as a json structured as follows: {\"10345\": 0, ...} (replace 10345 with the actual instance id and 0 with the actual predicted label for each instance id).\\n\\nAdditional instructions:\\n1. Only load the first 10 examples of each (training/validation/test) set in the dataset.\\n\\nGit repository: https://github.com/adianliusie/robust-prompt-classifier\\n\\nI have already executed some of the required steps. Now, you should fix any remaining issues. Your goal is to successfully run\\n```\\n!python search/sentiment.py --transformer flan-t5-base --path outputs/flan-t5-large/ --lim 10 --epochs 1\\nls outputs/flan-t5-large/imdb/prompt_0/\\nimport pickle\\nwith open(r\"outputs/flan-t5-large/imdb/prompt_0/probs.pk\", \"rb\") as input_file:\\n   probs = pickle.load(input_file)\\nimport numpy as np\\nfor pid, prob in probs.items():\\n  print(pid, \"::\", np.argmax(prob))\\n```\\nand submit the answer.'}]\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09273b50",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0162124",
   "metadata": {},
   "source": [
    "Execution Start time: 00:00:26, End time: 00:00:26 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaeda40",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1f83e8",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57edd12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cloning into 'robust-prompt-classifier'...\nremote: Enumerating objects: 90, done.\u001b[K\nremote: Counting objects: 100% (90/90)\u001b[K\nremote: Counting objects: 100% (90/90), done.\u001b[K\nremote: Compressing objects: 100% (63/63)\u001b[K\nremote: Compressing objects: 100% (63/63), done.\u001b[K\nremote: Total 90 (delta 31), reused 79 (delta 25), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (90/90)\nReceiving objects: 100% (90/90), 1.29 MiB | 27.01 MiB/s, done.\nResolving deltas: 100% (31/31)\nResolving deltas: 100% (31/31), done.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!git clone https://github.com/adianliusie/robust-prompt-classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f801de84",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c9924e",
   "metadata": {},
   "source": [
    "Execution Start time: 00:00:26, End time: 00:00:27 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dfed17",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2e61b5",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e61623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/robust-prompt-classifier\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd robust-prompt-classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51878cdf",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2829eb15",
   "metadata": {},
   "source": [
    "Execution Start time: 00:00:27, End time: 00:00:27 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c273afd",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a54dff",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc3c51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n    train_parser = get_train_parser()\n\n    # Parse system input arguments \n    model_args, moargs = model_parser.parse_known_args()\n    train_args, toargs = train_parser.parse_known_args()\n    \n    # Making sure no unkown arguments are given\n    assert set(moargs).isdisjoint(toargs), f\"{set(moargs) & set(toargs)}\"\n    \n    # get experiment specific arguments\n    model_args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    train_args.device = model_args.device\n    lim = train_args.lim \n    output_path = model_args.path\n\n    # == Print out the arguments ==================================================================#\n    print(f\"Model Arguments: {model_args}\")\n    print(f\"Train Arguments: {train_args}\")\n\n    #== Set Up Zero Shot Model ====================================================================#\n    trainer = Trainer(f'models/{model_args.transformer}', model_args)\n    train_args.lim = 0\n    trainer.train(train_args)\n\n    #== Set Up Evaluation =========================================================================#\n    evaluater = deepcopy(trainer)\n    evaluater.__class__ = Evaluater"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/robust-prompt-classifier/search/prompt_search.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "    assert set(moargs).isdisjoint(toargs), f\"{set(moargs) & set(toargs)}\"\n",
    "    \n",
    "    # get experiment specific arguments\n",
    "    lim = train_args.lim \n",
    "    output_path = model_args.path\n",
    "\n",
    "    #== Set Up Zero Shot Model ====================================================================#\n",
    "    trainer = Trainer(f'models/{model_args.transformer}', model_args)\n",
    "<<<AFTER_EDIT>>>\n",
    "    assert set(moargs).isdisjoint(toargs), f\"{set(moargs) & set(toargs)}\"\n",
    "    \n",
    "    # get experiment specific arguments\n",
    "    model_args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    train_args.device = model_args.device\n",
    "    lim = train_args.lim \n",
    "    output_path = model_args.path\n",
    "\n",
    "    # == Print out the arguments ==================================================================#\n",
    "    print(f\"Model Arguments: {model_args}\")\n",
    "    print(f\"Train Arguments: {train_args}\")\n",
    "\n",
    "    #== Set Up Zero Shot Model ====================================================================#\n",
    "    trainer = Trainer(f'models/{model_args.transformer}', model_args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d21f65",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30ebb75",
   "metadata": {},
   "source": [
    "Execution Start time: 00:00:27, End time: 00:00:27 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9796d94a",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a409dc",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a82a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n    print(f\"Train Arguments: {train_args}\")\n\n    #== Set Up Zero Shot Model ====================================================================#\n    trainer = Trainer(f'models/{model_args.transformer}', model_args)\n    train_args.lim = 0\n    trainer.train(train_args)\n\n    #== Set Up Evaluation =========================================================================#\n    evaluater = deepcopy(trainer)\n    evaluater.__class__ = Evaluater\n    evaluater.device = model_args.device\n    evaluater.model.eval()\n    # update model rods used to get logits\n    label_words = [word for label_class in label_word_sets for word in label_class]\n    evaluater.model.update_label_words(label_words)\n    word_to_idx = {word:k for k, word in enumerate(label_words)}\n\n    # save prompts used in the experiment"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/robust-prompt-classifier/search/prompt_search.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "    #== Set Up Evaluation =========================================================================#\n",
    "    evaluater = deepcopy(trainer)\n",
    "    evaluater.__class__ = Evaluater\n",
    "    evaluater.device = 'cuda'\n",
    "    evaluater.model.eval()\n",
    "\n",
    "<<<AFTER_EDIT>>>\n",
    "    #== Set Up Evaluation =========================================================================#\n",
    "    evaluater = deepcopy(trainer)\n",
    "    evaluater.__class__ = Evaluater\n",
    "    evaluater.device = model_args.device\n",
    "    evaluater.model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c19af1",
   "metadata": {},
   "source": [
    "### Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199184b2",
   "metadata": {},
   "source": [
    "Execution Start time: 00:00:27, End time: 00:00:28 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc83471",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae701cc0",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18630fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\nfrom prompt_search import prompt_search_experiment\n\nif __name__ == '__main__':\n    #== Set Exp arguments (beforehand) ============================================================#\n    # datasets = ['imdb', 'rt', 'amazon-s']\n    datasets = ['imdb']\n\n    templates = [\n        'classify the following review: <t>',\n        'how was the movie? <t>',\n        'which word best describes the text? <t>',\n        'what is the sentiment? <t>',\n        \"what is the reviewer's verdict? <t>\",\n        'is the following movie good or bad? <t>'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/robust-prompt-classifier/search/sentiment.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #== Set Exp arguments (beforehand) ============================================================#\n",
    "    datasets = ['imdb', 'rt', 'amazon-s']\n",
    "\n",
    "    templates = [\n",
    "<<<AFTER_EDIT>>>\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #== Set Exp arguments (beforehand) ============================================================#\n",
    "    # datasets = ['imdb', 'rt', 'amazon-s']\n",
    "    datasets = ['imdb']\n",
    "\n",
    "    templates = [\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7633e5a2",
   "metadata": {},
   "source": [
    "### Step 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f87ca20",
   "metadata": {},
   "source": [
    "Execution Start time: 00:00:28, End time: 00:00:34 (duration: 6 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ffcd9f",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bc0900",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b093c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting wandb\n  Downloading wandb-0.17.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/site-packages (from wandb) (8.1.7)\nCollecting docker-pycreds>=0.4.0 (from wandb)\n  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\nCollecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/site-packages (from wandb) (4.2.2)\nRequirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/site-packages (from wandb) (5.27.1)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/site-packages (from wandb) (5.9.8)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/site-packages (from wandb) (2.31.0)\nCollecting sentry-sdk>=1.0.0 (from wandb)\n  Downloading sentry_sdk-2.13.0-py2.py3-none-any.whl.metadata (9.7 kB)\nCollecting setproctitle (from wandb)\n  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from wandb) (68.1.2)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nCollecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.6.2)\nCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\nDownloading wandb-0.17.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.3 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/9.3 MB ? eta -:--:--\n\n\n\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\nDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/207.3 kB ? eta -:--:--\n\n\u001b[?25hDownloading sentry_sdk-2.13.0-py2.py3-none-any.whl (309 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/309.1 kB ? eta -:--:--\n\n\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\nDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/62.7 kB ? eta -:--:--\n\n\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\nInstalling collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\nSuccessfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.13.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.7\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c6fc00",
   "metadata": {},
   "source": [
    "### Step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4097f024",
   "metadata": {},
   "source": [
    "Execution Start time: 00:00:34, End time: 00:00:34 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933e5ed8",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fc60c6",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6a69dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want latest dataset module, despite the repo suggest 2.12.0 we need 2.19.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df3b53b",
   "metadata": {},
   "source": [
    "### Step 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9b9aa0",
   "metadata": {},
   "source": [
    "Execution Start time: 00:00:34, End time: 00:00:41 (duration: 6 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddb3f02",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adf4243",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae6a1cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting datasets\n  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from datasets) (1.25.2)\nCollecting pyarrow>=15.0.0 (from datasets)\n  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\nCollecting dill<0.3.9,>=0.3.0 (from datasets)\n  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from datasets) (2.0.3)\nCollecting requests>=2.32.2 (from datasets)\n  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/site-packages (from datasets) (4.66.4)\nCollecting xxhash (from datasets)\n  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nCollecting multiprocess (from datasets)\n  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2023.6.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/site-packages (from datasets) (3.8.3)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/site-packages (from datasets) (0.23.3)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from datasets) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (2.1.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.0.7)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.6.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDownloading datasets-2.21.0-py3-none-any.whl (527 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/527.3 kB ? eta -:--:--\n\n\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/116.3 kB ? eta -:--:--\n\n\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/39.9 MB ? eta -:--:--\n\n\n\n\n\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/64.9 kB ? eta -:--:--\n\n\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/134.8 kB ? eta -:--:--\n\n\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/194.1 kB ? eta -:--:--\n\n\u001b[?25hInstalling collected packages: xxhash, requests, pyarrow, dill, multiprocess, datasets\n  Attempting uninstall: requests\n    Found existing installation: requests 2.31.0\n    Uninstalling requests-2.31.0:\n      Successfully uninstalled requests-2.31.0\nSuccessfully installed datasets-2.21.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 requests-2.32.3 xxhash-3.5.0\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8ad528",
   "metadata": {},
   "source": [
    "### Step 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db27cfc",
   "metadata": {},
   "source": [
    "Execution Start time: 00:00:42, End time: 00:00:45 (duration: 2 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a362432",
   "metadata": {},
   "source": [
    "**Thought**: The necessary packages have been installed, and the script has been modified to use only the IMDb dataset. Now, I will execute the `search/sentiment.py` script with the specified parameters to generate model outputs for sentiment classification on the IMDb dataset using the flan-t5-base model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1920524",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b8820f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Traceback (most recent call last):\n  File \"/content/robust-prompt-classifier/search/sentiment.py\", line 1, in <module>\n    from prompt_search import prompt_search_experiment\n  File \"/content/robust-prompt-classifier/search/prompt_search.py\", line 14, in <module>\n    from src.handlers.trainer import Trainer\nModuleNotFoundError: No module named 'src'\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python search/sentiment.py --transformer flan-t5-base --path outputs/flan-t5-large/ --lim 10 --epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfb4a2e",
   "metadata": {},
   "source": [
    "### Step 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2530f941",
   "metadata": {},
   "source": [
    "Execution Start time: 00:00:46, End time: 00:00:47 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817ca52f",
   "metadata": {},
   "source": [
    "**Thought**: It seems there is a `ModuleNotFoundError` for the `src` module. This indicates that the script is unable to locate the `src` directory, which likely contains the necessary modules. I need to check the directory structure to ensure that the `src` directory is correctly placed or adjust the import paths accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36520c67",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5b5338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "README.md  analysis  requirements.txt  search  src\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48740ce9",
   "metadata": {},
   "source": [
    "### Step 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a63d99",
   "metadata": {},
   "source": [
    "Execution Start time: 00:00:48, End time: 00:00:50 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b128df0",
   "metadata": {},
   "source": [
    "**Thought**: The `src` directory is present in the root of the repository. The issue might be related to the Python path not including the root directory. I will modify the `PYTHONPATH` to include the current directory so that the script can locate the `src` module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636d297a",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9d384d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Traceback (most recent call last):\n  File \"/content/robust-prompt-classifier/search/sentiment.py\", line 1, in <module>\n    from prompt_search import prompt_search_experiment\n  File \"/content/robust-prompt-classifier/search/prompt_search.py\", line 14, in <module>\n    from src.handlers.trainer import Trainer\nModuleNotFoundError: No module named 'src'\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('.')\n",
    "!python search/sentiment.py --transformer flan-t5-base --path outputs/flan-t5-large/ --lim 10 --epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dde0c2",
   "metadata": {},
   "source": [
    "### Step 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928db5df",
   "metadata": {},
   "source": [
    "Execution Start time: 00:00:51, End time: 00:01:41 (duration: 49 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81915f53",
   "metadata": {},
   "source": [
    "**Thought**: The `ModuleNotFoundError` persists, indicating that the `src` directory is still not being recognized. I will set the `PYTHONPATH` environment variable to include the current directory before running the script to ensure that Python can locate the `src` module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652080b7",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2975c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2024-08-21 00:00:56 INFO     generated new fontManager\nModel Arguments: Namespace(path='outputs/flan-t5-large/', transformer='flan-t5-base', prompt_finetuning=False, label_words=['bad', 'good'], template='<t>', loss=None, maxlen=512, num_classes=2, rand_seed=None, device='cpu')\nTrain Arguments: Namespace(dataset='sst', bias=None, lim=10, epochs=1, bsz=4, lr=1e-05, data_ordering=False, grad_clip=1, freeze_trans=None, log_every=400, val_every=50000, early_stop=3, wandb=False, device='cpu')\n2024-08-21 00:00:57 INFO     random seed set to 296\n/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\ntokenizer_config.json:   0%|                        | 0.00/2.54k [00:00<?, ?B/s]\n\n\nspiece.model:   0%|                                  | 0.00/792k [00:00<?, ?B/s]\n\n\ntokenizer.json:   0%|                               | 0.00/2.42M [00:00<?, ?B/s]\n\n\nspecial_tokens_map.json:   0%|                      | 0.00/2.20k [00:00<?, ?B/s]\n\n\nconfig.json:   0%|                                  | 0.00/1.40k [00:00<?, ?B/s]\n\n\nmodel.safetensors:   0%|                             | 0.00/990M [00:00<?, ?B/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngeneration_config.json:   0%|                         | 0.00/147 [00:00<?, ?B/s]\n\n2024-08-21 00:01:06 INFO     Number of parameters in model 247.6M\n[[1282], [9412], [2714], [17425], [2841], [207], [248], [1237], [2723], [1465]]\nimdb\n0\n\nDownloading readme:   0%|                           | 0.00/7.81k [00:00<?, ?B/s]\nDownloading readme: 100%|███████████████████| 7.81k/7.81k [00:00<00:00, 279kB/s]\n\nDownloading data:   0%|                             | 0.00/21.0M [00:00<?, ?B/s]\n\n\n\nDownloading data:   0%|                             | 0.00/20.5M [00:00<?, ?B/s]\n\n\n\nDownloading data:   0%|                             | 0.00/42.0M [00:00<?, ?B/s]\n\n\n\n\nGenerating train split:   0%|                  | 0/25000 [00:00<?, ? examples/s]\nGenerating train split:  84%|█▋| 21000/25000 [00:00<00:00, 196612.39 examples/s]\nGenerating train split: 100%|██| 25000/25000 [00:00<00:00, 204105.17 examples/s]\n\nGenerating test split:   0%|                   | 0/25000 [00:00<?, ? examples/s]\nGenerating test split: 100%|███| 25000/25000 [00:00<00:00, 248131.99 examples/s]\nGenerating test split: 100%|███| 25000/25000 [00:00<00:00, 247145.37 examples/s]\n\nGenerating unsupervised split:   0%|           | 0/50000 [00:00<?, ? examples/s]\nGenerating unsupervised split:  46%|▍| 23000/50000 [00:00<00:00, 223075.48 examp\nGenerating unsupervised split:  94%|▉| 47000/50000 [00:00<00:00, 228403.57 examp\nGenerating unsupervised split: 100%|█| 50000/50000 [00:00<00:00, 227443.51 examp\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\n100%|█████████████████████████████████████████| 10/10 [00:00<00:00, 1218.21it/s]\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\n 10%|████▍                                       | 1/10 [00:00<00:04,  2.12it/s]\n 20%|████████▊                                   | 2/10 [00:00<00:03,  2.29it/s]\n 30%|█████████████▏                              | 3/10 [00:01<00:02,  2.56it/s]\n 40%|█████████████████▌                          | 4/10 [00:01<00:02,  2.45it/s]\n 50%|██████████████████████                      | 5/10 [00:02<00:02,  1.84it/s]\n 60%|██████████████████████████▍                 | 6/10 [00:02<00:01,  2.22it/s]\n 70%|██████████████████████████████▊             | 7/10 [00:03<00:01,  1.58it/s]\n 80%|███████████████████████████████████▏        | 8/10 [00:03<00:01,  2.00it/s]\n 90%|███████████████████████████████████████▌    | 9/10 [00:04<00:00,  2.00it/s]\n100%|███████████████████████████████████████████| 10/10 [00:04<00:00,  2.19it/s]\n100%|███████████████████████████████████████████| 10/10 [00:04<00:00,  2.09it/s]\n1\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\n100%|█████████████████████████████████████████| 10/10 [00:00<00:00, 2299.00it/s]\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\n 10%|████▍                                       | 1/10 [00:00<00:03,  2.48it/s]\n 20%|████████▊                                   | 2/10 [00:00<00:03,  2.51it/s]\n 30%|█████████████▏                              | 3/10 [00:01<00:02,  2.72it/s]\n 40%|█████████████████▌                          | 4/10 [00:01<00:02,  2.53it/s]\n 50%|██████████████████████                      | 5/10 [00:02<00:02,  1.88it/s]\n 60%|██████████████████████████▍                 | 6/10 [00:02<00:01,  2.26it/s]\n 70%|██████████████████████████████▊             | 7/10 [00:03<00:01,  1.63it/s]\n 80%|███████████████████████████████████▏        | 8/10 [00:03<00:00,  2.06it/s]\n 90%|███████████████████████████████████████▌    | 9/10 [00:04<00:00,  2.05it/s]\n100%|███████████████████████████████████████████| 10/10 [00:04<00:00,  2.23it/s]\n100%|███████████████████████████████████████████| 10/10 [00:04<00:00,  2.16it/s]\n2\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\n100%|█████████████████████████████████████████| 10/10 [00:00<00:00, 2284.11it/s]\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\n 10%|████▍                                       | 1/10 [00:00<00:03,  2.41it/s]\n 20%|████████▊                                   | 2/10 [00:00<00:03,  2.44it/s]\n 30%|█████████████▏                              | 3/10 [00:01<00:02,  2.67it/s]\n 40%|█████████████████▌                          | 4/10 [00:01<00:02,  2.52it/s]\n 50%|██████████████████████                      | 5/10 [00:02<00:02,  1.86it/s]\n 60%|██████████████████████████▍                 | 6/10 [00:02<00:01,  2.24it/s]\n 70%|██████████████████████████████▊             | 7/10 [00:03<00:01,  1.63it/s]\n 80%|███████████████████████████████████▏        | 8/10 [00:03<00:00,  2.05it/s]\n 90%|███████████████████████████████████████▌    | 9/10 [00:04<00:00,  2.03it/s]\n100%|███████████████████████████████████████████| 10/10 [00:04<00:00,  2.20it/s]\n100%|███████████████████████████████████████████| 10/10 [00:04<00:00,  2.13it/s]\n3\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\n100%|█████████████████████████████████████████| 10/10 [00:00<00:00, 2296.86it/s]\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\n 10%|████▍                                       | 1/10 [00:00<00:03,  2.35it/s]\n 20%|████████▊                                   | 2/10 [00:00<00:03,  2.44it/s]\n 30%|█████████████▏                              | 3/10 [00:01<00:02,  2.70it/s]\n 40%|█████████████████▌                          | 4/10 [00:01<00:02,  2.52it/s]\n 50%|██████████████████████                      | 5/10 [00:02<00:02,  1.86it/s]\n 60%|██████████████████████████▍                 | 6/10 [00:02<00:01,  2.24it/s]\n 70%|██████████████████████████████▊             | 7/10 [00:03<00:01,  1.63it/s]\n 80%|███████████████████████████████████▏        | 8/10 [00:03<00:00,  2.06it/s]\n 90%|███████████████████████████████████████▌    | 9/10 [00:04<00:00,  2.06it/s]\n100%|███████████████████████████████████████████| 10/10 [00:04<00:00,  2.22it/s]\n100%|███████████████████████████████████████████| 10/10 [00:04<00:00,  2.14it/s]\n4\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\n100%|█████████████████████████████████████████| 10/10 [00:00<00:00, 2264.74it/s]\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\n 10%|████▍                                       | 1/10 [00:00<00:03,  2.41it/s]\n 20%|████████▊                                   | 2/10 [00:00<00:03,  2.45it/s]\n 30%|█████████████▏                              | 3/10 [00:01<00:02,  2.65it/s]\n 40%|█████████████████▌                          | 4/10 [00:01<00:02,  2.48it/s]\n 50%|██████████████████████                      | 5/10 [00:02<00:02,  1.84it/s]\n 60%|██████████████████████████▍                 | 6/10 [00:02<00:01,  2.23it/s]\n 70%|██████████████████████████████▊             | 7/10 [00:03<00:01,  1.62it/s]\n 80%|███████████████████████████████████▏        | 8/10 [00:03<00:00,  2.05it/s]\n 90%|███████████████████████████████████████▌    | 9/10 [00:04<00:00,  2.05it/s]\n100%|███████████████████████████████████████████| 10/10 [00:04<00:00,  2.21it/s]\n100%|███████████████████████████████████████████| 10/10 [00:04<00:00,  2.13it/s]\n5\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\n100%|█████████████████████████████████████████| 10/10 [00:00<00:00, 2291.84it/s]\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\n 10%|████▍                                       | 1/10 [00:00<00:03,  2.36it/s]\n 20%|████████▊                                   | 2/10 [00:00<00:03,  2.46it/s]\n 30%|█████████████▏                              | 3/10 [00:01<00:02,  2.66it/s]\n 40%|█████████████████▌                          | 4/10 [00:01<00:02,  2.52it/s]\n 50%|██████████████████████                      | 5/10 [00:02<00:02,  1.88it/s]\n 60%|██████████████████████████▍                 | 6/10 [00:02<00:01,  2.24it/s]\n 70%|██████████████████████████████▊             | 7/10 [00:03<00:01,  1.64it/s]\n 80%|███████████████████████████████████▏        | 8/10 [00:03<00:00,  2.06it/s]\n 90%|███████████████████████████████████████▌    | 9/10 [00:04<00:00,  2.03it/s]\n100%|███████████████████████████████████████████| 10/10 [00:04<00:00,  2.20it/s]\n100%|███████████████████████████████████████████| 10/10 [00:04<00:00,  2.13it/s]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!PYTHONPATH=. python search/sentiment.py --transformer flan-t5-base --path outputs/flan-t5-large/ --lim 10 --epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f6c337",
   "metadata": {},
   "source": [
    "### Step 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da78b75",
   "metadata": {},
   "source": [
    "Execution Start time: 00:01:43, End time: 00:01:43 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80ff2e8",
   "metadata": {},
   "source": [
    "**Thought**: The script executed successfully, and the model outputs for sentiment classification on the IMDb dataset have been generated. Now, I will list the contents of the `outputs/flan-t5-large/imdb/prompt_0/` directory to verify the presence of the `probs.pk` file, which contains the predicted probabilities for each instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9091b72d",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bed998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "info.json  logits.pk  probs.pk\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ls outputs/flan-t5-large/imdb/prompt_0/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caedc3b0",
   "metadata": {},
   "source": [
    "### Step 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b5a2c6",
   "metadata": {},
   "source": [
    "Execution Start time: 00:01:46, End time: 00:01:46 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3993814",
   "metadata": {},
   "source": [
    "**Thought**: The `probs.pk` file is present in the `outputs/flan-t5-large/imdb/prompt_0/` directory. I will now load this file and extract the predicted labels (label with the highest probability) for each instance ID, as specified in the query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b25903",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de57b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{22896: 9,\n 18555: 9,\n 2703: 4,\n 5196: 4,\n 7537: 4,\n 21732: 9,\n 17004: 0,\n 19284: 5,\n 16260: 4,\n 21790: 9}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open(\"outputs/flan-t5-large/imdb/prompt_0/probs.pk\", \"rb\") as input_file:\n",
    "    probs = pickle.load(input_file)\n",
    "\n",
    "predicted_labels = {pid: np.argmax(prob) for pid, prob in probs.items()}\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3380a0d4",
   "metadata": {},
   "source": [
    "### Step 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e2791a",
   "metadata": {},
   "source": [
    "Execution Start time: 00:01:48, End time: 00:01:48 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0ffa74",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3d5e89",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38903fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1929ee65",
   "metadata": {},
   "source": [
    "### Step 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0235c3fb",
   "metadata": {},
   "source": [
    "Execution Start time: 00:01:49, End time: 00:01:49 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2523c6cb",
   "metadata": {},
   "source": [
    "**Thought**: The predicted labels for each instance ID have been successfully extracted from the `probs.pk` file. I will now submit the results as a JSON object, as requested in the query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5295339",
   "metadata": {},
   "source": [
    "**Action (submit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b4ab99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'22896': 9, '18555': 9, '2703': 4, '5196': 4, '7537': 4, '21732': 9, '17004': 0, '19284': 5, '16260': 4, '21790': 9}"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
