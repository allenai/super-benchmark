{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b8f6e18",
   "metadata": {},
   "source": [
    "### Query\n",
    "Train a teacher model on the iwslt14-de-en dataset (available from the download script in the repo), then train a teacher model and evaluate it. Report the BLEU score on the test set for the student model as a json structured as follows: {\"bleu\": 0.0} (replace 0.0 with the actual value).\n",
    "\n",
    "Additional instructions:\n",
    "1. Once loaded/tokenized, keep only the first 10 rows of each set in the dataset.\n",
    "2. Train for only one epoch and with batch_size=1.\n",
    "3. To evaluate the teacher model, generate predictions on the test set using `generate.py` with beam_size=5. Then take the evaluation score with the `score.py` script, with `--ignore-case --sacrebleu`.\n",
    "\n",
    "Git repository: https://github.com/ncsoft/Align-to-Distill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b731d3",
   "metadata": {},
   "source": [
    "### Prompt\n",
    "<details>\n",
    "[{'role': 'system', 'content': 'Interact in a jupyter notebook to solve the request.\\nOutput a thought and an action.\\nThought can reason about the current situation, and action allows you to interact with the Jupyter notebook. There are three types of actions:\\n(1) execute, which executes a notebook cell with the command under the \"content\" field. This could be a bash command (beginning with !), a python command, or magic command (beginning with %).\\n(2) edit, which allows you to replace existing lines in a file with new lines. To replace lines, you will need to provide the lines as they appear before the replacement, and the lines to replace them with.\\nIn the \"content\" field, format should be ```{filename}\\n<<<BEFORE_EDIT>>>\\nlines before edit\\nlines before edit\\n<<<AFTER_EDIT>>>\\nlines after edit\\nlines after edit```\\n  - Include the entire lines WITH indentation. After editing, make sure indentation is correct.\\n(3) submit, which submits the final answer (or null if there\\'s no answer to submit), formatted as JSON (make sure to use double quotes for keys and values).\\nAlways output exactly one \"thought\" and one \"action\", and encapsulate the content with <content>...</content> tags.\\nI will provide the cell output (observation) for you.\\n\\nTips:\\n- When a github repository is given, you should start by cloning it. You can read the readme file and browse the repo for installation and usage instructions.\\n- Avoid using conda, instead use pip for package management.\\n- Each execution has a 5-minute timeout. After this period, you can decide whether to interrupt the kernel or continue waiting.\\n  You should interrupt if: (1) you are prompted with an interactive message (e.g., \"Do you want to continue? [Y/n]\"). In that case, you can bypass it by executing echo \"y\" | command, or (2) the execution is taking longer than expected because the wrong task is being executed, such as a different model or incorrect number of epochs.\\n  You should continue to wait if the current execution is correct and proceeding as expected.\\n\\nHere are some examples.\\nGit repository: EleutherAI/lm-evaluation-harness\\nQuery: Evaluate EleutherAI/gpt-j-6B on hellaswag. What\\'s the score?\\nThought: First, I need to clone the GitHub repository \\'EleutherAI/lm-evaluation-harness\\'.\\nAction: execute\\n<content>\\n!git clone https://github.com/EleutherAI/lm-evaluation-harness.git\\n</content>\\nObservation: ```\\nCloning into \\'lm-evaluation-harness\\'...\\n```\\nThought: The repository has been cloned. Next, I should check the README file for installation and usage instructions to proceed with evaluating EleutherAI/gpt-j-6B on hellaswag.\\nAction: execute\\n<content>\\n!cat lm-evaluation-harness/README.md\\n</content>\\nObservation: ```# Language Model Evaluation Harness ... truncated ...\\n```\\nThought: The README provides detailed instructions for installing the `lm-eval` package and evaluating models like GPT-J-6B on tasks like hellaswag. I need to install the package and its dependencies first.\\nAction: execute\\n<content>\\n!pip install -e lm-evaluation-harness/.\\n</content>\\nObservation: ```\\nObtaining file:///home/nora/lm-evaluation-harness\\n  Installing build dependencies ... truncated ...\\n```\\nThought: The `lm-eval` package and its dependencies have been installed successfully. Now, I can proceed with evaluating the EleutherAI/gpt-j-6B model on the hellaswag task as per the instructions in the README.\\nAction: execute\\n<content>\\n!lm_eval --model hf --model_args pretrained=EleutherAI/gpt-j-6B --tasks hellaswag --device cuda:0 --batch_size 8\\n</content>\\nObservation: ```\\n{\\n  \"hellaswag\": {\\n    \"acc\": 0.24603658536585363,\\n    \"acc_norm\": 0.34436059706446426,\\n  },  ... truncated ...\\n```\\nThought: The `lm-eval` was executed successfully, providing the metrics I needed. The `acc` metric is 0.2460. The `acc_norm` metric is 0.3444\\nAction: submit\\n<content>\\n{\"acc\": 0.2460, \"acc_norm\": 0.3444}\\n</content>\\n\\nQuery: use pandas to read the file \\'data.csv\\' and display the first 2 rows.\\nThought: First, I need to read the file \\'data.csv\\' using the pandas library.\\nAction: execute\\n<content>\\nimport pandas as pd\\npd.read_csv(\\'data.csv\\').head()\\n</content>\\nObservation: ```\\n    name\\n  0  Alice\\n   1    Bob\\n  2  Charlie\\n```\\nThought: The file \\'data.csv\\' has been read successfully and the first 2 rows have been displayed.\\nAction: submit\\n<content>\\n[\"Alice\", \"Bob\"]\\n</content>\\n\\nQuery: Change the model name in the \\'config.json\\' file from \\'roberta-base\\' to \\'roberta-large\\'.\\nThought: First, I need to read the \\'config.json\\' file to find the location where I can replace the model name.\\nAction: execute\\n<content>\\n!cat config.json\\n</content>\\nObservation: ```\\n{\\n    \"model\": \"roberta-base\",\\n    \"batch_size\": 8,\\n    \"epochs\": 10\\n}\\n```\\nThought: The file has been read successfully. Now, I can replace the model name in the \\'config.json\\' file.\\nAction: edit\\n<content>\\nconfig.json\\n<<<BEFORE_EDIT>>>\\n{\\n    \"model\": \"roberta-base\",\\n    \"batch_size\": 8,\\n<<<AFTER_EDIT>>>\\n{\\n    \"model\": \"roberta-large\",\\n    \"batch_size\": 8,\\n</content>\\nObservation: \"editing file config.json\"\\nThought: The model name has been replaced in the \\'config.json\\' file.\\nAction: submit\\n<content>\\nnull\\n</content>\\n'}, {'role': 'user', 'content': 'Query: Train a teacher model on the iwslt14-de-en dataset (available from the download script in the repo), then train a teacher model and evaluate it. Report the BLEU score on the test set for the student model as a json structured as follows: {\"bleu\": 0.0} (replace 0.0 with the actual value).\\n\\nAdditional instructions:\\n1. Once loaded/tokenized, keep only the first 10 rows of each set in the dataset.\\n2. Train for only one epoch and with batch_size=1.\\n3. To evaluate the teacher model, generate predictions on the test set using `generate.py` with beam_size=5. Then take the evaluation score with the `score.py` script, with `--ignore-case --sacrebleu`.\\n\\nGit repository: https://github.com/ncsoft/Align-to-Distill'}]\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7278b5",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1654e222",
   "metadata": {},
   "source": [
    "Execution Start time: 19:41:28, End time: 19:41:29 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7506f4",
   "metadata": {},
   "source": [
    "**Thought**: First, I need to clone the GitHub repository 'Align-to-Distill' to access the necessary scripts and dataset. Then, I will check the README or relevant documentation for instructions on downloading the iwslt14-de-en dataset and training the teacher model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87615392",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c447a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cloning into 'Align-to-Distill'...\nremote: Enumerating objects: 579, done.\u001b[K\nremote: Counting objects: 100% (579/579)\u001b[K\nremote: Counting objects: 100% (579/579), done.\u001b[K\nremote: Compressing objects: 100% (457/457)\u001b[K\nremote: Compressing objects: 100% (457/457), done.\u001b[K\nremote: Total 579 (delta 116), reused 579 (delta 116), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (579/579)\nReceiving objects: 100% (579/579), 932.36 KiB | 22.74 MiB/s, done.\nResolving deltas: 100% (116/116)\nResolving deltas: 100% (116/116), done.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!git clone https://github.com/ncsoft/Align-to-Distill.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcbb6b0",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66335828",
   "metadata": {},
   "source": [
    "Execution Start time: 19:41:30, End time: 19:41:30 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e9cf02",
   "metadata": {},
   "source": [
    "**Thought**: The repository has been cloned successfully. Next, I need to check the README file or other documentation in the repository to find instructions on downloading the iwslt14-de-en dataset and training the teacher model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81027b3",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955ecb10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# Align-to-Distill: Trainable Attention Alignment for Knowledge Distillation in Neural Machine Translation\n\nThis is the PyTorch implementation of paper: **[Align-to-Distill: Trainable Attention Alignment for Knowledge Distillation in Neural Machine Translation (LREC-COLING 2024)](<https://arxiv.org/abs/2403.01479>)**. \n\nWe carry out our experiments on standard Transformer with the  [fairseq](https://github.com/pytorch/fairseq) toolkit. If you use any source code included in this repo in your work, please cite the following paper.\n\n```bibtex\n@misc{jin2024aligntodistill,\n      title={Align-to-Distill: Trainable Attention Alignment for Knowledge Distillation in Neural Machine Translation}, \n      author={Heegon Jin and Seonil Son and Jemin Park and Youngseok Kim and Hyungjong Noh and Yeonsoo Lee},\n      year={2024},\n      eprint={2403.01479},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n# Requirements and Installation\n\n* [PyTorch](http://pytorch.org/) version >= 1.10.0\n* Python version >= 3.8\n* For training new models, you'll also need an NVIDIA GPU and [NCCL](https://github.com/NVIDIA/nccl)\n* **To install fairseq** and develop locally:\n\n``` bash\ngit clone this_repository\ncd fairseq\npip install --editable ./\n```\n\nWe require a few additional Python dependencies:\n\n``` bash\npip install sacremoses einops\n```\n\n# Prepare dataset\n\n### IWSLT'14 German to English\n\nThe following instructions can be used to train a Transformer model on the [IWSLT'14 German to English dataset](http://workshop2014.iwslt.org/downloads/proceeding.pdf).\n\nFirst download and preprocess the data:\n```bash\n# Download and prepare the data\ncd examples/translation/\nbash prepare-iwslt14.sh\ncd ../..\n\n# Preprocess/binarize the data\nTEXT=examples/translation/iwslt14.tokenized.de-en\nfairseq-preprocess --source-lang de --target-lang en \\\n    --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \\\n    --destdir data-bin/iwslt14.tokenized.de-en \\\n    --workers 20\n```\n# Training\nFirst, you need train a teacher model, the training script is the same with fairseq. \nSecond, use the trained teacher model to train an A2D student model. \nThe '--teacher-ckpt-path' argument is used to specify the path to the trained teacher model checkpoint from the first step.\n\nAdjustable arguments for experiments:\n- add '--alpha' (default=0.5) : This argument controls the weight between the cross-entropy loss and the response-based distillation loss.\n- add '--beta' (default=1) : This argument controls the weight between the response-based loss and the attention distillation loss.\n- add '--decay' (default=0.9) : This argument sets the decay rate for the attention distillation loss.\n\nTwo scripts are provided for running the training processes:\n- train_teacher.sh: This script is used to train the teacher model.\n- train_student.sh: This script is used to train the A2D student model using the trained teacher model.\n\n## Train a teacher model\n\n```bash\nbash train_teacher.sh\n```\n\n## Train a student model (with A2D method)\n\n```bash\nbash train_student.sh\n```\n\n## Test a student model (with A2D method)\n\n```bash\nbash test.sh\n```\n\n# Citation\n\nPlease cite as:\n\n``` bibtex\n@misc{jin2024aligntodistill,\n      title={Align-to-Distill: Trainable Attention Alignment for Knowledge Distillation in Neural Machine Translation}, \n      author={Heegon Jin and Seonil Son and Jemin Park and Youngseok Kim and Hyungjong Noh and Yeonsoo Lee},\n      year={2024},\n      eprint={2403.01479},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!cat Align-to-Distill/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f9d3ce",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8803be",
   "metadata": {},
   "source": [
    "Execution Start time: 19:41:32, End time: 19:42:33 (duration: 60 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa12f1cf",
   "metadata": {},
   "source": [
    "**Thought**: The README provides instructions for downloading and preparing the iwslt14-de-en dataset, as well as training the teacher model using the `train_teacher.sh` script. I need to follow these steps to prepare the dataset and train the teacher model. First, I'll navigate to the appropriate directory and run the script to download and preprocess the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d702bf",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b213269b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cloning Moses github repository (for tokenization scripts)...\nCloning into 'mosesdecoder'...\nremote: Enumerating objects: 148459, done.\u001b[K\nremote: Counting objects: 100% (887/887)\u001b[K\nremote: Counting objects: 100% (887/887), done.\u001b[K\nremote: Compressing objects: 100% (366/366)\u001b[K\nremote: Compressing objects: 100% (366/366), done.\u001b[K\nremote: Total 148459 (delta 565), reused 797 (delta 517), pack-reused 147572 (from 1)\u001b[K\nReceiving objects: 100% (148459/148459), 124.41 MiB | 31.10 MiB/s\nReceiving objects: 100% (148459/148459), 129.99 MiB | 31.20 MiB/s, done.\nResolving deltas: 100% (114591/114591)\nResolving deltas: 100% (114591/114591), done.\nCloning Subword NMT repository (for BPE pre-processing)...\nCloning into 'subword-nmt'...\nremote: Enumerating objects: 622, done.\u001b[K\nremote: Counting objects: 100% (46/46)\u001b[K\nremote: Counting objects: 100% (46/46), done.\u001b[K\nremote: Compressing objects: 100% (30/30)\u001b[K\nremote: Compressing objects: 100% (30/30), done.\u001b[K\nremote: Total 622 (delta 25), reused 31 (delta 16), pack-reused 576 (from 1)\u001b[K\nReceiving objects: 100% (622/622)\nReceiving objects: 100% (622/622), 261.27 KiB | 17.42 MiB/s, done.\nResolving deltas: 100% (374/374)\nResolving deltas: 100% (374/374), done.\nDownloading data from http://dl.fbaipublicfiles.com/fairseq/data/iwslt14/de-en.tgz...\n--2024-08-19 19:41:39--  http://dl.fbaipublicfiles.com/fairseq/data/iwslt14/de-en.tgz\nResolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.165.83.91, 18.165.83.79, 18.165.83.35, ...\nConnecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.165.83.91|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 19982877 (19M) [application/x-tar]\nSaving to: ‘de-en.tgz’\n\n\nde-en.tgz             0%[                    ]       0  --.-KB/s               \nde-en.tgz           100%[===================>]  19.06M  --.-KB/s    in 0.1s    \n\n\n\nData successfully downloaded.\nde-en/\nde-en/IWSLT14.TED.dev2010.de-en.de.xml\nde-en/IWSLT14.TED.dev2010.de-en.en.xml\nde-en/IWSLT14.TED.tst2010.de-en.de.xml\nde-en/IWSLT14.TED.tst2010.de-en.en.xml\nde-en/IWSLT14.TED.tst2011.de-en.de.xml\nde-en/IWSLT14.TED.tst2011.de-en.en.xml\nde-en/IWSLT14.TED.tst2012.de-en.de.xml\nde-en/IWSLT14.TED.tst2012.de-en.en.xml\nde-en/IWSLT14.TEDX.dev2012.de-en.de.xml\nde-en/IWSLT14.TEDX.dev2012.de-en.en.xml\nde-en/README\nde-en/train.en\nde-en/train.tags.de-en.de\nde-en/train.tags.de-en.en\npre-processing train data...\nTokenizer Version 1.1\nLanguage: de\nNumber of threads: 8\n\nTokenizer Version 1.1\nLanguage: en\nNumber of threads: 8\n\nclean-corpus.perl: processing iwslt14.tokenized.de-en/tmp/train.tags.de-en.tok.de & .en to iwslt14.tokenized.de-en/tmp/train.tags.de-en.clean, cutoff 1-175, ratio 1.5\n..........(100000).......\nInput sentences: 174443  Output sentences:  167522\npre-processing valid/test data...\norig/de-en/IWSLT14.TED.dev2010.de-en.de.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.dev2010.de-en.de\nTokenizer Version 1.1\nLanguage: de\nNumber of threads: 8\n\norig/de-en/IWSLT14.TED.tst2010.de-en.de.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2010.de-en.de\nTokenizer Version 1.1\nLanguage: de\nNumber of threads: 8\n\norig/de-en/IWSLT14.TED.tst2011.de-en.de.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2011.de-en.de\nTokenizer Version 1.1\nLanguage: de\nNumber of threads: 8\n\norig/de-en/IWSLT14.TED.tst2012.de-en.de.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2012.de-en.de\nTokenizer Version 1.1\nLanguage: de\nNumber of threads: 8\n\norig/de-en/IWSLT14.TEDX.dev2012.de-en.de.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TEDX.dev2012.de-en.de\nTokenizer Version 1.1\nLanguage: de\nNumber of threads: 8\n\norig/de-en/IWSLT14.TED.dev2010.de-en.en.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.dev2010.de-en.en\nTokenizer Version 1.1\nLanguage: en\nNumber of threads: 8\n\norig/de-en/IWSLT14.TED.tst2010.de-en.en.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2010.de-en.en\nTokenizer Version 1.1\nLanguage: en\nNumber of threads: 8\n\norig/de-en/IWSLT14.TED.tst2011.de-en.en.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2011.de-en.en\nTokenizer Version 1.1\nLanguage: en\nNumber of threads: 8\n\norig/de-en/IWSLT14.TED.tst2012.de-en.en.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2012.de-en.en\nTokenizer Version 1.1\nLanguage: en\nNumber of threads: 8\n\norig/de-en/IWSLT14.TEDX.dev2012.de-en.en.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TEDX.dev2012.de-en.en\nTokenizer Version 1.1\nLanguage: en\nNumber of threads: 8\n\ncreating train, valid, test...\nlearn_bpe.py on iwslt14.tokenized.de-en/tmp/train.en-de...\n\n  0%|                                                 | 0/10000 [00:00<?, ?it/s]\n  0%|                                         | 1/10000 [00:00<27:30,  6.06it/s]\n  0%|                                         | 3/10000 [00:00<17:18,  9.63it/s]\n  0%|                                         | 4/10000 [00:00<25:04,  6.64it/s]\n  0%|                                         | 5/10000 [00:00<22:35,  7.38it/s]\n  0%|                                         | 6/10000 [00:00<26:58,  6.18it/s]\n  0%|                                         | 8/10000 [00:01<23:06,  7.21it/s]\n  0%|                                         | 9/10000 [00:01<23:08,  7.20it/s]\n  0%|                                        | 11/10000 [00:01<16:54,  9.85it/s]\n  0%|                                        | 13/10000 [00:01<15:09, 10.98it/s]\n  0%|                                        | 15/10000 [00:01<13:00, 12.80it/s]\n  0%|                                        | 18/10000 [00:01<10:41, 15.56it/s]\n  0%|                                        | 20/10000 [00:01<12:15, 13.57it/s]\n  0%|1                                       | 25/10000 [00:02<08:03, 20.61it/s]\n  0%|1                                       | 28/10000 [00:02<07:30, 22.13it/s]\n  0%|1                                       | 31/10000 [00:02<07:33, 22.00it/s]\n  0%|1                                       | 37/10000 [00:02<05:20, 31.08it/s]\n  0%|1                                       | 48/10000 [00:02<03:19, 49.80it/s]\n  1%|2                                       | 54/10000 [00:02<04:34, 36.17it/s]\n  1%|2                                       | 59/10000 [00:02<04:41, 35.27it/s]\n  1%|2                                       | 65/10000 [00:03<04:26, 37.23it/s]\n  1%|2                                       | 73/10000 [00:03<03:44, 44.14it/s]\n  1%|3                                       | 81/10000 [00:03<03:20, 49.40it/s]\n  1%|3                                       | 90/10000 [00:03<02:48, 58.82it/s]\n  1%|3                                      | 101/10000 [00:03<02:22, 69.62it/s]\n  1%|4                                      | 110/10000 [00:03<02:13, 74.28it/s]\n  1%|4                                      | 118/10000 [00:03<02:30, 65.47it/s]\n  1%|4                                      | 125/10000 [00:03<02:40, 61.34it/s]\n  1%|5                                      | 135/10000 [00:04<02:19, 70.62it/s]\n  2%|5                                      | 150/10000 [00:04<01:48, 90.38it/s]\n  2%|6                                     | 164/10000 [00:04<01:35, 102.66it/s]\n  2%|6                                     | 178/10000 [00:04<01:29, 110.02it/s]\n  2%|7                                     | 190/10000 [00:04<01:28, 110.34it/s]\n  2%|7                                     | 202/10000 [00:04<01:27, 112.13it/s]\n  2%|8                                     | 214/10000 [00:04<01:25, 114.05it/s]\n  2%|8                                     | 226/10000 [00:04<01:25, 114.29it/s]\n  2%|9                                     | 238/10000 [00:04<01:24, 115.85it/s]\n  3%|9                                     | 252/10000 [00:04<01:19, 122.63it/s]\n  3%|#                                     | 265/10000 [00:05<01:19, 122.73it/s]\n  3%|#                                     | 280/10000 [00:05<01:14, 130.02it/s]\n  3%|#1                                    | 295/10000 [00:05<01:12, 134.48it/s]\n  3%|#1                                    | 310/10000 [00:05<01:10, 137.30it/s]\n  3%|#2                                    | 324/10000 [00:05<01:15, 128.96it/s]\n  3%|#3                                    | 343/10000 [00:05<01:06, 145.55it/s]\n  4%|#3                                    | 360/10000 [00:05<01:03, 151.05it/s]\n  4%|#4                                    | 376/10000 [00:05<01:07, 141.79it/s]\n  4%|#4                                    | 393/10000 [00:05<01:04, 147.90it/s]\n  4%|#5                                    | 413/10000 [00:06<00:59, 162.03it/s]\n  4%|#6                                    | 444/10000 [00:06<00:47, 201.76it/s]\n  5%|#7                                    | 471/10000 [00:06<00:43, 220.86it/s]\n  5%|#9                                    | 501/10000 [00:06<00:40, 235.14it/s]\n  5%|##                                    | 529/10000 [00:06<00:38, 243.49it/s]\n  6%|##1                                   | 561/10000 [00:06<00:35, 264.20it/s]\n  6%|##2                                   | 588/10000 [00:06<00:37, 253.16it/s]\n  6%|##3                                   | 616/10000 [00:06<00:36, 259.29it/s]\n  7%|##5                                   | 658/10000 [00:06<00:31, 299.78it/s]\n  7%|##6                                   | 689/10000 [00:07<00:36, 257.34it/s]\n  7%|##7                                   | 720/10000 [00:07<00:34, 266.82it/s]\n  8%|##8                                   | 757/10000 [00:07<00:31, 293.15it/s]\n  8%|##9                                   | 788/10000 [00:07<00:31, 295.96it/s]\n  8%|###1                                  | 819/10000 [00:07<00:31, 290.24it/s]\n  9%|###2                                  | 860/10000 [00:07<00:28, 323.48it/s]\n  9%|###4                                  | 897/10000 [00:07<00:27, 334.33it/s]\n  9%|###5                                  | 940/10000 [00:07<00:25, 361.64it/s]\n 10%|###7                                  | 985/10000 [00:07<00:23, 385.53it/s]\n 10%|###7                                 | 1026/10000 [00:08<00:23, 389.41it/s]\n 11%|###9                                 | 1074/10000 [00:08<00:21, 412.74it/s]\n 11%|####1                                | 1116/10000 [00:08<00:24, 356.02it/s]\n 12%|####3                                | 1168/10000 [00:08<00:22, 397.56it/s]\n 12%|####4                                | 1210/10000 [00:08<00:22, 386.27it/s]\n 13%|####6                                | 1263/10000 [00:08<00:20, 423.94it/s]\n 13%|####8                                | 1307/10000 [00:08<00:20, 423.31it/s]\n 14%|#####                                | 1366/10000 [00:08<00:18, 468.03it/s]\n 14%|#####2                               | 1415/10000 [00:08<00:18, 469.15it/s]\n 15%|#####4                               | 1473/10000 [00:08<00:17, 499.36it/s]\n 15%|#####6                               | 1524/10000 [00:09<00:17, 491.71it/s]\n 16%|#####8                               | 1574/10000 [00:09<00:17, 492.99it/s]\n 16%|######                               | 1624/10000 [00:09<00:19, 424.22it/s]\n 17%|######2                              | 1676/10000 [00:09<00:18, 448.82it/s]\n 17%|######4                              | 1735/10000 [00:09<00:17, 484.52it/s]\n 18%|######6                              | 1796/10000 [00:09<00:15, 518.71it/s]\n 19%|######8                              | 1858/10000 [00:09<00:14, 547.20it/s]\n 19%|#######                              | 1916/10000 [00:09<00:14, 554.28it/s]\n 20%|#######3                             | 1980/10000 [00:09<00:13, 575.93it/s]\n 20%|#######5                             | 2047/10000 [00:10<00:13, 601.02it/s]\n 21%|#######8                             | 2120/10000 [00:10<00:12, 637.47it/s]\n 22%|########                             | 2189/10000 [00:10<00:11, 651.52it/s]\n 23%|########3                            | 2258/10000 [00:10<00:11, 661.51it/s]\n 23%|########6                            | 2325/10000 [00:10<00:13, 564.59it/s]\n 24%|########9                            | 2408/10000 [00:10<00:11, 634.41it/s]\n 25%|#########2                           | 2500/10000 [00:10<00:10, 712.17it/s]\n 26%|#########5                           | 2576/10000 [00:10<00:10, 720.69it/s]\n 27%|#########8                           | 2665/10000 [00:10<00:09, 768.57it/s]\n 28%|##########1                          | 2752/10000 [00:11<00:09, 797.16it/s]\n 28%|##########4                          | 2833/10000 [00:11<00:09, 787.55it/s]\n 29%|##########8                          | 2925/10000 [00:11<00:08, 821.44it/s]\n 30%|###########2                         | 3035/10000 [00:11<00:07, 902.42it/s]\n 31%|###########6                         | 3145/10000 [00:11<00:07, 960.39it/s]\n 33%|###########7                        | 3262/10000 [00:11<00:06, 1021.90it/s]\n 34%|############1                       | 3376/10000 [00:11<00:06, 1056.02it/s]\n 35%|############8                        | 3482/10000 [00:11<00:07, 895.51it/s]\n 36%|#############3                       | 3611/10000 [00:11<00:06, 999.17it/s]\n 38%|#############5                      | 3766/10000 [00:12<00:05, 1148.79it/s]\n 39%|##############                      | 3907/10000 [00:12<00:04, 1221.90it/s]\n 40%|##############9                      | 4033/10000 [00:12<00:12, 459.10it/s]\n 41%|###############2                     | 4127/10000 [00:13<00:12, 453.96it/s]\n 42%|###############5                     | 4206/10000 [00:13<00:13, 444.62it/s]\n 43%|###############8                     | 4274/10000 [00:13<00:12, 453.54it/s]\n 43%|################                     | 4336/10000 [00:13<00:12, 445.59it/s]\n 44%|################2                    | 4392/10000 [00:13<00:12, 443.50it/s]\n 44%|################4                    | 4445/10000 [00:13<00:12, 449.32it/s]\n 45%|################6                    | 4496/10000 [00:13<00:12, 455.19it/s]\n 45%|################8                    | 4546/10000 [00:13<00:11, 457.24it/s]\n 46%|#################                    | 4596/10000 [00:14<00:11, 465.78it/s]\n 46%|#################2                   | 4649/10000 [00:14<00:11, 480.39it/s]\n 47%|#################3                   | 4699/10000 [00:14<00:11, 466.97it/s]\n 47%|#################5                   | 4749/10000 [00:14<00:11, 474.24it/s]\n 48%|#################7                   | 4799/10000 [00:14<00:10, 479.91it/s]\n 49%|#################9                   | 4853/10000 [00:14<00:10, 494.67it/s]\n 49%|##################1                  | 4904/10000 [00:14<00:10, 492.61it/s]\n 50%|##################3                  | 4962/10000 [00:14<00:09, 517.00it/s]\n 50%|##################5                  | 5015/10000 [00:14<00:09, 510.79it/s]\n 51%|##################7                  | 5074/10000 [00:14<00:09, 531.18it/s]\n 51%|##################9                  | 5128/10000 [00:15<00:09, 530.17it/s]\n 52%|###################1                 | 5182/10000 [00:15<00:09, 527.58it/s]\n 52%|###################3                 | 5235/10000 [00:15<00:12, 372.35it/s]\n 53%|###################5                 | 5288/10000 [00:15<00:11, 407.48it/s]\n 53%|###################7                 | 5344/10000 [00:15<00:10, 444.23it/s]\n 54%|###################9                 | 5394/10000 [00:15<00:10, 456.98it/s]\n 55%|####################2                | 5461/10000 [00:15<00:08, 514.30it/s]\n 55%|####################4                | 5532/10000 [00:15<00:07, 567.71it/s]\n 56%|####################7                | 5604/10000 [00:16<00:07, 610.72it/s]\n 57%|#####################                | 5683/10000 [00:16<00:06, 661.59it/s]\n 58%|#####################3               | 5758/10000 [00:16<00:06, 685.30it/s]\n 58%|#####################5               | 5833/10000 [00:16<00:05, 703.70it/s]\n 59%|#####################8               | 5908/10000 [00:16<00:05, 716.53it/s]\n 60%|######################1              | 5995/10000 [00:16<00:05, 760.40it/s]\n 61%|######################4              | 6075/10000 [00:16<00:05, 771.40it/s]\n 62%|######################7              | 6153/10000 [00:16<00:04, 769.71it/s]\n 62%|#######################              | 6238/10000 [00:16<00:04, 790.95it/s]\n 63%|#######################3             | 6322/10000 [00:16<00:04, 803.65it/s]\n 64%|#######################6             | 6403/10000 [00:17<00:04, 803.49it/s]\n 65%|########################             | 6496/10000 [00:17<00:04, 840.88it/s]\n 66%|########################3            | 6588/10000 [00:17<00:03, 863.09it/s]\n 67%|########################7            | 6680/10000 [00:17<00:03, 879.98it/s]\n 68%|#########################            | 6772/10000 [00:17<00:03, 891.44it/s]\n 69%|#########################4           | 6871/10000 [00:17<00:03, 918.98it/s]\n 70%|#########################7           | 6963/10000 [00:17<00:03, 915.58it/s]\n 71%|##########################1          | 7062/10000 [00:17<00:03, 935.91it/s]\n 72%|##########################5          | 7165/10000 [00:17<00:02, 963.52it/s]\n 73%|##########################2         | 7278/10000 [00:17<00:02, 1011.56it/s]\n 74%|##########################6         | 7391/10000 [00:18<00:02, 1044.74it/s]\n 75%|###########################         | 7511/10000 [00:18<00:02, 1089.77it/s]\n 76%|###########################5        | 7645/10000 [00:18<00:02, 1163.18it/s]\n 78%|###########################9        | 7777/10000 [00:18<00:01, 1207.78it/s]\n 79%|############################4       | 7901/10000 [00:18<00:01, 1214.45it/s]\n 80%|############################9       | 8050/10000 [00:18<00:01, 1295.72it/s]\n 82%|#############################5      | 8200/10000 [00:18<00:01, 1354.40it/s]\n 84%|##############################1     | 8368/10000 [00:18<00:01, 1451.25it/s]\n 85%|##############################6     | 8514/10000 [00:18<00:01, 1007.49it/s]\n 87%|###############################3    | 8701/10000 [00:19<00:01, 1197.12it/s]\n 89%|################################1   | 8924/10000 [00:19<00:00, 1449.76it/s]\n 92%|#################################   | 9180/10000 [00:19<00:00, 1736.83it/s]\n 95%|###################################  | 9474/10000 [00:20<00:00, 634.25it/s]\n 96%|###################################5 | 9618/10000 [00:20<00:00, 592.64it/s]\n 97%|#################################### | 9733/10000 [00:20<00:00, 584.31it/s]\n 98%|####################################3| 9830/10000 [00:20<00:00, 575.29it/s]\n 99%|####################################6| 9914/10000 [00:21<00:00, 563.51it/s]\n100%|####################################9| 9989/10000 [00:21<00:00, 560.05it/s]\n100%|####################################| 10000/10000 [00:21<00:00, 470.72it/s]\napply_bpe.py to train.de...\napply_bpe.py to valid.de...\napply_bpe.py to test.de...\napply_bpe.py to train.en...\napply_bpe.py to valid.en...\napply_bpe.py to test.en...\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!cd Align-to-Distill/examples/translation && bash prepare-iwslt14.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5554bc9",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0419270",
   "metadata": {},
   "source": [
    "Execution Start time: 19:42:39, End time: 19:42:39 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d7b314",
   "metadata": {},
   "source": [
    "**Thought**: The dataset has been successfully downloaded and preprocessed. Now, I need to modify the training script to ensure that only the first 10 rows of each set in the dataset are used, and to set the training parameters to one epoch and batch_size=1. I will locate the training script and make the necessary modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d16853",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212f0894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: File Align-to-Distill/examples/translation/train_teacher.sh does not exist."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Align-to-Distill/examples/translation/train_teacher.sh\n",
    "<<<BEFORE_EDIT>>>\n",
    "# Original script content\n",
    "<<<AFTER_EDIT>>>\n",
    "# Modified script content to use only the first 10 rows, 1 epoch, and batch_size=1\n",
    "head -n 10 train.de > train_10.de\n",
    "head -n 10 train.en > train_10.en\n",
    "head -n 10 valid.de > valid_10.de\n",
    "head -n 10 valid.en > valid_10.en\n",
    "head -n 10 test.de > test_10.de\n",
    "head -n 10 test.en > test_10.en\n",
    "\n",
    "# Update training command with new parameters\n",
    "fairseq-train \\\n",
    "    --task translation \\\n",
    "    --arch transformer_iwslt_de_en \\\n",
    "    --share-decoder-input-output-embed \\\n",
    "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
    "    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
    "    --dropout 0.3 --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --max-tokens 4096 \\\n",
    "    --source-lang de --target-lang en \\\n",
    "    --train-subset train_10 --valid-subset valid_10 --test-subset test_10 \\\n",
    "    --max-epoch 1 --batch-size 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977db49d",
   "metadata": {},
   "source": [
    "### Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02fba7b",
   "metadata": {},
   "source": [
    "Execution Start time: 19:42:41, End time: 19:42:41 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58d61f9",
   "metadata": {},
   "source": [
    "**Thought**: It seems that the `train_teacher.sh` script does not exist in the specified directory. I need to explore the repository to locate the correct script or file where the training process is defined, so I can make the necessary modifications for training the teacher model with the specified parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a496362",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f36d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iwslt14.tokenized.de-en  mosesdecoder  orig  prepare-iwslt14.sh  subword-nmt\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ls Align-to-Distill/examples/translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9087e459",
   "metadata": {},
   "source": [
    "### Step 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a95709",
   "metadata": {},
   "source": [
    "Execution Start time: 19:42:42, End time: 19:42:42 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef72d969",
   "metadata": {},
   "source": [
    "**Thought**: The directory does not contain a `train_teacher.sh` script. It seems I need to explore the repository further to locate the correct script or instructions for training the teacher model. I will check the main directory of the repository for any training scripts or instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f508c975",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a794aa16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CODE_OF_CONDUCT.md  RELEASE.md\t hubconf.py\t   setup.py\nCONTRIBUTING.md     custom\t pyproject.toml    test.sh\nLICENSE\t\t    examples\t release_utils.py  train.py\nMANIFEST.in\t    fairseq\t scripts\t   train_student.sh\nREADME.md\t    fairseq_cli  setup.cfg\t   train_teacher.sh\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ls Align-to-Distill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd04054c",
   "metadata": {},
   "source": [
    "### Step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7025408",
   "metadata": {},
   "source": [
    "Execution Start time: 19:42:52, End time: 19:42:52 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68f6f83",
   "metadata": {},
   "source": [
    "**Thought**: The `train_teacher.sh` script is located in the main directory of the repository. I will now edit this script to modify it for training with only the first 10 rows of each dataset, for one epoch, and with a batch size of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7605b729",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c164e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Could not find the following lines to replace in the file content:\n```\n# Original script content\n```"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Align-to-Distill/train_teacher.sh\n",
    "<<<BEFORE_EDIT>>>\n",
    "# Original script content\n",
    "<<<AFTER_EDIT>>>\n",
    "# Modified script content to use only the first 10 rows, 1 epoch, and batch_size=1\n",
    "head -n 10 examples/translation/iwslt14.tokenized.de-en/train.de > examples/translation/iwslt14.tokenized.de-en/train_10.de\n",
    "head -n 10 examples/translation/iwslt14.tokenized.de-en/train.en > examples/translation/iwslt14.tokenized.de-en/train_10.en\n",
    "head -n 10 examples/translation/iwslt14.tokenized.de-en/valid.de > examples/translation/iwslt14.tokenized.de-en/valid_10.de\n",
    "head -n 10 examples/translation/iwslt14.tokenized.de-en/valid.en > examples/translation/iwslt14.tokenized.de-en/valid_10.en\n",
    "head -n 10 examples/translation/iwslt14.tokenized.de-en/test.de > examples/translation/iwslt14.tokenized.de-en/test_10.de\n",
    "head -n 10 examples/translation/iwslt14.tokenized.de-en/test.en > examples/translation/iwslt14.tokenized.de-en/test_10.en\n",
    "\n",
    "# Update training command with new parameters\n",
    "fairseq-train \\\n",
    "    --task translation \\\n",
    "    --arch transformer_iwslt_de_en \\\n",
    "    --share-decoder-input-output-embed \\\n",
    "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
    "    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
    "    --dropout 0.3 --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --max-tokens 4096 \\\n",
    "    --source-lang de --target-lang en \\\n",
    "    --train-subset train_10 --valid-subset valid_10 --test-subset test_10 \\\n",
    "    --max-epoch 1 --batch-size 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09648e91",
   "metadata": {},
   "source": [
    "### Step 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e18be2",
   "metadata": {},
   "source": [
    "Execution Start time: 19:42:53, End time: 19:42:54 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d35220",
   "metadata": {},
   "source": [
    "**Thought**: It seems I attempted to edit the script without knowing its current content. I need to first read the content of `train_teacher.sh` to understand its structure and make the necessary modifications for training with the specified parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ac31f4",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f86e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# SPDX-FileCopyrightText: Ⓒ 2024 NCSOFT Corporation. All Rights Reserved.\n# SPDX-License-Identifier: BSD-3-Clause\n\n#!/bin/bash\nbase_dir=$path_to_fairseq\nexport PYTHONPATH=\"${PYTHONPATH}:$base_dir\"\ndata_dir=$base_dir/data-bin\ndata=iwslt14.tokenized.de-en\ncustom_model_dir=$base_dir/custom/\nteacher_model=transformer_teacher\n\nmkdir -p $data_dir/$teacher_model/$data/\ntouch $data_dir/$teacher_model/$data/train.log\n\nCUDA_VISIBLE_DEVICES=$GPU_NUM fairseq-train $data_dir/$data \\\n    --arch $teacher_model --share-decoder-input-output-embed \\\n    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n    --dropout 0.3 --weight-decay 0.0001 \\\n    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n    --max-tokens 4096 \\\n    --eval-bleu \\\n    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\n    --eval-bleu-detok moses \\\n    --eval-bleu-remove-bpe \\\n    --eval-bleu-print-samples \\\n    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\n    --save-dir $data_dir/$teacher_model/$data --user-dir $custom_model_dir | tee -a $data_dir/$teacher_model/$data/train.log"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!cat Align-to-Distill/train_teacher.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a0a3ac",
   "metadata": {},
   "source": [
    "### Step 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e780677",
   "metadata": {},
   "source": [
    "Execution Start time: 19:43:14, End time: 19:43:14 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516be6a3",
   "metadata": {},
   "source": [
    "**Thought**: The `train_teacher.sh` script uses environment variables and paths to set up the training process. I need to modify this script to ensure it uses only the first 10 rows of each dataset, trains for one epoch, and uses a batch size of 1. I will create a new script to preprocess the dataset and modify the training command accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5881d3",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d223b1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n# SPDX-FileCopyrightText: Ⓒ 2024 NCSOFT Corporation. All Rights Reserved.\n# SPDX-License-Identifier: BSD-3-Clause\n\n#!/bin/bash\nbase_dir=$path_to_fairseq\nexport PYTHONPATH=\"${PYTHONPATH}:$base_dir\"\ndata_dir=$base_dir/data-bin\ndata=iwslt14.tokenized.de-en\ncustom_model_dir=$base_dir/custom/\nteacher_model=transformer_teacher\n\n# Preprocess to keep only the first 10 rows\nhead -n 10 examples/translation/iwslt14.tokenized.de-en/train.de > examples/translation/iwslt14.tokenized.de-en/train_10.de\nhead -n 10 examples/translation/iwslt14.tokenized.de-en/train.en > examples/translation/iwslt14.tokenized.de-en/train_10.en\nhead -n 10 examples/translation/iwslt14.tokenized.de-en/valid.de > examples/translation/iwslt14.tokenized.de-en/valid_10.de\nhead -n 10 examples/translation/iwslt14.tokenized.de-en/valid.en > examples/translation/iwslt14.tokenized.de-en/valid_10.en\nhead -n 10 examples/translation/iwslt14.tokenized.de-en/test.de > examples/translation/iwslt14.tokenized.de-en/test_10.de\nhead -n 10 examples/translation/iwslt14.tokenized.de-en/test.en > examples/translation/iwslt14.tokenized.de-en/test_10.en\n\nmkdir -p $data_dir/$teacher_model/$data/\ntouch $data_dir/$teacher_model/$data/train.log\n\nCUDA_VISIBLE_DEVICES=$GPU_NUM fairseq-train $data_dir/$data \\\n    --arch $teacher_model --share-decoder-input-output-embed \\\n    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n    --dropout 0.3 --weight-decay 0.0001 \\\n    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n    --max-tokens 4096 \\\n    --max-epoch 1 --batch-size 1 \\\n    --eval-bleu \\\n    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\n    --eval-bleu-detok moses \\\n    --eval-bleu-remove-bpe \\\n    --eval-bleu-print-samples \\\n    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\n    --save-dir $data_dir/$teacher_model/$data --user-dir $custom_model_dir | tee -a $data_dir/$teacher_model/$data/train.log"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Align-to-Distill/train_teacher.sh\n",
    "<<<BEFORE_EDIT>>>\n",
    "#!/bin/bash\n",
    "base_dir=$path_to_fairseq\n",
    "export PYTHONPATH=\"${PYTHONPATH}:$base_dir\"\n",
    "data_dir=$base_dir/data-bin\n",
    "data=iwslt14.tokenized.de-en\n",
    "custom_model_dir=$base_dir/custom/\n",
    "teacher_model=transformer_teacher\n",
    "\n",
    "mkdir -p $data_dir/$teacher_model/$data/\n",
    "touch $data_dir/$teacher_model/$data/train.log\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=$GPU_NUM fairseq-train $data_dir/$data \\\n",
    "    --arch $teacher_model --share-decoder-input-output-embed \\\n",
    "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
    "    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
    "    --dropout 0.3 --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --max-tokens 4096 \\\n",
    "    --eval-bleu \\\n",
    "    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\n",
    "    --eval-bleu-detok moses \\\n",
    "    --eval-bleu-remove-bpe \\\n",
    "    --eval-bleu-print-samples \\\n",
    "    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\n",
    "    --save-dir $data_dir/$teacher_model/$data --user-dir $custom_model_dir | tee -a $data_dir/$teacher_model/$data/train.log\n",
    "<<<AFTER_EDIT>>>\n",
    "#!/bin/bash\n",
    "base_dir=$path_to_fairseq\n",
    "export PYTHONPATH=\"${PYTHONPATH}:$base_dir\"\n",
    "data_dir=$base_dir/data-bin\n",
    "data=iwslt14.tokenized.de-en\n",
    "custom_model_dir=$base_dir/custom/\n",
    "teacher_model=transformer_teacher\n",
    "\n",
    "# Preprocess to keep only the first 10 rows\n",
    "head -n 10 examples/translation/iwslt14.tokenized.de-en/train.de > examples/translation/iwslt14.tokenized.de-en/train_10.de\n",
    "head -n 10 examples/translation/iwslt14.tokenized.de-en/train.en > examples/translation/iwslt14.tokenized.de-en/train_10.en\n",
    "head -n 10 examples/translation/iwslt14.tokenized.de-en/valid.de > examples/translation/iwslt14.tokenized.de-en/valid_10.de\n",
    "head -n 10 examples/translation/iwslt14.tokenized.de-en/valid.en > examples/translation/iwslt14.tokenized.de-en/valid_10.en\n",
    "head -n 10 examples/translation/iwslt14.tokenized.de-en/test.de > examples/translation/iwslt14.tokenized.de-en/test_10.de\n",
    "head -n 10 examples/translation/iwslt14.tokenized.de-en/test.en > examples/translation/iwslt14.tokenized.de-en/test_10.en\n",
    "\n",
    "mkdir -p $data_dir/$teacher_model/$data/\n",
    "touch $data_dir/$teacher_model/$data/train.log\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=$GPU_NUM fairseq-train $data_dir/$data \\\n",
    "    --arch $teacher_model --share-decoder-input-output-embed \\\n",
    "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
    "    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
    "    --dropout 0.3 --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --max-tokens 4096 \\\n",
    "    --max-epoch 1 --batch-size 1 \\\n",
    "    --eval-bleu \\\n",
    "    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\n",
    "    --eval-bleu-detok moses \\\n",
    "    --eval-bleu-remove-bpe \\\n",
    "    --eval-bleu-print-samples \\\n",
    "    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\n",
    "    --save-dir $data_dir/$teacher_model/$data --user-dir $custom_model_dir | tee -a $data_dir/$teacher_model/$data/train.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f2189a",
   "metadata": {},
   "source": [
    "### Step 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3fde0d",
   "metadata": {},
   "source": [
    "Execution Start time: 19:43:15, End time: 19:43:16 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab17a11a",
   "metadata": {},
   "source": [
    "**Thought**: The `train_teacher.sh` script has been successfully modified to preprocess the dataset to only include the first 10 rows and to train the model for one epoch with a batch size of 1. Now, I will execute this script to train the teacher model with the specified parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5e99d4",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbfa516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Align-to-Distill/train_teacher.sh: line 13: examples/translation/iwslt14.tokenized.de-en/train_10.de: No such file or directory\nAlign-to-Distill/train_teacher.sh: line 14: examples/translation/iwslt14.tokenized.de-en/train_10.en: No such file or directory\nAlign-to-Distill/train_teacher.sh: line 15: examples/translation/iwslt14.tokenized.de-en/valid_10.de: No such file or directory\nAlign-to-Distill/train_teacher.sh: line 16: examples/translation/iwslt14.tokenized.de-en/valid_10.en: No such file or directory\nAlign-to-Distill/train_teacher.sh: line 17: examples/translation/iwslt14.tokenized.de-en/test_10.de: No such file or directory\nAlign-to-Distill/train_teacher.sh: line 18: examples/translation/iwslt14.tokenized.de-en/test_10.en: No such file or directory\nAlign-to-Distill/train_teacher.sh: line 23: fairseq-train: command not found\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!bash Align-to-Distill/train_teacher.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d5adb9",
   "metadata": {},
   "source": [
    "### Step 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b65b144",
   "metadata": {},
   "source": [
    "Execution Start time: 19:43:18, End time: 19:43:18 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e1eacf",
   "metadata": {},
   "source": [
    "**Thought**: It seems that the paths to the dataset files are incorrect, and the `fairseq-train` command is not found, indicating that the Fairseq library might not be installed or the environment is not set up correctly. I need to ensure that the dataset paths are correct and install Fairseq to proceed with training the teacher model. First, I'll check the correct paths for the dataset files and then install Fairseq."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbe5e1b",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de26c6a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code  test.de  test.en\ttmp  train.de  train.en  valid.de  valid.en\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ls Align-to-Distill/examples/translation/iwslt14.tokenized.de-en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef00159",
   "metadata": {},
   "source": [
    "### Step 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eeb6f8",
   "metadata": {},
   "source": [
    "Execution Start time: 19:43:21, End time: 19:45:50 (duration: 149 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80705cb8",
   "metadata": {},
   "source": [
    "**Thought**: The dataset files are located in the `Align-to-Distill/examples/translation/iwslt14.tokenized.de-en` directory. I will correct the paths in the script to point to these files. Additionally, I need to install the Fairseq library to ensure the `fairseq-train` command is available for training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb5ab2f",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f93697a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting fairseq\n  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n\u001b[?25l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/9.6 MB ? eta -:--:--\n\n\n\u001b[?25h  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \b\\\b \bdone\n\u001b[?25hRequirement already satisfied: cffi in /usr/local/lib/python3.10/site-packages (from fairseq) (1.16.0)\nRequirement already satisfied: cython in /usr/local/lib/python3.10/site-packages (from fairseq) (3.0.10)\nCollecting hydra-core<1.1,>=1.0.7 (from fairseq)\n  Downloading hydra_core-1.0.7-py3-none-any.whl.metadata (3.7 kB)\nCollecting omegaconf<2.1 (from fairseq)\n  Downloading omegaconf-2.0.6-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.10/site-packages (from fairseq) (2023.12.25)\nCollecting sacrebleu>=1.4.12 (from fairseq)\n  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n\u001b[?25l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/51.8 kB ? eta -:--:--\n\n\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/site-packages (from fairseq) (2.2.1+cpu)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from fairseq) (4.66.4)\nCollecting bitarray (from fairseq)\n  Downloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\nCollecting torchaudio>=0.8.0 (from fairseq)\n  Downloading torchaudio-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from fairseq) (1.25.2)\nCollecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq)\n  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n\u001b[?25l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/112.4 kB ? eta -:--:--\n\n\u001b[?25h  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\n\u001b[?25hRequirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/site-packages (from omegaconf<2.1->fairseq) (6.0.1)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/site-packages (from omegaconf<2.1->fairseq) (4.12.2)\nCollecting portalocker (from sacrebleu>=1.4.12->fairseq)\n  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\nCollecting tabulate>=0.8.9 (from sacrebleu>=1.4.12->fairseq)\n  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\nCollecting colorama (from sacrebleu>=1.4.12->fairseq)\n  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\nCollecting lxml (from sacrebleu>=1.4.12->fairseq)\n  Downloading lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\nCollecting torch (from fairseq)\n  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from torch->fairseq) (3.15.1)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/site-packages (from torch->fairseq) (1.12)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch->fairseq) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch->fairseq) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/site-packages (from torch->fairseq) (2023.6.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->fairseq)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->fairseq)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->fairseq)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch->fairseq)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch->fairseq)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch->fairseq)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch->fairseq)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch->fairseq)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch->fairseq)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.20.5 (from torch->fairseq)\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch->fairseq)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==3.0.0 (from torch->fairseq)\n  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->fairseq)\n  Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/site-packages (from cffi->fairseq) (2.22)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch->fairseq) (2.1.5)\nRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/site-packages (from sympy->torch->fairseq) (1.3.0)\nDownloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/123.8 kB ? eta -:--:--\n\n\u001b[?25hDownloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\nDownloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/104.0 kB ? eta -:--:--\n\n\u001b[?25hDownloading torchaudio-2.4.0-cp310-cp310-manylinux1_x86_64.whl (3.4 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/3.4 MB ? eta -:--:--\n\n\u001b[?25hDownloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/797.2 MB ? eta -:--:--\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/410.6 MB ? eta -:--:--\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/14.1 MB ? eta -:--:--\n\n\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/23.7 MB ? eta -:--:--\n\n\n\n\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/823.6 kB ? eta -:--:--\n\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/664.8 MB ? eta -:--:--\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/121.6 MB ? eta -:--:--\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/56.5 MB ? eta -:--:--\n\n\n\n\n\n\n\n\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/124.2 MB ? eta -:--:--\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/196.0 MB ? eta -:--:--\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/176.2 MB ? eta -:--:--\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/99.1 kB ? eta -:--:--\n\n\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/209.4 MB ? eta -:--:--\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[?25hDownloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/288.3 kB ? eta -:--:--\n\n\u001b[?25hDownloading tabulate-0.9.0-py3-none-any.whl (35 kB)\nDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\nDownloading lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (5.0 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/5.0 MB ? eta -:--:--\n\n\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\nDownloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/19.7 MB ? eta -:--:--\n\n\n\n\u001b[?25hBuilding wheels for collected packages: fairseq, antlr4-python3-runtime\n  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\n\u001b[?25h  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=10854433 sha256=2b42bd94065b70c681b2499902c79a5fce6dedd9f595d40b872b48bdb09772b1\n  Stored in directory: /tmp/pip-ephem-wheel-cache-byibz5f9/wheels/e4/35/55/9c66f65ec7c83fd6fbc2b9502a0ac81b2448a1196159dacc32\n  Building wheel for antlr4-python3-runtime (pyproject.toml) ... \u001b[?25l-\b \b\\\b \bdone\n\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141210 sha256=940a9209c623b3eb2b1c28f1ced80467ab889f14dd95cb184ab45960d888f947\n  Stored in directory: /tmp/pip-ephem-wheel-cache-byibz5f9/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\nSuccessfully built fairseq antlr4-python3-runtime\nDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\nInstalling collected packages: bitarray, antlr4-python3-runtime, triton, tabulate, portalocker, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lxml, colorama, sacrebleu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, hydra-core, nvidia-cusolver-cu12, torch, torchaudio, fairseq\n  Attempting uninstall: torch\n    Found existing installation: torch 2.2.1+cpu\n    Uninstalling torch-2.2.1+cpu:\n      Successfully uninstalled torch-2.2.1+cpu\nSuccessfully installed antlr4-python3-runtime-4.8 bitarray-2.9.2 colorama-0.4.6 fairseq-0.12.2 hydra-core-1.0.7 lxml-5.3.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 omegaconf-2.0.6 portalocker-2.10.1 sacrebleu-2.4.3 tabulate-0.9.0 torch-2.4.0 torchaudio-2.4.0 triton-3.0.0\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install fairseq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c338cbf9",
   "metadata": {},
   "source": [
    "### Step 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288c1e66",
   "metadata": {},
   "source": [
    "Execution Start time: 19:45:59, End time: 19:45:59 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb86984",
   "metadata": {},
   "source": [
    "**Thought**: The Fairseq library has been successfully installed. Now, I will correct the paths in the `train_teacher.sh` script to point to the correct dataset files and execute the script to train the teacher model with the specified parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99c89dc",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f20a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\nbase_dir=$path_to_fairseq\nexport PYTHONPATH=\"${PYTHONPATH}:$base_dir\"\ndata_dir=$base_dir/data-bin\ndata=iwslt14.tokenized.de-en\ncustom_model_dir=$base_dir/custom/\nteacher_model=transformer_teacher\n\n# Preprocess to keep only the first 10 rows\nhead -n 10 Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/train.de > Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/train_10.de\nhead -n 10 Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/train.en > Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/train_10.en\nhead -n 10 Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/valid.de > Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/valid_10.de\nhead -n 10 Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/valid.en > Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/valid_10.en\nhead -n 10 Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/test.de > Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/test_10.de\nhead -n 10 Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/test.en > Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/test_10.en\n\nmkdir -p $data_dir/$teacher_model/$data/\ntouch $data_dir/$teacher_model/$data/train.log\n\nCUDA_VISIBLE_DEVICES=$GPU_NUM fairseq-train $data_dir/$data \\\n    --arch $teacher_model --share-decoder-input-output-embed \\"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Align-to-Distill/train_teacher.sh\n",
    "<<<BEFORE_EDIT>>>\n",
    "# Preprocess to keep only the first 10 rows\n",
    "head -n 10 examples/translation/iwslt14.tokenized.de-en/train.de > examples/translation/iwslt14.tokenized.de-en/train_10.de\n",
    "head -n 10 examples/translation/iwslt14.tokenized.de-en/train.en > examples/translation/iwslt14.tokenized.de-en/train_10.en\n",
    "head -n 10 examples/translation/iwslt14.tokenized.de-en/valid.de > examples/translation/iwslt14.tokenized.de-en/valid_10.de\n",
    "head -n 10 examples/translation/iwslt14.tokenized.de-en/valid.en > examples/translation/iwslt14.tokenized.de-en/valid_10.en\n",
    "head -n 10 examples/translation/iwslt14.tokenized.de-en/test.de > examples/translation/iwslt14.tokenized.de-en/test_10.de\n",
    "head -n 10 examples/translation/iwslt14.tokenized.de-en/test.en > examples/translation/iwslt14.tokenized.de-en/test_10.en\n",
    "<<<AFTER_EDIT>>>\n",
    "# Preprocess to keep only the first 10 rows\n",
    "head -n 10 Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/train.de > Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/train_10.de\n",
    "head -n 10 Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/train.en > Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/train_10.en\n",
    "head -n 10 Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/valid.de > Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/valid_10.de\n",
    "head -n 10 Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/valid.en > Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/valid_10.en\n",
    "head -n 10 Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/test.de > Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/test_10.de\n",
    "head -n 10 Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/test.en > Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/test_10.en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ee5587",
   "metadata": {},
   "source": [
    "### Step 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee37b971",
   "metadata": {},
   "source": [
    "Execution Start time: 19:46:00, End time: 19:46:05 (duration: 4 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd48fe00",
   "metadata": {},
   "source": [
    "**Thought**: The paths in the `train_teacher.sh` script have been corrected. Now, I will execute the script to train the teacher model with the specified parameters, ensuring that the environment is set up correctly for Fairseq."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558fdc02",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d5bc41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2024-08-19 19:46:03 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\nTraceback (most recent call last):\n  File \"/usr/local/bin/fairseq-train\", line 8, in <module>\n    sys.exit(cli_main())\n  File \"/usr/local/lib/python3.10/site-packages/fairseq_cli/train.py\", line 541, in cli_main\n    parser = options.get_training_parser()\n  File \"/usr/local/lib/python3.10/site-packages/fairseq/options.py\", line 38, in get_training_parser\n    parser = get_parser(\"Trainer\", default_task)\n  File \"/usr/local/lib/python3.10/site-packages/fairseq/options.py\", line 234, in get_parser\n    utils.import_user_module(usr_args)\n  File \"/usr/local/lib/python3.10/site-packages/fairseq/utils.py\", line 481, in import_user_module\n    raise FileNotFoundError(module_path)\nFileNotFoundError: /custom\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!bash Align-to-Distill/train_teacher.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff231eb",
   "metadata": {},
   "source": [
    "### Step 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a994d370",
   "metadata": {},
   "source": [
    "Execution Start time: 19:46:07, End time: 19:46:10 (duration: 2 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae3eb38",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the script is trying to access a custom module directory (`/custom`) that does not exist. This might be due to an incorrect path or missing configuration. I need to check the script for any hardcoded paths or environment variables that need to be set correctly. Additionally, I should ensure that all required dependencies, such as `tensorboardX`, are installed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddc940b",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42e8b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting tensorboardX\n  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from tensorboardX) (1.25.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from tensorboardX) (24.1)\nRequirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/site-packages (from tensorboardX) (5.27.1)\nDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/101.7 kB ? eta -:--:--\n\n\u001b[?25hDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\nInstalling collected packages: tensorboardX\nSuccessfully installed tensorboardX-2.6.2.2\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d42b66",
   "metadata": {},
   "source": [
    "### Step 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41aa86b",
   "metadata": {},
   "source": [
    "Execution Start time: 19:46:14, End time: 19:46:15 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a86425",
   "metadata": {},
   "source": [
    "**Thought**: The `tensorboardX` package has been installed successfully. The error in the script execution seems to be related to the `--user-dir` argument pointing to a non-existent directory. I need to check the `train_teacher.sh` script for any hardcoded paths or incorrect configurations related to the `--user-dir` argument and correct them. I will review the script again to identify the issue and make necessary adjustments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8ff232",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ac9d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Could not find the following lines to replace in the file content:\n```\n--save-dir $data_dir/$teacher_model/$data --user-dir $custom_model_dir | tee -a $data_dir/$teacher_model/$data/train.log\n```\nDid you mean to replace the following lines (notice leading/trailing whitespaces difference)?\n```\n    --save-dir $data_dir/$teacher_model/$data --user-dir $custom_model_dir | tee -a $data_dir/$teacher_model/$data/train.log\n```"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Align-to-Distill/train_teacher.sh\n",
    "<<<BEFORE_EDIT>>>\n",
    "--save-dir $data_dir/$teacher_model/$data --user-dir $custom_model_dir | tee -a $data_dir/$teacher_model/$data/train.log\n",
    "<<<AFTER_EDIT>>>\n",
    "--save-dir $data_dir/$teacher_model/$data | tee -a $data_dir/$teacher_model/$data/train.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ab32b9",
   "metadata": {},
   "source": [
    "### Step 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be46054",
   "metadata": {},
   "source": [
    "Execution Start time: 19:46:17, End time: 19:46:18 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536c6cfd",
   "metadata": {},
   "source": [
    "**Thought**: It seems there was a whitespace issue in the line I attempted to edit. I will correct the line by removing the `--user-dir` argument and its associated variable, as it appears to be causing the error due to a non-existent directory. I will make sure to match the line exactly as it appears in the script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5bfe92",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1219b1b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n    --max-epoch 1 --batch-size 1 \\\n    --eval-bleu \\\n    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\n    --eval-bleu-detok moses \\\n    --eval-bleu-remove-bpe \\\n    --eval-bleu-print-samples \\\n    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\n    --save-dir $data_dir/$teacher_model/$data | tee -a $data_dir/$teacher_model/$data/train.log\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Align-to-Distill/train_teacher.sh\n",
    "<<<BEFORE_EDIT>>>\n",
    "    --save-dir $data_dir/$teacher_model/$data --user-dir $custom_model_dir | tee -a $data_dir/$teacher_model/$data/train.log\n",
    "<<<AFTER_EDIT>>>\n",
    "    --save-dir $data_dir/$teacher_model/$data | tee -a $data_dir/$teacher_model/$data/train.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12add58c",
   "metadata": {},
   "source": [
    "### Step 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27c880f",
   "metadata": {},
   "source": [
    "Execution Start time: 19:46:19, End time: 19:46:23 (duration: 4 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adde9006",
   "metadata": {},
   "source": [
    "**Thought**: The `train_teacher.sh` script has been successfully modified to remove the `--user-dir` argument, which was causing the error. Now, I will execute the script again to train the teacher model with the specified parameters, ensuring that the environment is set up correctly for Fairseq."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbf1e04",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b96b227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "usage: fairseq-train [-h] [--no-progress-bar] [--log-interval LOG_INTERVAL]\n                     [--log-format {json,none,simple,tqdm}]\n                     [--log-file LOG_FILE] [--aim-repo AIM_REPO]\n                     [--aim-run-hash AIM_RUN_HASH]\n                     [--tensorboard-logdir TENSORBOARD_LOGDIR]\n                     [--wandb-project WANDB_PROJECT] [--azureml-logging]\n                     [--seed SEED] [--cpu] [--tpu] [--bf16]\n                     [--memory-efficient-bf16] [--fp16]\n                     [--memory-efficient-fp16] [--fp16-no-flatten-grads]\n                     [--fp16-init-scale FP16_INIT_SCALE]\n                     [--fp16-scale-window FP16_SCALE_WINDOW]\n                     [--fp16-scale-tolerance FP16_SCALE_TOLERANCE]\n                     [--on-cpu-convert-precision]\n                     [--min-loss-scale MIN_LOSS_SCALE]\n                     [--threshold-loss-scale THRESHOLD_LOSS_SCALE] [--amp]\n                     [--amp-batch-retries AMP_BATCH_RETRIES]\n                     [--amp-init-scale AMP_INIT_SCALE]\n                     [--amp-scale-window AMP_SCALE_WINDOW]\n                     [--user-dir USER_DIR]\n                     [--empty-cache-freq EMPTY_CACHE_FREQ]\n                     [--all-gather-list-size ALL_GATHER_LIST_SIZE]\n                     [--model-parallel-size MODEL_PARALLEL_SIZE]\n                     [--quantization-config-path QUANTIZATION_CONFIG_PATH]\n                     [--profile] [--reset-logging] [--suppress-crashes]\n                     [--use-plasma-view] [--plasma-path PLASMA_PATH]\n                     [--criterion {adaptive_loss,composite_loss,cross_entropy,ctc,fastspeech2,hubert,label_smoothed_cross_entropy,latency_augmented_label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,label_smoothed_cross_entropy_with_ctc,legacy_masked_lm_loss,masked_lm,model,nat_loss,sentence_prediction,sentence_prediction_adapters,sentence_ranking,tacotron2,speech_to_unit,speech_to_spectrogram,speech_unit_lm_criterion,wav2vec,vocab_parallel_cross_entropy}]\n                     [--tokenizer {moses,nltk,space}]\n                     [--bpe {byte_bpe,bytes,characters,fastbpe,gpt2,bert,hf_byte_bpe,sentencepiece,subword_nmt}]\n                     [--optimizer {adadelta,adafactor,adagrad,adam,adamax,composite,cpu_adam,lamb,nag,sgd}]\n                     [--lr-scheduler {cosine,fixed,inverse_sqrt,manual,pass_through,polynomial_decay,reduce_lr_on_plateau,step,tri_stage,triangular}]\n                     [--scoring {bert_score,sacrebleu,bleu,chrf,meteor,wer}]\n                     [--task TASK] [--num-workers NUM_WORKERS]\n                     [--skip-invalid-size-inputs-valid-test]\n                     [--max-tokens MAX_TOKENS] [--batch-size BATCH_SIZE]\n                     [--required-batch-size-multiple REQUIRED_BATCH_SIZE_MULTIPLE]\n                     [--required-seq-len-multiple REQUIRED_SEQ_LEN_MULTIPLE]\n                     [--dataset-impl {raw,lazy,cached,mmap,fasta,huffman}]\n                     [--data-buffer-size DATA_BUFFER_SIZE]\n                     [--train-subset TRAIN_SUBSET]\n                     [--valid-subset VALID_SUBSET] [--combine-valid-subsets]\n                     [--ignore-unused-valid-subsets]\n                     [--validate-interval VALIDATE_INTERVAL]\n                     [--validate-interval-updates VALIDATE_INTERVAL_UPDATES]\n                     [--validate-after-updates VALIDATE_AFTER_UPDATES]\n                     [--fixed-validation-seed FIXED_VALIDATION_SEED]\n                     [--disable-validation]\n                     [--max-tokens-valid MAX_TOKENS_VALID]\n                     [--batch-size-valid BATCH_SIZE_VALID]\n                     [--max-valid-steps MAX_VALID_STEPS]\n                     [--curriculum CURRICULUM] [--gen-subset GEN_SUBSET]\n                     [--num-shards NUM_SHARDS] [--shard-id SHARD_ID]\n                     [--grouped-shuffling]\n                     [--update-epoch-batch-itr UPDATE_EPOCH_BATCH_ITR]\n                     [--update-ordered-indices-seed]\n                     [--distributed-world-size DISTRIBUTED_WORLD_SIZE]\n                     [--distributed-num-procs DISTRIBUTED_NUM_PROCS]\n                     [--distributed-rank DISTRIBUTED_RANK]\n                     [--distributed-backend DISTRIBUTED_BACKEND]\n                     [--distributed-init-method DISTRIBUTED_INIT_METHOD]\n                     [--distributed-port DISTRIBUTED_PORT]\n                     [--device-id DEVICE_ID] [--distributed-no-spawn]\n                     [--ddp-backend {c10d,fully_sharded,legacy_ddp,no_c10d,pytorch_ddp,slowmo}]\n                     [--ddp-comm-hook {none,fp16}]\n                     [--bucket-cap-mb BUCKET_CAP_MB] [--fix-batches-to-gpus]\n                     [--find-unused-parameters] [--gradient-as-bucket-view]\n                     [--fast-stat-sync]\n                     [--heartbeat-timeout HEARTBEAT_TIMEOUT]\n                     [--broadcast-buffers] [--slowmo-momentum SLOWMO_MOMENTUM]\n                     [--slowmo-base-algorithm SLOWMO_BASE_ALGORITHM]\n                     [--localsgd-frequency LOCALSGD_FREQUENCY]\n                     [--nprocs-per-node NPROCS_PER_NODE]\n                     [--pipeline-model-parallel]\n                     [--pipeline-balance PIPELINE_BALANCE]\n                     [--pipeline-devices PIPELINE_DEVICES]\n                     [--pipeline-chunks PIPELINE_CHUNKS]\n                     [--pipeline-encoder-balance PIPELINE_ENCODER_BALANCE]\n                     [--pipeline-encoder-devices PIPELINE_ENCODER_DEVICES]\n                     [--pipeline-decoder-balance PIPELINE_DECODER_BALANCE]\n                     [--pipeline-decoder-devices PIPELINE_DECODER_DEVICES]\n                     [--pipeline-checkpoint {always,never,except_last}]\n                     [--zero-sharding {none,os}] [--no-reshard-after-forward]\n                     [--fp32-reduce-scatter] [--cpu-offload]\n                     [--use-sharded-state] [--not-fsdp-flatten-parameters]\n                     [--arch ARCH] [--max-epoch MAX_EPOCH]\n                     [--max-update MAX_UPDATE]\n                     [--stop-time-hours STOP_TIME_HOURS]\n                     [--clip-norm CLIP_NORM] [--sentence-avg]\n                     [--update-freq UPDATE_FREQ] [--lr LR]\n                     [--stop-min-lr STOP_MIN_LR] [--use-bmuf]\n                     [--skip-remainder-batch] [--save-dir SAVE_DIR]\n                     [--restore-file RESTORE_FILE]\n                     [--continue-once CONTINUE_ONCE]\n                     [--finetune-from-model FINETUNE_FROM_MODEL]\n                     [--reset-dataloader] [--reset-lr-scheduler]\n                     [--reset-meters] [--reset-optimizer]\n                     [--optimizer-overrides OPTIMIZER_OVERRIDES]\n                     [--save-interval SAVE_INTERVAL]\n                     [--save-interval-updates SAVE_INTERVAL_UPDATES]\n                     [--keep-interval-updates KEEP_INTERVAL_UPDATES]\n                     [--keep-interval-updates-pattern KEEP_INTERVAL_UPDATES_PATTERN]\n                     [--keep-last-epochs KEEP_LAST_EPOCHS]\n                     [--keep-best-checkpoints KEEP_BEST_CHECKPOINTS]\n                     [--no-save] [--no-epoch-checkpoints]\n                     [--no-last-checkpoints] [--no-save-optimizer-state]\n                     [--best-checkpoint-metric BEST_CHECKPOINT_METRIC]\n                     [--maximize-best-checkpoint-metric] [--patience PATIENCE]\n                     [--checkpoint-suffix CHECKPOINT_SUFFIX]\n                     [--checkpoint-shard-count CHECKPOINT_SHARD_COUNT]\n                     [--load-checkpoint-on-all-dp-ranks]\n                     [--write-checkpoints-asynchronously] [--store-ema]\n                     [--ema-decay EMA_DECAY]\n                     [--ema-start-update EMA_START_UPDATE]\n                     [--ema-seed-model EMA_SEED_MODEL]\n                     [--ema-update-freq EMA_UPDATE_FREQ] [--ema-fp32]\nfairseq-train: error: argument --arch/-a: invalid choice: 'transformer_teacher' (choose from 'transformer_tiny', 'transformer', 'transformer_iwslt_de_en', 'transformer_wmt_en_de', 'transformer_vaswani_wmt_en_de_big', 'transformer_vaswani_wmt_en_fr_big', 'transformer_wmt_en_de_big', 'transformer_wmt_en_de_big_t2t', 'fconv', 'fconv_iwslt_de_en', 'fconv_wmt_en_ro', 'fconv_wmt_en_de', 'fconv_wmt_en_fr', 'fconv_lm', 'fconv_lm_dauphin_wikitext103', 'fconv_lm_dauphin_gbw', 'fconv_self_att', 'fconv_self_att_wp', 'lightconv', 'lightconv_iwslt_de_en', 'lightconv_wmt_en_de', 'lightconv_wmt_en_de_big', 'lightconv_wmt_en_fr_big', 'lightconv_wmt_zh_en_big', 'lightconv_lm', 'lightconv_lm_gbw', 'lstm', 'lstm_wiseman_iwslt_de_en', 'lstm_luong_wmt_en_de', 'lstm_lm', 'masked_lm', 'bert_base', 'bert_large', 'xlm_base', 'multilingual_transformer', 'multilingual_transformer_iwslt_de_en', 'transformer_align', 'transformer_wmt_en_de_big_align', 'transformer_from_pretrained_xlm', 'transformer_lm', 'transformer_lm_big', 'transformer_lm_baevski_wiki103', 'transformer_lm_wiki103', 'transformer_lm_baevski_gbw', 'transformer_lm_gbw', 'transformer_lm_gpt', 'transformer_lm_gpt2_small', 'transformer_lm_gpt2_tiny', 'transformer_lm_gpt2_medium', 'transformer_lm_gpt2_big', 'transformer_lm_gpt2_big_wide', 'transformer_lm_gpt2_bigger', 'transformer_lm_gpt3_small', 'transformer_lm_gpt3_medium', 'transformer_lm_gpt3_large', 'transformer_lm_gpt3_xl', 'transformer_lm_gpt3_2_7', 'transformer_lm_gpt3_6_7', 'transformer_lm_gpt3_13', 'transformer_lm_gpt3_175', 'transformer_ulm', 'transformer_ulm_big', 'transformer_ulm_tiny', 'bart_large', 'bart_base', 'mbart_large', 'mbart_base', 'mbart_base_wmt20', 'wav2vec', 'wav2vec2', 'wav2vec_ctc', 'wav2vec_seq2seq', 'hubert', 'hubert_ctc', 'hf_gpt2', 'hf_gpt2_medium', 'hf_gpt2_large', 'hf_gpt2_xl', 'nonautoregressive_transformer', 'nonautoregressive_transformer_wmt_en_de', 'nacrf_transformer', 'iterative_nonautoregressive_transformer', 'iterative_nonautoregressive_transformer_wmt_en_de', 'cmlm_transformer', 'cmlm_transformer_wmt_en_de', 'levenshtein_transformer', 'levenshtein_transformer_wmt_en_de', 'levenshtein_transformer_vaswani_wmt_en_de_big', 'levenshtein_transformer_wmt_en_de_big', 'insertion_transformer', 'roberta', 'roberta_prenorm', 'roberta_base', 'roberta_large', 'xlm', 'roberta_enc_dec', 's2t_berard', 's2t_berard_256_3_3', 's2t_berard_512_3_2', 's2t_berard_512_5_3', 'convtransformer', 'convtransformer_espnet', 's2t_transformer', 's2t_transformer_s', 's2t_transformer_xs', 's2t_transformer_sp', 's2t_transformer_m', 's2t_transformer_mp', 's2t_transformer_l', 's2t_transformer_lp', 'xm_transformer', 's2t_conformer', 'tacotron_2', 'tts_transformer', 'fastspeech2', 's2ut_transformer', 's2ut_transformer_fisher', 's2spect_transformer', 's2spect_transformer_fisher', 's2ut_conformer', 'xmod_base_13', 'xmod_base_30', 'xmod_base_60', 'xmod_base_75', 'xmod_base', 'xmod_large_prenorm', 'dummy_model', 'transformer_lm_megatron', 'transformer_lm_megatron_11b', 'transformer_iwslt_de_en_pipeline_parallel', 'transformer_wmt_en_de_big_pipeline_parallel', 'model_parallel_roberta', 'model_parallel_roberta_v1', 'model_parallel_roberta_postnorm', 'model_parallel_roberta_base', 'model_parallel_roberta_large')\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!bash Align-to-Distill/train_teacher.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e6f17a",
   "metadata": {},
   "source": [
    "### Step 19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0025c46a",
   "metadata": {},
   "source": [
    "Execution Start time: 19:46:26, End time: 19:46:27 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8faf5fb",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the architecture `transformer_teacher` is not recognized by Fairseq. I need to use a valid architecture name that is compatible with the iwslt14-de-en dataset. The architecture `transformer_iwslt_de_en` is a valid choice for this dataset. I will update the script to use this architecture and then re-run the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154261fd",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7a8909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\nhead -n 10 Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/test.de > Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/test_10.de\nhead -n 10 Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/test.en > Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/test_10.en\n\nmkdir -p $data_dir/$teacher_model/$data/\ntouch $data_dir/$teacher_model/$data/train.log\n\nCUDA_VISIBLE_DEVICES=$GPU_NUM fairseq-train $data_dir/$data \\\n    --arch transformer_iwslt_de_en --share-decoder-input-output-embed \\\n    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n    --dropout 0.3 --weight-decay 0.0001 \\\n    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n    --max-tokens 4096 \\\n    --max-epoch 1 --batch-size 1 \\"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Align-to-Distill/train_teacher.sh\n",
    "<<<BEFORE_EDIT>>>\n",
    "    --arch $teacher_model --share-decoder-input-output-embed \\\n",
    "<<<AFTER_EDIT>>>\n",
    "    --arch transformer_iwslt_de_en --share-decoder-input-output-embed \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6bf18b",
   "metadata": {},
   "source": [
    "### Step 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1bae44",
   "metadata": {},
   "source": [
    "Execution Start time: 19:46:29, End time: 19:46:35 (duration: 6 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf460b1b",
   "metadata": {},
   "source": [
    "**Thought**: The architecture in the `train_teacher.sh` script has been updated to `transformer_iwslt_de_en`, which is compatible with the iwslt14-de-en dataset. Now, I will execute the script again to train the teacher model with the specified parameters. This should proceed without the previous architecture error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ea0a96",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40c3f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2024-08-19 19:46:34 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 0, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': 1, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 1, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/data-bin/transformer_teacher/iwslt14.tokenized.de-en', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4096, batch_size=1, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4096, batch_size_valid=1, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer_iwslt_de_en', max_epoch=1, max_update=0, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0005], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/data-bin/transformer_teacher/iwslt14.tokenized.de-en', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='bleu', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='/data-bin/iwslt14.tokenized.de-en', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=True, eval_bleu_args='{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe='@@ ', eval_bleu_print_samples=True, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0001, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, share_decoder_input_output_embed=True, dropout=0.3, no_seed_provided=False, encoder_embed_dim=512, encoder_ffn_embed_dim=1024, encoder_attention_heads=4, encoder_layers=6, decoder_embed_dim=512, decoder_ffn_embed_dim=1024, decoder_attention_heads=4, decoder_layers=6, encoder_embed_path=None, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_all_embeddings=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=512, decoder_input_dim=512, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer_iwslt_de_en'), 'task': {'_name': 'translation', 'data': '/data-bin/iwslt14.tokenized.de-en', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': True}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\nTraceback (most recent call last):\n  File \"/usr/local/bin/fairseq-train\", line 8, in <module>\n    sys.exit(cli_main())\n  File \"/usr/local/lib/python3.10/site-packages/fairseq_cli/train.py\", line 557, in cli_main\n    distributed_utils.call_main(cfg, main)\n  File \"/usr/local/lib/python3.10/site-packages/fairseq/distributed/utils.py\", line 369, in call_main\n    main(cfg, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/fairseq_cli/train.py\", line 87, in main\n    task = tasks.setup_task(cfg.task)\n  File \"/usr/local/lib/python3.10/site-packages/fairseq/tasks/__init__.py\", line 46, in setup_task\n    return task.setup_task(cfg, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/fairseq/tasks/translation.py\", line 301, in setup_task\n    cfg.source_lang, cfg.target_lang = data_utils.infer_language_pair(paths[0])\n  File \"/usr/local/lib/python3.10/site-packages/fairseq/data/data_utils.py\", line 30, in infer_language_pair\n    for filename in PathManager.ls(path):\n  File \"/usr/local/lib/python3.10/site-packages/fairseq/file_io.py\", line 100, in ls\n    return os.listdir(path)\nFileNotFoundError: [Errno 2] No such file or directory: '/data-bin/iwslt14.tokenized.de-en'\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!bash Align-to-Distill/train_teacher.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe0728a",
   "metadata": {},
   "source": [
    "### Step 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeaf040",
   "metadata": {},
   "source": [
    "Execution Start time: 19:46:38, End time: 19:46:38 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415fd660",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the directory `/data-bin/iwslt14.tokenized.de-en` does not exist. This suggests that the data preprocessing step might not have been completed or the data was not correctly placed in the expected directory. I need to ensure that the data is correctly preprocessed and placed in the `/data-bin` directory. I will check the data preprocessing steps and ensure the data is correctly placed before attempting to train the model again. Additionally, I will verify the paths used in the script to ensure they match the actual directory structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b4b99b",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb1788f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ls: cannot access 'Align-to-Distill/data-bin': No such file or directory\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ls Align-to-Distill/data-bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efdc635",
   "metadata": {},
   "source": [
    "### Step 22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db799999",
   "metadata": {},
   "source": [
    "Execution Start time: 19:46:42, End time: 19:46:42 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e05b111",
   "metadata": {},
   "source": [
    "**Thought**: It appears that the `data-bin` directory does not exist, which suggests that the data preprocessing step did not complete successfully or the data was not placed in the correct directory. I need to ensure that the data is preprocessed and placed in the `data-bin` directory. I will review the data preparation script and execute it to ensure the data is correctly preprocessed and placed in the expected directory structure for training. \n",
    "\n",
    "I will also verify the paths used in the `train_teacher.sh` script to ensure they match the actual directory structure. If necessary, I will create the `data-bin` directory and move the preprocessed data there. \n",
    "\n",
    "First, I will check the data preparation script to understand how the data is supposed to be preprocessed and where it should be placed. Then, I will execute the necessary steps to ensure the data is correctly prepared for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b127f5d",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738f9a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#!/usr/bin/env bash\n#\n# Adapted from https://github.com/facebookresearch/MIXER/blob/master/prepareData.sh\n\necho 'Cloning Moses github repository (for tokenization scripts)...'\ngit clone https://github.com/moses-smt/mosesdecoder.git\n\necho 'Cloning Subword NMT repository (for BPE pre-processing)...'\ngit clone https://github.com/rsennrich/subword-nmt.git\n\nSCRIPTS=mosesdecoder/scripts\nTOKENIZER=$SCRIPTS/tokenizer/tokenizer.perl\nLC=$SCRIPTS/tokenizer/lowercase.perl\nCLEAN=$SCRIPTS/training/clean-corpus-n.perl\nBPEROOT=subword-nmt/subword_nmt\nBPE_TOKENS=10000\n\nURL=\"http://dl.fbaipublicfiles.com/fairseq/data/iwslt14/de-en.tgz\"\nGZ=de-en.tgz\n\nif [ ! -d \"$SCRIPTS\" ]; then\n    echo \"Please set SCRIPTS variable correctly to point to Moses scripts.\"\n    exit\nfi\n\nsrc=de\ntgt=en\nlang=de-en\nprep=iwslt14.tokenized.de-en\ntmp=$prep/tmp\norig=orig\n\nmkdir -p $orig $tmp $prep\n\necho \"Downloading data from ${URL}...\"\ncd $orig\nwget \"$URL\"\n\nif [ -f $GZ ]; then\n    echo \"Data successfully downloaded.\"\nelse\n    echo \"Data not successfully downloaded.\"\n    exit\nfi\n\ntar zxvf $GZ\ncd ..\n\necho \"pre-processing train data...\"\nfor l in $src $tgt; do\n    f=train.tags.$lang.$l\n    tok=train.tags.$lang.tok.$l\n\n    cat $orig/$lang/$f | \\\n    grep -v '<url>' | \\\n    grep -v '<talkid>' | \\\n    grep -v '<keywords>' | \\\n    sed -e 's/<title>//g' | \\\n    sed -e 's/<\\/title>//g' | \\\n    sed -e 's/<description>//g' | \\\n    sed -e 's/<\\/description>//g' | \\\n    perl $TOKENIZER -threads 8 -l $l > $tmp/$tok\n    echo \"\"\ndone\nperl $CLEAN -ratio 1.5 $tmp/train.tags.$lang.tok $src $tgt $tmp/train.tags.$lang.clean 1 175\nfor l in $src $tgt; do\n    perl $LC < $tmp/train.tags.$lang.clean.$l > $tmp/train.tags.$lang.$l\ndone\n\necho \"pre-processing valid/test data...\"\nfor l in $src $tgt; do\n    for o in `ls $orig/$lang/IWSLT14.TED*.$l.xml`; do\n    fname=${o##*/}\n    f=$tmp/${fname%.*}\n    echo $o $f\n    grep '<seg id' $o | \\\n        sed -e 's/<seg id=\"[0-9]*\">\\s*//g' | \\\n        sed -e 's/\\s*<\\/seg>\\s*//g' | \\\n        sed -e \"s/\\’/\\'/g\" | \\\n    perl $TOKENIZER -threads 8 -l $l | \\\n    perl $LC > $f\n    echo \"\"\n    done\ndone\n\n\necho \"creating train, valid, test...\"\nfor l in $src $tgt; do\n    awk '{if (NR%23 == 0)  print $0; }' $tmp/train.tags.de-en.$l > $tmp/valid.$l\n    awk '{if (NR%23 != 0)  print $0; }' $tmp/train.tags.de-en.$l > $tmp/train.$l\n\n    cat $tmp/IWSLT14.TED.dev2010.de-en.$l \\\n        $tmp/IWSLT14.TEDX.dev2012.de-en.$l \\\n        $tmp/IWSLT14.TED.tst2010.de-en.$l \\\n        $tmp/IWSLT14.TED.tst2011.de-en.$l \\\n        $tmp/IWSLT14.TED.tst2012.de-en.$l \\\n        > $tmp/test.$l\ndone\n\nTRAIN=$tmp/train.en-de\nBPE_CODE=$prep/code\nrm -f $TRAIN\nfor l in $src $tgt; do\n    cat $tmp/train.$l >> $TRAIN\ndone\n\necho \"learn_bpe.py on ${TRAIN}...\"\npython $BPEROOT/learn_bpe.py -s $BPE_TOKENS < $TRAIN > $BPE_CODE\n\nfor L in $src $tgt; do\n    for f in train.$L valid.$L test.$L; do\n        echo \"apply_bpe.py to ${f}...\"\n        python $BPEROOT/apply_bpe.py -c $BPE_CODE < $tmp/$f > $prep/$f\n    done\ndone\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!cat Align-to-Distill/examples/translation/prepare-iwslt14.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4767adff",
   "metadata": {},
   "source": [
    "### Step 23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b682927",
   "metadata": {},
   "source": [
    "Execution Start time: 19:46:47, End time: 19:47:19 (duration: 32 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8aa935",
   "metadata": {},
   "source": [
    "**Thought**: The `prepare-iwslt14.sh` script is responsible for downloading, tokenizing, and preparing the dataset. It places the processed data in the `iwslt14.tokenized.de-en` directory. However, it seems the data is not being moved to the `data-bin` directory, which is expected by the training script. I need to ensure that the data is correctly preprocessed and then move it to the `data-bin` directory. I will create the `data-bin` directory and move the preprocessed data there.\n",
    "\n",
    "Additionally, I will ensure that the Fairseq preprocessing step (`fairseq-preprocess`) is executed to convert the tokenized data into a format suitable for Fairseq training. This step is crucial to ensure the data is correctly prepared for training.\n",
    "\n",
    "First, I will create the `data-bin` directory and then execute the Fairseq preprocessing step to prepare the data for training. After that, I will attempt to train the teacher model again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a6a864",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3189d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2024-08-19 19:46:50 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='de', target_lang='en', trainpref='Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/train', validpref='Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/valid', testpref='Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/test', align_suffix=None, destdir='Align-to-Distill/data-bin/iwslt14.tokenized.de-en', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=4, dict_only=False)\n2024-08-19 19:46:53 | INFO | fairseq_cli.preprocess | [de] Dictionary: 8848 types\n2024-08-19 19:47:05 | INFO | fairseq_cli.preprocess | [de] Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/train.de: 160239 sents, 4035591 tokens, 0.0% replaced (by <unk>)\n2024-08-19 19:47:05 | INFO | fairseq_cli.preprocess | [de] Dictionary: 8848 types\n2024-08-19 19:47:06 | INFO | fairseq_cli.preprocess | [de] Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/valid.de: 7283 sents, 182592 tokens, 0.0192% replaced (by <unk>)\n2024-08-19 19:47:06 | INFO | fairseq_cli.preprocess | [de] Dictionary: 8848 types\n2024-08-19 19:47:06 | INFO | fairseq_cli.preprocess | [de] Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/test.de: 6750 sents, 161838 tokens, 0.0636% replaced (by <unk>)\n2024-08-19 19:47:06 | INFO | fairseq_cli.preprocess | [en] Dictionary: 6632 types\n2024-08-19 19:47:17 | INFO | fairseq_cli.preprocess | [en] Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/train.en: 160239 sents, 3949114 tokens, 0.0% replaced (by <unk>)\n2024-08-19 19:47:17 | INFO | fairseq_cli.preprocess | [en] Dictionary: 6632 types\n2024-08-19 19:47:18 | INFO | fairseq_cli.preprocess | [en] Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/valid.en: 7283 sents, 178622 tokens, 0.00448% replaced (by <unk>)\n2024-08-19 19:47:18 | INFO | fairseq_cli.preprocess | [en] Dictionary: 6632 types\n2024-08-19 19:47:19 | INFO | fairseq_cli.preprocess | [en] Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/test.en: 6750 sents, 156928 tokens, 0.00892% replaced (by <unk>)\n2024-08-19 19:47:19 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to Align-to-Distill/data-bin/iwslt14.tokenized.de-en\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!mkdir -p Align-to-Distill/data-bin && fairseq-preprocess --source-lang de --target-lang en --trainpref Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/train --validpref Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/valid --testpref Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/test --destdir Align-to-Distill/data-bin/iwslt14.tokenized.de-en --workers 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c623398",
   "metadata": {},
   "source": [
    "### Step 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8593a0",
   "metadata": {},
   "source": [
    "Execution Start time: 19:47:21, End time: 19:47:28 (duration: 6 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d1a0d9",
   "metadata": {},
   "source": [
    "**Thought**: The Fairseq preprocessing step has been completed successfully, and the preprocessed data has been written to the `Align-to-Distill/data-bin/iwslt14.tokenized.de-en` directory. Now, I will attempt to train the teacher model again using the `train_teacher.sh` script, as the data is now correctly prepared and placed in the expected directory structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7128bcf3",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e04f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2024-08-19 19:47:26 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 0, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': 1, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 1, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/data-bin/transformer_teacher/iwslt14.tokenized.de-en', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4096, batch_size=1, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4096, batch_size_valid=1, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer_iwslt_de_en', max_epoch=1, max_update=0, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0005], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/data-bin/transformer_teacher/iwslt14.tokenized.de-en', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='bleu', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='/data-bin/iwslt14.tokenized.de-en', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=True, eval_bleu_args='{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe='@@ ', eval_bleu_print_samples=True, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0001, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, share_decoder_input_output_embed=True, dropout=0.3, no_seed_provided=False, encoder_embed_dim=512, encoder_ffn_embed_dim=1024, encoder_attention_heads=4, encoder_layers=6, decoder_embed_dim=512, decoder_ffn_embed_dim=1024, decoder_attention_heads=4, decoder_layers=6, encoder_embed_path=None, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_all_embeddings=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=512, decoder_input_dim=512, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer_iwslt_de_en'), 'task': {'_name': 'translation', 'data': '/data-bin/iwslt14.tokenized.de-en', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': True}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\nTraceback (most recent call last):\n  File \"/usr/local/bin/fairseq-train\", line 8, in <module>\n    sys.exit(cli_main())\n  File \"/usr/local/lib/python3.10/site-packages/fairseq_cli/train.py\", line 557, in cli_main\n    distributed_utils.call_main(cfg, main)\n  File \"/usr/local/lib/python3.10/site-packages/fairseq/distributed/utils.py\", line 369, in call_main\n    main(cfg, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/fairseq_cli/train.py\", line 87, in main\n    task = tasks.setup_task(cfg.task)\n  File \"/usr/local/lib/python3.10/site-packages/fairseq/tasks/__init__.py\", line 46, in setup_task\n    return task.setup_task(cfg, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/fairseq/tasks/translation.py\", line 301, in setup_task\n    cfg.source_lang, cfg.target_lang = data_utils.infer_language_pair(paths[0])\n  File \"/usr/local/lib/python3.10/site-packages/fairseq/data/data_utils.py\", line 30, in infer_language_pair\n    for filename in PathManager.ls(path):\n  File \"/usr/local/lib/python3.10/site-packages/fairseq/file_io.py\", line 100, in ls\n    return os.listdir(path)\nFileNotFoundError: [Errno 2] No such file or directory: '/data-bin/iwslt14.tokenized.de-en'\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!bash Align-to-Distill/train_teacher.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4e3aff",
   "metadata": {},
   "source": [
    "### Step 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea827d5",
   "metadata": {},
   "source": [
    "Execution Start time: 19:47:30, End time: 19:47:30 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79831d6",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804470e0",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23279c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "711185c9",
   "metadata": {},
   "source": [
    "### Step 26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296aa75c",
   "metadata": {},
   "source": [
    "Execution Start time: 19:47:33, End time: 19:47:34 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdd71cc",
   "metadata": {},
   "source": [
    "**Thought**: The error persists, indicating that the script is still looking for the data in the `/data-bin/iwslt14.tokenized.de-en` directory. This suggests that the path in the script might not be correctly set to the relative path where the data is actually located. I need to ensure that the `data_dir` variable in the `train_teacher.sh` script is set correctly to point to the `Align-to-Distill/data-bin` directory. I will update the script to use the correct relative path for the data directory and then attempt to train the model again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf74572",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f75e06d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n# SPDX-FileCopyrightText: Ⓒ 2024 NCSOFT Corporation. All Rights Reserved.\n# SPDX-License-Identifier: BSD-3-Clause\n\n#!/bin/bash\nbase_dir=$path_to_fairseq\nexport PYTHONPATH=\"${PYTHONPATH}:$base_dir\"\ndata_dir=Align-to-Distill/data-bin\ndata=iwslt14.tokenized.de-en\ncustom_model_dir=$base_dir/custom/\nteacher_model=transformer_teacher\n\n# Preprocess to keep only the first 10 rows\nhead -n 10 Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/train.de > Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/train_10.de"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Align-to-Distill/train_teacher.sh\n",
    "<<<BEFORE_EDIT>>>\n",
    "data_dir=$base_dir/data-bin\n",
    "<<<AFTER_EDIT>>>\n",
    "data_dir=Align-to-Distill/data-bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebcca8d",
   "metadata": {},
   "source": [
    "### Step 27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e3640d",
   "metadata": {},
   "source": [
    "Execution Start time: 19:47:36, End time: 19:47:36 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dfc08a",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ed4a19",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd609030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7808db78",
   "metadata": {},
   "source": [
    "### Step 28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7b1a22",
   "metadata": {},
   "source": [
    "Execution Start time: 19:47:38, End time: 19:47:45 (duration: 6 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a86183d",
   "metadata": {},
   "source": [
    "**Thought**: The `data_dir` variable in the `train_teacher.sh` script has been updated to point to the correct relative path `Align-to-Distill/data-bin`. Now, I will execute the script again to train the teacher model, as the data directory path should now be correctly set to the location where the preprocessed data is stored. This should resolve the previous error related to the data directory not being found."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975099e2",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8973936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2024-08-19 19:47:43 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 0, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': 1, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 1, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'Align-to-Distill/data-bin/transformer_teacher/iwslt14.tokenized.de-en', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4096, batch_size=1, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4096, batch_size_valid=1, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer_iwslt_de_en', max_epoch=1, max_update=0, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0005], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='Align-to-Distill/data-bin/transformer_teacher/iwslt14.tokenized.de-en', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='bleu', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='Align-to-Distill/data-bin/iwslt14.tokenized.de-en', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=True, eval_bleu_args='{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe='@@ ', eval_bleu_print_samples=True, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0001, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, share_decoder_input_output_embed=True, dropout=0.3, no_seed_provided=False, encoder_embed_dim=512, encoder_ffn_embed_dim=1024, encoder_attention_heads=4, encoder_layers=6, decoder_embed_dim=512, decoder_ffn_embed_dim=1024, decoder_attention_heads=4, decoder_layers=6, encoder_embed_path=None, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_all_embeddings=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=512, decoder_input_dim=512, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer_iwslt_de_en'), 'task': {'_name': 'translation', 'data': 'Align-to-Distill/data-bin/iwslt14.tokenized.de-en', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': True}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n2024-08-19 19:47:43 | INFO | fairseq.tasks.translation | [de] dictionary: 8848 types\n2024-08-19 19:47:43 | INFO | fairseq.tasks.translation | [en] dictionary: 6632 types\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/fairseq/data/encoders/moses_tokenizer.py\", line 31, in __init__\n    from sacremoses import MosesTokenizer, MosesDetokenizer\nModuleNotFoundError: No module named 'sacremoses'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/bin/fairseq-train\", line 8, in <module>\n    sys.exit(cli_main())\n  File \"/usr/local/lib/python3.10/site-packages/fairseq_cli/train.py\", line 557, in cli_main\n    distributed_utils.call_main(cfg, main)\n  File \"/usr/local/lib/python3.10/site-packages/fairseq/distributed/utils.py\", line 369, in call_main\n    main(cfg, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/fairseq_cli/train.py\", line 96, in main\n    model = task.build_model(cfg.model)\n  File \"/usr/local/lib/python3.10/site-packages/fairseq/tasks/translation.py\", line 372, in build_model\n    self.tokenizer = encoders.build_tokenizer(\n  File \"/usr/local/lib/python3.10/site-packages/fairseq/registry.py\", line 61, in build_x\n    return builder(cfg, *extra_args, **extra_kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/fairseq/data/encoders/moses_tokenizer.py\", line 36, in __init__\n    raise ImportError(\nImportError: Please install Moses tokenizer with: pip install sacremoses\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!bash Align-to-Distill/train_teacher.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55feeb2",
   "metadata": {},
   "source": [
    "### Step 29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05563003",
   "metadata": {},
   "source": [
    "Execution Start time: 19:47:47, End time: 19:47:50 (duration: 2 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3df8e8",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the `sacremoses` package, which is required for the Moses tokenizer, is not installed. I need to install this package to proceed with training the teacher model. Once installed, I will re-run the training script to ensure the model is trained correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861b880d",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952be827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting sacremoses\n  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.10/site-packages (from sacremoses) (2023.12.25)\nRequirement already satisfied: click in /usr/local/lib/python3.10/site-packages (from sacremoses) (8.1.7)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/site-packages (from sacremoses) (1.4.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from sacremoses) (4.66.4)\nDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/897.5 kB ? eta -:--:--\n\n\u001b[?25hDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\nInstalling collected packages: sacremoses\nSuccessfully installed sacremoses-0.1.1\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install sacremoses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfe23b7",
   "metadata": {},
   "source": [
    "### Step 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3d0512",
   "metadata": {},
   "source": [
    "Execution Start time: 19:47:51, End time: 19:52:52 (duration: 300 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def40eb2",
   "metadata": {},
   "source": [
    "**Thought**: The `sacremoses` package has been successfully installed. Now, I will execute the `train_teacher.sh` script again to train the teacher model, as all necessary dependencies should now be in place. This should allow the training process to proceed without encountering the previous errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffc5a8f",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7937ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2024-08-19 19:47:56 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 0, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': 1, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 1, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'Align-to-Distill/data-bin/transformer_teacher/iwslt14.tokenized.de-en', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4096, batch_size=1, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4096, batch_size_valid=1, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer_iwslt_de_en', max_epoch=1, max_update=0, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0005], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='Align-to-Distill/data-bin/transformer_teacher/iwslt14.tokenized.de-en', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='bleu', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='Align-to-Distill/data-bin/iwslt14.tokenized.de-en', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=True, eval_bleu_args='{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe='@@ ', eval_bleu_print_samples=True, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0001, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, share_decoder_input_output_embed=True, dropout=0.3, no_seed_provided=False, encoder_embed_dim=512, encoder_ffn_embed_dim=1024, encoder_attention_heads=4, encoder_layers=6, decoder_embed_dim=512, decoder_ffn_embed_dim=1024, decoder_attention_heads=4, decoder_layers=6, encoder_embed_path=None, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_all_embeddings=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=512, decoder_input_dim=512, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer_iwslt_de_en'), 'task': {'_name': 'translation', 'data': 'Align-to-Distill/data-bin/iwslt14.tokenized.de-en', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': True}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n2024-08-19 19:47:56 | INFO | fairseq.tasks.translation | [de] dictionary: 8848 types\n2024-08-19 19:47:56 | INFO | fairseq.tasks.translation | [en] dictionary: 6632 types\n2024-08-19 19:47:57 | INFO | fairseq_cli.train | TransformerModel(\n  (encoder): TransformerEncoderBase(\n    (dropout_module): FairseqDropout()\n    (embed_tokens): Embedding(8848, 512, padding_idx=1)\n    (embed_positions): SinusoidalPositionalEmbedding()\n    (layers): ModuleList(\n      (0-5): 6 x TransformerEncoderLayerBase(\n        (self_attn): MultiheadAttention(\n          (dropout_module): FairseqDropout()\n        )\n        (dropout_module): FairseqDropout()\n        (activation_dropout_module): FairseqDropout()\n      )\n    )\n  )\n  (decoder): TransformerDecoderBase(\n    (dropout_module): FairseqDropout()\n    (embed_tokens): Embedding(6632, 512, padding_idx=1)\n    (embed_positions): SinusoidalPositionalEmbedding()\n    (layers): ModuleList(\n      (0-5): 6 x TransformerDecoderLayerBase(\n        (dropout_module): FairseqDropout()\n        (self_attn): MultiheadAttention(\n          (dropout_module): FairseqDropout()\n        )\n        (activation_dropout_module): FairseqDropout()\n        (encoder_attn): MultiheadAttention(\n          (dropout_module): FairseqDropout()\n        )\n      )\n    )\n  )\n)\n2024-08-19 19:47:57 | INFO | fairseq_cli.train | task: TranslationTask\n2024-08-19 19:47:57 | INFO | fairseq_cli.train | model: TransformerModel\n2024-08-19 19:47:57 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n2024-08-19 19:47:57 | INFO | fairseq_cli.train | num. shared model params: 52,073,472 (num. trained: 52,073,472)\n2024-08-19 19:47:57 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n2024-08-19 19:47:57 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: Align-to-Distill/data-bin/iwslt14.tokenized.de-en/valid.de-en.de\n2024-08-19 19:47:57 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: Align-to-Distill/data-bin/iwslt14.tokenized.de-en/valid.de-en.en\n2024-08-19 19:47:57 | INFO | fairseq.tasks.translation | Align-to-Distill/data-bin/iwslt14.tokenized.de-en valid de-en 7283 examples\n2024-08-19 19:47:57 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n2024-08-19 19:47:57 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n2024-08-19 19:47:57 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = 1\n2024-08-19 19:47:57 | INFO | fairseq.trainer | Preparing to load checkpoint Align-to-Distill/data-bin/transformer_teacher/iwslt14.tokenized.de-en/checkpoint_last.pt\n2024-08-19 19:47:57 | INFO | fairseq.trainer | No existing checkpoint found Align-to-Distill/data-bin/transformer_teacher/iwslt14.tokenized.de-en/checkpoint_last.pt\n2024-08-19 19:47:57 | INFO | fairseq.trainer | loading train data for epoch 1\n2024-08-19 19:47:57 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: Align-to-Distill/data-bin/iwslt14.tokenized.de-en/train.de-en.de\n2024-08-19 19:47:57 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: Align-to-Distill/data-bin/iwslt14.tokenized.de-en/train.de-en.en\n2024-08-19 19:47:57 | INFO | fairseq.tasks.translation | Align-to-Distill/data-bin/iwslt14.tokenized.de-en train de-en 160239 examples\n2024-08-19 19:47:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 160239\n\nepoch 001:   0%|                                     | 0/160239 [00:00<?, ?it/s]2024-08-19 19:47:58 | INFO | fairseq.trainer | begin training epoch 1\n2024-08-19 19:47:58 | INFO | fairseq_cli.train | Start iterating over samples\n/usr/local/lib/python3.10/site-packages/fairseq/tasks/fairseq_task.py:514: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):\n\nepoch 001:   0%|                          | 1/160239 [00:01<46:37:11,  1.05s/it]\nepoch 001:   0%|                          | 2/160239 [00:01<28:22:40,  1.57it/s]\nepoch 001:   0%|                          | 3/160239 [00:01<21:31:00,  2.07it/s]\nepoch 001:   0%|                          | 4/160239 [00:02<18:34:55,  2.40it/s]\nepoch 001:   0%|                          | 5/160239 [00:02<16:31:16,  2.69it/s]\nepoch 001:   0%|                          | 6/160239 [00:02<15:39:15,  2.84it/s]\nepoch 001:   0%|                          | 7/160239 [00:02<15:42:40,  2.83it/s]\nepoch 001:   0%|                          | 8/160239 [00:03<14:58:07,  2.97it/s]\nepoch 001:   0%|                          | 9/160239 [00:03<14:27:19,  3.08it/s]\nepoch 001:   0%|                         | 10/160239 [00:03<13:54:08,  3.20it/s]\nepoch 001:   0%|                         | 11/160239 [00:04<13:37:56,  3.26it/s]\nepoch 001:   0%|                         | 12/160239 [00:04<13:24:53,  3.32it/s]\nepoch 001:   0%|                         | 13/160239 [00:04<13:13:00,  3.37it/s]\nepoch 001:   0%|                         | 14/160239 [00:05<15:19:42,  2.90it/s]\nepoch 001:   0%|                         | 15/160239 [00:05<15:06:04,  2.95it/s]\nepoch 001:   0%|                         | 16/160239 [00:05<14:25:47,  3.08it/s]\nepoch 001:   0%|                         | 17/160239 [00:06<14:00:06,  3.18it/s]\nepoch 001:   0%|                         | 18/160239 [00:06<13:38:13,  3.26it/s]\nepoch 001:   0%|                         | 19/160239 [00:06<15:15:41,  2.92it/s]\nepoch 001:   0%|                         | 20/160239 [00:07<14:44:06,  3.02it/s]\nepoch 001:   0%|                         | 21/160239 [00:07<14:12:40,  3.13it/s]\nepoch 001:   0%|                         | 22/160239 [00:07<13:57:26,  3.19it/s]\nepoch 001:   0%|                         | 23/160239 [00:07<13:36:43,  3.27it/s]\nepoch 001:   0%|                         | 24/160239 [00:08<13:35:33,  3.27it/s]\nepoch 001:   0%|                         | 25/160239 [00:08<13:53:32,  3.20it/s]\nepoch 001:   0%|                         | 26/160239 [00:08<13:41:53,  3.25it/s]\nepoch 001:   0%|                         | 27/160239 [00:09<13:33:36,  3.28it/s]\nepoch 001:   0%|                         | 28/160239 [00:09<13:20:47,  3.33it/s]\nepoch 001:   0%|                         | 29/160239 [00:09<14:10:37,  3.14it/s]\nepoch 001:   0%|                         | 30/160239 [00:10<14:17:52,  3.11it/s]\nepoch 001:   0%|                         | 31/160239 [00:10<14:02:28,  3.17it/s]\nepoch 001:   0%|                         | 32/160239 [00:10<14:16:24,  3.12it/s]\nepoch 001:   0%|                         | 33/160239 [00:11<14:25:15,  3.09it/s]\nepoch 001:   0%|                         | 34/160239 [00:11<14:14:27,  3.12it/s]\nepoch 001:   0%|                         | 35/160239 [00:11<13:51:49,  3.21it/s]\nepoch 001:   0%|                         | 36/160239 [00:12<14:20:56,  3.10it/s]\nepoch 001:   0%|                         | 37/160239 [00:12<13:55:23,  3.20it/s]\nepoch 001:   0%|                         | 38/160239 [00:12<15:29:15,  2.87it/s]\nepoch 001:   0%|                         | 39/160239 [00:13<15:27:31,  2.88it/s]\nepoch 001:   0%|                         | 40/160239 [00:13<15:12:47,  2.93it/s]\nepoch 001:   0%|                         | 41/160239 [00:13<14:56:17,  2.98it/s]\nepoch 001:   0%|                         | 42/160239 [00:14<14:41:43,  3.03it/s]\nepoch 001:   0%|                         | 43/160239 [00:14<14:13:30,  3.13it/s]\nepoch 001:   0%|                         | 44/160239 [00:14<13:43:43,  3.24it/s]\nepoch 001:   0%|                         | 45/160239 [00:15<14:00:13,  3.18it/s]\nepoch 001:   0%|                         | 46/160239 [00:15<13:59:27,  3.18it/s]\nepoch 001:   0%|                         | 47/160239 [00:15<13:48:33,  3.22it/s]\nepoch 001:   0%|                         | 48/160239 [00:15<13:44:07,  3.24it/s]\nepoch 001:   0%|                         | 49/160239 [00:16<13:40:53,  3.25it/s]\nepoch 001:   0%|                         | 50/160239 [00:16<13:27:59,  3.30it/s]\nepoch 001:   0%|                         | 51/160239 [00:16<13:31:12,  3.29it/s]\nepoch 001:   0%|                         | 52/160239 [00:17<13:46:10,  3.23it/s]\nepoch 001:   0%|                         | 53/160239 [00:17<14:03:59,  3.16it/s]\nepoch 001:   0%|                         | 54/160239 [00:17<13:39:46,  3.26it/s]\nepoch 001:   0%|                         | 55/160239 [00:18<14:35:22,  3.05it/s]\nepoch 001:   0%|                         | 56/160239 [00:18<14:05:17,  3.16it/s]\nepoch 001:   0%|                         | 57/160239 [00:18<13:40:22,  3.25it/s]\nepoch 001:   0%|                         | 58/160239 [00:19<13:42:20,  3.25it/s]\nepoch 001:   0%|                         | 59/160239 [00:19<14:41:59,  3.03it/s]\nepoch 001:   0%|                         | 60/160239 [00:19<14:37:10,  3.04it/s]\nepoch 001:   0%|                         | 61/160239 [00:20<14:38:32,  3.04it/s]\nepoch 001:   0%|                         | 62/160239 [00:20<14:16:21,  3.12it/s]\nepoch 001:   0%|                         | 63/160239 [00:20<14:06:25,  3.15it/s]\nepoch 001:   0%|                         | 64/160239 [00:21<14:10:55,  3.14it/s]\nepoch 001:   0%|                         | 65/160239 [00:21<14:04:25,  3.16it/s]\nepoch 001:   0%|                         | 66/160239 [00:21<13:49:44,  3.22it/s]\nepoch 001:   0%|                         | 67/160239 [00:21<13:24:15,  3.32it/s]\nepoch 001:   0%|                         | 68/160239 [00:22<12:59:24,  3.43it/s]\nepoch 001:   0%|                         | 69/160239 [00:22<13:25:10,  3.32it/s]\nepoch 001:   0%|                         | 70/160239 [00:22<13:32:27,  3.29it/s]\nepoch 001:   0%|                         | 71/160239 [00:23<13:39:01,  3.26it/s]\nepoch 001:   0%|                         | 72/160239 [00:23<13:46:39,  3.23it/s]\nepoch 001:   0%|                         | 73/160239 [00:23<13:48:45,  3.22it/s]\nepoch 001:   0%|                         | 74/160239 [00:24<13:36:23,  3.27it/s]\nepoch 001:   0%|                         | 75/160239 [00:24<14:48:09,  3.01it/s]\nepoch 001:   0%|                         | 76/160239 [00:24<14:24:00,  3.09it/s]\nepoch 001:   0%|                         | 77/160239 [00:25<13:49:18,  3.22it/s]\nepoch 001:   0%|                         | 78/160239 [00:25<14:43:26,  3.02it/s]\nepoch 001:   0%|                         | 79/160239 [00:25<14:07:11,  3.15it/s]\nepoch 001:   0%|                         | 80/160239 [00:26<13:47:09,  3.23it/s]\nepoch 001:   0%|                         | 81/160239 [00:26<13:39:18,  3.26it/s]\nepoch 001:   0%|                         | 82/160239 [00:26<13:13:08,  3.37it/s]\nepoch 001:   0%|                         | 83/160239 [00:26<13:17:39,  3.35it/s]\nepoch 001:   0%|                         | 84/160239 [00:27<13:08:49,  3.38it/s]\nepoch 001:   0%|                         | 85/160239 [00:27<13:11:09,  3.37it/s]\nepoch 001:   0%|                         | 86/160239 [00:27<13:10:13,  3.38it/s]\nepoch 001:   0%|                         | 87/160239 [00:28<13:24:20,  3.32it/s]\nepoch 001:   0%|                         | 88/160239 [00:28<13:13:44,  3.36it/s]\nepoch 001:   0%|                         | 89/160239 [00:28<13:02:59,  3.41it/s]\nepoch 001:   0%|                         | 90/160239 [00:28<12:54:07,  3.45it/s]\nepoch 001:   0%|                         | 91/160239 [00:29<13:04:33,  3.40it/s]\nepoch 001:   0%|                         | 92/160239 [00:29<13:02:53,  3.41it/s]\nepoch 001:   0%|                         | 93/160239 [00:29<12:57:24,  3.43it/s]\nepoch 001:   0%|                         | 94/160239 [00:30<12:41:50,  3.50it/s]\nepoch 001:   0%|                         | 95/160239 [00:30<12:53:06,  3.45it/s]\nepoch 001:   0%|                         | 96/160239 [00:30<13:47:04,  3.23it/s]\nepoch 001:   0%|                         | 97/160239 [00:31<13:50:20,  3.21it/s]\nepoch 001:   0%|                         | 98/160239 [00:31<13:49:25,  3.22it/s]\nepoch 001:   0%|                         | 99/160239 [00:31<13:45:05,  3.23it/s]\nepoch 001:   0%| | 100/160239 [00:31<13:50:00,  3.22it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 101/160239 [00:32<13:57:52,  3.19it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 102/160239 [00:32<13:48:38,  3.22it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 103/160239 [00:33<15:01:38,  2.96it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 104/160239 [00:33<14:38:17,  3.04it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 105/160239 [00:33<14:46:21,  3.01it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 106/160239 [00:33<14:48:33,  3.00it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 107/160239 [00:34<14:31:27,  3.06it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 108/160239 [00:34<14:55:42,  2.98it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 109/160239 [00:34<14:24:21,  3.09it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 110/160239 [00:35<14:13:27,  3.13it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 111/160239 [00:35<13:44:06,  3.24it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 112/160239 [00:35<13:10:53,  3.37it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 113/160239 [00:36<13:04:24,  3.40it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 114/160239 [00:36<13:07:36,  3.39it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 115/160239 [00:36<13:15:11,  3.36it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 116/160239 [00:37<13:41:23,  3.25it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 117/160239 [00:37<13:42:18,  3.25it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 118/160239 [00:37<14:05:41,  3.16it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 119/160239 [00:37<13:42:01,  3.25it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 120/160239 [00:38<13:35:41,  3.27it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 121/160239 [00:38<13:59:20,  3.18it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 122/160239 [00:38<13:50:56,  3.21it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 123/160239 [00:39<14:03:14,  3.16it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 124/160239 [00:39<13:41:23,  3.25it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 125/160239 [00:39<13:20:24,  3.33it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 126/160239 [00:40<13:15:53,  3.35it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 127/160239 [00:40<13:09:24,  3.38it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 128/160239 [00:40<13:12:50,  3.37it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 129/160239 [00:40<13:13:01,  3.36it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 130/160239 [00:41<13:09:19,  3.38it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 131/160239 [00:41<13:10:12,  3.38it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 132/160239 [00:41<13:22:01,  3.33it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 133/160239 [00:42<13:09:05,  3.38it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 134/160239 [00:42<13:05:52,  3.40it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 135/160239 [00:42<13:06:22,  3.39it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 136/160239 [00:43<13:10:11,  3.38it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 137/160239 [00:43<13:12:09,  3.37it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 138/160239 [00:43<15:02:22,  2.96it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 139/160239 [00:44<15:51:37,  2.80it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 140/160239 [00:44<15:38:58,  2.84it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 141/160239 [00:44<14:59:25,  2.97it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 142/160239 [00:45<15:14:30,  2.92it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 143/160239 [00:45<14:27:23,  3.08it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 144/160239 [00:45<13:59:18,  3.18it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 145/160239 [00:46<13:50:31,  3.21it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 146/160239 [00:46<13:33:07,  3.28it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 147/160239 [00:46<13:16:46,  3.35it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 148/160239 [00:46<13:05:15,  3.40it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 149/160239 [00:47<13:16:15,  3.35it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 150/160239 [00:47<13:06:32,  3.39it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 151/160239 [00:47<13:18:12,  3.34it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 152/160239 [00:48<12:58:48,  3.43it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 153/160239 [00:48<12:43:42,  3.49it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 154/160239 [00:48<12:45:30,  3.49it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 155/160239 [00:48<12:57:49,  3.43it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 156/160239 [00:49<13:08:27,  3.38it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 157/160239 [00:49<12:57:47,  3.43it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 158/160239 [00:49<12:55:19,  3.44it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 159/160239 [00:50<12:57:54,  3.43it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 160/160239 [00:50<13:02:07,  3.41it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 161/160239 [00:50<13:17:22,  3.35it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 162/160239 [00:51<13:07:44,  3.39it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 163/160239 [00:51<13:15:12,  3.36it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 164/160239 [00:51<13:07:35,  3.39it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 165/160239 [00:51<12:54:52,  3.44it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 166/160239 [00:52<13:31:50,  3.29it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 167/160239 [00:52<13:14:03,  3.36it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 168/160239 [00:52<12:58:32,  3.43it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 169/160239 [00:53<14:43:52,  3.02it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 170/160239 [00:53<14:29:11,  3.07it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 171/160239 [00:53<14:35:55,  3.05it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 172/160239 [00:54<14:18:14,  3.11it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 173/160239 [00:54<14:44:14,  3.02it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 174/160239 [00:54<14:43:39,  3.02it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 175/160239 [00:55<14:40:28,  3.03it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 176/160239 [00:55<14:29:02,  3.07it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 177/160239 [00:55<13:59:50,  3.18it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 178/160239 [00:56<13:38:03,  3.26it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 179/160239 [00:56<13:25:26,  3.31it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 180/160239 [00:56<13:33:28,  3.28it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 181/160239 [00:56<13:25:55,  3.31it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 182/160239 [00:57<13:24:42,  3.32it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 183/160239 [00:57<13:38:50,  3.26it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 184/160239 [00:57<13:42:44,  3.24it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 185/160239 [00:58<13:31:42,  3.29it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 186/160239 [00:58<13:15:29,  3.35it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 187/160239 [00:58<13:21:34,  3.33it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 188/160239 [00:59<13:27:25,  3.30it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 189/160239 [00:59<13:28:08,  3.30it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 190/160239 [00:59<13:23:08,  3.32it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 191/160239 [01:00<13:25:39,  3.31it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 192/160239 [01:00<13:25:24,  3.31it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 193/160239 [01:00<13:20:40,  3.33it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 194/160239 [01:00<13:20:39,  3.33it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 195/160239 [01:01<13:15:22,  3.35it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 196/160239 [01:01<13:24:35,  3.32it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 197/160239 [01:01<13:17:51,  3.34it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 198/160239 [01:02<13:02:40,  3.41it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 199/160239 [01:02<14:16:10,  3.12it/s, loss=12.738, nll_loss=\nepoch 001:   0%| | 200/160239 [01:02<13:42:17,  3.24it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 201/160239 [01:03<13:09:01,  3.38it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 202/160239 [01:03<12:42:54,  3.50it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 203/160239 [01:03<12:38:29,  3.52it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 204/160239 [01:03<13:02:57,  3.41it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 205/160239 [01:04<12:53:43,  3.45it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 206/160239 [01:04<12:50:25,  3.46it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 207/160239 [01:04<13:59:53,  3.18it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 208/160239 [01:05<13:34:00,  3.28it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 209/160239 [01:05<13:23:18,  3.32it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 210/160239 [01:05<13:10:39,  3.37it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 211/160239 [01:05<12:59:00,  3.42it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 212/160239 [01:06<12:48:26,  3.47it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 213/160239 [01:06<12:48:00,  3.47it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 214/160239 [01:06<12:52:23,  3.45it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 215/160239 [01:07<12:49:30,  3.47it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 216/160239 [01:07<12:46:37,  3.48it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 217/160239 [01:07<12:49:39,  3.47it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 218/160239 [01:07<12:31:43,  3.55it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 219/160239 [01:08<12:51:44,  3.46it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 220/160239 [01:08<13:39:12,  3.26it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 221/160239 [01:08<13:25:42,  3.31it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 222/160239 [01:09<13:00:41,  3.42it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 223/160239 [01:09<12:51:24,  3.46it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 224/160239 [01:09<12:57:13,  3.43it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 225/160239 [01:10<13:05:45,  3.39it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 226/160239 [01:10<13:07:24,  3.39it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 227/160239 [01:10<13:00:47,  3.42it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 228/160239 [01:10<13:30:09,  3.29it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 229/160239 [01:11<13:32:25,  3.28it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 230/160239 [01:11<13:40:31,  3.25it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 231/160239 [01:11<14:19:31,  3.10it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 232/160239 [01:12<14:46:31,  3.01it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 233/160239 [01:12<14:17:39,  3.11it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 234/160239 [01:12<13:54:31,  3.20it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 235/160239 [01:13<14:10:49,  3.13it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 236/160239 [01:13<14:37:46,  3.04it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 237/160239 [01:13<14:28:25,  3.07it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 238/160239 [01:14<14:06:35,  3.15it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 239/160239 [01:14<14:06:15,  3.15it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 240/160239 [01:14<14:11:14,  3.13it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 241/160239 [01:15<14:21:49,  3.09it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 242/160239 [01:15<14:27:38,  3.07it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 243/160239 [01:15<14:04:01,  3.16it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 244/160239 [01:16<13:48:22,  3.22it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 245/160239 [01:16<13:38:24,  3.26it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 246/160239 [01:16<13:50:30,  3.21it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 247/160239 [01:17<14:10:53,  3.13it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 248/160239 [01:17<14:00:02,  3.17it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 249/160239 [01:17<14:59:15,  2.97it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 250/160239 [01:18<15:32:10,  2.86it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 251/160239 [01:18<15:18:23,  2.90it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 252/160239 [01:18<15:25:55,  2.88it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 253/160239 [01:19<15:13:56,  2.92it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 254/160239 [01:19<14:37:59,  3.04it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 255/160239 [01:19<14:08:10,  3.14it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 256/160239 [01:20<13:42:02,  3.24it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 257/160239 [01:20<13:29:32,  3.29it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 258/160239 [01:20<13:28:19,  3.30it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 259/160239 [01:20<13:31:35,  3.29it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 260/160239 [01:21<13:38:32,  3.26it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 261/160239 [01:21<14:11:31,  3.13it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 262/160239 [01:21<13:44:42,  3.23it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 263/160239 [01:22<13:21:05,  3.33it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 264/160239 [01:22<13:11:10,  3.37it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 265/160239 [01:22<13:09:21,  3.38it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 266/160239 [01:23<12:56:00,  3.44it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 267/160239 [01:23<12:42:49,  3.50it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 268/160239 [01:23<12:40:16,  3.51it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 269/160239 [01:23<12:59:37,  3.42it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 270/160239 [01:24<12:39:46,  3.51it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 271/160239 [01:24<12:35:58,  3.53it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 272/160239 [01:24<12:33:58,  3.54it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 273/160239 [01:25<13:17:33,  3.34it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 274/160239 [01:25<13:09:17,  3.38it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 275/160239 [01:25<13:01:44,  3.41it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 276/160239 [01:25<12:46:57,  3.48it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 277/160239 [01:26<12:51:14,  3.46it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 278/160239 [01:26<13:01:47,  3.41it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 279/160239 [01:26<13:35:36,  3.27it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 280/160239 [01:27<13:21:17,  3.33it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 281/160239 [01:27<13:18:22,  3.34it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 282/160239 [01:27<14:10:58,  3.13it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 283/160239 [01:28<14:28:30,  3.07it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 284/160239 [01:28<14:16:42,  3.11it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 285/160239 [01:28<13:54:43,  3.19it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 286/160239 [01:29<14:04:22,  3.16it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 287/160239 [01:29<15:54:19,  2.79it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 288/160239 [01:29<15:51:40,  2.80it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 289/160239 [01:30<14:53:18,  2.98it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 290/160239 [01:30<14:12:50,  3.13it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 291/160239 [01:30<13:50:10,  3.21it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 292/160239 [01:31<13:43:31,  3.24it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 293/160239 [01:31<13:42:12,  3.24it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 294/160239 [01:31<13:15:54,  3.35it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 295/160239 [01:31<13:21:40,  3.33it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 296/160239 [01:32<13:30:54,  3.29it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 297/160239 [01:32<14:24:10,  3.08it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 298/160239 [01:32<14:12:09,  3.13it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 299/160239 [01:33<13:54:27,  3.19it/s, loss=11.238, nll_loss=\nepoch 001:   0%| | 300/160239 [01:33<13:43:45,  3.24it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 301/160239 [01:33<13:42:25,  3.24it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 302/160239 [01:34<13:52:53,  3.20it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 303/160239 [01:34<14:00:13,  3.17it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 304/160239 [01:34<13:47:08,  3.22it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 305/160239 [01:35<13:34:50,  3.27it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 306/160239 [01:35<13:30:27,  3.29it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 307/160239 [01:35<13:24:46,  3.31it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 308/160239 [01:35<13:28:50,  3.30it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 309/160239 [01:36<13:35:55,  3.27it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 310/160239 [01:36<13:46:30,  3.22it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 311/160239 [01:36<14:05:17,  3.15it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 312/160239 [01:37<14:27:51,  3.07it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 313/160239 [01:37<14:50:16,  2.99it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 314/160239 [01:37<14:26:52,  3.07it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 315/160239 [01:38<14:03:24,  3.16it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 316/160239 [01:38<13:48:43,  3.22it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 317/160239 [01:38<13:28:17,  3.30it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 318/160239 [01:39<13:19:01,  3.34it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 319/160239 [01:39<13:24:14,  3.31it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 320/160239 [01:39<13:25:39,  3.31it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 321/160239 [01:39<13:23:28,  3.32it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 322/160239 [01:40<13:23:15,  3.32it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 323/160239 [01:40<13:36:19,  3.26it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 324/160239 [01:40<13:37:36,  3.26it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 325/160239 [01:41<14:03:22,  3.16it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 326/160239 [01:41<13:58:17,  3.18it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 327/160239 [01:41<13:41:52,  3.24it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 328/160239 [01:42<13:32:59,  3.28it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 329/160239 [01:42<13:23:30,  3.32it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 330/160239 [01:42<13:09:47,  3.37it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 331/160239 [01:43<13:57:37,  3.18it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 332/160239 [01:43<13:46:34,  3.22it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 333/160239 [01:43<13:42:49,  3.24it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 334/160239 [01:44<13:43:34,  3.24it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 335/160239 [01:44<15:32:31,  2.86it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 336/160239 [01:44<14:46:29,  3.01it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 337/160239 [01:45<14:44:43,  3.01it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 338/160239 [01:45<14:25:13,  3.08it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 339/160239 [01:45<14:54:18,  2.98it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 340/160239 [01:46<14:15:04,  3.12it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 341/160239 [01:46<13:52:22,  3.20it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 342/160239 [01:46<13:50:33,  3.21it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 343/160239 [01:46<13:36:02,  3.27it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 344/160239 [01:47<13:28:33,  3.30it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 345/160239 [01:47<13:21:14,  3.33it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 346/160239 [01:47<13:23:21,  3.32it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 347/160239 [01:48<13:32:56,  3.28it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 348/160239 [01:48<13:20:29,  3.33it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 349/160239 [01:48<14:21:26,  3.09it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 350/160239 [01:49<13:42:14,  3.24it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 351/160239 [01:49<13:29:38,  3.29it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 352/160239 [01:49<13:18:20,  3.34it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 353/160239 [01:49<13:17:29,  3.34it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 354/160239 [01:50<13:36:07,  3.27it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 355/160239 [01:50<13:37:24,  3.26it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 356/160239 [01:50<13:18:55,  3.34it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 357/160239 [01:51<13:50:12,  3.21it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 358/160239 [01:51<13:48:04,  3.22it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 359/160239 [01:51<13:49:13,  3.21it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 360/160239 [01:52<13:42:05,  3.24it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 361/160239 [01:52<13:41:15,  3.24it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 362/160239 [01:52<13:26:24,  3.30it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 363/160239 [01:53<13:29:34,  3.29it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 364/160239 [01:53<14:21:33,  3.09it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 365/160239 [01:53<14:21:08,  3.09it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 366/160239 [01:54<14:28:46,  3.07it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 367/160239 [01:54<14:02:31,  3.16it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 368/160239 [01:54<13:41:52,  3.24it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 369/160239 [01:54<13:36:36,  3.26it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 370/160239 [01:55<13:32:07,  3.28it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 371/160239 [01:55<16:04:01,  2.76it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 372/160239 [01:56<15:17:14,  2.90it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 373/160239 [01:56<14:56:50,  2.97it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 374/160239 [01:56<15:35:01,  2.85it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 375/160239 [01:57<14:54:43,  2.98it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 376/160239 [01:57<15:42:51,  2.83it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 377/160239 [01:57<15:05:40,  2.94it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 378/160239 [01:58<15:35:32,  2.85it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 379/160239 [01:58<14:55:49,  2.97it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 380/160239 [01:58<14:28:22,  3.07it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 381/160239 [01:59<14:30:37,  3.06it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 382/160239 [01:59<14:09:17,  3.14it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 383/160239 [01:59<15:27:13,  2.87it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 384/160239 [02:00<15:14:59,  2.91it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 385/160239 [02:00<14:51:27,  2.99it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 386/160239 [02:00<15:38:05,  2.84it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 387/160239 [02:01<14:53:36,  2.98it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 388/160239 [02:01<14:45:56,  3.01it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 389/160239 [02:01<14:29:11,  3.07it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 390/160239 [02:02<14:36:55,  3.04it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 391/160239 [02:02<14:24:40,  3.08it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 392/160239 [02:02<15:35:43,  2.85it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 393/160239 [02:03<14:52:19,  2.99it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 394/160239 [02:03<14:22:54,  3.09it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 395/160239 [02:03<14:30:17,  3.06it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 396/160239 [02:04<14:03:16,  3.16it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 397/160239 [02:04<13:47:06,  3.22it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 398/160239 [02:04<13:33:33,  3.27it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 399/160239 [02:04<13:29:38,  3.29it/s, loss=10.586, nll_loss=\nepoch 001:   0%| | 400/160239 [02:05<13:19:32,  3.33it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 401/160239 [02:05<13:53:04,  3.20it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 402/160239 [02:05<13:31:50,  3.28it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 403/160239 [02:06<13:12:17,  3.36it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 404/160239 [02:06<12:58:39,  3.42it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 405/160239 [02:06<12:49:26,  3.46it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 406/160239 [02:06<12:48:23,  3.47it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 407/160239 [02:07<12:39:39,  3.51it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 408/160239 [02:07<12:58:36,  3.42it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 409/160239 [02:07<12:55:56,  3.43it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 410/160239 [02:08<12:44:07,  3.49it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 411/160239 [02:08<12:40:02,  3.50it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 412/160239 [02:08<12:35:49,  3.52it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 413/160239 [02:08<12:31:25,  3.54it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 414/160239 [02:09<12:35:35,  3.53it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 415/160239 [02:09<12:47:51,  3.47it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 416/160239 [02:09<12:49:21,  3.46it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 417/160239 [02:10<13:12:21,  3.36it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 418/160239 [02:10<13:47:37,  3.22it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 419/160239 [02:10<14:13:04,  3.12it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 420/160239 [02:11<13:31:39,  3.28it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 421/160239 [02:11<15:13:21,  2.92it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 422/160239 [02:11<14:33:59,  3.05it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 423/160239 [02:12<14:10:09,  3.13it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 424/160239 [02:12<13:42:30,  3.24it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 425/160239 [02:12<13:23:43,  3.31it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 426/160239 [02:12<13:09:46,  3.37it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 427/160239 [02:13<12:53:44,  3.44it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 428/160239 [02:13<14:38:44,  3.03it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 429/160239 [02:13<14:17:57,  3.10it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 430/160239 [02:14<13:36:25,  3.26it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 431/160239 [02:14<13:18:04,  3.34it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 432/160239 [02:14<13:07:09,  3.38it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 433/160239 [02:15<13:02:15,  3.40it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 434/160239 [02:15<12:53:31,  3.44it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 435/160239 [02:15<12:55:24,  3.43it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 436/160239 [02:15<12:50:05,  3.46it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 437/160239 [02:16<12:50:15,  3.46it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 438/160239 [02:16<12:53:08,  3.44it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 439/160239 [02:16<12:57:13,  3.43it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 440/160239 [02:17<13:05:21,  3.39it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 441/160239 [02:17<13:08:14,  3.38it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 442/160239 [02:17<13:03:19,  3.40it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 443/160239 [02:18<13:27:15,  3.30it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 444/160239 [02:18<13:33:01,  3.28it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 445/160239 [02:18<13:43:11,  3.24it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 446/160239 [02:19<14:15:54,  3.11it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 447/160239 [02:19<13:47:30,  3.22it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 448/160239 [02:19<13:41:16,  3.24it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 449/160239 [02:19<13:29:38,  3.29it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 450/160239 [02:20<13:51:10,  3.20it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 451/160239 [02:20<13:17:34,  3.34it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 452/160239 [02:20<13:13:57,  3.35it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 453/160239 [02:21<13:37:12,  3.26it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 454/160239 [02:21<13:18:07,  3.34it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 455/160239 [02:21<13:17:22,  3.34it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 456/160239 [02:22<13:20:10,  3.33it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 457/160239 [02:22<13:08:12,  3.38it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 458/160239 [02:22<13:42:03,  3.24it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 459/160239 [02:22<13:24:36,  3.31it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 460/160239 [02:23<13:06:31,  3.39it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 461/160239 [02:23<12:52:21,  3.45it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 462/160239 [02:23<12:52:15,  3.45it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 463/160239 [02:24<12:54:57,  3.44it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 464/160239 [02:24<13:49:21,  3.21it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 465/160239 [02:24<13:38:14,  3.25it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 466/160239 [02:25<13:37:17,  3.26it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 467/160239 [02:25<13:26:22,  3.30it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 468/160239 [02:25<13:27:12,  3.30it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 469/160239 [02:26<16:14:17,  2.73it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 470/160239 [02:26<15:32:09,  2.86it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 471/160239 [02:26<15:00:37,  2.96it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 472/160239 [02:27<14:39:57,  3.03it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 473/160239 [02:27<14:28:02,  3.07it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 474/160239 [02:27<14:07:06,  3.14it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 475/160239 [02:28<13:57:54,  3.18it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 476/160239 [02:28<13:50:08,  3.21it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 477/160239 [02:28<13:41:34,  3.24it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 478/160239 [02:28<13:41:01,  3.24it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 479/160239 [02:29<13:35:15,  3.27it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 480/160239 [02:29<13:45:13,  3.23it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 481/160239 [02:29<14:37:17,  3.04it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 482/160239 [02:30<14:21:15,  3.09it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 483/160239 [02:30<13:55:36,  3.19it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 484/160239 [02:30<13:50:49,  3.20it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 485/160239 [02:31<13:34:18,  3.27it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 486/160239 [02:31<13:27:20,  3.30it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 487/160239 [02:31<13:14:53,  3.35it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 488/160239 [02:32<13:08:50,  3.38it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 489/160239 [02:32<12:58:03,  3.42it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 490/160239 [02:32<13:02:59,  3.40it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 491/160239 [02:32<13:00:47,  3.41it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 492/160239 [02:33<12:59:48,  3.41it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 493/160239 [02:33<13:06:05,  3.39it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 494/160239 [02:33<13:30:29,  3.28it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 495/160239 [02:34<13:28:03,  3.29it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 496/160239 [02:34<13:21:00,  3.32it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 497/160239 [02:34<13:47:12,  3.22it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 498/160239 [02:35<14:06:40,  3.14it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 499/160239 [02:35<14:02:57,  3.16it/s, loss=10.378, nll_loss=\nepoch 001:   0%| | 500/160239 [02:35<15:02:42,  2.95it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 501/160239 [02:36<14:54:05,  2.98it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 502/160239 [02:36<14:27:12,  3.07it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 503/160239 [02:36<13:56:39,  3.18it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 504/160239 [02:36<13:39:20,  3.25it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 505/160239 [02:37<13:54:27,  3.19it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 506/160239 [02:37<14:17:26,  3.10it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 507/160239 [02:37<14:25:39,  3.08it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 508/160239 [02:38<14:06:33,  3.14it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 509/160239 [02:38<13:42:00,  3.24it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 510/160239 [02:38<13:26:01,  3.30it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 511/160239 [02:39<13:37:54,  3.25it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 512/160239 [02:39<13:45:34,  3.22it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 513/160239 [02:39<13:34:15,  3.27it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 514/160239 [02:40<13:47:20,  3.22it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 515/160239 [02:40<13:38:19,  3.25it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 516/160239 [02:40<13:31:31,  3.28it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 517/160239 [02:41<13:28:20,  3.29it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 518/160239 [02:41<13:14:33,  3.35it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 519/160239 [02:41<13:27:46,  3.30it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 520/160239 [02:41<13:22:08,  3.32it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 521/160239 [02:42<13:14:52,  3.35it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 522/160239 [02:42<13:13:11,  3.36it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 523/160239 [02:42<14:26:10,  3.07it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 524/160239 [02:43<14:13:53,  3.12it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 525/160239 [02:43<14:17:37,  3.10it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 526/160239 [02:43<14:07:12,  3.14it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 527/160239 [02:44<14:15:11,  3.11it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 528/160239 [02:44<14:14:41,  3.11it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 529/160239 [02:44<14:01:22,  3.16it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 530/160239 [02:45<13:52:14,  3.20it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 531/160239 [02:45<13:41:50,  3.24it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 532/160239 [02:45<13:23:46,  3.31it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 533/160239 [02:46<15:20:03,  2.89it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 534/160239 [02:46<14:32:34,  3.05it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 535/160239 [02:46<13:45:25,  3.22it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 536/160239 [02:46<13:40:02,  3.25it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 537/160239 [02:47<13:12:45,  3.36it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 538/160239 [02:47<12:57:14,  3.42it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 539/160239 [02:47<12:47:14,  3.47it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 540/160239 [02:48<12:54:05,  3.44it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 541/160239 [02:48<12:42:02,  3.49it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 542/160239 [02:48<12:28:54,  3.55it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 543/160239 [02:48<12:30:17,  3.55it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 544/160239 [02:49<12:35:43,  3.52it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 545/160239 [02:49<12:36:03,  3.52it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 546/160239 [02:49<12:34:31,  3.53it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 547/160239 [02:50<13:25:46,  3.30it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 548/160239 [02:50<13:07:32,  3.38it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 549/160239 [02:50<13:36:11,  3.26it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 550/160239 [02:51<14:22:19,  3.09it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 551/160239 [02:51<14:43:17,  3.01it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 552/160239 [02:51<14:53:45,  2.98it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 553/160239 [02:52<14:12:20,  3.12it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 554/160239 [02:52<13:49:40,  3.21it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 555/160239 [02:52<13:32:43,  3.27it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 556/160239 [02:52<13:17:07,  3.34it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 557/160239 [02:53<13:09:05,  3.37it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 558/160239 [02:53<12:58:01,  3.42it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 559/160239 [02:53<12:55:46,  3.43it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 560/160239 [02:54<12:51:48,  3.45it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 561/160239 [02:54<12:57:57,  3.42it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 562/160239 [02:54<12:49:54,  3.46it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 563/160239 [02:54<12:40:23,  3.50it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 564/160239 [02:55<12:28:09,  3.56it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 565/160239 [02:55<12:29:14,  3.55it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 566/160239 [02:55<12:50:36,  3.45it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 567/160239 [02:56<13:49:28,  3.21it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 568/160239 [02:56<13:46:30,  3.22it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 569/160239 [02:56<13:41:06,  3.24it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 570/160239 [02:57<13:37:34,  3.25it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 571/160239 [02:57<13:45:43,  3.22it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 572/160239 [02:57<13:29:46,  3.29it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 573/160239 [02:58<13:42:11,  3.24it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 574/160239 [02:58<13:25:07,  3.31it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 575/160239 [02:58<13:47:23,  3.22it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 576/160239 [02:58<13:42:38,  3.23it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 577/160239 [02:59<13:39:31,  3.25it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 578/160239 [02:59<13:32:52,  3.27it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 579/160239 [02:59<13:22:33,  3.32it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 580/160239 [03:00<13:19:15,  3.33it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 581/160239 [03:00<13:01:35,  3.40it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 582/160239 [03:00<13:07:48,  3.38it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 583/160239 [03:01<13:09:29,  3.37it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 584/160239 [03:01<13:03:22,  3.40it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 585/160239 [03:01<13:01:09,  3.41it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 586/160239 [03:02<14:44:35,  3.01it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 587/160239 [03:02<14:38:12,  3.03it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 588/160239 [03:02<14:02:06,  3.16it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 589/160239 [03:02<13:55:58,  3.18it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 590/160239 [03:03<13:53:09,  3.19it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 591/160239 [03:03<13:40:56,  3.24it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 592/160239 [03:03<13:32:42,  3.27it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 593/160239 [03:04<13:24:29,  3.31it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 594/160239 [03:04<13:39:46,  3.25it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 595/160239 [03:04<13:20:41,  3.32it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 596/160239 [03:05<13:18:48,  3.33it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 597/160239 [03:05<13:25:54,  3.30it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 598/160239 [03:05<14:20:50,  3.09it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 599/160239 [03:06<14:03:45,  3.15it/s, loss=10.176, nll_loss=\nepoch 001:   0%| | 600/160239 [03:06<13:47:21,  3.22it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 601/160239 [03:06<13:42:57,  3.23it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 602/160239 [03:06<13:18:31,  3.33it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 603/160239 [03:07<13:08:51,  3.37it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 604/160239 [03:07<13:55:09,  3.19it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 605/160239 [03:07<14:04:03,  3.15it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 606/160239 [03:08<13:48:59,  3.21it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 607/160239 [03:08<13:44:21,  3.23it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 608/160239 [03:08<13:39:39,  3.25it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 609/160239 [03:09<13:11:24,  3.36it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 610/160239 [03:09<13:09:41,  3.37it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 611/160239 [03:09<12:57:13,  3.42it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 612/160239 [03:09<12:49:58,  3.46it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 613/160239 [03:10<12:48:18,  3.46it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 614/160239 [03:10<12:43:56,  3.48it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 615/160239 [03:10<12:41:36,  3.49it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 616/160239 [03:11<12:45:26,  3.48it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 617/160239 [03:11<12:54:11,  3.44it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 618/160239 [03:11<12:45:33,  3.48it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 619/160239 [03:11<12:45:18,  3.48it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 620/160239 [03:12<12:51:25,  3.45it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 621/160239 [03:12<13:06:53,  3.38it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 622/160239 [03:12<13:48:56,  3.21it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 623/160239 [03:13<13:30:48,  3.28it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 624/160239 [03:13<13:11:00,  3.36it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 625/160239 [03:13<13:09:18,  3.37it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 626/160239 [03:14<14:25:37,  3.07it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 627/160239 [03:14<13:56:22,  3.18it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 628/160239 [03:14<13:32:51,  3.27it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 629/160239 [03:15<13:24:12,  3.31it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 630/160239 [03:15<13:24:29,  3.31it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 631/160239 [03:15<13:29:00,  3.29it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 632/160239 [03:15<13:25:57,  3.30it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 633/160239 [03:16<13:38:37,  3.25it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 634/160239 [03:16<13:51:50,  3.20it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 635/160239 [03:17<16:08:43,  2.75it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 636/160239 [03:17<15:10:43,  2.92it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 637/160239 [03:17<14:29:50,  3.06it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 638/160239 [03:17<14:44:51,  3.01it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 639/160239 [03:18<14:12:00,  3.12it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 640/160239 [03:18<13:58:40,  3.17it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 641/160239 [03:18<13:53:23,  3.19it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 642/160239 [03:19<13:58:31,  3.17it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 643/160239 [03:19<14:50:59,  2.99it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 644/160239 [03:19<14:47:55,  3.00it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 645/160239 [03:20<15:20:16,  2.89it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 646/160239 [03:20<14:55:54,  2.97it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 647/160239 [03:20<14:14:47,  3.11it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 648/160239 [03:21<13:46:10,  3.22it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 649/160239 [03:21<13:31:09,  3.28it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 650/160239 [03:21<13:04:08,  3.39it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 651/160239 [03:22<12:56:49,  3.42it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 652/160239 [03:22<12:38:53,  3.50it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 653/160239 [03:22<12:47:12,  3.47it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 654/160239 [03:22<12:41:31,  3.49it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 655/160239 [03:23<12:31:44,  3.54it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 656/160239 [03:23<13:05:18,  3.39it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 657/160239 [03:23<12:55:48,  3.43it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 658/160239 [03:24<12:44:09,  3.48it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 659/160239 [03:24<12:38:11,  3.51it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 660/160239 [03:24<12:52:23,  3.44it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 661/160239 [03:24<13:36:45,  3.26it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 662/160239 [03:25<13:41:17,  3.24it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 663/160239 [03:25<13:45:09,  3.22it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 664/160239 [03:25<13:37:43,  3.25it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 665/160239 [03:26<13:41:43,  3.24it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 666/160239 [03:26<13:31:58,  3.28it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 667/160239 [03:26<14:01:28,  3.16it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 668/160239 [03:27<13:52:19,  3.20it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 669/160239 [03:27<13:36:12,  3.26it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 670/160239 [03:27<13:18:35,  3.33it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 671/160239 [03:28<13:12:21,  3.36it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 672/160239 [03:28<13:10:32,  3.36it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 673/160239 [03:28<13:01:05,  3.40it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 674/160239 [03:28<13:46:19,  3.22it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 675/160239 [03:29<13:39:15,  3.25it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 676/160239 [03:29<13:42:02,  3.24it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 677/160239 [03:29<13:51:43,  3.20it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 678/160239 [03:30<13:40:09,  3.24it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 679/160239 [03:30<13:34:14,  3.27it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 680/160239 [03:30<13:35:39,  3.26it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 681/160239 [03:31<13:23:41,  3.31it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 682/160239 [03:31<14:11:19,  3.12it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 683/160239 [03:31<13:42:31,  3.23it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 684/160239 [03:32<13:20:28,  3.32it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 685/160239 [03:32<13:15:27,  3.34it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 686/160239 [03:32<13:05:12,  3.39it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 687/160239 [03:32<13:06:14,  3.38it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 688/160239 [03:33<13:11:51,  3.36it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 689/160239 [03:33<13:14:50,  3.35it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 690/160239 [03:33<13:41:47,  3.24it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 691/160239 [03:34<13:41:55,  3.24it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 692/160239 [03:34<13:36:43,  3.26it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 693/160239 [03:34<13:19:35,  3.33it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 694/160239 [03:35<13:10:52,  3.36it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 695/160239 [03:35<13:09:05,  3.37it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 696/160239 [03:35<13:11:27,  3.36it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 697/160239 [03:35<12:57:23,  3.42it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 698/160239 [03:36<12:49:18,  3.46it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 699/160239 [03:36<12:45:37,  3.47it/s, loss=10.063, nll_loss=\nepoch 001:   0%| | 700/160239 [03:36<13:51:22,  3.20it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 701/160239 [03:37<13:44:21,  3.23it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 702/160239 [03:37<13:33:30,  3.27it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 703/160239 [03:37<13:17:46,  3.33it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 704/160239 [03:38<13:20:32,  3.32it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 705/160239 [03:38<13:51:28,  3.20it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 706/160239 [03:38<13:39:22,  3.25it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 707/160239 [03:38<13:22:33,  3.31it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 708/160239 [03:39<13:13:16,  3.35it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 709/160239 [03:39<13:11:36,  3.36it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 710/160239 [03:39<13:00:34,  3.41it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 711/160239 [03:40<12:58:29,  3.42it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 712/160239 [03:40<13:21:53,  3.32it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 713/160239 [03:40<13:13:36,  3.35it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 714/160239 [03:41<13:20:08,  3.32it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 715/160239 [03:41<13:15:03,  3.34it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 716/160239 [03:41<13:17:11,  3.34it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 717/160239 [03:41<13:25:51,  3.30it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 718/160239 [03:42<13:15:48,  3.34it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 719/160239 [03:42<13:24:30,  3.30it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 720/160239 [03:42<13:15:15,  3.34it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 721/160239 [03:43<13:13:18,  3.35it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 722/160239 [03:43<13:18:33,  3.33it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 723/160239 [03:43<13:22:19,  3.31it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 724/160239 [03:44<13:20:24,  3.32it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 725/160239 [03:44<13:06:27,  3.38it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 726/160239 [03:44<12:54:19,  3.43it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 727/160239 [03:44<13:05:01,  3.39it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 728/160239 [03:45<13:42:28,  3.23it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 729/160239 [03:45<13:57:52,  3.17it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 730/160239 [03:45<13:46:26,  3.22it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 731/160239 [03:46<13:38:22,  3.25it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 732/160239 [03:46<13:44:16,  3.23it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 733/160239 [03:46<13:46:22,  3.22it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 734/160239 [03:47<13:58:56,  3.17it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 735/160239 [03:47<14:00:16,  3.16it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 736/160239 [03:47<13:52:52,  3.19it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 737/160239 [03:48<14:48:50,  2.99it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 738/160239 [03:48<14:27:14,  3.07it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 739/160239 [03:48<14:55:52,  2.97it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 740/160239 [03:49<14:31:16,  3.05it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 741/160239 [03:49<14:12:33,  3.12it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 742/160239 [03:49<14:04:37,  3.15it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 743/160239 [03:50<13:55:16,  3.18it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 744/160239 [03:50<14:05:29,  3.14it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 745/160239 [03:50<14:04:24,  3.15it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 746/160239 [03:50<14:05:33,  3.14it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 747/160239 [03:51<13:39:55,  3.24it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 748/160239 [03:51<13:39:41,  3.24it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 749/160239 [03:51<13:32:22,  3.27it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 750/160239 [03:52<15:06:29,  2.93it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 751/160239 [03:52<17:02:24,  2.60it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 752/160239 [03:53<16:10:15,  2.74it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 753/160239 [03:53<15:25:25,  2.87it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 754/160239 [03:53<15:35:02,  2.84it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 755/160239 [03:54<15:05:24,  2.94it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 756/160239 [03:54<15:46:48,  2.81it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 757/160239 [03:54<15:00:09,  2.95it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 758/160239 [03:55<14:20:51,  3.09it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 759/160239 [03:55<14:01:42,  3.16it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 760/160239 [03:55<13:46:58,  3.21it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 761/160239 [03:56<14:24:15,  3.08it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 762/160239 [03:56<13:58:37,  3.17it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 763/160239 [03:56<13:34:40,  3.26it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 764/160239 [03:56<13:20:12,  3.32it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 765/160239 [03:57<12:57:39,  3.42it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 766/160239 [03:57<12:54:39,  3.43it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 767/160239 [03:57<12:48:49,  3.46it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 768/160239 [03:58<12:48:33,  3.46it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 769/160239 [03:58<13:37:54,  3.25it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 770/160239 [03:58<13:17:32,  3.33it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 771/160239 [03:58<13:11:31,  3.36it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 772/160239 [03:59<13:12:22,  3.35it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 773/160239 [03:59<12:54:46,  3.43it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 774/160239 [03:59<13:11:30,  3.36it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 775/160239 [04:00<13:17:08,  3.33it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 776/160239 [04:00<13:21:42,  3.32it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 777/160239 [04:00<13:30:37,  3.28it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 778/160239 [04:01<13:53:37,  3.19it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 779/160239 [04:01<13:53:42,  3.19it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 780/160239 [04:01<13:34:54,  3.26it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 781/160239 [04:02<13:44:58,  3.22it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 782/160239 [04:02<14:40:44,  3.02it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 783/160239 [04:02<14:10:33,  3.12it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 784/160239 [04:03<16:03:50,  2.76it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 785/160239 [04:03<15:40:29,  2.83it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 786/160239 [04:03<14:59:54,  2.95it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 787/160239 [04:04<14:54:23,  2.97it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 788/160239 [04:04<14:26:09,  3.07it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 789/160239 [04:04<13:44:12,  3.22it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 790/160239 [04:05<13:46:54,  3.21it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 791/160239 [04:05<13:44:57,  3.22it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 792/160239 [04:05<13:46:31,  3.22it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 793/160239 [04:05<14:19:15,  3.09it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 794/160239 [04:06<14:15:08,  3.11it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 795/160239 [04:06<13:57:03,  3.17it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 796/160239 [04:06<13:49:14,  3.20it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 797/160239 [04:07<13:57:39,  3.17it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 798/160239 [04:07<13:54:19,  3.19it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 799/160239 [04:07<13:57:47,  3.17it/s, loss=10.231, nll_loss=\nepoch 001:   0%| | 800/160239 [04:08<14:00:50,  3.16it/s, loss=10.09, nll_loss=9\nepoch 001:   0%| | 801/160239 [04:08<13:56:10,  3.18it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 802/160239 [04:08<13:49:36,  3.20it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 803/160239 [04:09<14:46:17,  3.00it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 804/160239 [04:09<14:17:17,  3.10it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 805/160239 [04:09<14:04:03,  3.15it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 806/160239 [04:10<13:53:39,  3.19it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 807/160239 [04:10<13:35:28,  3.26it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 808/160239 [04:10<13:42:00,  3.23it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 809/160239 [04:11<13:48:03,  3.21it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 810/160239 [04:11<13:48:06,  3.21it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 811/160239 [04:11<14:10:12,  3.13it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 812/160239 [04:12<14:20:15,  3.09it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 813/160239 [04:12<14:22:50,  3.08it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 814/160239 [04:12<14:03:34,  3.15it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 815/160239 [04:12<14:10:19,  3.12it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 816/160239 [04:13<14:09:11,  3.13it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 817/160239 [04:13<14:07:12,  3.14it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 818/160239 [04:13<13:39:56,  3.24it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 819/160239 [04:14<13:23:52,  3.31it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 820/160239 [04:14<13:10:50,  3.36it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 821/160239 [04:14<13:09:21,  3.37it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 822/160239 [04:15<13:07:00,  3.38it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 823/160239 [04:15<13:29:36,  3.28it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 824/160239 [04:15<14:32:09,  3.05it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 825/160239 [04:16<14:05:29,  3.14it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 826/160239 [04:16<13:52:12,  3.19it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 827/160239 [04:16<13:32:29,  3.27it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 828/160239 [04:16<13:22:09,  3.31it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 829/160239 [04:17<13:35:56,  3.26it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 830/160239 [04:17<13:49:06,  3.20it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 831/160239 [04:17<13:36:55,  3.25it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 832/160239 [04:18<13:28:07,  3.29it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 833/160239 [04:18<13:22:19,  3.31it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 834/160239 [04:18<13:57:59,  3.17it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 835/160239 [04:19<13:43:27,  3.23it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 836/160239 [04:19<13:44:23,  3.22it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 837/160239 [04:19<13:58:20,  3.17it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 838/160239 [04:20<14:50:34,  2.98it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 839/160239 [04:20<14:20:39,  3.09it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 840/160239 [04:20<14:13:37,  3.11it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 841/160239 [04:21<14:06:21,  3.14it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 842/160239 [04:21<14:02:04,  3.15it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 843/160239 [04:21<13:46:45,  3.21it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 844/160239 [04:21<13:59:29,  3.16it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 845/160239 [04:22<14:00:54,  3.16it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 846/160239 [04:22<14:16:46,  3.10it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 847/160239 [04:22<14:10:06,  3.12it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 848/160239 [04:23<14:10:07,  3.12it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 849/160239 [04:23<13:58:29,  3.17it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 850/160239 [04:23<13:54:58,  3.18it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 851/160239 [04:24<13:39:48,  3.24it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 852/160239 [04:24<13:56:55,  3.17it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 853/160239 [04:24<13:44:58,  3.22it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 854/160239 [04:25<13:49:02,  3.20it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 855/160239 [04:25<14:01:22,  3.16it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 856/160239 [04:25<13:57:10,  3.17it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 857/160239 [04:26<14:14:06,  3.11it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 858/160239 [04:26<14:10:04,  3.12it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 859/160239 [04:26<14:05:07,  3.14it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 860/160239 [04:27<14:07:02,  3.14it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 861/160239 [04:27<14:04:21,  3.15it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 862/160239 [04:27<14:22:57,  3.08it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 863/160239 [04:28<14:31:27,  3.05it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 864/160239 [04:28<14:15:27,  3.11it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 865/160239 [04:28<14:07:45,  3.13it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 866/160239 [04:28<14:08:07,  3.13it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 867/160239 [04:29<14:03:02,  3.15it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 868/160239 [04:29<13:55:20,  3.18it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 869/160239 [04:29<13:52:59,  3.19it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 870/160239 [04:30<13:57:23,  3.17it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 871/160239 [04:30<13:58:01,  3.17it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 872/160239 [04:30<14:03:46,  3.15it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 873/160239 [04:31<14:05:05,  3.14it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 874/160239 [04:31<14:01:50,  3.16it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 875/160239 [04:31<14:09:57,  3.12it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 876/160239 [04:32<14:17:05,  3.10it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 877/160239 [04:32<14:19:57,  3.09it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 878/160239 [04:32<14:45:09,  3.00it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 879/160239 [04:33<14:24:20,  3.07it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 880/160239 [04:33<14:15:28,  3.10it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 881/160239 [04:33<15:01:03,  2.95it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 882/160239 [04:34<14:41:52,  3.01it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 883/160239 [04:34<14:54:46,  2.97it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 884/160239 [04:34<14:36:37,  3.03it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 885/160239 [04:35<14:35:52,  3.03it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 886/160239 [04:35<14:15:29,  3.10it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 887/160239 [04:35<14:16:37,  3.10it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 888/160239 [04:36<13:52:04,  3.19it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 889/160239 [04:36<13:40:51,  3.24it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 890/160239 [04:36<13:38:44,  3.24it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 891/160239 [04:37<13:40:26,  3.24it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 892/160239 [04:37<13:33:19,  3.27it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 893/160239 [04:37<13:45:21,  3.22it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 894/160239 [04:37<14:00:18,  3.16it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 895/160239 [04:38<13:56:37,  3.17it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 896/160239 [04:38<13:51:29,  3.19it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 897/160239 [04:38<13:38:26,  3.24it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 898/160239 [04:39<13:44:27,  3.22it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 899/160239 [04:39<13:40:02,  3.24it/s, loss=10.09, nll_loss=9\nepoch 001:   1%| | 900/160239 [04:39<13:36:01,  3.25it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 901/160239 [04:40<14:07:33,  3.13it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 902/160239 [04:40<13:56:31,  3.17it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 903/160239 [04:40<13:44:58,  3.22it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 904/160239 [04:41<13:35:55,  3.25it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 905/160239 [04:41<13:34:18,  3.26it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 906/160239 [04:41<13:30:15,  3.28it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 907/160239 [04:41<13:43:44,  3.22it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 908/160239 [04:42<13:43:55,  3.22it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 909/160239 [04:42<14:04:10,  3.15it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 910/160239 [04:42<14:01:36,  3.16it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 911/160239 [04:43<14:04:46,  3.14it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 912/160239 [04:43<14:21:02,  3.08it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 913/160239 [04:43<14:26:38,  3.06it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 914/160239 [04:44<15:05:52,  2.93it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 915/160239 [04:44<14:52:48,  2.97it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 916/160239 [04:44<14:24:15,  3.07it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 917/160239 [04:45<14:15:09,  3.11it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 918/160239 [04:45<16:12:36,  2.73it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 919/160239 [04:46<15:28:50,  2.86it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 920/160239 [04:46<14:50:44,  2.98it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 921/160239 [04:46<14:33:13,  3.04it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 922/160239 [04:47<15:02:36,  2.94it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 923/160239 [04:47<14:44:57,  3.00it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 924/160239 [04:47<14:28:18,  3.06it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 925/160239 [04:47<14:21:37,  3.08it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 926/160239 [04:48<14:20:24,  3.09it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 927/160239 [04:48<14:05:23,  3.14it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 928/160239 [04:48<14:05:35,  3.14it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 929/160239 [04:49<14:43:06,  3.01it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 930/160239 [04:49<14:13:08,  3.11it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 931/160239 [04:49<14:02:31,  3.15it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 932/160239 [04:50<13:48:36,  3.20it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 933/160239 [04:50<13:57:10,  3.17it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 934/160239 [04:50<14:02:01,  3.15it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 935/160239 [04:51<15:01:56,  2.94it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 936/160239 [04:51<15:11:27,  2.91it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 937/160239 [04:51<14:52:52,  2.97it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 938/160239 [04:52<14:44:57,  3.00it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 939/160239 [04:52<14:45:07,  3.00it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 940/160239 [04:52<14:35:46,  3.03it/s, loss=9.924, nll_loss=9\nepoch 001:   1%| | 941/160239 [04:53<14:50:16,  2.98it/s, loss=9.924, nll_loss=9\n\nYour command has already run for 300 seconds. It is still running. You can continue to wait or interrupt it with Thought: ... \nAction: interrupt\n```\n``` \nor:\nThought: ... \nAction: continue_wait\n```\n```"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!bash Align-to-Distill/train_teacher.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb555bd",
   "metadata": {},
   "source": [
    "### Step 31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71760a80",
   "metadata": {},
   "source": [
    "Execution Start time: 19:52:55, End time: 19:52:56 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f814f2",
   "metadata": {},
   "source": [
    "**Thought**: The training process is taking longer than expected due to the large dataset size, even though we are only using the first 10 rows. This might be because the script is not correctly configured to use the reduced dataset. I will interrupt the training process to investigate further and ensure that the training script is using the correct subset of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4379696",
   "metadata": {},
   "source": [
    "**Action (interrupt)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f769b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Kernel interrupted."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52f536ee",
   "metadata": {},
   "source": [
    "### Step 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7d4908",
   "metadata": {},
   "source": [
    "Execution Start time: 19:53:00, End time: 19:53:00 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a89b1f",
   "metadata": {},
   "source": [
    "**Thought**: The training process was interrupted because it was taking longer than expected, likely due to the script not using the reduced dataset as intended. I need to verify that the training script is correctly configured to use only the first 10 rows of each dataset. I will check the `train_teacher.sh` script again to ensure that the correct subset of the data is being used for training. Additionally, I will verify that the preprocessed data files with only 10 rows each are correctly created and used in the training process.\n",
    "\n",
    "I will first check the contents of the preprocessed data files to ensure they contain only the first 10 rows, and then verify the script configuration to ensure it uses these files for training. If necessary, I will adjust the script to ensure it uses the correct subset of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4717c4",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92369d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "und was menschliche gesundheit ist , kann auch ziemlich kompliziert sein .\nund diese zwei zusammen zu bringen , erscheint vielleicht wie eine gewal@@ tige aufgabe . aber was ich ihnen zu sagen versuche ist , dass es trotz dieser komplexität einige einfache themen gibt , von denen ich denke , wenn wir diese verstehen , können wir uns wirklich weiter entwickeln .\nund diese einfachen themen sind eigentlich keine komplex@@ en wissenschaftlichen zusammen@@ hän@@ ge , sondern tat@@ sachen , die wir alle gut kennen .\nund ich werde mit dieser hier anfangen : wenn die mama nicht glücklich ist , ist keiner glücklich .\nkennen wir das nicht alle ? das haben wir alle schon erlebt .\nwenn wir das nehmen und darauf aufbauen , dann können wir einen schritt weiter gehen : wenn das meer nicht glücklich ist , ist keiner glücklich .\ndarum geht es in meinem vortrag .\nwir machen das meer auf viele verschiedene arten ziemlich un@@ glücklich .\ndas ist ein bild der can@@ ner@@ y row von 19@@ 32 .\ndamals war die can@@ ner@@ y row die größte industri@@ elle konser@@ ven@@ fabrik an der west@@ küste .\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!head -n 10 Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/train_10.de"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fd4c74",
   "metadata": {},
   "source": [
    "### Step 33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711a56e6",
   "metadata": {},
   "source": [
    "Execution Start time: 19:53:03, End time: 19:53:03 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2221e870",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba1d21e",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5accafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4ad2340",
   "metadata": {},
   "source": [
    "### Step 34"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746871d9",
   "metadata": {},
   "source": [
    "Execution Start time: 19:53:06, End time: 19:53:06 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c6f8f4",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae05b1d",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a731d91e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "153cadd4",
   "metadata": {},
   "source": [
    "### Step 35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b161f212",
   "metadata": {},
   "source": [
    "Execution Start time: 19:53:10, End time: 19:53:10 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf1cc91",
   "metadata": {},
   "source": [
    "**Thought**: The `train_10.de` file contains the first 10 rows as expected. I need to ensure that the training script is using these files for training. I will verify the script configuration to ensure it uses the `train_10.de` and `train_10.en` files for training. If necessary, I will adjust the script to ensure it uses the correct subset of the data. Additionally, I will check the other preprocessed files to ensure they are also correctly limited to 10 rows.\n",
    "\n",
    "I will proceed to check the contents of the `train_10.en` file to confirm it contains only the first 10 rows. Then, I will verify the script configuration to ensure it uses these files for training. If necessary, I will adjust the script to ensure it uses the correct subset of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7c8942",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f81e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "and it can be a very complicated thing , what human health is .\nand bringing those two together might seem a very da@@ un@@ ting task , but what i &apos;m going to try to say is that even in that complexity , there &apos;s some simple the@@ mes that i think , if we understand , we can really move forward .\nand those simple the@@ mes aren &apos;t really the@@ mes about the complex science of what &apos;s going on , but things that we all pretty well know .\nand i &apos;m going to start with this one : if mom@@ ma a@@ in &apos;t happy , a@@ in &apos;t nobody happy .\nwe know that , right ? we &apos;ve experienced that .\nand if we just take that and we build from there , then we can go to the next step , which is that if the ocean a@@ in &apos;t happy , a@@ in &apos;t nobody happy .\nthat &apos;s the theme of my talk .\nand we &apos;re making the ocean pretty un@@ happy in a lot of different ways .\nthis is a shot of can@@ ner@@ y row in 19@@ 32 .\ncan@@ ner@@ y row , at the time , had the biggest industrial can@@ ning operation on the west coast .\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!head -n 10 Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/train_10.en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3918b51e",
   "metadata": {},
   "source": [
    "### Step 36"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c31afb",
   "metadata": {},
   "source": [
    "Execution Start time: 19:53:13, End time: 19:53:13 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a9db7a",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76807f82",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb5a01a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4caa8422",
   "metadata": {},
   "source": [
    "### Step 37"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56ab84f",
   "metadata": {},
   "source": [
    "Execution Start time: 19:53:17, End time: 19:53:17 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d2ba54",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f98fd38",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8994b8a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff4a4c93",
   "metadata": {},
   "source": [
    "### Step 38"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a6d38e",
   "metadata": {},
   "source": [
    "Execution Start time: 19:53:21, End time: 19:53:21 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7cb6a4",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96deac98",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9914f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "554bc335",
   "metadata": {},
   "source": [
    "### Step 39"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcf6012",
   "metadata": {},
   "source": [
    "Execution Start time: 19:53:24, End time: 19:53:24 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef35c72",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4041343e",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35df68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f38d2a5",
   "metadata": {},
   "source": [
    "### Step 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a00c152",
   "metadata": {},
   "source": [
    "Execution Start time: 19:53:32, End time: 19:53:32 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e1ec6a",
   "metadata": {},
   "source": [
    "**Thought**: The `train_10.en` file also contains the first 10 rows as expected. This confirms that the data files are correctly limited to 10 rows. I need to ensure that the training script is using these files for training. I will verify the script configuration to ensure it uses the `train_10.de` and `train_10.en` files for training. If necessary, I will adjust the script to ensure it uses the correct subset of the data.\n",
    "\n",
    "I will proceed to check the contents of the `valid_10.de` and `valid_10.en` files to confirm they contain only the first 10 rows. Then, I will verify the script configuration to ensure it uses these files for validation. If necessary, I will adjust the script to ensure it uses the correct subset of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f3e120",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594f6452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "es ist diese pyrami@@ de .\ndurch die mutter@@ mil@@ ch .\nes enth@@ i@@ elt das zwei@@ - , drei@@ - , bis 40@@ 0-@@ fa@@ che des gren@@ z@@ wer@@ ts an sch@@ ad@@ stoffen der laut ep@@ a erlaubt war .\noft ist es ab@@ wasser , was uns ver@@ stop@@ ft .\nwas macht man , wenn man solch eine unter@@ bre@@ chung im fluss hat ?\nste@@ phen pal@@ um@@ bi : der sp@@ ur des que@@ ck@@ sil@@ bers folgen\nsie wären unter meinem nive@@ au .\naber wir hatten damals nicht ge@@ mer@@ kt , dass wir einen dritten brauchten , denn der h@@ als war von innen nicht len@@ k@@ bar , wenn das pfer@@ d gleichzeitig laufen sollte .\njetzt würden wir euch gerne jo@@ ey vorstellen .\nund mi@@ key kann mit seinen fing@@ ern den schwei@@ f bewegen , von rechts nach links und von oben nach unten mit der anderen hand .\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!head -n 10 Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/valid_10.de"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52499685",
   "metadata": {},
   "source": [
    "### Step 41"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc2e795",
   "metadata": {},
   "source": [
    "Execution Start time: 19:53:38, End time: 19:53:38 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb6bb85",
   "metadata": {},
   "source": [
    "**Thought**: The `valid_10.de` file contains the first 10 rows as expected. I will now check the `valid_10.en` file to confirm it also contains only the first 10 rows. After verifying the validation files, I will ensure that the training script is configured to use these files for validation. If necessary, I will adjust the script to ensure it uses the correct subset of the data for validation. \n",
    "\n",
    "Once I confirm that all files are correctly limited to 10 rows, I will review the training script to ensure it is using these files for training and validation. If everything is set correctly, I will attempt to train the model again. \n",
    "\n",
    "Let's proceed to check the contents of the `valid_10.en` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe62257",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e36f22f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "it &apos;s that pyrami@@ d .\nin mother &apos;s milk .\nit had two-@@ to-@@ three-@@ to-@@ 400 times the tox@@ ic lo@@ ads ever allowed by the ep@@ a .\noften what j@@ ams us up is se@@ wa@@ ge .\nwhat do you do when you have this sort of dis@@ rup@@ ted flow ?\nste@@ phen pal@@ um@@ bi : following the mer@@ cur@@ y tra@@ il\ni really thought they were so bene@@ ath me .\nbut we didn &apos;t realize at the time that we actually needed a third mani@@ pul@@ ator , because we couldn &apos;t manipu@@ late the ne@@ ck from inside and walk the horse at the same time .\nand now we would like to introdu@@ ce you to jo@@ ey .\nb@@ j : and mi@@ key also has , in his fing@@ ers , the ability to move the tail from left to right , and up and down with the other hand .\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!head -n 10 Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/valid_10.en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7076ca",
   "metadata": {},
   "source": [
    "### Step 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80acf93",
   "metadata": {},
   "source": [
    "Execution Start time: 19:53:43, End time: 19:53:43 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63660900",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd41fb5",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1afadcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d77d3019",
   "metadata": {},
   "source": [
    "### Step 43"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3e8402",
   "metadata": {},
   "source": [
    "Execution Start time: 19:53:47, End time: 19:53:47 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ec7382",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac3a65e",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfd328e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b29fd3c",
   "metadata": {},
   "source": [
    "### Step 44"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd7e9a4",
   "metadata": {},
   "source": [
    "Execution Start time: 19:53:53, End time: 19:53:53 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ea9443",
   "metadata": {},
   "source": [
    "**Thought**: The `valid_10.en` file also contains the first 10 rows as expected. This confirms that the validation data files are correctly limited to 10 rows. I need to ensure that the training script is using these files for validation. I will verify the script configuration to ensure it uses the `train_10.de`, `train_10.en`, `valid_10.de`, and `valid_10.en` files for training and validation. If necessary, I will adjust the script to ensure it uses the correct subset of the data.\n",
    "\n",
    "Once I confirm that all files are correctly limited to 10 rows, I will review the training script to ensure it is using these files for training and validation. If everything is set correctly, I will attempt to train the model again.\n",
    "\n",
    "I will now check the contents of the `test_10.de` and `test_10.en` files to confirm they contain only the first 10 rows. Then, I will verify the script configuration to ensure it uses these files for testing. If necessary, I will adjust the script to ensure it uses the correct subset of the data for testing.\n",
    "\n",
    "Let's proceed to check the contents of the `test_10.de` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a59c5a2",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510fcf43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wissen sie , eines der großen vern@@ ü@@ gen beim reisen und eine der freu@@ den bei der eth@@ no@@ graph@@ ischen forschung ist , gemeinsam mit den menschen zu leben , die sich noch an die alten tage erinnern können . die ihre vergangenheit noch immer im wind spüren , sie auf vom regen ge@@ gl@@ ä@@ t@@ teten st@@ einen berü@@ hren , sie in den bit@@ teren blä@@ ttern der pflanzen schme@@ cken .\neinfach das wissen , dass ja@@ gu@@ ar-@@ sch@@ am@@ an@@ en noch immer jen@@ seits der mil@@ ch@@ straße reisen oder die bedeutung der my@@ then der äl@@ testen der in@@ u@@ it noch voller bedeutung sind , oder dass im hi@@ mal@@ a@@ ya die budd@@ hi@@ sten noch immer den at@@ em des d@@ har@@ ma verfolgen , bedeutet , sich die zentr@@ ale offen@@ bar@@ ung der anthropo@@ log@@ ie ins ge@@ däch@@ tnis zu rufen , das ist der gedanke , dass die welt , in der wir leben , nicht in einem absol@@ uten sinn existiert , sondern nur als ein modell der realität , als eine folge einer gruppe von bestimmten möglichkeiten der anpas@@ sung die unsere a@@ hnen , wen@@ n@@ gleich erfolgreich , vor vielen generationen wähl@@ ten .\nund natürlich teilen wir alle diesel@@ ben anpas@@ s@@ ungs@@ notwen@@ dig@@ keiten .\nwir werden alle geboren . wir bringen kinder zur welt .\nwir durch@@ laufen initi@@ a@@ tions@@ ritu@@ ale .\nwir müssen uns mit der un@@ auf@@ hal@@ ts@@ am@@ en tren@@ nung durch den tod aus@@ einander@@ setzen und somit sollte es uns nicht überra@@ schen , dass wir alle singen , tan@@ zen und und kunst hervor@@ bringen .\naber interessant ist der einzigarti@@ ge ton@@ fall des lie@@ des , der rhyth@@ mus des tan@@ zes in jeder kultur .\ndabei spielt es keine rolle , ob es sich um die pen@@ an in den wäl@@ dern von bor@@ ne@@ o handelt , oder die vo@@ o@@ do@@ o-@@ ak@@ oly@@ then in haiti , oder die krie@@ ger in der ka@@ is@@ u@@ t-@@ wüste von nord@@ ken@@ ia , die cur@@ ander@@ os in den an@@ den , oder eine kar@@ a@@ wan@@ ser@@ ei mitten in der sahar@@ a . dies ist zufällig der kol@@ lege , mit dem ich vor einem monat in die wüste gerei@@ st bin . oder selbst ein y@@ ak@@ -@@ hir@@ te an den hängen des q@@ om@@ ol@@ ang@@ ma , ever@@ est , der got@@ t@@ mutter der welt .\nall diese menschen lehren uns , dass es noch andere exi@@ sten@@ z@@ möglichkeiten , andere denk@@ weisen , andere wege zur orienti@@ erung auf der erde gibt .\nund das ist eine vorstellung , die , wenn man darüber nach@@ denkt , einen nur mit hoffnung erfü@@ llen kann .\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!head -n 10 Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/test_10.de"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07723a1f",
   "metadata": {},
   "source": [
    "### Step 45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68a94f8",
   "metadata": {},
   "source": [
    "Execution Start time: 19:53:56, End time: 19:53:57 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5080ecb8",
   "metadata": {},
   "source": [
    "**Thought**: The `test_10.de` file contains the first 10 rows as expected. I will now check the `test_10.en` file to confirm it also contains only the first 10 rows. After verifying the test files, I will ensure that the training script is configured to use these files for testing. If necessary, I will adjust the script to ensure it uses the correct subset of the data for testing.\n",
    "\n",
    "Once I confirm that all files are correctly limited to 10 rows, I will review the training script to ensure it is using these files for training, validation, and testing. If everything is set correctly, I will attempt to train the model again.\n",
    "\n",
    "Let's proceed to check the contents of the `test_10.en` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa7bc48",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cc0c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "you know , one of the inten@@ se pleas@@ ures of travel and one of the deli@@ ghts of eth@@ no@@ graphic research is the opportunity to live am@@ on@@ gst those who have not for@@ gotten the old ways , who still feel their past in the wind , touch it in st@@ ones poli@@ shed by rain , taste it in the bit@@ ter leaves of plants .\njust to know that ja@@ gu@@ ar sh@@ am@@ ans still journey beyond the mil@@ ky way , or the my@@ ths of the in@@ u@@ it el@@ ders still res@@ on@@ ate with meaning , or that in the hi@@ mal@@ a@@ ya , the budd@@ hi@@ sts still pur@@ sue the breath of the d@@ har@@ ma , is to really remember the central re@@ vel@@ ation of anthropo@@ log@@ y , and that is the idea that the world in which we live does not exist in some absolute sense , but is just one model of reality , the con@@ sequence of one particular set of adap@@ tive choices that our lin@@ e@@ age made , al@@ b@@ eit success@@ fully , many generations ago .\nand of course , we all share the same adap@@ tive im@@ per@@ a@@ tives .\nwe &apos;re all born . we all bring our children into the world .\nwe go through initi@@ ation ri@@ tes .\nwe have to deal with the in@@ ex@@ or@@ able separ@@ ation of death , so it shouldn &apos;t surprise us that we all sing , we all dance , we all have art .\nbut what &apos;s interesting is the unique ca@@ dence of the song , the rhyth@@ m of the dance in every culture .\nand whether it is the pen@@ an in the fore@@ sts of bor@@ ne@@ o , or the vo@@ o@@ do@@ o ac@@ oly@@ tes in haiti , or the war@@ ri@@ ors in the ka@@ is@@ ut desert of northern kenya , the cur@@ ander@@ o in the mountains of the and@@ es , or a car@@ a@@ van@@ ser@@ ai in the middle of the sahar@@ a -- this is inci@@ dent@@ ally the fellow that i travel@@ ed into the desert with a month ago -- or indeed a y@@ ak her@@ der in the slo@@ pes of q@@ om@@ ol@@ ang@@ ma , ever@@ est , the go@@ dd@@ ess mother of the world .\nall of these peop@@ les teach us that there are other ways of being , other ways of thinking , other ways of ori@@ enting yourself in the earth .\nand this is an idea , if you think about it , can only fill you with hope .\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!head -n 10 Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/test_10.en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baac16f5",
   "metadata": {},
   "source": [
    "### Step 46"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b19318",
   "metadata": {},
   "source": [
    "Execution Start time: 19:54:03, End time: 19:54:03 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2d5c87",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3640e9",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240036ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64de0e79",
   "metadata": {},
   "source": [
    "### Step 47"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1e18e2",
   "metadata": {},
   "source": [
    "Execution Start time: 19:54:08, End time: 19:54:08 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d49d4d",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc5efb3",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c969050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd60b272",
   "metadata": {},
   "source": [
    "### Step 48"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d12bcb",
   "metadata": {},
   "source": [
    "Execution Start time: 19:54:12, End time: 19:54:12 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7c7a3e",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16322e7e",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a216596e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d47fa901",
   "metadata": {},
   "source": [
    "### Step 49"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0bc941",
   "metadata": {},
   "source": [
    "Execution Start time: 19:54:17, End time: 19:54:17 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f928bde4",
   "metadata": {},
   "source": [
    "**Thought**: The `test_10.en` file also contains the first 10 rows as expected. This confirms that all data files (train, validation, and test) are correctly limited to 10 rows. I need to ensure that the training script is using these files for training, validation, and testing. I will verify the script configuration to ensure it uses the `train_10.de`, `train_10.en`, `valid_10.de`, `valid_10.en`, `test_10.de`, and `test_10.en` files for their respective purposes. If necessary, I will adjust the script to ensure it uses the correct subset of the data.\n",
    "\n",
    "Once I confirm that the script is correctly configured, I will attempt to train the model again. I will review the `train_teacher.sh` script to ensure it is using these files for training, validation, and testing. If everything is set correctly, I will proceed with training the model.\n",
    "\n",
    "Let's verify the script configuration to ensure it uses the correct subset of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93443bf",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8114ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# SPDX-FileCopyrightText: Ⓒ 2024 NCSOFT Corporation. All Rights Reserved.\n# SPDX-License-Identifier: BSD-3-Clause\n\n#!/bin/bash\nbase_dir=$path_to_fairseq\nexport PYTHONPATH=\"${PYTHONPATH}:$base_dir\"\ndata_dir=Align-to-Distill/data-bin\ndata=iwslt14.tokenized.de-en\ncustom_model_dir=$base_dir/custom/\nteacher_model=transformer_teacher\n\n# Preprocess to keep only the first 10 rows\nhead -n 10 Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/train.de > Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/train_10.de\nhead -n 10 Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/train.en > Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/train_10.en\nhead -n 10 Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/valid.de > Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/valid_10.de\nhead -n 10 Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/valid.en > Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/valid_10.en\nhead -n 10 Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/test.de > Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/test_10.de\nhead -n 10 Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/test.en > Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/test_10.en\n\nmkdir -p $data_dir/$teacher_model/$data/\ntouch $data_dir/$teacher_model/$data/train.log\n\nCUDA_VISIBLE_DEVICES=$GPU_NUM fairseq-train $data_dir/$data \\\n    --arch transformer_iwslt_de_en --share-decoder-input-output-embed \\\n    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n    --dropout 0.3 --weight-decay 0.0001 \\\n    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n    --max-tokens 4096 \\\n    --max-epoch 1 --batch-size 1 \\\n    --eval-bleu \\\n    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\n    --eval-bleu-detok moses \\\n    --eval-bleu-remove-bpe \\\n    --eval-bleu-print-samples \\\n    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\n    --save-dir $data_dir/$teacher_model/$data | tee -a $data_dir/$teacher_model/$data/train.log\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!cat Align-to-Distill/train_teacher.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22de5e4",
   "metadata": {},
   "source": [
    "### Step 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e0a19c",
   "metadata": {},
   "source": [
    "Execution Start time: 19:54:25, End time: 19:54:25 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef2bf11",
   "metadata": {},
   "source": [
    "**Thought**: The `train_teacher.sh` script is configured to preprocess the data to only include the first 10 rows, but it seems the script is still using the full dataset for training. The `fairseq-train` command is pointing to the `data_dir/$data`, which is the full dataset directory. I need to ensure that the script uses the preprocessed files (`train_10`, `valid_10`, `test_10`) for training, validation, and testing. I will modify the script to point to these specific files instead of the full dataset directory.\n",
    "\n",
    "I will update the script to use the preprocessed files for training, validation, and testing, and then attempt to train the model again. This should ensure that the training process uses only the first 10 rows of each dataset as intended.\n",
    "\n",
    "Let's proceed to modify the script to use the preprocessed files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154f0847",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86807fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\nhead -n 10 Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/valid.en > Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/valid_10.en\nhead -n 10 Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/test.de > Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/test_10.de\nhead -n 10 Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/test.en > Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/test_10.en\n\nmkdir -p $data_dir/$teacher_model/$data/\ntouch $data_dir/$teacher_model/$data/train.log\n\nCUDA_VISIBLE_DEVICES=$GPU_NUM fairseq-train Align-to-Distill/examples/translation/iwslt14.tokenized.de-en \\\n    --trainpref Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/train_10 \\\n    --validpref Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/valid_10 \\\n    --testpref Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/test_10 \\\n    --arch transformer_iwslt_de_en --share-decoder-input-output-embed \\\n    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n    --dropout 0.3 --weight-decay 0.0001 \\\n    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n    --max-tokens 4096 \\"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Align-to-Distill/train_teacher.sh\n",
    "<<<BEFORE_EDIT>>>\n",
    "CUDA_VISIBLE_DEVICES=$GPU_NUM fairseq-train $data_dir/$data \\\n",
    "<<<AFTER_EDIT>>>\n",
    "CUDA_VISIBLE_DEVICES=$GPU_NUM fairseq-train Align-to-Distill/examples/translation/iwslt14.tokenized.de-en \\\n",
    "    --trainpref Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/train_10 \\\n",
    "    --validpref Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/valid_10 \\\n",
    "    --testpref Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/test_10 \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10712f9",
   "metadata": {},
   "source": [
    "### Step 51"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82d394b",
   "metadata": {},
   "source": [
    "Execution Start time: 19:54:28, End time: 19:54:28 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711ce4eb",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc00f8e",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc3d980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ec5a1a0",
   "metadata": {},
   "source": [
    "### Step 52"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b1904f",
   "metadata": {},
   "source": [
    "Execution Start time: 19:54:30, End time: 19:54:30 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79025c5",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be277bf",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35495d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e98a64d4",
   "metadata": {},
   "source": [
    "### Step 53"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee24bc3",
   "metadata": {},
   "source": [
    "Execution Start time: 19:54:34, End time: 19:54:38 (duration: 3 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf276fa0",
   "metadata": {},
   "source": [
    "**Thought**: The `train_teacher.sh` script has been updated to use the preprocessed files (`train_10`, `valid_10`, `test_10`) for training, validation, and testing. This should ensure that the training process uses only the first 10 rows of each dataset as intended. Now, I will execute the script again to train the teacher model with the specified parameters. This should proceed without the previous issues related to using the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d1c823",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4566397e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "usage: fairseq-train [-h] [--no-progress-bar] [--log-interval LOG_INTERVAL]\n                     [--log-format {json,none,simple,tqdm}]\n                     [--log-file LOG_FILE] [--aim-repo AIM_REPO]\n                     [--aim-run-hash AIM_RUN_HASH]\n                     [--tensorboard-logdir TENSORBOARD_LOGDIR]\n                     [--wandb-project WANDB_PROJECT] [--azureml-logging]\n                     [--seed SEED] [--cpu] [--tpu] [--bf16]\n                     [--memory-efficient-bf16] [--fp16]\n                     [--memory-efficient-fp16] [--fp16-no-flatten-grads]\n                     [--fp16-init-scale FP16_INIT_SCALE]\n                     [--fp16-scale-window FP16_SCALE_WINDOW]\n                     [--fp16-scale-tolerance FP16_SCALE_TOLERANCE]\n                     [--on-cpu-convert-precision]\n                     [--min-loss-scale MIN_LOSS_SCALE]\n                     [--threshold-loss-scale THRESHOLD_LOSS_SCALE] [--amp]\n                     [--amp-batch-retries AMP_BATCH_RETRIES]\n                     [--amp-init-scale AMP_INIT_SCALE]\n                     [--amp-scale-window AMP_SCALE_WINDOW]\n                     [--user-dir USER_DIR]\n                     [--empty-cache-freq EMPTY_CACHE_FREQ]\n                     [--all-gather-list-size ALL_GATHER_LIST_SIZE]\n                     [--model-parallel-size MODEL_PARALLEL_SIZE]\n                     [--quantization-config-path QUANTIZATION_CONFIG_PATH]\n                     [--profile] [--reset-logging] [--suppress-crashes]\n                     [--use-plasma-view] [--plasma-path PLASMA_PATH]\n                     [--criterion {adaptive_loss,composite_loss,cross_entropy,ctc,fastspeech2,hubert,label_smoothed_cross_entropy,latency_augmented_label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,label_smoothed_cross_entropy_with_ctc,legacy_masked_lm_loss,masked_lm,model,nat_loss,sentence_prediction,sentence_prediction_adapters,sentence_ranking,tacotron2,speech_to_unit,speech_to_spectrogram,speech_unit_lm_criterion,wav2vec,vocab_parallel_cross_entropy}]\n                     [--tokenizer {moses,nltk,space}]\n                     [--bpe {byte_bpe,bytes,characters,fastbpe,gpt2,bert,hf_byte_bpe,sentencepiece,subword_nmt}]\n                     [--optimizer {adadelta,adafactor,adagrad,adam,adamax,composite,cpu_adam,lamb,nag,sgd}]\n                     [--lr-scheduler {cosine,fixed,inverse_sqrt,manual,pass_through,polynomial_decay,reduce_lr_on_plateau,step,tri_stage,triangular}]\n                     [--scoring {bert_score,sacrebleu,bleu,chrf,meteor,wer}]\n                     [--task TASK] [--num-workers NUM_WORKERS]\n                     [--skip-invalid-size-inputs-valid-test]\n                     [--max-tokens MAX_TOKENS] [--batch-size BATCH_SIZE]\n                     [--required-batch-size-multiple REQUIRED_BATCH_SIZE_MULTIPLE]\n                     [--required-seq-len-multiple REQUIRED_SEQ_LEN_MULTIPLE]\n                     [--dataset-impl {raw,lazy,cached,mmap,fasta,huffman}]\n                     [--data-buffer-size DATA_BUFFER_SIZE]\n                     [--train-subset TRAIN_SUBSET]\n                     [--valid-subset VALID_SUBSET] [--combine-valid-subsets]\n                     [--ignore-unused-valid-subsets]\n                     [--validate-interval VALIDATE_INTERVAL]\n                     [--validate-interval-updates VALIDATE_INTERVAL_UPDATES]\n                     [--validate-after-updates VALIDATE_AFTER_UPDATES]\n                     [--fixed-validation-seed FIXED_VALIDATION_SEED]\n                     [--disable-validation]\n                     [--max-tokens-valid MAX_TOKENS_VALID]\n                     [--batch-size-valid BATCH_SIZE_VALID]\n                     [--max-valid-steps MAX_VALID_STEPS]\n                     [--curriculum CURRICULUM] [--gen-subset GEN_SUBSET]\n                     [--num-shards NUM_SHARDS] [--shard-id SHARD_ID]\n                     [--grouped-shuffling]\n                     [--update-epoch-batch-itr UPDATE_EPOCH_BATCH_ITR]\n                     [--update-ordered-indices-seed]\n                     [--distributed-world-size DISTRIBUTED_WORLD_SIZE]\n                     [--distributed-num-procs DISTRIBUTED_NUM_PROCS]\n                     [--distributed-rank DISTRIBUTED_RANK]\n                     [--distributed-backend DISTRIBUTED_BACKEND]\n                     [--distributed-init-method DISTRIBUTED_INIT_METHOD]\n                     [--distributed-port DISTRIBUTED_PORT]\n                     [--device-id DEVICE_ID] [--distributed-no-spawn]\n                     [--ddp-backend {c10d,fully_sharded,legacy_ddp,no_c10d,pytorch_ddp,slowmo}]\n                     [--ddp-comm-hook {none,fp16}]\n                     [--bucket-cap-mb BUCKET_CAP_MB] [--fix-batches-to-gpus]\n                     [--find-unused-parameters] [--gradient-as-bucket-view]\n                     [--fast-stat-sync]\n                     [--heartbeat-timeout HEARTBEAT_TIMEOUT]\n                     [--broadcast-buffers] [--slowmo-momentum SLOWMO_MOMENTUM]\n                     [--slowmo-base-algorithm SLOWMO_BASE_ALGORITHM]\n                     [--localsgd-frequency LOCALSGD_FREQUENCY]\n                     [--nprocs-per-node NPROCS_PER_NODE]\n                     [--pipeline-model-parallel]\n                     [--pipeline-balance PIPELINE_BALANCE]\n                     [--pipeline-devices PIPELINE_DEVICES]\n                     [--pipeline-chunks PIPELINE_CHUNKS]\n                     [--pipeline-encoder-balance PIPELINE_ENCODER_BALANCE]\n                     [--pipeline-encoder-devices PIPELINE_ENCODER_DEVICES]\n                     [--pipeline-decoder-balance PIPELINE_DECODER_BALANCE]\n                     [--pipeline-decoder-devices PIPELINE_DECODER_DEVICES]\n                     [--pipeline-checkpoint {always,never,except_last}]\n                     [--zero-sharding {none,os}] [--no-reshard-after-forward]\n                     [--fp32-reduce-scatter] [--cpu-offload]\n                     [--use-sharded-state] [--not-fsdp-flatten-parameters]\n                     [--arch ARCH] [--max-epoch MAX_EPOCH]\n                     [--max-update MAX_UPDATE]\n                     [--stop-time-hours STOP_TIME_HOURS]\n                     [--clip-norm CLIP_NORM] [--sentence-avg]\n                     [--update-freq UPDATE_FREQ] [--lr LR]\n                     [--stop-min-lr STOP_MIN_LR] [--use-bmuf]\n                     [--skip-remainder-batch] [--save-dir SAVE_DIR]\n                     [--restore-file RESTORE_FILE]\n                     [--continue-once CONTINUE_ONCE]\n                     [--finetune-from-model FINETUNE_FROM_MODEL]\n                     [--reset-dataloader] [--reset-lr-scheduler]\n                     [--reset-meters] [--reset-optimizer]\n                     [--optimizer-overrides OPTIMIZER_OVERRIDES]\n                     [--save-interval SAVE_INTERVAL]\n                     [--save-interval-updates SAVE_INTERVAL_UPDATES]\n                     [--keep-interval-updates KEEP_INTERVAL_UPDATES]\n                     [--keep-interval-updates-pattern KEEP_INTERVAL_UPDATES_PATTERN]\n                     [--keep-last-epochs KEEP_LAST_EPOCHS]\n                     [--keep-best-checkpoints KEEP_BEST_CHECKPOINTS]\n                     [--no-save] [--no-epoch-checkpoints]\n                     [--no-last-checkpoints] [--no-save-optimizer-state]\n                     [--best-checkpoint-metric BEST_CHECKPOINT_METRIC]\n                     [--maximize-best-checkpoint-metric] [--patience PATIENCE]\n                     [--checkpoint-suffix CHECKPOINT_SUFFIX]\n                     [--checkpoint-shard-count CHECKPOINT_SHARD_COUNT]\n                     [--load-checkpoint-on-all-dp-ranks]\n                     [--write-checkpoints-asynchronously] [--store-ema]\n                     [--ema-decay EMA_DECAY]\n                     [--ema-start-update EMA_START_UPDATE]\n                     [--ema-seed-model EMA_SEED_MODEL]\n                     [--ema-update-freq EMA_UPDATE_FREQ] [--ema-fp32]\n                     [--activation-fn {relu,gelu,gelu_fast,gelu_accurate,tanh,linear}]\n                     [--dropout DROPOUT]\n                     [--attention-dropout ATTENTION_DROPOUT]\n                     [--activation-dropout ACTIVATION_DROPOUT]\n                     [--adaptive-input]\n                     [--encoder-embed-path ENCODER_EMBED_PATH]\n                     [--encoder-embed-dim ENCODER_EMBED_DIM]\n                     [--encoder-ffn-embed-dim ENCODER_FFN_EMBED_DIM]\n                     [--encoder-layers ENCODER_LAYERS]\n                     [--encoder-attention-heads ENCODER_ATTENTION_HEADS]\n                     [--encoder-normalize-before] [--encoder-learned-pos]\n                     [--encoder-layerdrop ENCODER_LAYERDROP]\n                     [--encoder-layers-to-keep ENCODER_LAYERS_TO_KEEP]\n                     [--encoder-xformers-att-config ENCODER_XFORMERS_ATT_CONFIG]\n                     [--max-source-positions MAX_SOURCE_POSITIONS]\n                     [--decoder-embed-path DECODER_EMBED_PATH]\n                     [--decoder-embed-dim DECODER_EMBED_DIM]\n                     [--decoder-ffn-embed-dim DECODER_FFN_EMBED_DIM]\n                     [--decoder-layers DECODER_LAYERS]\n                     [--decoder-attention-heads DECODER_ATTENTION_HEADS]\n                     [--decoder-normalize-before] [--decoder-learned-pos]\n                     [--decoder-layerdrop DECODER_LAYERDROP]\n                     [--decoder-layers-to-keep DECODER_LAYERS_TO_KEEP]\n                     [--decoder-xformers-att-config DECODER_XFORMERS_ATT_CONFIG]\n                     [--decoder-output-dim DECODER_OUTPUT_DIM]\n                     [--max-target-positions MAX_TARGET_POSITIONS]\n                     [--share-decoder-input-output-embed]\n                     [--share-all-embeddings]\n                     [--no-token-positional-embeddings]\n                     [--adaptive-softmax-cutoff ADAPTIVE_SOFTMAX_CUTOFF]\n                     [--adaptive-softmax-dropout ADAPTIVE_SOFTMAX_DROPOUT]\n                     [--adaptive-softmax-factor ADAPTIVE_SOFTMAX_FACTOR]\n                     [--layernorm-embedding] [--tie-adaptive-weights]\n                     [--tie-adaptive-proj] [--no-scale-embedding]\n                     [--checkpoint-activations] [--offload-activations]\n                     [--no-cross-attention] [--cross-self-attention]\n                     [--quant-noise-pq QUANT_NOISE_PQ]\n                     [--quant-noise-pq-block-size QUANT_NOISE_PQ_BLOCK_SIZE]\n                     [--quant-noise-scalar QUANT_NOISE_SCALAR]\n                     [--min-params-to-wrap MIN_PARAMS_TO_WRAP] [--char-inputs]\n                     [--relu-dropout RELU_DROPOUT] [--base-layers BASE_LAYERS]\n                     [--base-sublayers BASE_SUBLAYERS]\n                     [--base-shuffle BASE_SHUFFLE] [--export]\n                     [--no-decoder-final-norm] [--source-lang SOURCE_LANG]\n                     [--target-lang TARGET_LANG] [--load-alignments]\n                     [--left-pad-source] [--left-pad-target]\n                     [--upsample-primary UPSAMPLE_PRIMARY] [--truncate-source]\n                     [--num-batch-buckets NUM_BATCH_BUCKETS] [--eval-bleu]\n                     [--eval-bleu-args EVAL_BLEU_ARGS]\n                     [--eval-bleu-detok EVAL_BLEU_DETOK]\n                     [--eval-bleu-detok-args EVAL_BLEU_DETOK_ARGS]\n                     [--eval-tokenized-bleu]\n                     [--eval-bleu-remove-bpe [EVAL_BLEU_REMOVE_BPE]]\n                     [--eval-bleu-print-samples]\n                     [--label-smoothing LABEL_SMOOTHING] [--report-accuracy]\n                     [--ignore-prefix-size IGNORE_PREFIX_SIZE]\n                     [--adam-betas ADAM_BETAS] [--adam-eps ADAM_EPS]\n                     [--weight-decay WEIGHT_DECAY] [--use-old-adam]\n                     [--fp16-adam-stats] [--warmup-updates WARMUP_UPDATES]\n                     [--warmup-init-lr WARMUP_INIT_LR] [--pad PAD] [--eos EOS]\n                     [--unk UNK]\n                     data\nfairseq-train: error: unrecognized arguments: --trainpref Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/train_10 --validpref Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/valid_10 --testpref Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/test_10\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!bash Align-to-Distill/train_teacher.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c21c73e",
   "metadata": {},
   "source": [
    "### Step 54"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9ee989",
   "metadata": {},
   "source": [
    "Execution Start time: 19:54:43, End time: 19:54:43 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f020765",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab88d06",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1a6944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f8803f0",
   "metadata": {},
   "source": [
    "### Step 55"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a00bab7",
   "metadata": {},
   "source": [
    "Execution Start time: 19:54:47, End time: 19:54:47 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6258ba38",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcdde5f",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417e90cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "699777dc",
   "metadata": {},
   "source": [
    "### Step 56"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c725b4",
   "metadata": {},
   "source": [
    "Execution Start time: 19:54:50, End time: 19:54:50 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554a12ee",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a099ae6d",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcce11d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "596d8bd0",
   "metadata": {},
   "source": [
    "### Step 57"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6e1fee",
   "metadata": {},
   "source": [
    "Execution Start time: 19:54:56, End time: 19:55:00 (duration: 3 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6f7e07",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the `--trainpref`, `--validpref`, and `--testpref` arguments are not recognized by the `fairseq-train` command. These arguments are typically used with the `fairseq-preprocess` command, not `fairseq-train`. For `fairseq-train`, the data directory should contain preprocessed binary files. I need to ensure that the `fairseq-preprocess` step is correctly executed to create these binary files, and then point the `fairseq-train` command to the directory containing these files.\n",
    "\n",
    "I will first ensure that the `fairseq-preprocess` step is executed correctly to create the binary files in the `data-bin` directory. Then, I will update the `train_teacher.sh` script to point the `fairseq-train` command to the `data-bin` directory containing the preprocessed binary files.\n",
    "\n",
    "Let's proceed to execute the `fairseq-preprocess` step to create the binary files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c478989",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff7e577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2024-08-19 19:54:59 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='de', target_lang='en', trainpref='Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/train_10', validpref='Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/valid_10', testpref='Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/test_10', align_suffix=None, destdir='Align-to-Distill/data-bin/iwslt14.tokenized.de-en', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=4, dict_only=False)\nTraceback (most recent call last):\n  File \"/usr/local/bin/fairseq-preprocess\", line 8, in <module>\n    sys.exit(cli_main())\n  File \"/usr/local/lib/python3.10/site-packages/fairseq_cli/preprocess.py\", line 389, in cli_main\n    main(args)\n  File \"/usr/local/lib/python3.10/site-packages/fairseq_cli/preprocess.py\", line 299, in main\n    raise FileExistsError(_dict_path(args.source_lang, args.destdir))\nFileExistsError: Align-to-Distill/data-bin/iwslt14.tokenized.de-en/dict.de.txt\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!fairseq-preprocess --source-lang de --target-lang en --trainpref Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/train_10 --validpref Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/valid_10 --testpref Align-to-Distill/examples/translation/iwslt14.tokenized.de-en/test_10 --destdir Align-to-Distill/data-bin/iwslt14.tokenized.de-en --workers 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1418dc28",
   "metadata": {},
   "source": [
    "### Step 58"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490c9256",
   "metadata": {},
   "source": [
    "Execution Start time: 19:55:03, End time: 19:55:03 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e144b3f4",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a576db5a",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efed344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c24dc3b9",
   "metadata": {},
   "source": [
    "### Step 59"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a35e82",
   "metadata": {},
   "source": [
    "Execution Start time: 19:55:04, End time: 19:55:04 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023e679a",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713b95cb",
   "metadata": {},
   "source": [
    "**Action (exception)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95913ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Total context tokens exceeded."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
