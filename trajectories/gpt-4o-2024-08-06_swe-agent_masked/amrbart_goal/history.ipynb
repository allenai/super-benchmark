{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba743e5d",
   "metadata": {},
   "source": [
    "### System Prompt\n",
    "<details>\n",
    "SETTING: You are an autonomous programmer, and you're working directly in the command line with a special Jupyter notebook interface.\n",
    "\n",
    "The special Jupyter notebook interface consists of a file editor that shows you 100 lines of a file at a time.\n",
    "You can execute commands in the notebook using:\n",
    "\n",
    "1. Bash commands: Commands starting with !.\n",
    "2. Python commands: Standard Python code.\n",
    "3. Magic commands: Commands starting with %, e.g., %cd <path>.\n",
    "\n",
    "Additionally, you can also use the following commands to help you navigate and edit files.\n",
    "\n",
    "COMMANDS:\n",
    "\n",
    "open:\n",
    "  docstring: opens the file at the given path in the editor. If line_number is provided, the window will be move to include that line\n",
    "  signature: open <path> [<line_number>]\n",
    "  arguments:\n",
    "    - path (string) [required]: the path to the file to open\n",
    "    - line_number (integer) [optional]: the line number to move the window to (if not provided, the window will start at the top of the file)\n",
    "\n",
    "create:\n",
    "  docstring: creates and opens a new file with the given name\n",
    "  signature: create <filename>\n",
    "  arguments:\n",
    "    - filename (string) [required]: the name of the file to create\n",
    "\n",
    "goto:\n",
    "  docstring: moves the window to show <line_number>\n",
    "  signature: goto <line_number>\n",
    "  arguments:\n",
    "    - line_number (integer) [required]: the line number to move the window to\n",
    "\n",
    "scroll_down:\n",
    "  docstring: moves the window down {WINDOW} lines\n",
    "  signature: scroll_down\n",
    "\n",
    "scroll_up:\n",
    "  docstring: moves the window down {WINDOW} lines\n",
    "  signature: scroll_up\n",
    "\n",
    "submit:\n",
    "  docstring: submits your final answer\n",
    "  signature: submit <answer>\n",
    "  arguments:\n",
    "    - answer (string) [required]: the answer to be submitted\n",
    "\n",
    "search_dir:\n",
    "  docstring: searches for search_term in all files in dir_path. If dir_path is not provided, searches in the current directory\n",
    "  signature: search_dir <search_term> [<dir_path>]\n",
    "  arguments:\n",
    "    - search_term (string) [required]: the term to search for\n",
    "    - dir_path (string) [optional]: the directory to search in (if not provided, searches in the current directory)\n",
    "\n",
    "search_file:\n",
    "  docstring: searches for search_term in file. If file is not provided, searches in the current open file\n",
    "  signature: search_file <search_term> [<file>]\n",
    "  arguments:\n",
    "    - search_term (string) [required]: the term to search for\n",
    "    - file (string) [optional]: the file to search in (if not provided, searches in the current open file)\n",
    "\n",
    "find_file:\n",
    "  docstring: finds all files with the given name in dir_path. If dir_path is not provided, searches in the current directory\n",
    "  signature: find_file <file_name> [<dir_path>]\n",
    "  arguments:\n",
    "    - file_name (string) [required]: the name of the file to search for\n",
    "    - dir_path (string) [optional]: the directory to search in (if not provided, searches in the current directory)\n",
    "\n",
    "edit:\n",
    "  docstring: replaces lines <start_line> through <end_line> (inclusive) with the given text in the open file. The replacement text is terminated by a line with only end_of_edit on it. All of the <replacement text> will be entered, so make sure your indentation is formatted properly. Python files will be checked for syntax errors after the edit. If the system detects a syntax error, the edit will not be executed. Simply try to edit the file again, but make sure to read the error message and modify the edit command you issue accordingly. Issuing the same command a second time will just lead to the same error message again.\n",
    "  signature: edit <start_line>:<end_line>\n",
    "<replacement_text>\n",
    "end_of_edit\n",
    "  arguments:\n",
    "    - start_line (integer) [required]: the line number to start the edit at\n",
    "    - end_line (integer) [required]: the line number to end the edit at (inclusive)\n",
    "    - replacement_text (string) [required]: the text to replace the current selection with\n",
    "\n",
    "\n",
    "\n",
    "Please note that THE EDIT COMMAND REQUIRES PROPER INDENTATION. \n",
    "If you'd like to add the line '        print(x)' you must fully write that out, with all those spaces before the code! Indentation is important and code that is not indented correctly will fail and require fixing before it can be run.\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "Your shell prompt is formatted as follows:\n",
    "(Open file: <path>)\n",
    "(Current directory: <cwd>)\n",
    "In [ ]\n",
    "\n",
    "You need to format your output using two fields: discussion and command.\n",
    "Your output should always include _one_ discussion and _one_ command field EXACTLY as in the following example:\n",
    "DISCUSSION\n",
    "First I'll start by using ls to see what files are in the current directory. Then maybe we can look at some relevant files to see what they look like.\n",
    "```\n",
    "!ls -a\n",
    "```\n",
    "\n",
    "You should only include a *SINGLE* command in the command section and then wait for a response from the shell before continuing with more discussion and commands. Everything you include in the DISCUSSION section will be saved for future reference.\n",
    "If you'd like to issue two commands at once, PLEASE DO NOT DO THAT! Please instead first submit just the first command, and then after receiving a response you'll be able to issue the second command. \n",
    "You're free to use any other bash commands you want (e.g. find, grep, cat, ls, cd) in addition to the special commands listed above.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf3a3f4",
   "metadata": {},
   "source": [
    "### Query\n",
    "Finetune a text-to-AMR parser starting with \"facebook/bart-base\" on the train/val/test files provided in the examples directory. Report the last training loss as a json structured as follows: {\"last_loss\": 0.0} (replace 0.0 with the actual value).\n",
    "\n",
    "Additional instructions:\n",
    "1. Train only one epoch.\n",
    "2. Limit the max source and target length to 128.\n",
    "3. Limit the max generation tokens to 128.\n",
    "4. Limit the number of beams to 1.\n",
    "\n",
    "Git repository: https://github.com/goodbai-nlp/AMRBART\n",
    "\n",
    "I have already executed some of the required steps. Now, your goal is to complete the remaining steps and submit the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5d17a6",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38122712",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a2d419",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e845d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Preparing code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10511b4",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d62fa4",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e666735",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e8d877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Clone repository"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4505044",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ccb415",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310a4625",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722b72ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cloning into 'AMRBART'...\r\nremote: Enumerating objects: 400, done.\u001b[K\r\nremote: Counting objects:   1% (1/67)\u001b[K\rremote: Counting objects:   2% (2/67)\u001b[K\rremote: Counting objects:   4% (3/67)\u001b[K\rremote: Counting objects:   5% (4/67)\u001b[K\rremote: Counting objects:   7% (5/67)\u001b[K\rremote: Counting objects:   8% (6/67)\u001b[K\rremote: Counting objects:  10% (7/67)\u001b[K\rremote: Counting objects:  11% (8/67)\u001b[K\rremote: Counting objects:  13% (9/67)\u001b[K\rremote: Counting objects:  14% (10/67)\u001b[K\rremote: Counting objects:  16% (11/67)\u001b[K\rremote: Counting objects:  17% (12/67)\u001b[K\rremote: Counting objects:  19% (13/67)\u001b[K\rremote: Counting objects:  20% (14/67)\u001b[K\rremote: Counting objects:  22% (15/67)\u001b[K\rremote: Counting objects:  23% (16/67)\u001b[K\rremote: Counting objects:  25% (17/67)\u001b[K\rremote: Counting objects:  26% (18/67)\u001b[K\rremote: Counting objects:  28% (19/67)\u001b[K\rremote: Counting objects:  29% (20/67)\u001b[K\rremote: Counting objects:  31% (21/67)\u001b[K\rremote: Counting objects:  32% (22/67)\u001b[K\rremote: Counting objects:  34% (23/67)\u001b[K\rremote: Counting objects:  35% (24/67)\u001b[K\rremote: Counting objects:  37% (25/67)\u001b[K\rremote: Counting objects:  38% (26/67)\u001b[K\rremote: Counting objects:  40% (27/67)\u001b[K\rremote: Counting objects:  41% (28/67)\u001b[K\rremote: Counting objects:  43% (29/67)\u001b[K\rremote: Counting objects:  44% (30/67)\u001b[K\rremote: Counting objects:  46% (31/67)\u001b[K\rremote: Counting objects:  47% (32/67)\u001b[K\rremote: Counting objects:  49% (33/67)\u001b[K\rremote: Counting objects:  50% (34/67)\u001b[K\rremote: Counting objects:  52% (35/67)\u001b[K\rremote: Counting objects:  53% (36/67)\u001b[K\rremote: Counting objects:  55% (37/67)\u001b[K\rremote: Counting objects:  56% (38/67)\u001b[K\rremote: Counting objects:  58% (39/67)\u001b[K\rremote: Counting objects:  59% (40/67)\u001b[K\rremote: Counting objects:  61% (41/67)\u001b[K\rremote: Counting objects:  62% (42/67)\u001b[K\rremote: Counting objects:  64% (43/67)\u001b[K\rremote: Counting objects:  65% (44/67)\u001b[K\rremote: Counting objects:  67% (45/67)\u001b[K\rremote: Counting objects:  68% (46/67)\u001b[K\rremote: Counting objects:  70% (47/67)\u001b[K\rremote: Counting objects:  71% (48/67)\u001b[K\rremote: Counting objects:  73% (49/67)\u001b[K\rremote: Counting objects:  74% (50/67)\u001b[K\rremote: Counting objects:  76% (51/67)\u001b[K\rremote: Counting objects:  77% (52/67)\u001b[K\rremote: Counting objects:  79% (53/67)\u001b[K\rremote: Counting objects:  80% (54/67)\u001b[K\rremote: Counting objects:  82% (55/67)\u001b[K\rremote: Counting objects:  83% (56/67)\u001b[K\rremote: Counting objects:  85% (57/67)\u001b[K\rremote: Counting objects:  86% (58/67)\u001b[K\rremote: Counting objects:  88% (59/67)\u001b[K\rremote: Counting objects:  89% (60/67)\u001b[K\rremote: Counting objects:  91% (61/67)\u001b[K\rremote: Counting objects:  92% (62/67)\u001b[K\rremote: Counting objects:  94% (63/67)\u001b[K\rremote: Counting objects:  95% (64/67)\u001b[K\rremote: Counting objects:  97% (65/67)\u001b[K\rremote: Counting objects:  98% (66/67)\u001b[K\rremote: Counting objects: 100% (67/67)\u001b[K\rremote: Counting objects: 100% (67/67), done.\u001b[K\r\nremote: Compressing objects:   2% (1/40)\u001b[K\rremote: Compressing objects:   5% (2/40)\u001b[K\rremote: Compressing objects:   7% (3/40)\u001b[K\rremote: Compressing objects:  10% (4/40)\u001b[K\rremote: Compressing objects:  12% (5/40)\u001b[K\rremote: Compressing objects:  15% (6/40)\u001b[K\rremote: Compressing objects:  17% (7/40)\u001b[K\rremote: Compressing objects:  20% (8/40)\u001b[K\rremote: Compressing objects:  22% (9/40)\u001b[K\rremote: Compressing objects:  25% (10/40)\u001b[K\rremote: Compressing objects:  27% (11/40)\u001b[K\rremote: Compressing objects:  30% (12/40)\u001b[K\rremote: Compressing objects:  32% (13/40)\u001b[K\rremote: Compressing objects:  35% (14/40)\u001b[K\rremote: Compressing objects:  37% (15/40)\u001b[K\rremote: Compressing objects:  40% (16/40)\u001b[K\rremote: Compressing objects:  42% (17/40)\u001b[K\rremote: Compressing objects:  45% (18/40)\u001b[K\rremote: Compressing objects:  47% (19/40)\u001b[K\rremote: Compressing objects:  50% (20/40)\u001b[K\rremote: Compressing objects:  52% (21/40)\u001b[K\rremote: Compressing objects:  55% (22/40)\u001b[K\rremote: Compressing objects:  57% (23/40)\u001b[K\rremote: Compressing objects:  60% (24/40)\u001b[K\rremote: Compressing objects:  62% (25/40)\u001b[K\rremote: Compressing objects:  65% (26/40)\u001b[K\rremote: Compressing objects:  67% (27/40)\u001b[K\rremote: Compressing objects:  70% (28/40)\u001b[K\rremote: Compressing objects:  72% (29/40)\u001b[K\rremote: Compressing objects:  75% (30/40)\u001b[K\rremote: Compressing objects:  77% (31/40)\u001b[K\rremote: Compressing objects:  80% (32/40)\u001b[K\rremote: Compressing objects:  82% (33/40)\u001b[K\rremote: Compressing objects:  85% (34/40)\u001b[K\rremote: Compressing objects:  87% (35/40)\u001b[K\rremote: Compressing objects:  90% (36/40)\u001b[K\rremote: Compressing objects:  92% (37/40)\u001b[K\rremote: Compressing objects:  95% (38/40)\u001b[K\rremote: Compressing objects:  97% (39/40)\u001b[K\rremote: Compressing objects: 100% (40/40)\u001b[K\rremote: Compressing objects: 100% (40/40), done.\u001b[K\r\nReceiving objects:   0% (1/400)\rReceiving objects:   1% (4/400)\rReceiving objects:   2% (8/400)\rReceiving objects:   3% (12/400)\rReceiving objects:   4% (16/400)\rReceiving objects:   5% (20/400)\rReceiving objects:   6% (24/400)\rReceiving objects:   7% (28/400)\rReceiving objects:   8% (32/400)\rReceiving objects:   9% (36/400)\rReceiving objects:  10% (40/400)\rReceiving objects:  11% (44/400)\rReceiving objects:  12% (48/400)\rReceiving objects:  13% (52/400)\rReceiving objects:  14% (56/400)\rReceiving objects:  15% (60/400)\rReceiving objects:  16% (64/400)\rReceiving objects:  17% (68/400)\rReceiving objects:  18% (72/400)\rReceiving objects:  19% (76/400)\rReceiving objects:  20% (80/400)\rReceiving objects:  21% (84/400)\rReceiving objects:  22% (88/400)\rReceiving objects:  23% (92/400)\rReceiving objects:  24% (96/400)\rReceiving objects:  25% (100/400)\rReceiving objects:  26% (104/400)\rReceiving objects:  27% (108/400)\rReceiving objects:  28% (112/400)\rReceiving objects:  29% (116/400)\rReceiving objects:  30% (120/400)\rReceiving objects:  31% (124/400)\rReceiving objects:  32% (128/400)\rReceiving objects:  33% (132/400)\rReceiving objects:  34% (136/400)\rReceiving objects:  35% (140/400)\rReceiving objects:  36% (144/400)\rReceiving objects:  37% (148/400)\rReceiving objects:  38% (152/400)\rReceiving objects:  39% (156/400)\rReceiving objects:  40% (160/400)\rReceiving objects:  41% (164/400)\rReceiving objects:  42% (168/400)\rReceiving objects:  43% (172/400)\rReceiving objects:  44% (176/400)\rReceiving objects:  45% (180/400)\rReceiving objects:  46% (184/400)\rReceiving objects:  47% (188/400)\rReceiving objects:  48% (192/400)\rReceiving objects:  49% (196/400)\rReceiving objects:  50% (200/400)\rReceiving objects:  51% (204/400)\rReceiving objects:  52% (208/400)\rReceiving objects:  53% (212/400)\rReceiving objects:  54% (216/400)\rReceiving objects:  55% (220/400)\rReceiving objects:  56% (224/400)\rReceiving objects:  57% (228/400)\rReceiving objects:  58% (232/400)\rReceiving objects:  59% (236/400)\rReceiving objects:  60% (240/400)\rReceiving objects:  61% (244/400)\rReceiving objects:  62% (248/400)\rReceiving objects:  63% (252/400)\rReceiving objects:  64% (256/400)\rReceiving objects:  65% (260/400)\rReceiving objects:  66% (264/400)\rReceiving objects:  67% (268/400)\rReceiving objects:  68% (272/400)\rReceiving objects:  69% (276/400)\rReceiving objects:  70% (280/400)\rReceiving objects:  71% (284/400)\rReceiving objects:  72% (288/400)\rReceiving objects:  73% (292/400)\rReceiving objects:  74% (296/400)\rReceiving objects:  75% (300/400)\rReceiving objects:  76% (304/400)\rReceiving objects:  77% (308/400)\rReceiving objects:  78% (312/400)\rReceiving objects:  79% (316/400)\rReceiving objects:  80% (320/400)\rReceiving objects:  81% (324/400)\rremote: Total 400 (delta 43), reused 28 (delta 27), pack-reused 333 (from 1)\u001b[K\r\nReceiving objects:  82% (328/400)\rReceiving objects:  83% (332/400)\rReceiving objects:  84% (336/400)\rReceiving objects:  85% (340/400)\rReceiving objects:  86% (344/400)\rReceiving objects:  87% (348/400)\rReceiving objects:  88% (352/400)\rReceiving objects:  89% (356/400)\rReceiving objects:  90% (360/400)\rReceiving objects:  91% (364/400)\rReceiving objects:  92% (368/400)\rReceiving objects:  93% (372/400)\rReceiving objects:  94% (376/400)\rReceiving objects:  95% (380/400)\rReceiving objects:  96% (384/400)\rReceiving objects:  97% (388/400)\rReceiving objects:  98% (392/400)\rReceiving objects:  99% (396/400)\rReceiving objects: 100% (400/400)\rReceiving objects: 100% (400/400), 6.79 MiB | 48.59 MiB/s, done.\r\nResolving deltas:   0% (0/186)\rResolving deltas:   1% (2/186)\rResolving deltas:   2% (4/186)\rResolving deltas:   3% (6/186)\rResolving deltas:   4% (8/186)\rResolving deltas:   5% (10/186)\rResolving deltas:   6% (12/186)\rResolving deltas:   7% (14/186)\rResolving deltas:   8% (15/186)\rResolving deltas:   9% (17/186)\rResolving deltas:  10% (19/186)\rResolving deltas:  11% (21/186)\rResolving deltas:  12% (23/186)\rResolving deltas:  13% (25/186)\rResolving deltas:  14% (27/186)\rResolving deltas:  15% (28/186)\rResolving deltas:  16% (30/186)\rResolving deltas:  17% (32/186)\rResolving deltas:  18% (34/186)\rResolving deltas:  19% (36/186)\rResolving deltas:  20% (38/186)\rResolving deltas:  21% (40/186)\rResolving deltas:  22% (41/186)\rResolving deltas:  23% (43/186)\rResolving deltas:  24% (45/186)\rResolving deltas:  25% (47/186)\rResolving deltas:  26% (49/186)\rResolving deltas:  27% (51/186)\rResolving deltas:  28% (53/186)\rResolving deltas:  29% (54/186)\rResolving deltas:  30% (56/186)\rResolving deltas:  31% (58/186)\rResolving deltas:  32% (60/186)\rResolving deltas:  33% (62/186)\rResolving deltas:  34% (64/186)\rResolving deltas:  35% (66/186)\rResolving deltas:  36% (67/186)\rResolving deltas:  37% (69/186)\rResolving deltas:  38% (71/186)\rResolving deltas:  39% (73/186)\rResolving deltas:  40% (75/186)\rResolving deltas:  41% (77/186)\rResolving deltas:  42% (79/186)\rResolving deltas:  43% (80/186)\rResolving deltas:  44% (82/186)\rResolving deltas:  45% (84/186)\rResolving deltas:  46% (86/186)\rResolving deltas:  47% (88/186)\rResolving deltas:  48% (90/186)\rResolving deltas:  49% (92/186)\rResolving deltas:  50% (93/186)\rResolving deltas:  51% (95/186)\rResolving deltas:  52% (97/186)\rResolving deltas:  53% (99/186)\rResolving deltas:  54% (101/186)\rResolving deltas:  55% (103/186)\rResolving deltas:  56% (105/186)\rResolving deltas:  57% (107/186)\rResolving deltas:  58% (108/186)\rResolving deltas:  59% (110/186)\rResolving deltas:  60% (112/186)\rResolving deltas:  61% (114/186)\rResolving deltas:  62% (116/186)\rResolving deltas:  63% (118/186)\rResolving deltas:  64% (120/186)\rResolving deltas:  65% (121/186)\rResolving deltas:  66% (123/186)\rResolving deltas:  67% (125/186)\rResolving deltas:  68% (127/186)\rResolving deltas:  69% (129/186)\rResolving deltas:  70% (131/186)\rResolving deltas:  71% (133/186)\rResolving deltas:  72% (134/186)\rResolving deltas:  73% (136/186)\rResolving deltas:  74% (138/186)\rResolving deltas:  75% (140/186)\rResolving deltas:  76% (142/186)\rResolving deltas:  77% (144/186)\rResolving deltas:  78% (146/186)\rResolving deltas:  79% (147/186)\rResolving deltas:  80% (149/186)\rResolving deltas:  81% (151/186)\rResolving deltas:  82% (153/186)\rResolving deltas:  83% (155/186)\rResolving deltas:  84% (157/186)\rResolving deltas:  85% (159/186)\rResolving deltas:  86% (160/186)\rResolving deltas:  87% (162/186)\rResolving deltas:  88% (164/186)\rResolving deltas:  89% (166/186)\rResolving deltas:  90% (168/186)\rResolving deltas:  91% (170/186)\rResolving deltas:  92% (172/186)\rResolving deltas:  93% (173/186)\rResolving deltas:  94% (175/186)\rResolving deltas:  95% (177/186)\rResolving deltas:  96% (179/186)\rResolving deltas:  97% (181/186)\rResolving deltas:  98% (183/186)\rResolving deltas:  99% (185/186)\rResolving deltas: 100% (186/186)\rResolving deltas: 100% (186/186), done.\r\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!git clone https://github.com/goodbai-nlp/AMRBART.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a7d9a6",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be9bb62",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219f282a",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3bc1f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Attempt to run example scirpt as it is\n",
    "# \n",
    "# The first part of the task is fine-tuning, so I base on the example command\n",
    "# \n",
    "# ```\n",
    "# bash train-AMRBART-large-AMRParsing.sh \"xfbai/AMRBART-large-v2\"\n",
    "# ```\n",
    "# \n",
    "# I attempt to run it as it is to check for missing dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9914e76",
   "metadata": {},
   "source": [
    "### Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635ae855",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b54383",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5391674a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/AMRBART/fine-tune\n/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\nTraceback (most recent call last):\r\n  File \"/content/AMRBART/fine-tune/main.py\", line 10, in <module>\r\n    import penman\r\nModuleNotFoundError: No module named 'penman'\r\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/AMRBART/fine-tune\n",
    "!bash train-AMRBART-large-AMRParsing.sh \"facebook/bart-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df419419",
   "metadata": {},
   "source": [
    "### Step 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21737da2",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998757ce",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141d119e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Install dependencies\n",
    "# \n",
    "# I install the minimum dependencies to avoid running-time errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5d6b9d",
   "metadata": {},
   "source": [
    "### Step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7da123",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fdd05a",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c00251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting penman\r\n  Downloading penman-1.3.1-py3-none-any.whl.metadata (7.7 kB)\r\nDownloading penman-1.3.1-py3-none-any.whl (43 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hInstalling collected packages: penman\r\nSuccessfully installed penman-1.3.1\r\n\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\nCollecting datasets==2.19.0\r\n  Downloading datasets-2.19.0-py3-none-any.whl.metadata (19 kB)\r\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (3.15.1)\r\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (1.25.2)\r\nCollecting pyarrow>=12.0.0 (from datasets==2.19.0)\r\n  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\r\nCollecting pyarrow-hotfix (from datasets==2.19.0)\r\n  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\r\nCollecting dill<0.3.9,>=0.3.0 (from datasets==2.19.0)\r\n  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\r\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (2.0.3)\r\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (2.31.0)\r\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (4.66.4)\r\nCollecting xxhash (from datasets==2.19.0)\r\n  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\r\nCollecting multiprocess (from datasets==2.19.0)\r\n  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\r\nRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets==2.19.0) (2023.6.0)\r\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (3.8.3)\r\nRequirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (0.23.3)\r\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (24.1)\r\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (6.0.1)\r\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0) (23.2.0)\r\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0) (2.1.1)\r\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0) (6.0.5)\r\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0) (4.0.3)\r\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0) (1.9.4)\r\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0) (1.4.1)\r\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0) (1.3.1)\r\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets==2.19.0) (4.12.2)\r\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.19.0) (3.7)\r\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.19.0) (2.0.7)\r\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.19.0) (2024.6.2)\r\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas->datasets==2.19.0) (2.9.0.post0)\r\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets==2.19.0) (2024.1)\r\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets==2.19.0) (2024.1)\r\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.19.0) (1.16.0)\r\nDownloading datasets-2.19.0-py3-none-any.whl (542 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/542.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/542.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m193.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/39.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/39.9 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/39.9 MB\u001b[0m \u001b[31m98.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/39.9 MB\u001b[0m \u001b[31m116.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/39.9 MB\u001b[0m \u001b[31m120.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.7/39.9 MB\u001b[0m \u001b[31m111.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.4/39.9 MB\u001b[0m \u001b[31m112.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m26.4/39.9 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m30.4/39.9 MB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m34.3/39.9 MB\u001b[0m \u001b[31m108.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m37.8/39.9 MB\u001b[0m \u001b[31m113.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m107.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/134.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m186.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\r\nDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/194.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m119.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hInstalling collected packages: xxhash, pyarrow-hotfix, pyarrow, dill, multiprocess, datasets\r\nSuccessfully installed datasets-2.19.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 pyarrow-hotfix-0.6 xxhash-3.5.0\r\n\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\nCollecting smatch\r\n  Downloading smatch-1.0.4.tar.gz (26 kB)\r\n  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\r\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\r\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n\u001b[?25hBuilding wheels for collected packages: smatch\r\n  Building wheel for smatch (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n\u001b[?25h  Created wheel for smatch: filename=smatch-1.0.4-py3-none-any.whl size=24052 sha256=819ec1703edf1f2dcfd6b3babd12bce7fbb4fd036101287a87ad7a915b2800fa\r\n  Stored in directory: /tmp/pip-ephem-wheel-cache-cfobrftm/wheels/2a/5e/2d/61b88bc74b337fbf3e998b9aa6b43bec72227e18a84a8335e8\r\nSuccessfully built smatch\r\nInstalling collected packages: smatch\r\nSuccessfully installed smatch-1.0.4\r\n\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\nCollecting rouge-score\r\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\r\n  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\r\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\r\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/site-packages (from rouge-score) (2.1.0)\r\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/site-packages (from rouge-score) (3.8.1)\r\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from rouge-score) (1.25.2)\r\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/site-packages (from rouge-score) (1.16.0)\r\nRequirement already satisfied: click in /usr/local/lib/python3.10/site-packages (from nltk->rouge-score) (8.1.7)\r\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/site-packages (from nltk->rouge-score) (1.4.2)\r\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/site-packages (from nltk->rouge-score) (2023.12.25)\r\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from nltk->rouge-score) (4.66.4)\r\nBuilding wheels for collected packages: rouge-score\r\n  Building wheel for rouge-score (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=c15ce7de762dbb8d15e553182c19cf2551cdba98d87be4d06649852dcca98abd\r\n  Stored in directory: /tmp/pip-ephem-wheel-cache-97ab03xj/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\r\nSuccessfully built rouge-score\r\nInstalling collected packages: rouge-score\r\nSuccessfully installed rouge-score-0.1.2\r\n\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\nCollecting sacrebleu\r\n  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\r\n\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hCollecting portalocker (from sacrebleu)\r\n  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\r\nRequirement already satisfied: regex in /usr/local/lib/python3.10/site-packages (from sacrebleu) (2023.12.25)\r\nCollecting tabulate>=0.8.9 (from sacrebleu)\r\n  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\r\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from sacrebleu) (1.25.2)\r\nCollecting colorama (from sacrebleu)\r\n  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\r\nCollecting lxml (from sacrebleu)\r\n  Downloading lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\r\nDownloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/104.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading tabulate-0.9.0-py3-none-any.whl (35 kB)\r\nDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\r\nDownloading lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (5.0 MB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.9/5.0 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m4.4/5.0 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\r\nInstalling collected packages: tabulate, portalocker, lxml, colorama, sacrebleu\r\nSuccessfully installed colorama-0.4.6 lxml-5.3.0 portalocker-2.10.1 sacrebleu-2.4.3 tabulate-0.9.0\r\n\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\nCollecting transformers==4.21.3\r\n  Downloading transformers-4.21.3-py3-none-any.whl.metadata (81 kB)\r\n\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/82.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from transformers==4.21.3) (3.15.1)\r\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/site-packages (from transformers==4.21.3) (0.23.3)\r\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from transformers==4.21.3) (1.25.2)\r\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from transformers==4.21.3) (24.1)\r\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers==4.21.3) (6.0.1)\r\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers==4.21.3) (2023.12.25)\r\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers==4.21.3) (2.31.0)\r\nCollecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers==4.21.3)\r\n  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.5 kB)\r\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/site-packages (from transformers==4.21.3) (4.66.4)\r\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.21.3) (2023.6.0)\r\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.21.3) (4.12.2)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.21.3) (2.1.1)\r\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.21.3) (3.7)\r\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.21.3) (2.0.7)\r\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.21.3) (2024.6.2)\r\nDownloading transformers-4.21.3-py3-none-any.whl (4.7 MB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/4.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/4.7 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/4.7 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/6.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/6.6 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m5.6/6.6 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hInstalling collected packages: tokenizers, transformers\r\n  Attempting uninstall: tokenizers\r\n    Found existing installation: tokenizers 0.19.1\r\n    Uninstalling tokenizers-0.19.1:\r\n      Successfully uninstalled tokenizers-0.19.1\r\n  Attempting uninstall: transformers\r\n    Found existing installation: transformers 4.40.2\r\n    Uninstalling transformers-4.40.2:\r\n      Successfully uninstalled transformers-4.40.2\r\nSuccessfully installed tokenizers-0.12.1 transformers-4.21.3\r\n\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install penman\n",
    "!pip install datasets==2.19.0\n",
    "!pip install smatch\n",
    "!pip install rouge-score\n",
    "!pip install sacrebleu\n",
    "!pip install transformers==4.21.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce114ff2",
   "metadata": {},
   "source": [
    "### Step 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b191f041",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36348e9c",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afc289f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before running the command, I patch the script to remove the input prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7213870a",
   "metadata": {},
   "source": [
    "### Step 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6877b658",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdd3d31",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab7c89f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\nModelCache=$BasePath/.cache\nDataCache=$DataPath/.cache/dump-amrparsing\n\nlr=1e-5\n\nOutputDir=${RootDir}/outputs/$Dataset-${ModelCate}-AMRParing-bsz16-lr-${lr}-UnifiedInp\n\nif [ ! -d ${OutputDir} ];then\n  mkdir -p ${OutputDir}\nelse\n  rm -rf ${OutputDir}; mkdir -p ${OutputDir};\nfi\nexport HF_DATASETS_CACHE=$DataCache\n\nif [ ! -d ${DataCache} ];then\n  mkdir -p ${DataCache}\nfi\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/AMRBART/fine-tune/train-AMRBART-large-AMRParsing.sh. CHANGING:\n",
    "\n",
    "\n",
    "if [ ! -d ${OutputDir} ];then\n",
    "  mkdir -p ${OutputDir}\n",
    "else\n",
    "  read -p \"${OutputDir} already exists, delete origin one [y/n]?\" yn\n",
    "  case $yn in\n",
    "    [Yy]* ) rm -rf ${OutputDir}; mkdir -p ${OutputDir};;\n",
    "    [Nn]* ) echo \"exiting...\"; exit;;\n",
    "    * ) echo \"Please answer yes or no.\";;\n",
    "  esac\n",
    "fi\n",
    "\n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "if [ ! -d ${OutputDir} ];then\n",
    "  mkdir -p ${OutputDir}\n",
    "else\n",
    "  rm -rf ${OutputDir}; mkdir -p ${OutputDir};\n",
    "fi\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601de762",
   "metadata": {},
   "source": [
    "### Step 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8528582d",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defb7e69",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fb30e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/AMRBART/fine-tune\n/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\nTraceback (most recent call last):\r\n  File \"/content/AMRBART/fine-tune/main.py\", line 506, in <module>\r\n    main()\r\n  File \"/content/AMRBART/fine-tune/main.py\", line 79, in main\r\n    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\r\n  File \"/usr/local/lib/python3.10/site-packages/transformers/hf_argparser.py\", line 224, in parse_args_into_dataclasses\r\n    obj = dtype(**inputs)\r\n  File \"<string>\", line 110, in __init__\r\n  File \"/content/AMRBART/fine-tune/common/training_args.py\", line 1070, in __post_init__\r\n    raise ValueError(\r\nValueError: FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation (`--fp16_full_eval`) can only be used on CUDA devices.\r\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/AMRBART/fine-tune\n",
    "!bash train-AMRBART-large-AMRParsing.sh \"facebook/bart-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbf7c8c",
   "metadata": {},
   "source": [
    "### Step 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5c37a9",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b8abc6",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b43fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/AMRBART\n/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/AMRBART/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3e51fe",
   "metadata": {},
   "source": [
    "### Step 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e1ac3c",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffce1142",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25251de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Command-line argument `--fp16` is only available for GPU, so I patch the script again to remove it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d738a067",
   "metadata": {},
   "source": [
    "### Step 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34192d66",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec7c155",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543b115c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n    --max_steps -1 \\\n    --predict_with_generate \\\n    --smart_init False \\\n    --use_fast_tokenizer False \\\n    --logging_dir $OutputDir/logs \\\n    --logging_first_step True \\\n    --logging_steps 20 \\\n    --save_strategy \"epoch\" \\\n    --save_total_limit 1 \\\n    --seed 42 \\\n    --fp16_backend \"auto\" \\\n    --dataloader_num_workers 8 \\\n    --eval_dataloader_num_workers 2 \\\n    --load_best_model_at_end True \\\n    --metric_for_best_model \"eval_smatch\" \\\n    --include_inputs_for_metrics \\\n    --greater_is_better True \\\n    --do_train \\"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/AMRBART/fine-tune/train-AMRBART-large-AMRParsing.sh. CHANGING:\n",
    "\n",
    "\n",
    "    --save_strategy \"epoch\" \\\n",
    "    --save_total_limit 1 \\\n",
    "    --seed 42 \\\n",
    "    --fp16 \\\n",
    "    --fp16_backend \"auto\" \\\n",
    "    --dataloader_num_workers 8 \\\n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "    --save_strategy \"epoch\" \\\n",
    "    --save_total_limit 1 \\\n",
    "    --seed 42 \\\n",
    "    --fp16_backend \"auto\" \\\n",
    "    --dataloader_num_workers 8 \\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250a308a",
   "metadata": {},
   "source": [
    "### Step 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdeb435",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ce1d16",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fccdb57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/AMRBART/fine-tune\n/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n08/22/2024 04:46:54 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\r\n08/22/2024 04:46:54 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\r\n_n_gpu=0,\r\nadafactor=False,\r\nadam_beta1=0.9,\r\nadam_beta2=0.999,\r\nadam_epsilon=1e-08,\r\nauto_find_batch_size=False,\r\nbf16=False,\r\nbf16_full_eval=False,\r\ndata_seed=None,\r\ndataloader_drop_last=False,\r\ndataloader_num_workers=8,\r\ndataloader_pin_memory=True,\r\nddp_bucket_cap_mb=None,\r\nddp_find_unused_parameters=False,\r\ndebug=[],\r\ndeepspeed=None,\r\ndisable_tqdm=False,\r\ndo_eval=True,\r\ndo_predict=True,\r\ndo_train=True,\r\nearly_stopping=10,\r\neval_accumulation_steps=None,\r\neval_dataloader_num_workers=2,\r\neval_delay=0,\r\neval_lenpen=1.0,\r\neval_steps=None,\r\nevaluation_strategy=epoch,\r\nfp16=False,\r\nfp16_backend=auto,\r\nfp16_full_eval=False,\r\nfp16_opt_level=O1,\r\nfsdp=[],\r\nfsdp_min_num_params=0,\r\nfull_determinism=False,\r\ngeneration_max_length=1024,\r\ngeneration_num_beams=5,\r\ngradient_accumulation_steps=1,\r\ngradient_checkpointing=False,\r\ngreater_is_better=True,\r\ngroup_by_length=False,\r\nhalf_precision_backend=cuda_amp,\r\nhub_model_id=None,\r\nhub_private_repo=False,\r\nhub_strategy=every_save,\r\nhub_token=<HUB_TOKEN>,\r\nignore_data_skip=False,\r\ninclude_inputs_for_metrics=True,\r\njit_mode_eval=False,\r\nlabel_names=None,\r\nlabel_smoothing_factor=0.1,\r\nlearning_rate=1e-05,\r\nlength_column_name=length,\r\nload_best_model_at_end=True,\r\nlocal_rank=-1,\r\nlog_level=-1,\r\nlog_level_replica=-1,\r\nlog_on_each_node=True,\r\nlogging_dir=/content/AMRBART/fine-tune/outputs/LDC2020-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp/logs,\r\nlogging_first_step=True,\r\nlogging_nan_inf_filter=True,\r\nlogging_steps=20,\r\nlogging_strategy=steps,\r\nlr_scheduler_type=polynomial,\r\nmax_grad_norm=0.0,\r\nmax_steps=-1,\r\nmetric_for_best_model=eval_smatch,\r\nmp_parameters=,\r\nno_cuda=False,\r\nnum_train_epochs=30.0,\r\noptim=adamw_hf,\r\noutput_dir=/content/AMRBART/fine-tune/outputs/LDC2020-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp,\r\noverwrite_output_dir=True,\r\npast_index=-1,\r\nper_device_eval_batch_size=8,\r\nper_device_train_batch_size=16,\r\npredict_with_generate=True,\r\nprediction_loss_only=False,\r\npush_to_hub=False,\r\npush_to_hub_model_id=None,\r\npush_to_hub_organization=None,\r\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\r\nray_scope=last,\r\nremove_unused_columns=True,\r\nreport_to=['tensorboard'],\r\nresume_from_checkpoint=None,\r\nrun_name=/content/AMRBART/fine-tune/outputs/LDC2020-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp,\r\nsave_on_each_node=False,\r\nsave_steps=500,\r\nsave_strategy=epoch,\r\nsave_total_limit=1,\r\nseed=42,\r\nsharded_ddp=[],\r\nskip_memory_metrics=True,\r\nsmart_init=False,\r\nsortish_sampler=False,\r\ntask=text2amr,\r\ntf32=None,\r\ntorchdynamo=None,\r\ntpu_metrics_debug=False,\r\ntpu_num_cores=None,\r\nuse_ipex=False,\r\nuse_legacy_prediction_loop=False,\r\nwarmup_ratio=0.0,\r\nwarmup_steps=200,\r\nweight_decay=0.01,\r\nxpu_backend=None,\r\n)\r\n[INFO|hub.py:600] 2024-08-22 04:46:54,403 >> https://huggingface.co/facebook/bart-base/resolve/main/config.json not found in cache or force_download set to True, downloading to /mnt/nfs-storage/data/.cache/tmpg30ww9e4\r\n\rDownloading config.json:   0%|          | 0.00/1.68k [00:00<?, ?B/s]\rDownloading config.json: 100%|██████████| 1.68k/1.68k [00:00<00:00, 9.15MB/s]\r\n[INFO|hub.py:613] 2024-08-22 04:46:54,446 >> storing https://huggingface.co/facebook/bart-base/resolve/main/config.json in cache at /mnt/nfs-storage/data/.cache/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\r\n[INFO|hub.py:621] 2024-08-22 04:46:54,447 >> creating metadata file for /mnt/nfs-storage/data/.cache/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\r\n[INFO|configuration_utils.py:681] 2024-08-22 04:46:54,449 >> loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /mnt/nfs-storage/data/.cache/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\r\n[INFO|configuration_utils.py:730] 2024-08-22 04:46:54,450 >> Model config BartConfig {\r\n  \"_name_or_path\": \"facebook/bart-base\",\r\n  \"activation_dropout\": 0.1,\r\n  \"activation_function\": \"gelu\",\r\n  \"add_bias_logits\": false,\r\n  \"add_final_layer_norm\": false,\r\n  \"architectures\": [\r\n    \"BartModel\"\r\n  ],\r\n  \"attention_dropout\": 0.1,\r\n  \"bos_token_id\": 0,\r\n  \"classif_dropout\": 0.1,\r\n  \"classifier_dropout\": 0.0,\r\n  \"d_model\": 768,\r\n  \"decoder_attention_heads\": 12,\r\n  \"decoder_ffn_dim\": 3072,\r\n  \"decoder_layerdrop\": 0.0,\r\n  \"decoder_layers\": 6,\r\n  \"decoder_start_token_id\": 2,\r\n  \"dropout\": 0.1,\r\n  \"early_stopping\": true,\r\n  \"encoder_attention_heads\": 12,\r\n  \"encoder_ffn_dim\": 3072,\r\n  \"encoder_layerdrop\": 0.0,\r\n  \"encoder_layers\": 6,\r\n  \"eos_token_id\": 2,\r\n  \"forced_bos_token_id\": 0,\r\n  \"forced_eos_token_id\": 2,\r\n  \"gradient_checkpointing\": false,\r\n  \"id2label\": {\r\n    \"0\": \"LABEL_0\",\r\n    \"1\": \"LABEL_1\",\r\n    \"2\": \"LABEL_2\"\r\n  },\r\n  \"init_std\": 0.02,\r\n  \"is_encoder_decoder\": true,\r\n  \"label2id\": {\r\n    \"LABEL_0\": 0,\r\n    \"LABEL_1\": 1,\r\n    \"LABEL_2\": 2\r\n  },\r\n  \"max_position_embeddings\": 1024,\r\n  \"model_type\": \"bart\",\r\n  \"no_repeat_ngram_size\": 3,\r\n  \"normalize_before\": false,\r\n  \"normalize_embedding\": true,\r\n  \"num_beams\": 4,\r\n  \"num_hidden_layers\": 6,\r\n  \"pad_token_id\": 1,\r\n  \"scale_embedding\": false,\r\n  \"task_specific_params\": {\r\n    \"summarization\": {\r\n      \"length_penalty\": 1.0,\r\n      \"max_length\": 128,\r\n      \"min_length\": 12,\r\n      \"num_beams\": 4\r\n    },\r\n    \"summarization_cnn\": {\r\n      \"length_penalty\": 2.0,\r\n      \"max_length\": 142,\r\n      \"min_length\": 56,\r\n      \"num_beams\": 4\r\n    },\r\n    \"summarization_xsum\": {\r\n      \"length_penalty\": 1.0,\r\n      \"max_length\": 62,\r\n      \"min_length\": 11,\r\n      \"num_beams\": 6\r\n    }\r\n  },\r\n  \"torch_dtype\": \"float32\",\r\n  \"transformers_version\": \"4.21.3\",\r\n  \"use_cache\": true,\r\n  \"vocab_size\": 50265\r\n}\r\n\r\n[INFO|hub.py:600] 2024-08-22 04:46:54,488 >> https://huggingface.co/facebook/bart-large/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /mnt/nfs-storage/data/.cache/tmpe8qosbu2\r\n\rDownloading vocab.json:   0%|          | 0.00/878k [00:00<?, ?B/s]\rDownloading vocab.json: 100%|██████████| 878k/878k [00:00<00:00, 16.6MB/s]\r\n[INFO|hub.py:613] 2024-08-22 04:46:54,580 >> storing https://huggingface.co/facebook/bart-large/resolve/main/vocab.json in cache at /mnt/nfs-storage/data/.cache/0d6fc8b2ef1860c1f8f0baff4b021e3426cc7d11b153f98e563b799603ee2f25.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\r\n[INFO|hub.py:621] 2024-08-22 04:46:54,581 >> creating metadata file for /mnt/nfs-storage/data/.cache/0d6fc8b2ef1860c1f8f0baff4b021e3426cc7d11b153f98e563b799603ee2f25.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\r\n[INFO|hub.py:600] 2024-08-22 04:46:54,620 >> https://huggingface.co/facebook/bart-large/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /mnt/nfs-storage/data/.cache/tmp5gj1_fkk\r\n\rDownloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]\rDownloading merges.txt: 100%|██████████| 446k/446k [00:00<00:00, 17.0MB/s]\r\n[INFO|hub.py:613] 2024-08-22 04:46:54,688 >> storing https://huggingface.co/facebook/bart-large/resolve/main/merges.txt in cache at /mnt/nfs-storage/data/.cache/6e75e35f0bdd15870c98387e13b93a8e100237eb33ad99c36277a0562bd6d850.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\r\n[INFO|hub.py:621] 2024-08-22 04:46:54,688 >> creating metadata file for /mnt/nfs-storage/data/.cache/6e75e35f0bdd15870c98387e13b93a8e100237eb33ad99c36277a0562bd6d850.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\r\n[INFO|hub.py:600] 2024-08-22 04:46:54,835 >> https://huggingface.co/facebook/bart-large/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /mnt/nfs-storage/data/.cache/tmpoww2v3__\r\n\rDownloading tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]\rDownloading tokenizer_config.json: 100%|██████████| 26.0/26.0 [00:00<00:00, 192kB/s]\r\n[INFO|hub.py:613] 2024-08-22 04:46:54,872 >> storing https://huggingface.co/facebook/bart-large/resolve/main/tokenizer_config.json in cache at /mnt/nfs-storage/data/.cache/1abf196c889c24daca2909359ca2090e5fcbfa21a9ea36d763f70adbafb500d7.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8\r\n[INFO|hub.py:621] 2024-08-22 04:46:54,872 >> creating metadata file for /mnt/nfs-storage/data/.cache/1abf196c889c24daca2909359ca2090e5fcbfa21a9ea36d763f70adbafb500d7.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8\r\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:46:54,874 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/vocab.json from cache at /mnt/nfs-storage/data/.cache/0d6fc8b2ef1860c1f8f0baff4b021e3426cc7d11b153f98e563b799603ee2f25.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\r\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:46:54,874 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/merges.txt from cache at /mnt/nfs-storage/data/.cache/6e75e35f0bdd15870c98387e13b93a8e100237eb33ad99c36277a0562bd6d850.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\r\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:46:54,874 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/added_tokens.json from cache at None\r\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:46:54,874 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/special_tokens_map.json from cache at None\r\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:46:54,874 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/tokenizer_config.json from cache at /mnt/nfs-storage/data/.cache/1abf196c889c24daca2909359ca2090e5fcbfa21a9ea36d763f70adbafb500d7.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8\r\n[INFO|hub.py:600] 2024-08-22 04:46:54,913 >> https://huggingface.co/facebook/bart-large/resolve/main/config.json not found in cache or force_download set to True, downloading to /mnt/nfs-storage/data/.cache/tmpfigpv4i6\r\n\rDownloading config.json:   0%|          | 0.00/1.59k [00:00<?, ?B/s]\rDownloading config.json: 100%|██████████| 1.59k/1.59k [00:00<00:00, 10.3MB/s]\r\n[INFO|hub.py:613] 2024-08-22 04:46:54,953 >> storing https://huggingface.co/facebook/bart-large/resolve/main/config.json in cache at /mnt/nfs-storage/data/.cache/3f12fb71b844fcb7d591fdd4e55027da90d7b5dd6aa5430ad00ec6d76585f26c.bc22f15dc7ba074ee0a60bdd34c5f2fe3b6d746f89e765303376c51aff04e260\r\n[INFO|hub.py:621] 2024-08-22 04:46:54,953 >> creating metadata file for /mnt/nfs-storage/data/.cache/3f12fb71b844fcb7d591fdd4e55027da90d7b5dd6aa5430ad00ec6d76585f26c.bc22f15dc7ba074ee0a60bdd34c5f2fe3b6d746f89e765303376c51aff04e260\r\n[INFO|configuration_utils.py:681] 2024-08-22 04:46:54,955 >> loading configuration file https://huggingface.co/facebook/bart-large/resolve/main/config.json from cache at /mnt/nfs-storage/data/.cache/3f12fb71b844fcb7d591fdd4e55027da90d7b5dd6aa5430ad00ec6d76585f26c.bc22f15dc7ba074ee0a60bdd34c5f2fe3b6d746f89e765303376c51aff04e260\r\n[INFO|configuration_utils.py:730] 2024-08-22 04:46:54,956 >> Model config BartConfig {\r\n  \"_name_or_path\": \"facebook/bart-large\",\r\n  \"activation_dropout\": 0.1,\r\n  \"activation_function\": \"gelu\",\r\n  \"add_bias_logits\": false,\r\n  \"add_final_layer_norm\": false,\r\n  \"architectures\": [\r\n    \"BartModel\"\r\n  ],\r\n  \"attention_dropout\": 0.1,\r\n  \"bos_token_id\": 0,\r\n  \"classif_dropout\": 0.1,\r\n  \"classifier_dropout\": 0.0,\r\n  \"d_model\": 1024,\r\n  \"decoder_attention_heads\": 16,\r\n  \"decoder_ffn_dim\": 4096,\r\n  \"decoder_layerdrop\": 0.0,\r\n  \"decoder_layers\": 12,\r\n  \"decoder_start_token_id\": 2,\r\n  \"dropout\": 0.1,\r\n  \"early_stopping\": true,\r\n  \"encoder_attention_heads\": 16,\r\n  \"encoder_ffn_dim\": 4096,\r\n  \"encoder_layerdrop\": 0.0,\r\n  \"encoder_layers\": 12,\r\n  \"eos_token_id\": 2,\r\n  \"forced_bos_token_id\": 0,\r\n  \"forced_eos_token_id\": 2,\r\n  \"gradient_checkpointing\": false,\r\n  \"id2label\": {\r\n    \"0\": \"LABEL_0\",\r\n    \"1\": \"LABEL_1\",\r\n    \"2\": \"LABEL_2\"\r\n  },\r\n  \"init_std\": 0.02,\r\n  \"is_encoder_decoder\": true,\r\n  \"label2id\": {\r\n    \"LABEL_0\": 0,\r\n    \"LABEL_1\": 1,\r\n    \"LABEL_2\": 2\r\n  },\r\n  \"max_position_embeddings\": 1024,\r\n  \"model_type\": \"bart\",\r\n  \"no_repeat_ngram_size\": 3,\r\n  \"normalize_before\": false,\r\n  \"num_beams\": 4,\r\n  \"num_hidden_layers\": 12,\r\n  \"pad_token_id\": 1,\r\n  \"scale_embedding\": false,\r\n  \"task_specific_params\": {\r\n    \"summarization\": {\r\n      \"length_penalty\": 1.0,\r\n      \"max_length\": 128,\r\n      \"min_length\": 12,\r\n      \"num_beams\": 4\r\n    },\r\n    \"summarization_cnn\": {\r\n      \"length_penalty\": 2.0,\r\n      \"max_length\": 142,\r\n      \"min_length\": 56,\r\n      \"num_beams\": 4\r\n    },\r\n    \"summarization_xsum\": {\r\n      \"length_penalty\": 1.0,\r\n      \"max_length\": 62,\r\n      \"min_length\": 11,\r\n      \"num_beams\": 6\r\n    }\r\n  },\r\n  \"transformers_version\": \"4.21.3\",\r\n  \"use_cache\": true,\r\n  \"vocab_size\": 50265\r\n}\r\n\r\n[WARNING|tokenization_utils_base.py:1896] 2024-08-22 04:46:54,956 >> The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \r\nThe tokenizer class you load from this checkpoint is 'BartTokenizer'. \r\nThe class this function is called from is 'AMRBartTokenizer'.\r\nAdded 2965 AMR tokens\r\n[INFO|hub.py:600] 2024-08-22 04:46:55,136 >> https://huggingface.co/facebook/bart-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /mnt/nfs-storage/data/.cache/tmpd4jtggi3\r\n\rDownloading pytorch_model.bin:   0%|          | 0.00/532M [00:00<?, ?B/s]\rDownloading pytorch_model.bin:   1%|          | 6.05M/532M [00:00<00:08, 63.4MB/s]\rDownloading pytorch_model.bin:   2%|▏         | 12.3M/532M [00:00<00:08, 64.7MB/s]\rDownloading pytorch_model.bin:   3%|▎         | 18.5M/532M [00:00<00:08, 64.7MB/s]\rDownloading pytorch_model.bin:   5%|▍         | 24.9M/532M [00:00<00:08, 65.6MB/s]\rDownloading pytorch_model.bin:   6%|▌         | 31.1M/532M [00:00<00:08, 64.8MB/s]\rDownloading pytorch_model.bin:   7%|▋         | 37.5M/532M [00:00<00:07, 65.6MB/s]\rDownloading pytorch_model.bin:   8%|▊         | 43.8M/532M [00:00<00:07, 64.7MB/s]\rDownloading pytorch_model.bin:   9%|▉         | 50.0M/532M [00:00<00:08, 62.1MB/s]\rDownloading pytorch_model.bin:  11%|█         | 56.2M/532M [00:00<00:07, 63.2MB/s]\rDownloading pytorch_model.bin:  12%|█▏        | 62.3M/532M [00:01<00:07, 63.4MB/s]\rDownloading pytorch_model.bin:  13%|█▎        | 68.4M/532M [00:01<00:07, 63.3MB/s]\rDownloading pytorch_model.bin:  14%|█▍        | 74.6M/532M [00:01<00:07, 63.6MB/s]\rDownloading pytorch_model.bin:  15%|█▌        | 80.7M/532M [00:01<00:07, 63.9MB/s]\rDownloading pytorch_model.bin:  16%|█▋        | 86.8M/532M [00:01<00:07, 63.8MB/s]\rDownloading pytorch_model.bin:  17%|█▋        | 92.9M/532M [00:01<00:07, 63.9MB/s]\rDownloading pytorch_model.bin:  19%|█▊        | 99.5M/532M [00:01<00:06, 65.2MB/s]\rDownloading pytorch_model.bin:  20%|█▉        | 106M/532M [00:01<00:06, 65.2MB/s] \rDownloading pytorch_model.bin:  21%|██        | 112M/532M [00:01<00:06, 64.7MB/s]\rDownloading pytorch_model.bin:  22%|██▏       | 118M/532M [00:01<00:06, 63.2MB/s]\rDownloading pytorch_model.bin:  23%|██▎       | 124M/532M [00:02<00:06, 61.9MB/s]\rDownloading pytorch_model.bin:  25%|██▍       | 130M/532M [00:02<00:06, 63.0MB/s]\rDownloading pytorch_model.bin:  26%|██▌       | 137M/532M [00:02<00:06, 63.5MB/s]\rDownloading pytorch_model.bin:  27%|██▋       | 143M/532M [00:02<00:06, 64.5MB/s]\rDownloading pytorch_model.bin:  28%|██▊       | 149M/532M [00:02<00:06, 64.8MB/s]\rDownloading pytorch_model.bin:  29%|██▉       | 156M/532M [00:02<00:06, 65.4MB/s]\rDownloading pytorch_model.bin:  30%|███       | 162M/532M [00:02<00:05, 66.6MB/s]\rDownloading pytorch_model.bin:  32%|███▏      | 169M/532M [00:02<00:05, 67.2MB/s]\rDownloading pytorch_model.bin:  33%|███▎      | 175M/532M [00:02<00:05, 65.4MB/s]\rDownloading pytorch_model.bin:  34%|███▍      | 181M/532M [00:02<00:05, 64.0MB/s]\rDownloading pytorch_model.bin:  35%|███▌      | 188M/532M [00:03<00:05, 64.0MB/s]\rDownloading pytorch_model.bin:  36%|███▋      | 194M/532M [00:03<00:05, 64.5MB/s]\rDownloading pytorch_model.bin:  38%|███▊      | 200M/532M [00:03<00:05, 65.3MB/s]\rDownloading pytorch_model.bin:  39%|███▉      | 207M/532M [00:03<00:05, 65.8MB/s]\rDownloading pytorch_model.bin:  40%|████      | 213M/532M [00:03<00:05, 66.1MB/s]\rDownloading pytorch_model.bin:  41%|████▏     | 220M/532M [00:03<00:04, 67.0MB/s]\rDownloading pytorch_model.bin:  42%|████▏     | 226M/532M [00:03<00:04, 66.7MB/s]\rDownloading pytorch_model.bin:  44%|████▎     | 232M/532M [00:03<00:04, 66.7MB/s]\rDownloading pytorch_model.bin:  45%|████▍     | 239M/532M [00:03<00:04, 67.0MB/s]\rDownloading pytorch_model.bin:  46%|████▌     | 245M/532M [00:03<00:04, 66.5MB/s]\rDownloading pytorch_model.bin:  47%|████▋     | 252M/532M [00:04<00:04, 66.9MB/s]\rDownloading pytorch_model.bin:  49%|████▊     | 258M/532M [00:04<00:04, 67.1MB/s]\rDownloading pytorch_model.bin:  50%|████▉     | 264M/532M [00:04<00:04, 66.6MB/s]\rDownloading pytorch_model.bin:  51%|█████     | 271M/532M [00:04<00:04, 66.5MB/s]\rDownloading pytorch_model.bin:  52%|█████▏    | 277M/532M [00:04<00:04, 66.6MB/s]\rDownloading pytorch_model.bin:  53%|█████▎    | 284M/532M [00:04<00:03, 67.1MB/s]\rDownloading pytorch_model.bin:  55%|█████▍    | 290M/532M [00:04<00:03, 66.6MB/s]\rDownloading pytorch_model.bin:  56%|█████▌    | 296M/532M [00:04<00:03, 66.5MB/s]\rDownloading pytorch_model.bin:  57%|█████▋    | 303M/532M [00:04<00:03, 66.9MB/s]\rDownloading pytorch_model.bin:  58%|█████▊    | 309M/532M [00:04<00:03, 66.8MB/s]\rDownloading pytorch_model.bin:  59%|█████▉    | 316M/532M [00:05<00:03, 66.6MB/s]\rDownloading pytorch_model.bin:  61%|██████    | 322M/532M [00:05<00:03, 65.9MB/s]\rDownloading pytorch_model.bin:  62%|██████▏   | 328M/532M [00:05<00:03, 65.7MB/s]\rDownloading pytorch_model.bin:  63%|██████▎   | 335M/532M [00:05<00:03, 66.3MB/s]\rDownloading pytorch_model.bin:  64%|██████▍   | 341M/532M [00:05<00:02, 66.8MB/s]\rDownloading pytorch_model.bin:  65%|██████▌   | 348M/532M [00:05<00:02, 67.6MB/s]\rDownloading pytorch_model.bin:  67%|██████▋   | 354M/532M [00:05<00:02, 67.5MB/s]\rDownloading pytorch_model.bin:  68%|██████▊   | 361M/532M [00:05<00:02, 65.5MB/s]\rDownloading pytorch_model.bin:  69%|██████▉   | 367M/532M [00:05<00:02, 64.4MB/s]\rDownloading pytorch_model.bin:  70%|███████   | 373M/532M [00:05<00:02, 64.3MB/s]\rDownloading pytorch_model.bin:  71%|███████▏  | 379M/532M [00:06<00:02, 64.3MB/s]\rDownloading pytorch_model.bin:  72%|███████▏  | 385M/532M [00:06<00:02, 63.1MB/s]\rDownloading pytorch_model.bin:  74%|███████▎  | 392M/532M [00:06<00:02, 64.0MB/s]\rDownloading pytorch_model.bin:  75%|███████▍  | 398M/532M [00:06<00:02, 65.0MB/s]\rDownloading pytorch_model.bin:  76%|███████▌  | 404M/532M [00:06<00:02, 65.2MB/s]\rDownloading pytorch_model.bin:  77%|███████▋  | 411M/532M [00:06<00:01, 65.1MB/s]\rDownloading pytorch_model.bin:  78%|███████▊  | 417M/532M [00:06<00:01, 65.1MB/s]\rDownloading pytorch_model.bin:  80%|███████▉  | 423M/532M [00:06<00:01, 65.4MB/s]\rDownloading pytorch_model.bin:  81%|████████  | 430M/532M [00:06<00:01, 66.1MB/s]\rDownloading pytorch_model.bin:  82%|████████▏ | 436M/532M [00:07<00:01, 65.5MB/s]\rDownloading pytorch_model.bin:  83%|████████▎ | 442M/532M [00:07<00:01, 64.0MB/s]\rDownloading pytorch_model.bin:  84%|████████▍ | 448M/532M [00:07<00:01, 63.8MB/s]\rDownloading pytorch_model.bin:  85%|████████▌ | 454M/532M [00:07<00:01, 63.6MB/s]\rDownloading pytorch_model.bin:  87%|████████▋ | 460M/532M [00:07<00:01, 63.2MB/s]\rDownloading pytorch_model.bin:  88%|████████▊ | 467M/532M [00:07<00:01, 64.4MB/s]\rDownloading pytorch_model.bin:  89%|████████▉ | 473M/532M [00:07<00:00, 64.6MB/s]\rDownloading pytorch_model.bin:  90%|█████████ | 479M/532M [00:07<00:00, 64.7MB/s]\rDownloading pytorch_model.bin:  91%|█████████▏| 486M/532M [00:07<00:00, 65.4MB/s]\rDownloading pytorch_model.bin:  93%|█████████▎| 492M/532M [00:07<00:00, 66.5MB/s]\rDownloading pytorch_model.bin:  94%|█████████▍| 499M/532M [00:08<00:00, 67.2MB/s]\rDownloading pytorch_model.bin:  95%|█████████▍| 505M/532M [00:08<00:00, 67.0MB/s]\rDownloading pytorch_model.bin:  96%|█████████▌| 512M/532M [00:08<00:00, 66.9MB/s]\rDownloading pytorch_model.bin:  97%|█████████▋| 518M/532M [00:08<00:00, 67.2MB/s]\rDownloading pytorch_model.bin:  99%|█████████▊| 525M/532M [00:08<00:00, 66.1MB/s]\rDownloading pytorch_model.bin: 100%|█████████▉| 531M/532M [00:08<00:00, 66.2MB/s]\rDownloading pytorch_model.bin: 100%|██████████| 532M/532M [00:08<00:00, 65.3MB/s]\r\n[INFO|hub.py:613] 2024-08-22 04:47:03,771 >> storing https://huggingface.co/facebook/bart-base/resolve/main/pytorch_model.bin in cache at /mnt/nfs-storage/data/.cache/486355ec722ef05fd480e999d4c763be56549ae930f6a3742ee721a5d2a05647.f2f355ad2775769afc60592b43a46d72ca548375e3a1d65f381a751e711cbadd\r\n[INFO|hub.py:621] 2024-08-22 04:47:03,771 >> creating metadata file for /mnt/nfs-storage/data/.cache/486355ec722ef05fd480e999d4c763be56549ae930f6a3742ee721a5d2a05647.f2f355ad2775769afc60592b43a46d72ca548375e3a1d65f381a751e711cbadd\r\n[INFO|modeling_utils.py:2041] 2024-08-22 04:47:03,773 >> loading weights file https://huggingface.co/facebook/bart-base/resolve/main/pytorch_model.bin from cache at /mnt/nfs-storage/data/.cache/486355ec722ef05fd480e999d4c763be56549ae930f6a3742ee721a5d2a05647.f2f355ad2775769afc60592b43a46d72ca548375e3a1d65f381a751e711cbadd\r\n[INFO|modeling_utils.py:2435] 2024-08-22 04:47:06,539 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\r\n\r\n[INFO|modeling_utils.py:2443] 2024-08-22 04:47:06,539 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\r\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\r\nDataset cache dir: /mnt/nfs-storage/data/.cache\r\n/usr/local/lib/python3.10/site-packages/datasets/load.py:929: FutureWarning: The repository for data contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /content/AMRBART/fine-tune/data_interface/data.py\r\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\r\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\r\n  warnings.warn(\r\nTraceback (most recent call last):\r\n  File \"/content/AMRBART/fine-tune/main.py\", line 506, in <module>\r\n    main()\r\n  File \"/content/AMRBART/fine-tune/main.py\", line 194, in main\r\n    raw_datasets = DataSetCate(tokenizer, data_args, model_args)\r\n  File \"/content/AMRBART/fine-tune/data_interface/dataset.py\", line 63, in __init__\r\n    self.datasets = load_dataset(\r\n  File \"/usr/local/lib/python3.10/site-packages/datasets/load.py\", line 2587, in load_dataset\r\n    builder_instance = load_dataset_builder(\r\n  File \"/usr/local/lib/python3.10/site-packages/datasets/load.py\", line 2296, in load_dataset_builder\r\n    builder_instance: DatasetBuilder = builder_cls(\r\n  File \"/usr/local/lib/python3.10/site-packages/datasets/builder.py\", line 360, in __init__\r\n    data_files = DataFilesDict.from_patterns(\r\n  File \"/usr/local/lib/python3.10/site-packages/datasets/data_files.py\", line 715, in from_patterns\r\n    DataFilesList.from_patterns(\r\n  File \"/usr/local/lib/python3.10/site-packages/datasets/data_files.py\", line 620, in from_patterns\r\n    resolve_pattern(\r\n  File \"/usr/local/lib/python3.10/site-packages/datasets/data_files.py\", line 407, in resolve_pattern\r\n    raise FileNotFoundError(error_msg)\r\nFileNotFoundError: Unable to find '/content/AMRBART/fine-tune/data/LDC2020/train.jsonl'\r\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/AMRBART/fine-tune\n",
    "!bash train-AMRBART-large-AMRParsing.sh \"facebook/bart-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f64c0eb",
   "metadata": {},
   "source": [
    "### Step 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009b5a4f",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17646a0",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf80e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now the code asks for the data, so let's work on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa1feef",
   "metadata": {},
   "source": [
    "### Step 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950e96a9",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cd0fd1",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7262e0af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0194fa19",
   "metadata": {},
   "source": [
    "### Step 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9036b25",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf93e69",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09c2129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Download data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f11026",
   "metadata": {},
   "source": [
    "### Step 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8372a684",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dad591",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba5bf12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Requirement already satisfied: gdown in /usr/local/lib/python3.10/site-packages (5.1.0)\r\nCollecting gdown\r\n  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\r\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/site-packages (from gdown) (4.12.3)\r\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from gdown) (3.15.1)\r\nRequirement already satisfied: requests[socks] in /usr/local/lib/python3.10/site-packages (from gdown) (2.31.0)\r\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from gdown) (4.66.4)\r\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown) (2.1.1)\r\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown) (3.7)\r\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown) (2.0.7)\r\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.6.2)\r\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\r\nDownloading gdown-5.2.0-py3-none-any.whl (18 kB)\r\nInstalling collected packages: gdown\r\n  Attempting uninstall: gdown\r\n    Found existing installation: gdown 5.1.0\r\n    Uninstalling gdown-5.1.0:\r\n      Successfully uninstalled gdown-5.1.0\r\nSuccessfully installed gdown-5.2.0\r\n\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!# use gdown to download files\n",
    "!pip install gdown --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44808c6d",
   "metadata": {},
   "source": [
    "### Step 19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58239c77",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f772d01c",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28990e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\nRetrieving folder contents\n/content/AMRBART/fine-tune/data\nProcessing file 141pCNafqMmjx6uZ5IlNSHQmmdfTCqNLW dev.jsonl\nProcessing file 1vSSHzz3ns6DQzWjBhZkI37-einZQuZXb infer.txt\nProcessing file 1jjka6n4pbdbih3LUuJII-UYTloNgv_hp test.jsonl\nProcessing file 1TDkzzq6xOthZDC-UKuyHwLdeaok8D6GQ train.jsonl\nRetrieving folder contents completed\nBuilding directory structure\nBuilding directory structure completed\nDownloading...\nFrom: https://drive.google.com/uc?id=141pCNafqMmjx6uZ5IlNSHQmmdfTCqNLW\nTo: /content/AMRBART/fine-tune/data/AMRBART/dev.jsonl\n\r  0%|          | 0.00/8.22k [00:00<?, ?B/s]\r100%|██████████| 8.22k/8.22k [00:00<00:00, 14.7MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1vSSHzz3ns6DQzWjBhZkI37-einZQuZXb\nTo: /content/AMRBART/fine-tune/data/AMRBART/infer.txt\n\r  0%|          | 0.00/1.17k [00:00<?, ?B/s]\r100%|██████████| 1.17k/1.17k [00:00<00:00, 5.54MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1jjka6n4pbdbih3LUuJII-UYTloNgv_hp\nTo: /content/AMRBART/fine-tune/data/AMRBART/test.jsonl\n\r  0%|          | 0.00/6.17k [00:00<?, ?B/s]\r100%|██████████| 6.17k/6.17k [00:00<00:00, 19.8MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1TDkzzq6xOthZDC-UKuyHwLdeaok8D6GQ\nTo: /content/AMRBART/fine-tune/data/AMRBART/train.jsonl\n\r  0%|          | 0.00/7.86k [00:00<?, ?B/s]\r100%|██████████| 7.86k/7.86k [00:00<00:00, 26.2MB/s]\nDownload completed\n['/content/AMRBART/fine-tune/data/AMRBART/dev.jsonl',\n '/content/AMRBART/fine-tune/data/AMRBART/infer.txt',\n '/content/AMRBART/fine-tune/data/AMRBART/test.jsonl',\n '/content/AMRBART/fine-tune/data/AMRBART/train.jsonl']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "from pathlib import Path\n",
    "\n",
    "Path(\"/content/AMRBART/fine-tune/data\").mkdir(exist_ok=True)\n",
    "\n",
    "%cd /content/AMRBART/fine-tune/data\n",
    "gdown.download_folder(\"https://drive.google.com/drive/folders/1tgSf24gsTrSe6GRt-eSuz8vvB51Br9hG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360e4d46",
   "metadata": {},
   "source": [
    "### Step 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4c5086",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2731450",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417a8fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The script looks for a validation dataset named \"val.jsnol\". Assuming \"dev.jsonl\" contains the this dataset, I rename it accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ce72fe",
   "metadata": {},
   "source": [
    "### Step 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dde3fb7",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebee5dc0",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddcd7f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!mv AMRBART/dev.jsonl AMRBART/val.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83355687",
   "metadata": {},
   "source": [
    "### Step 22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0155d27f",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b13c2bc",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3bcad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The model expects sentences to be under the `sent` key, but in the downloaded datasets the key is `sentence`, so I replace it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed2d686",
   "metadata": {},
   "source": [
    "### Step 23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc4b015",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cf4fef",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28b683b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/AMRBART/fine-tune/data/AMRBART\n/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/AMRBART/fine-tune/data/AMRBART\n",
    "!sed -i 's/\"sentence\":/\"sent\":/' train.jsonl\n",
    "!sed -i 's/\"sentence\":/\"sent\":/' test.jsonl\n",
    "!sed -i 's/\"sentence\":/\"sent\":/' val.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bcd0da",
   "metadata": {},
   "source": [
    "### Step 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948b7f23",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50538336",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf8de03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Patch again the script, this time setting the correct `Dataset` variable and the number of epochs to `1`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b5c97c",
   "metadata": {},
   "source": [
    "### Step 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44124c7a",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2792779",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5101a79a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/AMRBART\n/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/AMRBART/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffde0e8",
   "metadata": {},
   "source": [
    "### Step 26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec930eda",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f85c8de",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08355d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\nexport CUDA_VISIBLE_DEVICES=0\nRootDir=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" >/dev/null 2>&1 && pwd )\"\n\nDataset=LDC2020\n#Dataset=LDC2017\nDataset=AMRBART\n\nBasePath=/mnt/nfs-storage/data        # change dir here\nDataPath=$RootDir/data/$Dataset\n\nModelCate=AMRBART-large\n\nMODEL=$1\nModelCache=$BasePath/.cache"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/AMRBART/fine-tune/train-AMRBART-large-AMRParsing.sh. CHANGING:\n",
    "\n",
    "\n",
    "\n",
    "Dataset=LDC2020\n",
    "#Dataset=LDC2017\n",
    "\n",
    "BasePath=/mnt/nfs-storage/data        # change dir here\n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "\n",
    "Dataset=LDC2020\n",
    "#Dataset=LDC2017\n",
    "Dataset=AMRBART\n",
    "\n",
    "BasePath=/mnt/nfs-storage/data        # change dir here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fd0aae",
   "metadata": {},
   "source": [
    "### Step 27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276802d2",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca5b025",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564fa222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n    --model_name_or_path $MODEL \\\n    --overwrite_output_dir \\\n    --unified_input True \\\n    --per_device_train_batch_size 16 \\\n    --per_device_eval_batch_size 8 \\\n    --gradient_accumulation_steps 1 \\\n    --learning_rate $lr \\\n    --optim \"adamw_hf\" \\\n    --lr_scheduler_type \"polynomial\" \\\n    --warmup_steps 200 \\\n    --num_train_epochs 1 \\\n    --early_stopping 10 \\\n    --max_source_length 128 \\\n    --max_target_length 128 \\\n    --val_max_target_length 128 \\\n    --generation_max_length 128 \\\n    --generation_num_beams 1 \\\n    --label_smoothing_factor 0.1 \\\n    --evaluation_strategy \"epoch\" \\\n    --weight_decay 0.01 \\\n    --max_grad_norm 0 \\\n    --max_steps -1 \\\n    --predict_with_generate \\\n    --smart_init False \\\n    --use_fast_tokenizer False \\"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/AMRBART/fine-tune/train-AMRBART-large-AMRParsing.sh. CHANGING:\n",
    "\n",
    "\n",
    "    --optim \"adamw_hf\" \\\n",
    "    --lr_scheduler_type \"polynomial\" \\\n",
    "    --warmup_steps 200 \\\n",
    "    --num_train_epochs 30 \\\n",
    "    --early_stopping 10 \\\n",
    "    --max_source_length 400 \\\n",
    "    --max_target_length 1024 \\\n",
    "    --val_max_target_length 1024 \\\n",
    "    --generation_max_length 1024 \\\n",
    "    --generation_num_beams 5 \\\n",
    "    --label_smoothing_factor 0.1 \\\n",
    "    --evaluation_strategy \"epoch\" \\\n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "    --optim \"adamw_hf\" \\\n",
    "    --lr_scheduler_type \"polynomial\" \\\n",
    "    --warmup_steps 200 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --early_stopping 10 \\\n",
    "    --max_source_length 128 \\\n",
    "    --max_target_length 128 \\\n",
    "    --val_max_target_length 128 \\\n",
    "    --generation_max_length 128 \\\n",
    "    --generation_num_beams 1 \\\n",
    "    --label_smoothing_factor 0.1 \\\n",
    "    --evaluation_strategy \"epoch\" \\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d883126a",
   "metadata": {},
   "source": [
    "### Step 28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e3d028",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8633d5",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebf8559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/AMRBART/fine-tune\n/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n08/22/2024 04:47:38 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\r\n08/22/2024 04:47:38 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\r\n_n_gpu=0,\r\nadafactor=False,\r\nadam_beta1=0.9,\r\nadam_beta2=0.999,\r\nadam_epsilon=1e-08,\r\nauto_find_batch_size=False,\r\nbf16=False,\r\nbf16_full_eval=False,\r\ndata_seed=None,\r\ndataloader_drop_last=False,\r\ndataloader_num_workers=8,\r\ndataloader_pin_memory=True,\r\nddp_bucket_cap_mb=None,\r\nddp_find_unused_parameters=False,\r\ndebug=[],\r\ndeepspeed=None,\r\ndisable_tqdm=False,\r\ndo_eval=True,\r\ndo_predict=True,\r\ndo_train=True,\r\nearly_stopping=10,\r\neval_accumulation_steps=None,\r\neval_dataloader_num_workers=2,\r\neval_delay=0,\r\neval_lenpen=1.0,\r\neval_steps=None,\r\nevaluation_strategy=epoch,\r\nfp16=False,\r\nfp16_backend=auto,\r\nfp16_full_eval=False,\r\nfp16_opt_level=O1,\r\nfsdp=[],\r\nfsdp_min_num_params=0,\r\nfull_determinism=False,\r\ngeneration_max_length=128,\r\ngeneration_num_beams=1,\r\ngradient_accumulation_steps=1,\r\ngradient_checkpointing=False,\r\ngreater_is_better=True,\r\ngroup_by_length=False,\r\nhalf_precision_backend=cuda_amp,\r\nhub_model_id=None,\r\nhub_private_repo=False,\r\nhub_strategy=every_save,\r\nhub_token=<HUB_TOKEN>,\r\nignore_data_skip=False,\r\ninclude_inputs_for_metrics=True,\r\njit_mode_eval=False,\r\nlabel_names=None,\r\nlabel_smoothing_factor=0.1,\r\nlearning_rate=1e-05,\r\nlength_column_name=length,\r\nload_best_model_at_end=True,\r\nlocal_rank=-1,\r\nlog_level=-1,\r\nlog_level_replica=-1,\r\nlog_on_each_node=True,\r\nlogging_dir=/content/AMRBART/fine-tune/outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp/logs,\r\nlogging_first_step=True,\r\nlogging_nan_inf_filter=True,\r\nlogging_steps=20,\r\nlogging_strategy=steps,\r\nlr_scheduler_type=polynomial,\r\nmax_grad_norm=0.0,\r\nmax_steps=-1,\r\nmetric_for_best_model=eval_smatch,\r\nmp_parameters=,\r\nno_cuda=False,\r\nnum_train_epochs=1.0,\r\noptim=adamw_hf,\r\noutput_dir=/content/AMRBART/fine-tune/outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp,\r\noverwrite_output_dir=True,\r\npast_index=-1,\r\nper_device_eval_batch_size=8,\r\nper_device_train_batch_size=16,\r\npredict_with_generate=True,\r\nprediction_loss_only=False,\r\npush_to_hub=False,\r\npush_to_hub_model_id=None,\r\npush_to_hub_organization=None,\r\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\r\nray_scope=last,\r\nremove_unused_columns=True,\r\nreport_to=['tensorboard'],\r\nresume_from_checkpoint=None,\r\nrun_name=/content/AMRBART/fine-tune/outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp,\r\nsave_on_each_node=False,\r\nsave_steps=500,\r\nsave_strategy=epoch,\r\nsave_total_limit=1,\r\nseed=42,\r\nsharded_ddp=[],\r\nskip_memory_metrics=True,\r\nsmart_init=False,\r\nsortish_sampler=False,\r\ntask=text2amr,\r\ntf32=None,\r\ntorchdynamo=None,\r\ntpu_metrics_debug=False,\r\ntpu_num_cores=None,\r\nuse_ipex=False,\r\nuse_legacy_prediction_loop=False,\r\nwarmup_ratio=0.0,\r\nwarmup_steps=200,\r\nweight_decay=0.01,\r\nxpu_backend=None,\r\n)\r\n[INFO|configuration_utils.py:681] 2024-08-22 04:47:38,245 >> loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /mnt/nfs-storage/data/.cache/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\r\n[INFO|configuration_utils.py:730] 2024-08-22 04:47:38,246 >> Model config BartConfig {\r\n  \"_name_or_path\": \"facebook/bart-base\",\r\n  \"activation_dropout\": 0.1,\r\n  \"activation_function\": \"gelu\",\r\n  \"add_bias_logits\": false,\r\n  \"add_final_layer_norm\": false,\r\n  \"architectures\": [\r\n    \"BartModel\"\r\n  ],\r\n  \"attention_dropout\": 0.1,\r\n  \"bos_token_id\": 0,\r\n  \"classif_dropout\": 0.1,\r\n  \"classifier_dropout\": 0.0,\r\n  \"d_model\": 768,\r\n  \"decoder_attention_heads\": 12,\r\n  \"decoder_ffn_dim\": 3072,\r\n  \"decoder_layerdrop\": 0.0,\r\n  \"decoder_layers\": 6,\r\n  \"decoder_start_token_id\": 2,\r\n  \"dropout\": 0.1,\r\n  \"early_stopping\": true,\r\n  \"encoder_attention_heads\": 12,\r\n  \"encoder_ffn_dim\": 3072,\r\n  \"encoder_layerdrop\": 0.0,\r\n  \"encoder_layers\": 6,\r\n  \"eos_token_id\": 2,\r\n  \"forced_bos_token_id\": 0,\r\n  \"forced_eos_token_id\": 2,\r\n  \"gradient_checkpointing\": false,\r\n  \"id2label\": {\r\n    \"0\": \"LABEL_0\",\r\n    \"1\": \"LABEL_1\",\r\n    \"2\": \"LABEL_2\"\r\n  },\r\n  \"init_std\": 0.02,\r\n  \"is_encoder_decoder\": true,\r\n  \"label2id\": {\r\n    \"LABEL_0\": 0,\r\n    \"LABEL_1\": 1,\r\n    \"LABEL_2\": 2\r\n  },\r\n  \"max_position_embeddings\": 1024,\r\n  \"model_type\": \"bart\",\r\n  \"no_repeat_ngram_size\": 3,\r\n  \"normalize_before\": false,\r\n  \"normalize_embedding\": true,\r\n  \"num_beams\": 4,\r\n  \"num_hidden_layers\": 6,\r\n  \"pad_token_id\": 1,\r\n  \"scale_embedding\": false,\r\n  \"task_specific_params\": {\r\n    \"summarization\": {\r\n      \"length_penalty\": 1.0,\r\n      \"max_length\": 128,\r\n      \"min_length\": 12,\r\n      \"num_beams\": 4\r\n    },\r\n    \"summarization_cnn\": {\r\n      \"length_penalty\": 2.0,\r\n      \"max_length\": 142,\r\n      \"min_length\": 56,\r\n      \"num_beams\": 4\r\n    },\r\n    \"summarization_xsum\": {\r\n      \"length_penalty\": 1.0,\r\n      \"max_length\": 62,\r\n      \"min_length\": 11,\r\n      \"num_beams\": 6\r\n    }\r\n  },\r\n  \"torch_dtype\": \"float32\",\r\n  \"transformers_version\": \"4.21.3\",\r\n  \"use_cache\": true,\r\n  \"vocab_size\": 50265\r\n}\r\n\r\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:47:38,505 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/vocab.json from cache at /mnt/nfs-storage/data/.cache/0d6fc8b2ef1860c1f8f0baff4b021e3426cc7d11b153f98e563b799603ee2f25.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\r\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:47:38,506 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/merges.txt from cache at /mnt/nfs-storage/data/.cache/6e75e35f0bdd15870c98387e13b93a8e100237eb33ad99c36277a0562bd6d850.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\r\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:47:38,506 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/added_tokens.json from cache at None\r\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:47:38,506 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/special_tokens_map.json from cache at None\r\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:47:38,506 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/tokenizer_config.json from cache at /mnt/nfs-storage/data/.cache/1abf196c889c24daca2909359ca2090e5fcbfa21a9ea36d763f70adbafb500d7.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8\r\n[INFO|configuration_utils.py:681] 2024-08-22 04:47:38,545 >> loading configuration file https://huggingface.co/facebook/bart-large/resolve/main/config.json from cache at /mnt/nfs-storage/data/.cache/3f12fb71b844fcb7d591fdd4e55027da90d7b5dd6aa5430ad00ec6d76585f26c.bc22f15dc7ba074ee0a60bdd34c5f2fe3b6d746f89e765303376c51aff04e260\r\n[INFO|configuration_utils.py:730] 2024-08-22 04:47:38,546 >> Model config BartConfig {\r\n  \"_name_or_path\": \"facebook/bart-large\",\r\n  \"activation_dropout\": 0.1,\r\n  \"activation_function\": \"gelu\",\r\n  \"add_bias_logits\": false,\r\n  \"add_final_layer_norm\": false,\r\n  \"architectures\": [\r\n    \"BartModel\"\r\n  ],\r\n  \"attention_dropout\": 0.1,\r\n  \"bos_token_id\": 0,\r\n  \"classif_dropout\": 0.1,\r\n  \"classifier_dropout\": 0.0,\r\n  \"d_model\": 1024,\r\n  \"decoder_attention_heads\": 16,\r\n  \"decoder_ffn_dim\": 4096,\r\n  \"decoder_layerdrop\": 0.0,\r\n  \"decoder_layers\": 12,\r\n  \"decoder_start_token_id\": 2,\r\n  \"dropout\": 0.1,\r\n  \"early_stopping\": true,\r\n  \"encoder_attention_heads\": 16,\r\n  \"encoder_ffn_dim\": 4096,\r\n  \"encoder_layerdrop\": 0.0,\r\n  \"encoder_layers\": 12,\r\n  \"eos_token_id\": 2,\r\n  \"forced_bos_token_id\": 0,\r\n  \"forced_eos_token_id\": 2,\r\n  \"gradient_checkpointing\": false,\r\n  \"id2label\": {\r\n    \"0\": \"LABEL_0\",\r\n    \"1\": \"LABEL_1\",\r\n    \"2\": \"LABEL_2\"\r\n  },\r\n  \"init_std\": 0.02,\r\n  \"is_encoder_decoder\": true,\r\n  \"label2id\": {\r\n    \"LABEL_0\": 0,\r\n    \"LABEL_1\": 1,\r\n    \"LABEL_2\": 2\r\n  },\r\n  \"max_position_embeddings\": 1024,\r\n  \"model_type\": \"bart\",\r\n  \"no_repeat_ngram_size\": 3,\r\n  \"normalize_before\": false,\r\n  \"num_beams\": 4,\r\n  \"num_hidden_layers\": 12,\r\n  \"pad_token_id\": 1,\r\n  \"scale_embedding\": false,\r\n  \"task_specific_params\": {\r\n    \"summarization\": {\r\n      \"length_penalty\": 1.0,\r\n      \"max_length\": 128,\r\n      \"min_length\": 12,\r\n      \"num_beams\": 4\r\n    },\r\n    \"summarization_cnn\": {\r\n      \"length_penalty\": 2.0,\r\n      \"max_length\": 142,\r\n      \"min_length\": 56,\r\n      \"num_beams\": 4\r\n    },\r\n    \"summarization_xsum\": {\r\n      \"length_penalty\": 1.0,\r\n      \"max_length\": 62,\r\n      \"min_length\": 11,\r\n      \"num_beams\": 6\r\n    }\r\n  },\r\n  \"transformers_version\": \"4.21.3\",\r\n  \"use_cache\": true,\r\n  \"vocab_size\": 50265\r\n}\r\n\r\n[WARNING|tokenization_utils_base.py:1896] 2024-08-22 04:47:38,546 >> The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \r\nThe tokenizer class you load from this checkpoint is 'BartTokenizer'. \r\nThe class this function is called from is 'AMRBartTokenizer'.\r\nAdded 2965 AMR tokens\r\n[INFO|modeling_utils.py:2041] 2024-08-22 04:47:38,708 >> loading weights file https://huggingface.co/facebook/bart-base/resolve/main/pytorch_model.bin from cache at /mnt/nfs-storage/data/.cache/486355ec722ef05fd480e999d4c763be56549ae930f6a3742ee721a5d2a05647.f2f355ad2775769afc60592b43a46d72ca548375e3a1d65f381a751e711cbadd\r\n[INFO|modeling_utils.py:2435] 2024-08-22 04:47:40,976 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\r\n\r\n[INFO|modeling_utils.py:2443] 2024-08-22 04:47:40,976 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\r\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\r\nDataset cache dir: /mnt/nfs-storage/data/.cache\r\n/usr/local/lib/python3.10/site-packages/datasets/load.py:929: FutureWarning: The repository for data contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /content/AMRBART/fine-tune/data_interface/data.py\r\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\r\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\r\n  warnings.warn(\r\nUsing custom data configuration default-0296a57fd4e8d68c\r\n08/22/2024 04:47:41 - INFO - datasets.builder - Using custom data configuration default-0296a57fd4e8d68c\r\nLoading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/data/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5\r\n08/22/2024 04:47:41 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/data/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5\r\nGenerating dataset data (/content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-0296a57fd4e8d68c/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5)\r\n08/22/2024 04:47:41 - INFO - datasets.builder - Generating dataset data (/content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-0296a57fd4e8d68c/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5)\r\nDownloading and preparing dataset data/default to /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-0296a57fd4e8d68c/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5...\r\n08/22/2024 04:47:41 - INFO - datasets.builder - Downloading and preparing dataset data/default to /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-0296a57fd4e8d68c/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5...\r\nGenerating train split\r\n08/22/2024 04:47:41 - INFO - datasets.builder - Generating train split\r\n\rGenerating train split: 0 examples [00:00, ? examples/s]\rGenerating train split: 10 examples [00:00, 640.40 examples/s]\r\nGenerating validation split\r\n08/22/2024 04:47:41 - INFO - datasets.builder - Generating validation split\r\n\rGenerating validation split: 0 examples [00:00, ? examples/s]\rGenerating validation split: 10 examples [00:00, 4993.81 examples/s]\r\nGenerating test split\r\n08/22/2024 04:47:41 - INFO - datasets.builder - Generating test split\r\n\rGenerating test split: 0 examples [00:00, ? examples/s]\rGenerating test split: 10 examples [00:00, 4977.81 examples/s]\r\nUnable to verify splits sizes.\r\n08/22/2024 04:47:41 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\r\nDataset data downloaded and prepared to /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-0296a57fd4e8d68c/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5. Subsequent calls will reuse this data.\r\n08/22/2024 04:47:41 - INFO - datasets.builder - Dataset data downloaded and prepared to /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-0296a57fd4e8d68c/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5. Subsequent calls will reuse this data.\r\ndatasets: DatasetDict({\r\n    train: Dataset({\r\n        features: ['src', 'tgt'],\r\n        num_rows: 10\r\n    })\r\n    validation: Dataset({\r\n        features: ['src', 'tgt'],\r\n        num_rows: 10\r\n    })\r\n    test: Dataset({\r\n        features: ['src', 'tgt'],\r\n        num_rows: 10\r\n    })\r\n})\r\ncolums: ['src', 'tgt']\r\n\rRunning tokenizer on train dataset:   0%|          | 0/10 [00:00<?, ? examples/s]Caching processed dataset at /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-0296a57fd4e8d68c/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5/cache-c1d8d623c8dbf300.arrow\r\n08/22/2024 04:47:42 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-0296a57fd4e8d68c/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5/cache-c1d8d623c8dbf300.arrow\r\n\rRunning tokenizer on train dataset: 100%|██████████| 10/10 [00:00<00:00, 354.90 examples/s]\r\nSaving cached train data ...\r\n\rSaving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s]\rSaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 3032.54 examples/s]\rSaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2892.22 examples/s]\r\n\rRunning tokenizer on validation dataset:   0%|          | 0/10 [00:00<?, ? examples/s]Caching processed dataset at /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-0296a57fd4e8d68c/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5/cache-93447e7651570c98.arrow\r\n08/22/2024 04:47:44 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-0296a57fd4e8d68c/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5/cache-93447e7651570c98.arrow\r\n\rRunning tokenizer on validation dataset: 100%|██████████| 10/10 [00:00<00:00, 403.17 examples/s]\r\nSaving cached validation data ...\r\n\rSaving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s]\rSaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 3290.68 examples/s]\rSaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 3132.42 examples/s]\r\n\rRunning tokenizer on prediction dataset:   0%|          | 0/10 [00:00<?, ? examples/s]Caching processed dataset at /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-0296a57fd4e8d68c/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5/cache-46ce74c14f288a3d.arrow\r\n08/22/2024 04:47:45 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-0296a57fd4e8d68c/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5/cache-46ce74c14f288a3d.arrow\r\n\rRunning tokenizer on prediction dataset: 100%|██████████| 10/10 [00:00<00:00, 514.39 examples/s]\r\nSaving cached test data ...\r\n\rSaving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s]\rSaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 3383.32 examples/s]\rSaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 3216.98 examples/s]\r\n/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 5, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\r\n  warnings.warn(_create_warning_msg(\r\n/usr/local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\r\n  warnings.warn(\r\nAll 1 steps, warm_up steps: 200\r\n\r  0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 5, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\r\n  warnings.warn(_create_warning_msg(\r\nTraceback (most recent call last):\r\n  File \"/content/AMRBART/fine-tune/main.py\", line 506, in <module>\r\n    main()\r\n  File \"/content/AMRBART/fine-tune/main.py\", line 423, in main\r\n    train_result = trainer.train(resume_from_checkpoint=checkpoint)\r\n  File \"/content/AMRBART/fine-tune/base_trainer.py\", line 1501, in train\r\n    return inner_training_loop(\r\n  File \"/content/AMRBART/fine-tune/base_trainer.py\", line 1717, in _inner_training_loop\r\n    for step, inputs in enumerate(epoch_iterator):\r\n  File \"/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 631, in __next__\r\n    data = self._next_data()\r\n  File \"/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1346, in _next_data\r\n    return self._process_data(data)\r\n  File \"/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1372, in _process_data\r\n    data.reraise()\r\n  File \"/usr/local/lib/python3.10/site-packages/torch/_utils.py\", line 722, in reraise\r\n    raise exception\r\nTypeError: Caught TypeError in DataLoader worker process 0.\r\nOriginal Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\r\n    data = fetcher.fetch(index)\r\n  File \"/usr/local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\r\n    return self.collate_fn(data)\r\n  File \"/content/AMRBART/fine-tune/data_interface/dataset.py\", line 134, in __call__\r\n    padding_func(\r\n  File \"/content/AMRBART/fine-tune/data_interface/dataset.py\", line 16, in padding_func\r\n    if pad_to_multiple_of > 1:\r\nTypeError: '>' not supported between instances of 'NoneType' and 'int'\r\n\r\n\r  0%|          | 0/1 [00:00<?, ?it/s]\r\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/AMRBART/fine-tune\n",
    "!bash train-AMRBART-large-AMRParsing.sh \"facebook/bart-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19467156",
   "metadata": {},
   "source": [
    "### Step 29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960cbaa2",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9893a9",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca9ec0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There seems to be a bug in file `main.py` related to the attribute `pad_to_multple_of`. The script fails when it is\n",
    "# equal to `None`. Aparently, it is resolved by replacing `None` with `1`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0777b09a",
   "metadata": {},
   "source": [
    "### Step 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bef570",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd3c4a4",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5cc572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/AMRBART\n/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/AMRBART/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bba58d7",
   "metadata": {},
   "source": [
    "### Step 31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eca72b7",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e8aa9b",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce1b8a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        else:\n            predict_dataset = load_from_disk(data_args.data_cache_dir + \"/test\", keep_in_memory=True)\n\n    # label_pad_token_id = -100 if data_args.ignore_pad_token_for_loss else tokenizer.pad_token_id\n    label_pad_token_id = tokenizer.pad_token_id\n    \n    DataCollatorCate = DataCollatorForAMR2Text if training_args.task == \"amr2text\" else DataCollatorForAMRParsing\n    data_collator = DataCollatorCate(\n        tokenizer,\n        label_pad_token_id=label_pad_token_id,\n        pad_to_multiple_of=8 if training_args.fp16 else 1,\n    )\n    \n    metric = load_metric(path=\"metric/sacrebleu.py\") if training_args.task == \"amr2text\" else None\n\n    def compute_metrics_parsing(eval_preds, global_step=0, prefix=\"val\"):\n        prefix = \"test\" if prefix == \"predict\" else \"val\"\n        preds, labels, inputs = eval_preds\n        # print(\"inputs:\", inputs)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/AMRBART/fine-tune/main.py. CHANGING:\n",
    "\n",
    "\n",
    "    data_collator = DataCollatorCate(\n",
    "        tokenizer,\n",
    "        label_pad_token_id=label_pad_token_id,\n",
    "        pad_to_multiple_of=8 if training_args.fp16 else None,\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "    data_collator = DataCollatorCate(\n",
    "        tokenizer,\n",
    "        label_pad_token_id=label_pad_token_id,\n",
    "        pad_to_multiple_of=8 if training_args.fp16 else 1,\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c97969",
   "metadata": {},
   "source": [
    "### Step 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec437cc4",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5fea37",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcea23ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ef6014",
   "metadata": {},
   "source": [
    "### Step 33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51957cc4",
   "metadata": {},
   "source": [
    "Execution Start time: 04:47:50, End time: 04:49:27 (duration: 96 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e574fdda",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c93bb66",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3eda8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/AMRBART/fine-tune\n/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n08/22/2024 04:47:56 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n08/22/2024 04:47:56 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n_n_gpu=0,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=8,\ndataloader_pin_memory=True,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=False,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=True,\ndo_train=True,\nearly_stopping=10,\neval_accumulation_steps=None,\neval_dataloader_num_workers=2,\neval_delay=0,\neval_lenpen=1.0,\neval_steps=None,\nevaluation_strategy=epoch,\nfp16=False,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_min_num_params=0,\nfull_determinism=False,\ngeneration_max_length=128,\ngeneration_num_beams=1,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngreater_is_better=True,\ngroup_by_length=False,\nhalf_precision_backend=cuda_amp,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=True,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.1,\nlearning_rate=1e-05,\nlength_column_name=length,\nload_best_model_at_end=True,\nlocal_rank=-1,\nlog_level=-1,\nlog_level_replica=-1,\nlog_on_each_node=True,\nlogging_dir=/content/AMRBART/fine-tune/outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp/logs,\nlogging_first_step=True,\nlogging_nan_inf_filter=True,\nlogging_steps=20,\nlogging_strategy=steps,\nlr_scheduler_type=polynomial,\nmax_grad_norm=0.0,\nmax_steps=-1,\nmetric_for_best_model=eval_smatch,\nmp_parameters=,\nno_cuda=False,\nnum_train_epochs=1.0,\noptim=adamw_hf,\noutput_dir=/content/AMRBART/fine-tune/outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp,\noverwrite_output_dir=True,\npast_index=-1,\nper_device_eval_batch_size=8,\nper_device_train_batch_size=16,\npredict_with_generate=True,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=/content/AMRBART/fine-tune/outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp,\nsave_on_each_node=False,\nsave_steps=500,\nsave_strategy=epoch,\nsave_total_limit=1,\nseed=42,\nsharded_ddp=[],\nskip_memory_metrics=True,\nsmart_init=False,\nsortish_sampler=False,\ntask=text2amr,\ntf32=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nwarmup_ratio=0.0,\nwarmup_steps=200,\nweight_decay=0.01,\nxpu_backend=None,\n)\n[INFO|configuration_utils.py:681] 2024-08-22 04:47:56,721 >> loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /mnt/nfs-storage/data/.cache/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\n[INFO|configuration_utils.py:730] 2024-08-22 04:47:56,722 >> Model config BartConfig {\n  \"_name_or_path\": \"facebook/bart-base\",\n  \"activation_dropout\": 0.1,\n  \"activation_function\": \"gelu\",\n  \"add_bias_logits\": false,\n  \"add_final_layer_norm\": false,\n  \"architectures\": [\n    \"BartModel\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"bos_token_id\": 0,\n  \"classif_dropout\": 0.1,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 768,\n  \"decoder_attention_heads\": 12,\n  \"decoder_ffn_dim\": 3072,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 6,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"early_stopping\": true,\n  \"encoder_attention_heads\": 12,\n  \"encoder_ffn_dim\": 3072,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 6,\n  \"eos_token_id\": 2,\n  \"forced_bos_token_id\": 0,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\"\n  },\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2\n  },\n  \"max_position_embeddings\": 1024,\n  \"model_type\": \"bart\",\n  \"no_repeat_ngram_size\": 3,\n  \"normalize_before\": false,\n  \"normalize_embedding\": true,\n  \"num_beams\": 4,\n  \"num_hidden_layers\": 6,\n  \"pad_token_id\": 1,\n  \"scale_embedding\": false,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 128,\n      \"min_length\": 12,\n      \"num_beams\": 4\n    },\n    \"summarization_cnn\": {\n      \"length_penalty\": 2.0,\n      \"max_length\": 142,\n      \"min_length\": 56,\n      \"num_beams\": 4\n    },\n    \"summarization_xsum\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 62,\n      \"min_length\": 11,\n      \"num_beams\": 6\n    }\n  },\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.21.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:47:56,938 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/vocab.json from cache at /mnt/nfs-storage/data/.cache/0d6fc8b2ef1860c1f8f0baff4b021e3426cc7d11b153f98e563b799603ee2f25.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:47:56,938 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/merges.txt from cache at /mnt/nfs-storage/data/.cache/6e75e35f0bdd15870c98387e13b93a8e100237eb33ad99c36277a0562bd6d850.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:47:56,938 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:47:56,938 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:1803] 2024-08-22 04:47:56,938 >> loading file https://huggingface.co/facebook/bart-large/resolve/main/tokenizer_config.json from cache at /mnt/nfs-storage/data/.cache/1abf196c889c24daca2909359ca2090e5fcbfa21a9ea36d763f70adbafb500d7.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8\n[INFO|configuration_utils.py:681] 2024-08-22 04:47:56,976 >> loading configuration file https://huggingface.co/facebook/bart-large/resolve/main/config.json from cache at /mnt/nfs-storage/data/.cache/3f12fb71b844fcb7d591fdd4e55027da90d7b5dd6aa5430ad00ec6d76585f26c.bc22f15dc7ba074ee0a60bdd34c5f2fe3b6d746f89e765303376c51aff04e260\n[INFO|configuration_utils.py:730] 2024-08-22 04:47:56,977 >> Model config BartConfig {\n  \"_name_or_path\": \"facebook/bart-large\",\n  \"activation_dropout\": 0.1,\n  \"activation_function\": \"gelu\",\n  \"add_bias_logits\": false,\n  \"add_final_layer_norm\": false,\n  \"architectures\": [\n    \"BartModel\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"bos_token_id\": 0,\n  \"classif_dropout\": 0.1,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 12,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"early_stopping\": true,\n  \"encoder_attention_heads\": 16,\n  \"encoder_ffn_dim\": 4096,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 12,\n  \"eos_token_id\": 2,\n  \"forced_bos_token_id\": 0,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\"\n  },\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2\n  },\n  \"max_position_embeddings\": 1024,\n  \"model_type\": \"bart\",\n  \"no_repeat_ngram_size\": 3,\n  \"normalize_before\": false,\n  \"num_beams\": 4,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"scale_embedding\": false,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 128,\n      \"min_length\": 12,\n      \"num_beams\": 4\n    },\n    \"summarization_cnn\": {\n      \"length_penalty\": 2.0,\n      \"max_length\": 142,\n      \"min_length\": 56,\n      \"num_beams\": 4\n    },\n    \"summarization_xsum\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 62,\n      \"min_length\": 11,\n      \"num_beams\": 6\n    }\n  },\n  \"transformers_version\": \"4.21.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\n[WARNING|tokenization_utils_base.py:1896] 2024-08-22 04:47:56,977 >> The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'BartTokenizer'. \nThe class this function is called from is 'AMRBartTokenizer'.\nAdded 2965 AMR tokens\n[INFO|modeling_utils.py:2041] 2024-08-22 04:47:57,138 >> loading weights file https://huggingface.co/facebook/bart-base/resolve/main/pytorch_model.bin from cache at /mnt/nfs-storage/data/.cache/486355ec722ef05fd480e999d4c763be56549ae930f6a3742ee721a5d2a05647.f2f355ad2775769afc60592b43a46d72ca548375e3a1d65f381a751e711cbadd\n[INFO|modeling_utils.py:2435] 2024-08-22 04:47:59,127 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n\n[INFO|modeling_utils.py:2443] 2024-08-22 04:47:59,127 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\nDataset cache dir: /mnt/nfs-storage/data/.cache\n/usr/local/lib/python3.10/site-packages/datasets/load.py:929: FutureWarning: The repository for data contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /content/AMRBART/fine-tune/data_interface/data.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\nUsing custom data configuration default-0296a57fd4e8d68c\n08/22/2024 04:47:59 - INFO - datasets.builder - Using custom data configuration default-0296a57fd4e8d68c\nLoading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/data/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5\n08/22/2024 04:47:59 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/data/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5\nOverwrite dataset info from restored data version if exists.\n08/22/2024 04:47:59 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\nLoading Dataset info from /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-0296a57fd4e8d68c/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5\n08/22/2024 04:47:59 - INFO - datasets.info - Loading Dataset info from /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-0296a57fd4e8d68c/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5\nFound cached dataset data (/content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-0296a57fd4e8d68c/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5)\n08/22/2024 04:47:59 - INFO - datasets.builder - Found cached dataset data (/content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-0296a57fd4e8d68c/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5)\nLoading Dataset info from /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-0296a57fd4e8d68c/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5\n08/22/2024 04:47:59 - INFO - datasets.info - Loading Dataset info from /content/AMRBART/fine-tune/data/AMRBART/.cache/dump-amrparsing/data/default-0296a57fd4e8d68c/1.0.0/de3d8b3b111683df6ff0ce882b325173dc75046029119f211c454c1ff50b54c5\ndatasets: DatasetDict({\n    train: Dataset({\n        features: ['src', 'tgt'],\n        num_rows: 10\n    })\n    validation: Dataset({\n        features: ['src', 'tgt'],\n        num_rows: 10\n    })\n    test: Dataset({\n        features: ['src', 'tgt'],\n        num_rows: 10\n    })\n})\ncolums: ['src', 'tgt']\n/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 5, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/usr/local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nAll 1 steps, warm_up steps: 200\n\n  0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 5, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n\n100%|██████████| 1/1 [00:11<00:00, 11.12s/it]\n                                             \n{'loss': 7.5741, 'learning_rate': 5.0000000000000004e-08, 'epoch': 1.0}\n\n100%|██████████| 1/1 [00:11<00:00, 11.12s/it]Generation Kwargs:\n{'max_length': 128, 'max_gen_length': 128, 'num_beams': 1}\nSaving dummy inputs...\n\n\n  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n\n100%|██████████| 2/2 [00:06<00:00,  3.11s/it]\u001b[A\n                                             \n\n\n                                             \n\u001b[A{'eval_loss': 8.709596633911133, 'eval_smatch': 0.0, 'eval_gen_len': 128.0, 'eval_runtime': 23.3563, 'eval_samples_per_second': 0.428, 'eval_steps_per_second': 0.086, 'epoch': 1.0}\n\n100%|██████████| 1/1 [00:34<00:00, 11.12s/it]\n\n100%|██████████| 2/2 [00:06<00:00,  3.11s/it]\u001b[A\n\n                                             \u001b[A[INFO|configuration_utils.py:451] 2024-08-22 04:48:35,346 >> Configuration saved in /content/AMRBART/fine-tune/outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp/checkpoint-1/config.json\n[INFO|modeling_utils.py:1566] 2024-08-22 04:48:35,823 >> Model weights saved in /content/AMRBART/fine-tune/outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp/checkpoint-1/pytorch_model.bin\n[INFO|tokenization_utils_base.py:2145] 2024-08-22 04:48:35,824 >> tokenizer config file saved in /content/AMRBART/fine-tune/outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp/checkpoint-1/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2152] 2024-08-22 04:48:35,824 >> Special tokens file saved in /content/AMRBART/fine-tune/outputs/AMRBART-AMRBART-large-AMRParing-bsz16-lr-1e-5-UnifiedInp/checkpoint-1/special_tokens_map.json\n\n                                             \n{'train_runtime': 36.6292, 'train_samples_per_second': 0.273, 'train_steps_per_second': 0.027, 'train_loss': 7.574057102203369, 'epoch': 1.0}\n\n100%|██████████| 1/1 [00:36<00:00, 11.12s/it]\n100%|██████████| 1/1 [00:36<00:00, 36.62s/it]\n***** train metrics *****\n  epoch                    =        1.0\n  train_loss               =     7.5741\n  train_runtime            = 0:00:36.62\n  train_samples            =         10\n  train_samples_per_second =      0.273\n  train_steps_per_second   =      0.027\n08/22/2024 04:48:37 - INFO - __main__ - *** Evaluate ***\nGeneration Kwargs:\n{'max_length': 128, 'num_beams': 1, 'max_gen_length': 128}\n\n  0%|          | 0/2 [00:00<?, ?it/s]\n100%|██████████| 2/2 [00:06<00:00,  3.10s/it]\n100%|██████████| 2/2 [00:06<00:00,  3.13s/it]\n***** eval metrics *****\n  epoch                   =        1.0\n  eval_gen_len            =      128.0\n  eval_loss               =     8.7096\n  eval_runtime            = 0:00:24.28\n  eval_samples            =         10\n  eval_samples_per_second =      0.412\n  eval_smatch             =        0.0\n  eval_steps_per_second   =      0.082\n08/22/2024 04:49:01 - INFO - __main__ - *** Predict ***\n\n  0%|          | 0/2 [00:00<?, ?it/s]\n100%|██████████| 2/2 [00:06<00:00,  3.16s/it]\n100%|██████████| 2/2 [00:06<00:00,  3.18s/it]\n***** predict metrics *****\n  predict_gen_len            =      128.0\n  predict_loss               =     7.6844\n  predict_runtime            = 0:00:24.23\n  predict_samples            =         10\n  predict_samples_per_second =      0.413\n  predict_smatch             =        0.0\n  predict_steps_per_second   =      0.083\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/AMRBART/fine-tune\n",
    "!bash train-AMRBART-large-AMRParsing.sh \"facebook/bart-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74c0ff0",
   "metadata": {},
   "source": [
    "### Step 34"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f778018b",
   "metadata": {},
   "source": [
    "Execution Start time: 04:49:29, End time: 04:49:29 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3d6567",
   "metadata": {},
   "source": [
    "**Thought**: The training has completed successfully. The last training loss is 7.5741. Now, I will submit the answer as requested in the task description.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ec47d8",
   "metadata": {},
   "source": [
    "**Action (submit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f9e226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{last_loss: 7.5741}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit {\"last_loss\": 7.5741}"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
