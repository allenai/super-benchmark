{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f81eac1",
   "metadata": {},
   "source": [
    "### Query\n",
    "Apply the decomposed prompt tuning method to fine-tune the t5-small model on our sentence pair classification dataset, available at https://drive.google.com/file/d/1raGLX-2vdX1uizYx2DdqZ_JzAkVeyeDF/view?usp=drive_link. Report the resulting metrics using the `main_ddp.py` script. Report accuracy on the development set as a json structured as follows: {\"accuracy\": 0.0} (replace 0.0 with the actual value).\n",
    "\n",
    "Additional instructions:\n",
    "1. Run for only 1 epoch.\n",
    "2. Use the following hyperparameters: --enc_prompt_tokens 100 -ts 16 -e 1 --bottle_neck 10\n",
    "\n",
    "Git repository: https://github.com/xyaoooo/dpt\n",
    "\n",
    "I have already executed some of the required steps. Now, you should install all required dependencies. Once dependencies are installed, you can re-run any of the pre-executed steps, and finally should successfully run\n",
    "```\n",
    "!python main_ddp.py --datasets=mrpc  --model_name=t5-small --enc_prompt_tokens 100 -ts 16 -e 1 --bottle_neck 10\n",
    "```\n",
    "to submit the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96eb541",
   "metadata": {},
   "source": [
    "### Prompt\n",
    "<details>\n",
    "[{'role': 'system', 'content': 'Interact in a jupyter notebook to solve the request.\\nOutput a thought and an action.\\nThought can reason about the current situation, and action allows you to interact with the Jupyter notebook. There are three types of actions:\\n(1) execute, which executes a notebook cell with the command under the \"content\" field. This could be a bash command (beginning with !), a python command, or magic command (beginning with %).\\n(2) edit, which allows you to replace existing lines in a file with new lines. To replace lines, you will need to provide the lines as they appear before the replacement, and the lines to replace them with.\\nIn the \"content\" field, format should be ```{filename}\\n<<<BEFORE_EDIT>>>\\nlines before edit\\nlines before edit\\n<<<AFTER_EDIT>>>\\nlines after edit\\nlines after edit```\\n  - Include the entire lines WITH indentation. After editing, make sure indentation is correct.\\n(3) submit, which submits the final answer (or null if there\\'s no answer to submit), formatted as JSON (make sure to use double quotes for keys and values).\\nAlways output exactly one \"thought\" and one \"action\", and encapsulate the content with <content>...</content> tags.\\nI will provide the cell output (observation) for you.\\n\\nTips:\\n- When a github repository is given, you should start by cloning it. You can read the readme file and browse the repo for installation and usage instructions.\\n- Avoid using conda, instead use pip for package management.\\n- Each execution has a 5-minute timeout. After this period, you can decide whether to interrupt the kernel or continue waiting.\\n  You should interrupt if: (1) you are prompted with an interactive message (e.g., \"Do you want to continue? [Y/n]\"). In that case, you can bypass it by executing echo \"y\" | command, or (2) the execution is taking longer than expected because the wrong task is being executed, such as a different model or incorrect number of epochs.\\n  You should continue to wait if the current execution is correct and proceeding as expected.\\n\\nHere are some examples.\\nGit repository: EleutherAI/lm-evaluation-harness\\nQuery: Evaluate EleutherAI/gpt-j-6B on hellaswag. What\\'s the score?\\nThought: First, I need to clone the GitHub repository \\'EleutherAI/lm-evaluation-harness\\'.\\nAction: execute\\n<content>\\n!git clone https://github.com/EleutherAI/lm-evaluation-harness.git\\n</content>\\nObservation: ```\\nCloning into \\'lm-evaluation-harness\\'...\\n```\\nThought: The repository has been cloned. Next, I should check the README file for installation and usage instructions to proceed with evaluating EleutherAI/gpt-j-6B on hellaswag.\\nAction: execute\\n<content>\\n!cat lm-evaluation-harness/README.md\\n</content>\\nObservation: ```# Language Model Evaluation Harness ... truncated ...\\n```\\nThought: The README provides detailed instructions for installing the `lm-eval` package and evaluating models like GPT-J-6B on tasks like hellaswag. I need to install the package and its dependencies first.\\nAction: execute\\n<content>\\n!pip install -e lm-evaluation-harness/.\\n</content>\\nObservation: ```\\nObtaining file:///home/nora/lm-evaluation-harness\\n  Installing build dependencies ... truncated ...\\n```\\nThought: The `lm-eval` package and its dependencies have been installed successfully. Now, I can proceed with evaluating the EleutherAI/gpt-j-6B model on the hellaswag task as per the instructions in the README.\\nAction: execute\\n<content>\\n!lm_eval --model hf --model_args pretrained=EleutherAI/gpt-j-6B --tasks hellaswag --device cuda:0 --batch_size 8\\n</content>\\nObservation: ```\\n{\\n  \"hellaswag\": {\\n    \"acc\": 0.24603658536585363,\\n    \"acc_norm\": 0.34436059706446426,\\n  },  ... truncated ...\\n```\\nThought: The `lm-eval` was executed successfully, providing the metrics I needed. The `acc` metric is 0.2460. The `acc_norm` metric is 0.3444\\nAction: submit\\n<content>\\n{\"acc\": 0.2460, \"acc_norm\": 0.3444}\\n</content>\\n\\nQuery: use pandas to read the file \\'data.csv\\' and display the first 2 rows.\\nThought: First, I need to read the file \\'data.csv\\' using the pandas library.\\nAction: execute\\n<content>\\nimport pandas as pd\\npd.read_csv(\\'data.csv\\').head()\\n</content>\\nObservation: ```\\n    name\\n  0  Alice\\n   1    Bob\\n  2  Charlie\\n```\\nThought: The file \\'data.csv\\' has been read successfully and the first 2 rows have been displayed.\\nAction: submit\\n<content>\\n[\"Alice\", \"Bob\"]\\n</content>\\n\\nQuery: Change the model name in the \\'config.json\\' file from \\'roberta-base\\' to \\'roberta-large\\'.\\nThought: First, I need to read the \\'config.json\\' file to find the location where I can replace the model name.\\nAction: execute\\n<content>\\n!cat config.json\\n</content>\\nObservation: ```\\n{\\n    \"model\": \"roberta-base\",\\n    \"batch_size\": 8,\\n    \"epochs\": 10\\n}\\n```\\nThought: The file has been read successfully. Now, I can replace the model name in the \\'config.json\\' file.\\nAction: edit\\n<content>\\nconfig.json\\n<<<BEFORE_EDIT>>>\\n{\\n    \"model\": \"roberta-base\",\\n    \"batch_size\": 8,\\n<<<AFTER_EDIT>>>\\n{\\n    \"model\": \"roberta-large\",\\n    \"batch_size\": 8,\\n</content>\\nObservation: \"editing file config.json\"\\nThought: The model name has been replaced in the \\'config.json\\' file.\\nAction: submit\\n<content>\\nnull\\n</content>\\n'}, {'role': 'user', 'content': 'Query: Apply the decomposed prompt tuning method to fine-tune the t5-small model on our sentence pair classification dataset, available at https://drive.google.com/file/d/1raGLX-2vdX1uizYx2DdqZ_JzAkVeyeDF/view?usp=drive_link. Report the resulting metrics using the `main_ddp.py` script. Report accuracy on the development set as a json structured as follows: {\"accuracy\": 0.0} (replace 0.0 with the actual value).\\n\\nAdditional instructions:\\n1. Run for only 1 epoch.\\n2. Use the following hyperparameters: --enc_prompt_tokens 100 -ts 16 -e 1 --bottle_neck 10\\n\\nGit repository: https://github.com/xyaoooo/dpt\\n\\nI have already executed some of the required steps. Now, you should install all required dependencies. Once dependencies are installed, you can re-run any of the pre-executed steps, and finally should successfully run\\n```\\n!python main_ddp.py --datasets=mrpc  --model_name=t5-small --enc_prompt_tokens 100 -ts 16 -e 1 --bottle_neck 10\\n```\\nto submit the answer.'}]\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea8f732",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376fd872",
   "metadata": {},
   "source": [
    "Execution Start time: 00:46:03, End time: 00:46:04 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b49a945",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c12d185",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb037ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cloning into 'dpt'...\nremote: Enumerating objects: 45, done.\u001b[K\nremote: Counting objects: 100% (45/45)\u001b[K\nremote: Counting objects: 100% (45/45), done.\u001b[K\nremote: Compressing objects: 100% (41/41)\u001b[K\nremote: Compressing objects: 100% (41/41), done.\u001b[K\nremote: Total 45 (delta 18), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (45/45)\nReceiving objects: 100% (45/45), 39.40 KiB | 13.13 MiB/s, done.\nResolving deltas: 100% (18/18)\nResolving deltas: 100% (18/18), done.\n/content/dpt\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!git clone https://github.com/xyaoooo/dpt\n",
    "%cd dpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bc4948",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd86cac",
   "metadata": {},
   "source": [
    "Execution Start time: 00:46:04, End time: 00:46:04 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ac31f1",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a45e79e",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2ade90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n    name = \"mrpc\"\n    labels_list = [\"0\", \"1\"]\n    metric = [metrics.accuracy, metrics.f1_score_with_invalid]\n    metric_names = [\"accuracy\", \"f1\"]\n    split_to_data_split = {\"train\": \"train\",\n                           \"validation\": \"validation\",\n                           \"test\": \"validation\"}\n\n    def load_dataset(self, split):\n        # return datasets.load_dataset('glue', 'mrpc', split=split, script_version=\"master\")\n        #return datasets.load_dataset('glue', 'mrpc', split=split)\n        files = {\n        'train': 'mrpc_csv/train.csv',\n        'validation': 'mrpc_csv/val.csv',\n        'test': 'mrpc_csv/test.csv'\n        }\n\n        # Load the dataset\n        return datasets.load_dataset('csv', data_files=files, split=split)\n\n    def preprocessor(self, example, add_prefix=True, add_vb=False):\n        src_texts = [\"sentence1:\", example['sentence1'],\n                     \"sentence2:\", example[\"sentence2\"]]\n        tgt_texts = [str(example['label'])]\n        if add_vb:\n            verbalizer = \"{ 0 : not equivalent, 1 : equivalent }\"\n        else:"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/dpt/data_utils.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "\n",
    "    def load_dataset(self, split):\n",
    "        # return datasets.load_dataset('glue', 'mrpc', split=split, script_version=\"master\")\n",
    "        return datasets.load_dataset('glue', 'mrpc', split=split)\n",
    "\n",
    "    def preprocessor(self, example, add_prefix=True, add_vb=False):\n",
    "<<<AFTER_EDIT>>>\n",
    "\n",
    "    def load_dataset(self, split):\n",
    "        # return datasets.load_dataset('glue', 'mrpc', split=split, script_version=\"master\")\n",
    "        #return datasets.load_dataset('glue', 'mrpc', split=split)\n",
    "        files = {\n",
    "        'train': 'mrpc_csv/train.csv',\n",
    "        'validation': 'mrpc_csv/val.csv',\n",
    "        'test': 'mrpc_csv/test.csv'\n",
    "        }\n",
    "\n",
    "        # Load the dataset\n",
    "        return datasets.load_dataset('csv', data_files=files, split=split)\n",
    "\n",
    "    def preprocessor(self, example, add_prefix=True, add_vb=False):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cef7dab",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d66dcf",
   "metadata": {},
   "source": [
    "Execution Start time: 00:46:04, End time: 00:46:08 (duration: 3 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e7d910",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b92be59",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604f95c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Downloading...\nFrom: https://drive.google.com/uc?id=1raGLX-2vdX1uizYx2DdqZ_JzAkVeyeDF\nTo: /content/dpt/mrpc_csv.zip\n\n  0%|          | 0.00/3.96k [00:00<?, ?B/s]\n\n   creating: mrpc_csv/\n  inflating: mrpc_csv/val.csv        \n  inflating: mrpc_csv/test.csv       \n  inflating: mrpc_csv/train.csv      \n\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# URL of the Google Drive file\n",
    "file_url = \"https://drive.google.com/uc?id=1raGLX-2vdX1uizYx2DdqZ_JzAkVeyeDF\"\n",
    "output_file = \"mrpc_csv.zip\"\n",
    "\n",
    "# Download the file from Google Driveraise ValueError(f'mutable default {type(f.default)} for field ' ValueError: mutable default <class 'datasets.utils.version.Version'> for field version is not allowed: use default_factory\n",
    "gdown.download(file_url, output_file, quiet=False)\n",
    "!unzip mrpc_csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91809e7",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1070b69",
   "metadata": {},
   "source": [
    "Execution Start time: 00:46:08, End time: 00:46:08 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a027fa47",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcccdfe",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b384fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\ndef run(local_rank, args):\n\n    is_master = local_rank == 0\n    world_size = args.world_size\n    is_mp = world_size > 1\n    # set the device\n    device = 'cpu' if not torch.cuda.is_available() else 'cuda'\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed_all(args.seed)\n    g = torch.Generator()\n    g.manual_seed(args.seed)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/dpt/main_ddp.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "    world_size = args.world_size\n",
    "    is_mp = world_size > 1\n",
    "    # set the device\n",
    "    device = local_rank\n",
    "\n",
    "    random.seed(args.seed)\n",
    "<<<AFTER_EDIT>>>\n",
    "    world_size = args.world_size\n",
    "    is_mp = world_size > 1\n",
    "    # set the device\n",
    "    device = 'cpu' if not torch.cuda.is_available() else 'cuda'\n",
    "\n",
    "    random.seed(args.seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a359fc2a",
   "metadata": {},
   "source": [
    "### Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2b6d20",
   "metadata": {},
   "source": [
    "Execution Start time: 00:46:08, End time: 00:46:09 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a005a84a",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05dbd1c",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0f9df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n\n                for step, batch in enumerate(TrainDataloader):\n                    global_step += 1\n\n                    if global_step <= resume_steps:\n\n                        if step % update_stride == 0:\n                            pbar.update(update_stride)                        \n                        continue\n                    if len(batch['source_ids'].shape) == 3:\n                        source_ids = batch['source_ids'].squeeze(0).to(device)\n                        source_mask = batch['source_mask'].squeeze(0).to(device)\n                        labels = batch['target_ids'].squeeze(0).to(device)\n                        task_ids = torch.tensor([x[0] for x in batch['task_ids']]).to(device)\n                    else:\n                        source_ids = batch['source_ids'].to(device)\n                        source_mask = batch['source_mask'].to(device)\n                        labels = batch['target_ids'].to(device)\n                        task_ids = batch['task_ids'].to(device)\n\n                    outputs = model(input_ids=source_ids, attention_mask=source_mask, labels=labels, task_ids=task_ids)\n                    loss = outputs['loss']\n\n                    loss = loss / args.accumulate_steps\n                    loss.backward()\n                    step_count += 1\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/dpt/main_ddp.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "                            pbar.update(update_stride)                        \n",
    "                        continue\n",
    "                    if len(batch['source_ids'].shape) == 3:\n",
    "                        source_ids = batch['source_ids'].squeeze(0).to(local_rank)\n",
    "                        source_mask = batch['source_mask'].squeeze(0).to(local_rank)\n",
    "                        labels = batch['target_ids'].squeeze(0).to(local_rank)\n",
    "                        task_ids = torch.tensor([x[0] for x in batch['task_ids']]).to(local_rank)\n",
    "                    else:\n",
    "                        source_ids = batch['source_ids'].to(local_rank)\n",
    "                        source_mask = batch['source_mask'].to(local_rank)\n",
    "                        labels = batch['target_ids'].to(local_rank)\n",
    "                        task_ids = batch['task_ids'].to(local_rank)\n",
    "\n",
    "                    outputs = model(input_ids=source_ids, attention_mask=source_mask, labels=labels, task_ids=task_ids)\n",
    "<<<AFTER_EDIT>>>\n",
    "                            pbar.update(update_stride)                        \n",
    "                        continue\n",
    "                    if len(batch['source_ids'].shape) == 3:\n",
    "                        source_ids = batch['source_ids'].squeeze(0).to(device)\n",
    "                        source_mask = batch['source_mask'].squeeze(0).to(device)\n",
    "                        labels = batch['target_ids'].squeeze(0).to(device)\n",
    "                        task_ids = torch.tensor([x[0] for x in batch['task_ids']]).to(device)\n",
    "                    else:\n",
    "                        source_ids = batch['source_ids'].to(device)\n",
    "                        source_mask = batch['source_mask'].to(device)\n",
    "                        labels = batch['target_ids'].to(device)\n",
    "                        task_ids = batch['task_ids'].to(device)\n",
    "\n",
    "                    outputs = model(input_ids=source_ids, attention_mask=source_mask, labels=labels, task_ids=task_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377b3773",
   "metadata": {},
   "source": [
    "### Step 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2cbcb7",
   "metadata": {},
   "source": [
    "Execution Start time: 00:46:10, End time: 00:48:30 (duration: 139 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a51649",
   "metadata": {},
   "source": [
    "**Thought**: Now that the necessary edits have been made to the code, I need to install the required dependencies for the project. This will allow me to run the `main_ddp.py` script with the specified hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e848861c",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09855a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting huggingface-hub==0.8.1 (from -r requirements.txt (line 1))\n  Downloading huggingface_hub-0.8.1-py3-none-any.whl.metadata (4.6 kB)\nCollecting numpy==1.22.4 (from -r requirements.txt (line 2))\n  Downloading numpy-1.22.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\nCollecting pandas==1.4.2 (from -r requirements.txt (line 3))\n  Downloading pandas-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nCollecting scikit-learn==1.1.1 (from -r requirements.txt (line 4))\n  Downloading scikit_learn-1.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nCollecting scipy==1.8.1 (from -r requirements.txt (line 5))\n  Downloading scipy-1.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\nCollecting sentencepiece==0.1.96 (from -r requirements.txt (line 6))\n  Downloading sentencepiece-0.1.96-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nCollecting spacy==3.3.0 (from -r requirements.txt (line 7))\n  Downloading spacy-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)\nCollecting tabulate==0.8.9 (from -r requirements.txt (line 8))\n  Downloading tabulate-0.8.9-py3-none-any.whl.metadata (23 kB)\nCollecting tokenizers==0.12.1 (from -r requirements.txt (line 9))\n  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.5 kB)\nCollecting torch==2.0.0 (from -r requirements.txt (line 10))\n  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\nCollecting torchvision (from -r requirements.txt (line 11))\n  Downloading torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.0 kB)\nCollecting tqdm==4.64.0 (from -r requirements.txt (line 12))\n  Downloading tqdm-4.64.0-py2.py3-none-any.whl.metadata (57 kB)\n\u001b[?25l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/57.3 kB ? eta -:--:--\n\n\u001b[?25hCollecting transformers==4.20.1 (from -r requirements.txt (line 13))\n  Downloading transformers-4.20.1-py3-none-any.whl.metadata (77 kB)\n\u001b[?25l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/77.3 kB ? eta -:--:--\n\n\u001b[?25hCollecting datasets==2.3.2 (from -r requirements.txt (line 14))\n  Downloading datasets-2.3.2-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from huggingface-hub==0.8.1->-r requirements.txt (line 1)) (3.15.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from huggingface-hub==0.8.1->-r requirements.txt (line 1)) (2.31.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from huggingface-hub==0.8.1->-r requirements.txt (line 1)) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub==0.8.1->-r requirements.txt (line 1)) (4.12.2)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/site-packages (from huggingface-hub==0.8.1->-r requirements.txt (line 1)) (24.1)\nRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/site-packages (from pandas==1.4.2->-r requirements.txt (line 3)) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas==1.4.2->-r requirements.txt (line 3)) (2024.1)\nRequirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn==1.1.1->-r requirements.txt (line 4)) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn==1.1.1->-r requirements.txt (line 4)) (3.5.0)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.10/site-packages (from spacy==3.3.0->-r requirements.txt (line 7)) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/site-packages (from spacy==3.3.0->-r requirements.txt (line 7)) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/site-packages (from spacy==3.3.0->-r requirements.txt (line 7)) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/site-packages (from spacy==3.3.0->-r requirements.txt (line 7)) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/site-packages (from spacy==3.3.0->-r requirements.txt (line 7)) (3.0.9)\nCollecting thinc<8.1.0,>=8.0.14 (from spacy==3.3.0->-r requirements.txt (line 7))\n  Downloading thinc-8.0.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\nRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.10/site-packages (from spacy==3.3.0->-r requirements.txt (line 7)) (0.7.11)\nCollecting wasabi<1.1.0,>=0.9.1 (from spacy==3.3.0->-r requirements.txt (line 7))\n  Downloading wasabi-0.10.1-py3-none-any.whl.metadata (28 kB)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/site-packages (from spacy==3.3.0->-r requirements.txt (line 7)) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/site-packages (from spacy==3.3.0->-r requirements.txt (line 7)) (2.0.10)\nCollecting typer<0.5.0,>=0.3.0 (from spacy==3.3.0->-r requirements.txt (line 7))\n  Downloading typer-0.4.2-py3-none-any.whl.metadata (12 kB)\nCollecting pathy>=0.3.5 (from spacy==3.3.0->-r requirements.txt (line 7))\n  Downloading pathy-0.11.0-py3-none-any.whl.metadata (16 kB)\nCollecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 (from spacy==3.3.0->-r requirements.txt (line 7))\n  Downloading pydantic-1.8.2-py3-none-any.whl.metadata (103 kB)\n\u001b[?25l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/103.1 kB ? eta -:--:--\n\n\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from spacy==3.3.0->-r requirements.txt (line 7)) (3.1.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from spacy==3.3.0->-r requirements.txt (line 7)) (68.1.2)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/site-packages (from spacy==3.3.0->-r requirements.txt (line 7)) (3.4.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/site-packages (from torch==2.0.0->-r requirements.txt (line 10)) (1.12)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch==2.0.0->-r requirements.txt (line 10)) (3.3)\nCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.0->-r requirements.txt (line 10))\n  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.0->-r requirements.txt (line 10))\n  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.0->-r requirements.txt (line 10))\n  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.0->-r requirements.txt (line 10))\n  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.0->-r requirements.txt (line 10))\n  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.0->-r requirements.txt (line 10))\n  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.0->-r requirements.txt (line 10))\n  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.0->-r requirements.txt (line 10))\n  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.0->-r requirements.txt (line 10))\n  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.0->-r requirements.txt (line 10))\n  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.0->-r requirements.txt (line 10))\n  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.0.0 (from torch==2.0.0->-r requirements.txt (line 10))\n  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers==4.20.1->-r requirements.txt (line 13)) (2023.12.25)\nCollecting pyarrow>=6.0.0 (from datasets==2.3.2->-r requirements.txt (line 14))\n  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\nCollecting dill<0.3.6 (from datasets==2.3.2->-r requirements.txt (line 14))\n  Downloading dill-0.3.5.1-py2.py3-none-any.whl.metadata (9.7 kB)\nCollecting xxhash (from datasets==2.3.2->-r requirements.txt (line 14))\n  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nCollecting multiprocess (from datasets==2.3.2->-r requirements.txt (line 14))\n  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->datasets==2.3.2->-r requirements.txt (line 14)) (2023.6.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/site-packages (from datasets==2.3.2->-r requirements.txt (line 14)) (3.8.3)\nCollecting responses<0.19 (from datasets==2.3.2->-r requirements.txt (line 14))\n  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\nCollecting wheel (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->-r requirements.txt (line 10))\n  Downloading wheel-0.44.0-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: cmake in /usr/local/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.0->-r requirements.txt (line 10)) (3.27.9)\nCollecting lit (from triton==2.0.0->torch==2.0.0->-r requirements.txt (line 10))\n  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\nINFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\nCollecting torchvision (from -r requirements.txt (line 11))\n  Downloading torchvision-0.18.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n  Downloading torchvision-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n  Downloading torchvision-0.17.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n  Downloading torchvision-0.17.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n  Downloading torchvision-0.17.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n  Downloading torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n  Downloading torchvision-0.16.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\nINFO: pip is still looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n  Downloading torchvision-0.16.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n  Downloading torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl.metadata (11 kB)\n  Downloading torchvision-0.15.1-cp310-cp310-manylinux1_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/site-packages (from torchvision->-r requirements.txt (line 11)) (10.3.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.3.2->-r requirements.txt (line 14)) (23.2.0)\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.3.2->-r requirements.txt (line 14)) (2.1.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.3.2->-r requirements.txt (line 14)) (6.0.5)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.3.2->-r requirements.txt (line 14)) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.3.2->-r requirements.txt (line 14)) (1.9.4)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.3.2->-r requirements.txt (line 14)) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.3.2->-r requirements.txt (line 14)) (1.3.1)\nRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.3.0->-r requirements.txt (line 7)) (1.2.0)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/site-packages (from pathy>=0.3.5->spacy==3.3.0->-r requirements.txt (line 7)) (6.4.0)\nCollecting pathlib-abc==0.1.1 (from pathy>=0.3.5->spacy==3.3.0->-r requirements.txt (line 7))\n  Downloading pathlib_abc-0.1.1-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas==1.4.2->-r requirements.txt (line 3)) (1.16.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub==0.8.1->-r requirements.txt (line 1)) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub==0.8.1->-r requirements.txt (line 1)) (2.0.7)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub==0.8.1->-r requirements.txt (line 1)) (2024.6.2)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy==3.3.0->-r requirements.txt (line 7)) (8.1.7)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->spacy==3.3.0->-r requirements.txt (line 7)) (2.1.5)\nINFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\nCollecting multiprocess (from datasets==2.3.2->-r requirements.txt (line 14))\n  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n  Downloading multiprocess-0.70.14-py310-none-any.whl.metadata (6.6 kB)\n  Downloading multiprocess-0.70.13-py310-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/site-packages (from sympy->torch==2.0.0->-r requirements.txt (line 10)) (1.3.0)\nRequirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.3.0->-r requirements.txt (line 7)) (1.2.0)\nDownloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/101.5 kB ? eta -:--:--\n\n\u001b[?25hDownloading numpy-1.22.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/16.8 MB ? eta -:--:--\n\n\n\u001b[?25hDownloading pandas-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/11.7 MB ? eta -:--:--\n\n\n\u001b[?25hDownloading scikit_learn-1.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.4 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/30.4 MB ? eta -:--:--\n\n\n\n\n\u001b[?25hDownloading scipy-1.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.2 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/42.2 MB ? eta -:--:--\n\n\n\n\n\n\u001b[?25hDownloading sentencepiece-0.1.96-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/1.2 MB ? eta -:--:--\n\n\u001b[?25hDownloading spacy-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.3 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/6.3 MB ? eta -:--:--\n\n\n\u001b[?25hDownloading tabulate-0.8.9-py3-none-any.whl (25 kB)\nDownloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/6.6 MB ? eta -:--:--\n\n\u001b[?25hDownloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/619.9 MB ? eta -:--:--\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[?25hDownloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/78.4 kB ? eta -:--:--\n\n\u001b[?25hDownloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/4.4 MB ? eta -:--:--\n\n\u001b[?25hDownloading datasets-2.3.2-py3-none-any.whl (362 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/362.3 kB ? eta -:--:--\n\n\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/317.1 MB ? eta -:--:--\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/11.8 MB ? eta -:--:--\n\n\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/21.0 MB ? eta -:--:--\n\n\n\n\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/849.3 kB ? eta -:--:--\n\n\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/557.1 MB ? eta -:--:--\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/168.4 MB ? eta -:--:--\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/54.6 MB ? eta -:--:--\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/102.6 MB ? eta -:--:--\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/173.2 MB ? eta -:--:--\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/177.1 MB ? eta -:--:--\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/98.6 kB ? eta -:--:--\n\n\u001b[?25hDownloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/63.3 MB ? eta -:--:--\n\n\n\n\n\n\n\n\n\n\u001b[?25hDownloading torchvision-0.15.1-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/6.0 MB ? eta -:--:--\n\n\u001b[?25hDownloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/95.8 kB ? eta -:--:--\n\n\u001b[?25hDownloading pathy-0.11.0-py3-none-any.whl (47 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/47.3 kB ? eta -:--:--\n\n\u001b[?25hDownloading pathlib_abc-0.1.1-py3-none-any.whl (23 kB)\nDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/39.9 MB ? eta -:--:--\n\n\n\n\n\n\u001b[?25hDownloading pydantic-1.8.2-py3-none-any.whl (126 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/126.0 kB ? eta -:--:--\n\n\u001b[?25hDownloading responses-0.18.0-py3-none-any.whl (38 kB)\nDownloading thinc-8.0.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (659 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/659.5 kB ? eta -:--:--\n\n\u001b[?25hDownloading typer-0.4.2-py3-none-any.whl (27 kB)\nDownloading wasabi-0.10.1-py3-none-any.whl (26 kB)\nDownloading multiprocess-0.70.13-py310-none-any.whl (133 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/133.1 kB ? eta -:--:--\n\n\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/194.1 kB ? eta -:--:--\n\n\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/96.4 kB ? eta -:--:--\n\n\u001b[?25hDownloading wheel-0.44.0-py3-none-any.whl (67 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/67.1 kB ? eta -:--:--\n\n\u001b[?25hInstalling collected packages: wasabi, tokenizers, tabulate, sentencepiece, lit, xxhash, wheel, typer, tqdm, pydantic, pathlib-abc, nvidia-nccl-cu11, nvidia-cufft-cu11, nvidia-cuda-nvrtc-cu11, numpy, dill, scipy, responses, pyarrow, pathy, pandas, nvidia-nvtx-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, multiprocess, huggingface-hub, transformers, thinc, scikit-learn, nvidia-cusolver-cu11, nvidia-cudnn-cu11, spacy, datasets, triton, torch, torchvision\n  Attempting uninstall: wasabi\n    Found existing installation: wasabi 1.1.3\n    Uninstalling wasabi-1.1.3:\n      Successfully uninstalled wasabi-1.1.3\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.1\n    Uninstalling tokenizers-0.19.1:\n      Successfully uninstalled tokenizers-0.19.1\n  Attempting uninstall: sentencepiece\n    Found existing installation: sentencepiece 0.1.99\n    Uninstalling sentencepiece-0.1.99:\n      Successfully uninstalled sentencepiece-0.1.99\n  Attempting uninstall: typer\n    Found existing installation: typer 0.6.1\n    Uninstalling typer-0.6.1:\n      Successfully uninstalled typer-0.6.1\n  Attempting uninstall: tqdm\n    Found existing installation: tqdm 4.66.4\n    Uninstalling tqdm-4.66.4:\n      Successfully uninstalled tqdm-4.66.4\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 1.10.16\n    Uninstalling pydantic-1.10.16:\n      Successfully uninstalled pydantic-1.10.16\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.25.2\n    Uninstalling numpy-1.25.2:\n      Successfully uninstalled numpy-1.25.2\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.11.4\n    Uninstalling scipy-1.11.4:\n      Successfully uninstalled scipy-1.11.4\n  Attempting uninstall: pandas\n    Found existing installation: pandas 2.0.3\n    Uninstalling pandas-2.0.3:\n      Successfully uninstalled pandas-2.0.3\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.23.3\n    Uninstalling huggingface-hub-0.23.3:\n      Successfully uninstalled huggingface-hub-0.23.3\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.40.2\n    Uninstalling transformers-4.40.2:\n      Successfully uninstalled transformers-4.40.2\n  Attempting uninstall: thinc\n    Found existing installation: thinc 8.2.4\n    Uninstalling thinc-8.2.4:\n      Successfully uninstalled thinc-8.2.4\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.5.0\n    Uninstalling scikit-learn-1.5.0:\n      Successfully uninstalled scikit-learn-1.5.0\n  Attempting uninstall: spacy\n    Found existing installation: spacy 3.7.4\n    Uninstalling spacy-3.7.4:\n      Successfully uninstalled spacy-3.7.4\n  Attempting uninstall: torch\n    Found existing installation: torch 2.2.1+cpu\n    Uninstalling torch-2.2.1+cpu:\n      Successfully uninstalled torch-2.2.1+cpu\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nmodal 0.62.65 requires synchronicity~=0.6.5, which is not installed.\nmodal 0.62.65 requires watchfiles, which is not installed.\nmodal 0.62.65 requires aiostream~=0.5.2, but you have aiostream 0.4.4 which is incompatible.\nmodal 0.62.65 requires grpclib==0.4.7, but you have grpclib 0.4.3 which is incompatible.\nmodal 0.62.65 requires typer~=0.9.0, but you have typer 0.4.2 which is incompatible.\nen-core-web-sm 3.7.1 requires spacy<3.8.0,>=3.7.2, but you have spacy 3.3.0 which is incompatible.\nSuccessfully installed datasets-2.3.2 dill-0.3.5.1 huggingface-hub-0.8.1 lit-18.1.8 multiprocess-0.70.13 numpy-1.22.4 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pandas-1.4.2 pathlib-abc-0.1.1 pathy-0.11.0 pyarrow-17.0.0 pydantic-1.8.2 responses-0.18.0 scikit-learn-1.1.1 scipy-1.8.1 sentencepiece-0.1.96 spacy-3.3.0 tabulate-0.8.9 thinc-8.0.17 tokenizers-0.12.1 torch-2.0.0 torchvision-0.15.1 tqdm-4.64.0 transformers-4.20.1 triton-2.0.0 typer-0.4.2 wasabi-0.10.1 wheel-0.44.0 xxhash-3.5.0\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0faf23",
   "metadata": {},
   "source": [
    "### Step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e54770",
   "metadata": {},
   "source": [
    "Execution Start time: 00:48:32, End time: 00:49:10 (duration: 38 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04138cff",
   "metadata": {},
   "source": [
    "**Thought**: The required dependencies have been successfully installed. Now, I can proceed to run the `main_ddp.py` script with the specified hyperparameters to fine-tune the `t5-small` model on the sentence pair classification dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04aef3d",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a80555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stating time:  08/21/2024 00:48:36\nRaw Arguments:  Namespace(seed=42, from_checkpoint='', auto_resume=False, save_ckpt_dir='./saved_models', load_source_path='', saving_steps=1000, saving_each_epoch=True, latest_ckpt_name='latest_checkpoint.pt', n_epochs=1, train_batch_size=16, eval_batch_size=128, lr=0.3, max_grad_norm=1.0, weight_decay=1e-05, accumulate_steps=1, lr_scheduler_type='linear', num_warmup_steps=0.1, max_train_steps=0, min_training_epoch=20, early_stopping_patience=10, mode='train', close_tqdm=True, max_epochs_before_stop=2, model_name='t5-small', datasets=['mrpc'], max_source_length=256, max_target_length=128, prompt_type='param', is_dynamic_share=False, eval_in_train=True, enc_prompt_tokens=100, dec_prompt_tokens=0, sharing_ratio=1.0, share_seed=42, add_task_prefix=True, add_verbalizer=False, port=12355, gupids=None, sampling_method='uniform', size_limit=262144, stoch_task=False, bottle_neck=10, model_output_path='./saved_outputs', local_file_prefix='/gpfs/u/home/DPTV/DPTVhnwz/scratch/mrqa_datasets/datasets', datasets_names=['mrpc'])\nProcess ID:  92\nUse single gpu!\n\nDownloading:   0%|                                  | 0.00/2.27k [00:00<?, ?B/s]\n\n\nDownloading:   0%|                                   | 0.00/773k [00:00<?, ?B/s]\n\n\nDownloading:   0%|                                  | 0.00/1.32M [00:00<?, ?B/s]\n\nDownloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-7b05eebbab7fdb15/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58...\nDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-7b05eebbab7fdb15/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58. Subsequent calls will reuse this data.\n0 mrpc Train\t {'source': 'sentence1: The companies uniformly declined to give specific numbers on customer turnover , saying they will release those figures only when they report overall company performance at year-end . sentence2: The companies , however , declined to give specifics on customer turnover , saying they would release figures only when they report their overall company performance .', 'target': '1', 'task': 'mrpc'}\n0 mrpc Val\t {'source': 'sentence1: The additional contribution brings total U.S. food aid to North Korea this year to 100,000 tonnes . sentence2: The donation of 60,000 tons brings the total of U.S. contributions for the year to 100,000 .', 'target': '1', 'task': 'mrpc'}\n0 10 10\n# all training samples: 10\n{'source': 'sentence1: The companies uniformly declined to give specific numbers on customer turnover , saying they will release those figures only when they report overall company performance at year-end . sentence2: The companies , however , declined to give specifics on customer turnover , saying they would release figures only when they report their overall company performance .', 'target': '1', 'task': 'mrpc'}\n\nDownloading:   0%|                                  | 0.00/1.18k [00:00<?, ?B/s]\n\n\nDownloading:   0%|                                   | 0.00/231M [00:00<?, ?B/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameters to optimize:  ['encoder.prefix_first', 'encoder.prefix_second']\nPrefix Parameters:  ['encoder.prefix_first', 'encoder.prefix_second']\nTrainable Parameters:  ['encoder.prefix_first', 'encoder.prefix_second']\n#Trainable Parameters:  6120\nAll trainable parameters: 6120; per task: 6120.0\n\n***** running training *****\n| batch_size: 16 | num_epochs: 1 | num_train: 1 |\nSaved soft prompts at: ./saved_models/mrpc.soft_prompts.epoch.0.pt\nEpoch 0 - Validation:  {'mrpc': {'accuracy': 0.0, 'f1': 0.0}}\nEpoch Best Val: 0.0 at Epoch 0.\n***** training ends *****\n\nbest dev acc: 0.00000 (at epoch 0)\nbest dev acc: -inf (at step 0)\n\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python main_ddp.py --datasets=mrpc --model_name=t5-small --enc_prompt_tokens 100 -ts 16 -e 1 --bottle_neck 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbdf4eb",
   "metadata": {},
   "source": [
    "### Step 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27458e1",
   "metadata": {},
   "source": [
    "Execution Start time: 00:49:12, End time: 00:49:12 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfb7d13",
   "metadata": {},
   "source": [
    "**Thought**: The `main_ddp.py` script has been executed successfully, and the resulting accuracy on the development set is 0.0. I will now submit this result in the required JSON format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44f1fcc",
   "metadata": {},
   "source": [
    "**Action (submit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b791df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'accuracy': 0.0}"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
