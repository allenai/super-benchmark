{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "effd7f23",
   "metadata": {},
   "source": [
    "### Query\n",
    "Use the lm-evaluation-harness to evaluate ai-forever/rugpt3small_based_on_gpt2 on the chegeka test set. Report \"metric\" and \"metric_stderr\" as a json structured as follows: {\"metric\": 0.0, \"metric_stderr\": 0.0} (replace 0.0 with the actual values).\n",
    "\n",
    "Additional instructions:\n",
    "1. Load only the first 10 rows of the dataset.\n",
    "2. Use the following hyperparameters: batch_size=1, num_fewshot=4\n",
    "\n",
    "Git repository: https://github.com/ai-forever/MERA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff161c31",
   "metadata": {},
   "source": [
    "### Prompt\n",
    "<details>\n",
    "[{'role': 'system', 'content': 'Interact in a jupyter notebook to solve the request.\\nOutput a thought and an action.\\nThought can reason about the current situation, and action allows you to interact with the Jupyter notebook. There are three types of actions:\\n(1) execute, which executes a notebook cell with the command under the \"content\" field. This could be a bash command (beginning with !), a python command, or magic command (beginning with %).\\n(2) edit, which allows you to replace existing lines in a file with new lines. To replace lines, you will need to provide the lines as they appear before the replacement, and the lines to replace them with.\\nIn the \"content\" field, format should be ```{filename}\\n<<<BEFORE_EDIT>>>\\nlines before edit\\nlines before edit\\n<<<AFTER_EDIT>>>\\nlines after edit\\nlines after edit```\\n  - Include the entire lines WITH indentation. After editing, make sure indentation is correct.\\n(3) submit, which submits the final answer (or null if there\\'s no answer to submit), formatted as JSON (make sure to use double quotes for keys and values).\\nAlways output exactly one \"thought\" and one \"action\", and encapsulate the content with <content>...</content> tags.\\nI will provide the cell output (observation) for you.\\n\\nTips:\\n- When a github repository is given, you should start by cloning it. You can read the readme file and browse the repo for installation and usage instructions.\\n- Avoid using conda, instead use pip for package management.\\n- Each execution has a 5-minute timeout. After this period, you can decide whether to interrupt the kernel or continue waiting.\\n  You should interrupt if: (1) you are prompted with an interactive message (e.g., \"Do you want to continue? [Y/n]\"). In that case, you can bypass it by executing echo \"y\" | command, or (2) the execution is taking longer than expected because the wrong task is being executed, such as a different model or incorrect number of epochs.\\n  You should continue to wait if the current execution is correct and proceeding as expected.\\n\\nHere are some examples.\\nGit repository: EleutherAI/lm-evaluation-harness\\nQuery: Evaluate EleutherAI/gpt-j-6B on hellaswag. What\\'s the score?\\nThought: First, I need to clone the GitHub repository \\'EleutherAI/lm-evaluation-harness\\'.\\nAction: execute\\n<content>\\n!git clone https://github.com/EleutherAI/lm-evaluation-harness.git\\n</content>\\nObservation: ```\\nCloning into \\'lm-evaluation-harness\\'...\\n```\\nThought: The repository has been cloned. Next, I should check the README file for installation and usage instructions to proceed with evaluating EleutherAI/gpt-j-6B on hellaswag.\\nAction: execute\\n<content>\\n!cat lm-evaluation-harness/README.md\\n</content>\\nObservation: ```# Language Model Evaluation Harness ... truncated ...\\n```\\nThought: The README provides detailed instructions for installing the `lm-eval` package and evaluating models like GPT-J-6B on tasks like hellaswag. I need to install the package and its dependencies first.\\nAction: execute\\n<content>\\n!pip install -e lm-evaluation-harness/.\\n</content>\\nObservation: ```\\nObtaining file:///home/nora/lm-evaluation-harness\\n  Installing build dependencies ... truncated ...\\n```\\nThought: The `lm-eval` package and its dependencies have been installed successfully. Now, I can proceed with evaluating the EleutherAI/gpt-j-6B model on the hellaswag task as per the instructions in the README.\\nAction: execute\\n<content>\\n!lm_eval --model hf --model_args pretrained=EleutherAI/gpt-j-6B --tasks hellaswag --device cuda:0 --batch_size 8\\n</content>\\nObservation: ```\\n{\\n  \"hellaswag\": {\\n    \"acc\": 0.24603658536585363,\\n    \"acc_norm\": 0.34436059706446426,\\n  },  ... truncated ...\\n```\\nThought: The `lm-eval` was executed successfully, providing the metrics I needed. The `acc` metric is 0.2460. The `acc_norm` metric is 0.3444\\nAction: submit\\n<content>\\n{\"acc\": 0.2460, \"acc_norm\": 0.3444}\\n</content>\\n\\nQuery: use pandas to read the file \\'data.csv\\' and display the first 2 rows.\\nThought: First, I need to read the file \\'data.csv\\' using the pandas library.\\nAction: execute\\n<content>\\nimport pandas as pd\\npd.read_csv(\\'data.csv\\').head()\\n</content>\\nObservation: ```\\n    name\\n  0  Alice\\n   1    Bob\\n  2  Charlie\\n```\\nThought: The file \\'data.csv\\' has been read successfully and the first 2 rows have been displayed.\\nAction: submit\\n<content>\\n[\"Alice\", \"Bob\"]\\n</content>\\n\\nQuery: Change the model name in the \\'config.json\\' file from \\'roberta-base\\' to \\'roberta-large\\'.\\nThought: First, I need to read the \\'config.json\\' file to find the location where I can replace the model name.\\nAction: execute\\n<content>\\n!cat config.json\\n</content>\\nObservation: ```\\n{\\n    \"model\": \"roberta-base\",\\n    \"batch_size\": 8,\\n    \"epochs\": 10\\n}\\n```\\nThought: The file has been read successfully. Now, I can replace the model name in the \\'config.json\\' file.\\nAction: edit\\n<content>\\nconfig.json\\n<<<BEFORE_EDIT>>>\\n{\\n    \"model\": \"roberta-base\",\\n    \"batch_size\": 8,\\n<<<AFTER_EDIT>>>\\n{\\n    \"model\": \"roberta-large\",\\n    \"batch_size\": 8,\\n</content>\\nObservation: \"editing file config.json\"\\nThought: The model name has been replaced in the \\'config.json\\' file.\\nAction: submit\\n<content>\\nnull\\n</content>\\n'}, {'role': 'user', 'content': 'Query: Use the lm-evaluation-harness to evaluate ai-forever/rugpt3small_based_on_gpt2 on the chegeka test set. Report \"metric\" and \"metric_stderr\" as a json structured as follows: {\"metric\": 0.0, \"metric_stderr\": 0.0} (replace 0.0 with the actual values).\\n\\nAdditional instructions:\\n1. Load only the first 10 rows of the dataset.\\n2. Use the following hyperparameters: batch_size=1, num_fewshot=4\\n\\nGit repository: https://github.com/ai-forever/MERA'}]\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2a14a6",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1211344c",
   "metadata": {},
   "source": [
    "Execution Start time: 19:41:28, End time: 19:41:29 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e81c47a",
   "metadata": {},
   "source": [
    "**Thought**: First, I need to clone the GitHub repository 'ai-forever/MERA' to access the lm-evaluation-harness and any specific instructions or scripts it might contain for evaluating models. After cloning, I will check the README or relevant documentation for installation and usage instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fb9d5e",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c9aee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cloning into 'MERA'...\nremote: Enumerating objects: 1488, done.\u001b[K\nremote: Counting objects: 100% (550/550)\u001b[K\nremote: Counting objects: 100% (550/550), done.\u001b[K\nremote: Compressing objects: 100% (394/394)\u001b[K\nremote: Compressing objects: 100% (394/394), done.\u001b[K\nremote: Total 1488 (delta 221), reused 362 (delta 153), pack-reused 938 (from 1)\u001b[K\nReceiving objects: 100% (1488/1488)\nReceiving objects: 100% (1488/1488), 8.66 MiB | 19.61 MiB/s, done.\nResolving deltas: 100% (616/616)\nResolving deltas: 100% (616/616), done.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!git clone https://github.com/ai-forever/MERA.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcc3a15",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c77f62e",
   "metadata": {},
   "source": [
    "Execution Start time: 19:41:31, End time: 19:41:31 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3220a4c7",
   "metadata": {},
   "source": [
    "**Thought**: The repository has been cloned successfully. Next, I should check the README file or other documentation in the repository to find installation and usage instructions for evaluating models using the lm-evaluation-harness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbaf7af",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30ee9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# MERA\n\n<p align=\"center\">\n  <picture>\n    <img alt=\"MERA\" src=\"docs/mera-logo.svg\" style=\"max-width: 100%;\">\n  </picture>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://opensource.org/licenses/MIT\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-yellow.svg\">\n    </a>\n    <a href=\"https://github.com/ai-forever/MERA/releases\">\n    <img alt=\"Release\" src=\"https://img.shields.io/badge/release-v1.1.0-blue\">\n    </a>\n\n</p>\n\n<h2 align=\"center\">\n    <p> MERA (Multimodal Evaluation for Russian-language Architectures) is a new open benchmark for the Russian language for evaluating fundamental models.\n</p>\n</h2>\n\n## About MERA\n\nMERA benchmark brings together all industry and academic players in one place to study the capabilities of fundamental models, draw attention to AI problems, develop collaboration within the Russian Federation and in the international arena and create an independent unified system for measuring all current models. This repository is a customized version of original [**Language Model Evaluation Harness**](https://github.com/EleutherAI/lm-evaluation-harness/tree/v0.3.0) (**LM-Harness** `v0.3.0`).\n\nOur contributions to this project are:\n\n- Instruction-based tasks available on 🤗 HuggingFace [dataset card](https://huggingface.co/datasets/ai-forever/MERA).\n- Customized version of LM-Harness evaluation code for models (`v0.3.0`).\n- Benchmark website with the [Leaderboard](https://mera.a-ai.ru/en/leaderboard) and the scoring submission system.\n- Baselines of the open models and Human Benchmark.\n\n\nThe MERA benchmark includes 21 text tasks (17 base tasks + 4 diagnostic tasks). See the task-table for a complete list.\n        \n| Name | Task Name | Task Type | Test Size | N-shots | Metrics |\n| --- | --- | --- | --- | --- | --- |\n| MathLogicQA | mathlogicqa | Math, Logic | 1143 | 5 | Acc |\n| MultiQ | multiq | Reasoning | 900 | 0 | EM / F1 |\n| PARus | parus | Common Sense | 500 | 0 | Acc |\n| RCB | rcb | NLI | 438 | 0 | Acc / F1_macro |\n| ruModAr | rumodar | Math, Logic | 6000 | 0 | Acc |\n| ruMultiAr | rumultiar | Math | 1024 | 5 | Acc |\n| ruOpenBookQA | ruopenbookqa | World Knowledge | 400 | 5 | Acc / F1_macro |\n| ruTiE | rutie | Reasoning, Dialogue Context, Memory | 430 | 0 | Acc |\n| ruWorldTree | ruworldtree | World Knowledge | 525 | 5 | Acc / F1_macro |\n| RWSD | rwsd | Reasoning | 260 | 0 | Acc |\n| SimpleAr | simplear | Math | 1000 | 5 | Acc |\n| BPS | bps | Code, Math | 1000 | 2 | Acc |\n| CheGeKa | chegeka | World Knowledge | 416 | 4 | EM / F1 |\n| LCS | lcs | Code, Math | 500 | 2 | Acc |\n| ruHumanEval | ruhumaneval | Code | 164 | 0 | Pass@k |\n| ruMMLU | rummlu | Reasoning | 961 | 5 | Acc |\n| USE | use | Exam | 900 | 0 | Grade_norm |\n| ruDetox | rudetox | Ethics | 800 | 0 | J(STA, SIM, FL) |\n| ruEthics | ruethics | Ethics | 1935 | 0 | 5 MCC |\n| ruHateSpeech | ruhatespeech | Ethics | 265 | 0 | Acc |\n| ruHHH | ruhhh | Ethics | 178 | 0 | Acc |\n\nOur aim is to evaluate all the models:\n\n- in the same scenarios;\n- using the same metrics;\n- with the same adaptation strategy (e.g., prompting); \n- provide an opportunity to make controlled and clear comparisons.\n\nMERA is a collaborative project created in a union of industry and academia with the **support of all the companies**, that are creating the foundation models, to ensure fair and transparent leaderboards for the models evaluation. \n\n*We express our gratitude to our team and partners:* \n\n*SberDevices, Sber AI, Yandex, Skoltech AI, MTS AI, NRU HSE, Russian Academy of Sciences, etc.*\n\n*Powered by [Aliance AI](https://a-ai.ru/)*\n\n## Contents\n\nThe repository has the following structure:\n\n- [`examples`](examples/instruction.ipynb) — the examples of loading and using data.\n- [`humanbenchmarks`](humanbenchmarks/README.md) — materials and code for human evaluation.\n- [`modules`](modules/scoring/README.md) — the examples of scoring scripts that are used on the website for scoring your submission.\n- [`lm-evaluation-harness`](lm-evaluation-harness) — a framework for few-shot evaluation of language models.\n    \n\n## The process of submission is the following:\n- to view the datasets use the [HuggingFace preview](https://huggingface.co/datasets/ai-forever/MERA/viewer/ruethics)  or run the prepared [instruction](https://github.com/ai-forever/MERA/blob/main/examples/instruction.ipynb);\n- clone MERA benchmark [repository](https://github.com/ai-forever/MERA);\n- to get submission files use [shell script](https://github.com/ai-forever/MERA/blob/main/lm-evaluation-harness/README.md\\#run-full-benchmark-with-bash-script) and the provided customized **lm-harness** code (the actual model is not required for submission and evaluation).\n- run your model on the all datasets using the code of lm-harness: the result of the code is the archive in ZIP format for the submission;\n- register on the website;\n- upload the submission files (ZIP) via the platform interface for the automatic assessment.\n\n*Note that, the evaluation result is then displayed in the user's account and is kept **private**. Those who want to make their submission results public could use the *''Publish''* function. After validation of the submission is approved, the model's overall score will be shown publicly.*\n*The parameters of the generation, prompts and few-shot/zero-shot are fixed. You can vary them for your own purposes. If you want to submit your results on the public leaderboard check that these parameters are the same and please add the logs. We have to be sure that the scenarios for the models evaluation are the same and reproducible.*\n\nWe provide the [sample submission](modules/scoring/examples) for you to check the format.\n\nThe process of the whole MERA evaluation is described on the Figure:\n\n![evaluation setup](docs/mera.png)\n\n------------------------------------\n\n📌 It’s the first text version of the benchmark. We are to expand and develop it in the future with new tasks and multimodality.\n\nFeel free to ask any questions regarding our work, write on email mera@a-ai.ru. If you have ideas and new tasks feel free to suggest them, **it’s important!** If you see any bugs, or you know how to make the code better please suggest the fixes via pull-requests and issues in this official github 🤗. We will be glad to get the feedback in any way.\n\n\n## Cite as\n\n```\n@misc{fenogenova2024mera,\n    title={{MERA}: A Comprehensive {LLM} Evaluation in {Russian}}, \n    author={Alena Fenogenova and Artem Chervyakov and Nikita Martynov and Anastasia Kozlova and Maria Tikhonova and Albina Akhmetgareeva and Anton Emelyanov and Denis Shevelev and Pavel Lebedev and Leonid Sinev and Ulyana Isaeva and Katerina Kolomeytseva and Daniil Moskovskiy and Elizaveta Goncharova and Nikita Savushkin and Polina Mikhailova and Denis Dimitrov and Alexander Panchenko and Sergei Markov},\n    year={2024},\n    eprint={2401.04531},\n    url = {https://arxiv.org/abs/2401.04531},\n    eprinttype={arXiv},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n    journal={arXiv},\n    volume={2401.04531}\n}\n```\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!cat MERA/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234a3174",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9274598f",
   "metadata": {},
   "source": [
    "Execution Start time: 19:41:33, End time: 19:41:33 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442a0388",
   "metadata": {},
   "source": [
    "**Thought**: The README file provides an overview of the MERA benchmark and mentions that it includes a customized version of the lm-evaluation-harness. It also provides a link to the lm-evaluation-harness directory, which likely contains the necessary scripts and instructions for evaluating models. I should navigate to the lm-evaluation-harness directory and check for any README or documentation specific to running evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1900d965",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5169d743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CITATION.bib   ignore.txt  pyproject.toml    run_mera_seq2seq.sh\nREADME.md      lm_eval\t   requirements.txt  scripts\nfasilitate.py  main.py\t   run_mera.sh\t     setup.py\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ls MERA/lm-evaluation-harness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8f6021",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495767c0",
   "metadata": {},
   "source": [
    "Execution Start time: 19:41:35, End time: 19:41:35 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d322ed8",
   "metadata": {},
   "source": [
    "**Thought**: The `lm-evaluation-harness` directory contains a `README.md` file, which likely contains instructions for running evaluations. I should check this file to understand how to evaluate the model using the harness and apply the specified hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9511745a",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700de34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# MERA with Language Model Evaluation Harness\n\nMERA: Multimodal Evaluation for Russian-language Architectures\n\nThe LM-harness support for the MERA benchmark datasets.\n\n## Overview\n\nThis project provides a unified framework to test generative language models on the MERA benchmark and its evaluation tasks.\n\n## Install\n\nTo install `lm-eval` from the repository main branch, run the following command:\n\n```bash\npip install -e .\n```\n\nTo support loading GPTQ quantized models, install the package with the `auto-gptq` extra:\n\n```bash\npip install -e \".[auto-gptq]\"\n```\n\n## MERA Benchmark:\n\n### Run full benchmark with the bash script\n\nBelow is a sample command to run the benchmark with `ai-forever/rugpt3large_based_on_gpt2` (`AutoModelForCausalLM` class compatible)\nmodel from Huggingface Hub:\n\n```linux\nCUDA_VISIBLE_DEVICES=0 MERA_FOLDER=\"$PWD/mera_results/rugpt3large_760m_defaults\" MERA_MODEL_STRING=\"pretrained=ai-forever/rugpt3large_based_on_gpt2,dtype=auto\" bash run_mera.sh\n```\n\nBelow is a sample command to run the benchmark with `ai-forever/FRED-T5-large` (`AutoModelForSeq2SeqLM` class compatible)\nmodel from Huggingface Hub:\n\n```linux\nCUDA_VISIBLE_DEVICES=0 MERA_FOLDER=\"$PWD/mera_results/FRED-T5-large_defaults\" MERA_MODEL_STRING=\"pretrained=ai-forever/FRED-T5-large,dtype=auto\" bash run_mera_seq2seq.sh\n```\n\nUse `CUDA_VISIBLE_DEVICES` to set cuda device visibility, `MERA_FOLDER` for path to store outputs,\n`MERA_MODEL_STRING` to setup `model_args` parameter of `lm-evaluation-harness`'s `main.py`.\nUse `MERA_COMMON_SETUP` to change default parameters for model inferencing with `main.py` (defaults are\n`--model hf-causal-experimental --device cuda --max_batch_size=64 --batch_size=auto --inference`).\nSee more on parameters in the next section.\n\nNotice two different bash scripts: `run_mera.sh` for models compatible with transformers' `AutoModelForCausalLM` class,\nand `run_mera_seq2seq.sh` for models compatible with transformers' `AutoModelForSeq2SeqLM` class.\n\n### Run specific benchmark tasks manually (ruMMLU example)\n\nRunning specific benchmark tasks is available with the `main.py` script.\n\nExample:\n```shell\nCUDA_VISIBLE_DEVICES=3 python main.py --model hf-causal-experimental --model_args pretrained=mistralai/Mistral-7B-v0.1,dtype=auto,max_length=11500 \\\n--device cuda --output_base_path=\"$PWD/mera_results/Mistral-7B-v0.1_defaults\" --batch_size=1 \\\n--inference --write_out --no_cache --tasks rummlu --num_fewshot=5 \\\n--output_path=\"$PWD/mera_results/Mistral-7B-v0.1_defaults/rummlu_result.json\"\n```\n\n#### Notes on `main.py` settings\n\nUse `--tasks` to provide comma separated list of tasks to run (available options are: `bps`, `chegeka`, `lcs`,\n`mathlogicqa`, `multiq`, `parus`, `rcb`, `rudetox`, `ruethics`, `ruhatespeech`, `ruhhh`, `ruhumaneval`, `rummlu`,\n`rumodar`, `rumultiar`, `ruopenbookqa`, `rutie`, `ruworldtree`, `rwsd`, `simplear`, `use`).\nAvoiding this argument will run all tasks with same provided settings.\n\n`--num_fewshot` sets fewshot count. MERA supposes to run tasks with the following fewshot count:\n* `--num_fewshot=0` (zeroshot) with `multiq`, `parus`, `rcb`, `rumodar`, `rwsd`, `use`, `rudetox`, `ruethics`,\n`ruhatespeech`, `ruhhh`, `rutie`, and `ruhumaneval`;\n* `--num_fewshot=2` with `bps` and `lcs`;\n* `--num_fewshot=4` with `chegeka`;\n* `--num_fewshot=5` with `mathlogicqa`, `ruworldtree`, `ruopenbookqa`, `simplear`, `rumultiar`, and `rummlu`.\n\nUse `CUDA_VISIBLE_DEVICES` to set cuda device visibility (setting `--device cuda:3` works inconsistently).\n\n`--model hf-causal-experimental` is used for models compatible with transformers' `AutoModelForCausalLM` class\nand `hf-seq2seq` is used for models compatible with transformers' `AutoModelForSeq2SeqLM` class.\n\n`--model_args` is for comma separated parameters of `from_pretrained` method of autoclass. One should be aware of\nhardware requirements to run big models and limit the maximum input length of models with the parameter `max_length`\nto avoid out-of-memory errors during a run.\n\n`--batch_size=1` is set to use a batch size of 1 to maximize benchmark results reproducibility.\n`--batch_size=auto` may be set to determine a batch size automatically based on the evaluated tasks and inputs maximum value\nto start to search down is set with `--max_batch_size`. Bigger batches may speed up running the whole MERA benchmark,\nbut results may become irreproducible, so it is not the default suggestion.\n\n`--output_base_path` is a path to dir (will be created) to store data for submission preparation and logs.\n\n`--inference` is important to use this key always. It allows to run on datasets without proper replies provided\n(score result 0 will be reported).\n\nThe `--write_out` command turns on extra logging necessary for public submissions. \n\n`--no_cache` is used to turn off the caching of tokenized inputs and model files (datasets are not cached).\n\n`--output_path` is a path to an extra log file with parameters of run and results of the task. It is preferred to be inside\n`output_base_path` directory.\n\n\n### Convert lm-harness to submission\nThe bash script above runs the submission zip-packing routine. Below is a way to run packing manually.\n\nFor converting run\n\n```shell\npython scripts/log_to_submission.py\n```\n\nCmd arguments:\n\n* `--outputs_dir` — path to directory with outputs (`MERA_FOLDER` from bash script above)\n* `--dst_dir` — directory for store submission zip\n* `--dataset_dir` — path to `lm_eval/datasets/`\n* `--logs_public_submit` (`--no-logs_public_submit`) — pack logs for public submission in separate file (true by default)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!cat MERA/lm-evaluation-harness/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332104d9",
   "metadata": {},
   "source": [
    "### Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c984c9d1",
   "metadata": {},
   "source": [
    "Execution Start time: 19:41:37, End time: 19:42:08 (duration: 31 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add5985d",
   "metadata": {},
   "source": [
    "**Thought**: The README provides detailed instructions for evaluating models using the lm-evaluation-harness. I need to run the `main.py` script with the specified model and hyperparameters. The task is `chegeka`, and I need to set `num_fewshot=4` and `batch_size=1`. Additionally, I need to load only the first 10 rows of the dataset. I will first install the necessary dependencies and then proceed with running the evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abc052c",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1c24bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Obtaining file:///content/MERA/lm-evaluation-harness\n  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build editable ... \u001b[?25l-\b \bdone\n\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\n\u001b[?25hCollecting datasets>=2.0.0 (from lm_eval==0.3.0)\n  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\nCollecting einops (from lm_eval==0.3.0)\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nCollecting jsonlines (from lm_eval==0.3.0)\n  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\nCollecting numexpr (from lm_eval==0.3.0)\n  Downloading numexpr-2.10.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.2 kB)\nCollecting openai>=0.6.4 (from lm_eval==0.3.0)\n  Downloading openai-1.41.0-py3-none-any.whl.metadata (22 kB)\nCollecting omegaconf>=2.2 (from lm_eval==0.3.0)\n  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\nCollecting peft>=0.2.0 (from lm_eval==0.3.0)\n  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\nCollecting pybind11>=2.6.2 (from lm_eval==0.3.0)\n  Downloading pybind11-2.13.4-py3-none-any.whl.metadata (9.5 kB)\nCollecting pycountry (from lm_eval==0.3.0)\n  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\nCollecting pytablewriter (from lm_eval==0.3.0)\n  Downloading pytablewriter-1.2.0-py3-none-any.whl.metadata (37 kB)\nCollecting rouge-score>=0.0.4 (from lm_eval==0.3.0)\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\n\u001b[?25hCollecting sacrebleu==1.5.0 (from lm_eval==0.3.0)\n  Downloading sacrebleu-1.5.0-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.10/site-packages (from lm_eval==0.3.0) (1.5.0)\nCollecting sqlitedict (from lm_eval==0.3.0)\n  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\n\u001b[?25hRequirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/site-packages (from lm_eval==0.3.0) (2.2.1+cpu)\nCollecting tqdm-multiprocess (from lm_eval==0.3.0)\n  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: transformers>=4.36.2 in /usr/local/lib/python3.10/site-packages (from lm_eval==0.3.0) (4.40.2)\nCollecting zstandard (from lm_eval==0.3.0)\n  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\nCollecting accelerate>=0.17.1 (from lm_eval==0.3.0)\n  Downloading accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\nCollecting portalocker (from sacrebleu==1.5.0->lm_eval==0.3.0)\n  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/site-packages (from accelerate>=0.17.1->lm_eval==0.3.0) (1.25.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from accelerate>=0.17.1->lm_eval==0.3.0) (24.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/site-packages (from accelerate>=0.17.1->lm_eval==0.3.0) (5.9.8)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/site-packages (from accelerate>=0.17.1->lm_eval==0.3.0) (6.0.1)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/site-packages (from accelerate>=0.17.1->lm_eval==0.3.0) (0.23.3)\nRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/site-packages (from accelerate>=0.17.1->lm_eval==0.3.0) (0.4.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from datasets>=2.0.0->lm_eval==0.3.0) (3.15.1)\nCollecting pyarrow>=15.0.0 (from datasets>=2.0.0->lm_eval==0.3.0)\n  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\nCollecting dill<0.3.9,>=0.3.0 (from datasets>=2.0.0->lm_eval==0.3.0)\n  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from datasets>=2.0.0->lm_eval==0.3.0) (2.0.3)\nCollecting requests>=2.32.2 (from datasets>=2.0.0->lm_eval==0.3.0)\n  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/site-packages (from datasets>=2.0.0->lm_eval==0.3.0) (4.66.4)\nCollecting xxhash (from datasets>=2.0.0->lm_eval==0.3.0)\n  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nCollecting multiprocess (from datasets>=2.0.0->lm_eval==0.3.0)\n  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.0.0->lm_eval==0.3.0) (2023.6.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/site-packages (from datasets>=2.0.0->lm_eval==0.3.0) (3.8.3)\nCollecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.2->lm_eval==0.3.0)\n  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n\u001b[?25l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/117.0 kB ? eta -:--:--\n\n\u001b[?25h  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\n\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/site-packages (from openai>=0.6.4->lm_eval==0.3.0) (4.4.0)\nCollecting distro<2,>=1.7.0 (from openai>=0.6.4->lm_eval==0.3.0)\n  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/site-packages (from openai>=0.6.4->lm_eval==0.3.0) (0.27.0)\nCollecting jiter<1,>=0.4.0 (from openai>=0.6.4->lm_eval==0.3.0)\n  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/site-packages (from openai>=0.6.4->lm_eval==0.3.0) (1.10.16)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/site-packages (from openai>=0.6.4->lm_eval==0.3.0) (1.3.1)\nRequirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/site-packages (from openai>=0.6.4->lm_eval==0.3.0) (4.12.2)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/site-packages (from rouge-score>=0.0.4->lm_eval==0.3.0) (2.1.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/site-packages (from rouge-score>=0.0.4->lm_eval==0.3.0) (3.8.1)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/site-packages (from rouge-score>=0.0.4->lm_eval==0.3.0) (1.16.0)\nRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn>=0.24.1->lm_eval==0.3.0) (1.11.4)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn>=0.24.1->lm_eval==0.3.0) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn>=0.24.1->lm_eval==0.3.0) (3.5.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/site-packages (from torch>=2.0->lm_eval==0.3.0) (1.12)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch>=2.0->lm_eval==0.3.0) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch>=2.0->lm_eval==0.3.0) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers>=4.36.2->lm_eval==0.3.0) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/site-packages (from transformers>=4.36.2->lm_eval==0.3.0) (0.19.1)\nRequirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/site-packages (from jsonlines->lm_eval==0.3.0) (23.2.0)\nRequirement already satisfied: setuptools>=38.3.0 in /usr/local/lib/python3.10/site-packages (from pytablewriter->lm_eval==0.3.0) (68.1.2)\nCollecting DataProperty<2,>=1.0.1 (from pytablewriter->lm_eval==0.3.0)\n  Downloading DataProperty-1.0.1-py3-none-any.whl.metadata (11 kB)\nCollecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm_eval==0.3.0)\n  Downloading mbstrdecoder-1.1.3-py3-none-any.whl.metadata (4.0 kB)\nCollecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm_eval==0.3.0)\n  Downloading pathvalidate-3.2.0-py3-none-any.whl.metadata (11 kB)\nCollecting tabledata<2,>=1.3.1 (from pytablewriter->lm_eval==0.3.0)\n  Downloading tabledata-1.3.3-py3-none-any.whl.metadata (3.7 kB)\nCollecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm_eval==0.3.0)\n  Downloading tcolorpy-0.1.6-py3-none-any.whl.metadata (6.4 kB)\nCollecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.3.0)\n  Downloading typepy-1.3.2-py3-none-any.whl.metadata (9.3 kB)\nCollecting colorama (from tqdm-multiprocess->lm_eval==0.3.0)\n  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=0.6.4->lm_eval==0.3.0) (3.7)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=0.6.4->lm_eval==0.3.0) (1.2.1)\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->lm_eval==0.3.0) (2.1.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->lm_eval==0.3.0) (6.0.5)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->lm_eval==0.3.0) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->lm_eval==0.3.0) (1.9.4)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->lm_eval==0.3.0) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->lm_eval==0.3.0) (1.3.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai>=0.6.4->lm_eval==0.3.0) (2024.6.2)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai>=0.6.4->lm_eval==0.3.0) (1.0.5)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=0.6.4->lm_eval==0.3.0) (0.14.0)\nCollecting chardet<6,>=3.0.4 (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm_eval==0.3.0)\n  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.0.0->lm_eval==0.3.0) (2.0.7)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.3.0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2018.9 in /usr/local/lib/python3.10/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.3.0) (2024.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch>=2.0->lm_eval==0.3.0) (2.1.5)\nRequirement already satisfied: click in /usr/local/lib/python3.10/site-packages (from nltk->rouge-score>=0.0.4->lm_eval==0.3.0) (8.1.7)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets>=2.0.0->lm_eval==0.3.0) (2024.1)\nRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/site-packages (from sympy->torch>=2.0->lm_eval==0.3.0) (1.3.0)\nDownloading sacrebleu-1.5.0-py3-none-any.whl (65 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/65.6 kB ? eta -:--:--\n\n\u001b[?25hDownloading accelerate-0.33.0-py3-none-any.whl (315 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/315.1 kB ? eta -:--:--\n\n\u001b[?25hDownloading datasets-2.21.0-py3-none-any.whl (527 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/527.3 kB ? eta -:--:--\n\n\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/79.5 kB ? eta -:--:--\n\n\u001b[?25hDownloading openai-1.41.0-py3-none-any.whl (362 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/362.4 kB ? eta -:--:--\n\n\u001b[?25hDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/296.4 kB ? eta -:--:--\n\n\u001b[?25hDownloading pybind11-2.13.4-py3-none-any.whl (240 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/240.9 kB ? eta -:--:--\n\n\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/43.2 kB ? eta -:--:--\n\n\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\nDownloading numexpr-2.10.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (405 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/405.0 kB ? eta -:--:--\n\n\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/6.3 MB ? eta -:--:--\n\n\u001b[?25hDownloading pytablewriter-1.2.0-py3-none-any.whl (111 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/111.1 kB ? eta -:--:--\n\n\u001b[?25hDownloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\nDownloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/5.4 MB ? eta -:--:--\n\n\u001b[?25hDownloading DataProperty-1.0.1-py3-none-any.whl (27 kB)\nDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/116.3 kB ? eta -:--:--\n\n\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\nDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/318.9 kB ? eta -:--:--\n\n\u001b[?25hDownloading mbstrdecoder-1.1.3-py3-none-any.whl (7.8 kB)\nDownloading pathvalidate-3.2.0-py3-none-any.whl (23 kB)\nDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/39.9 MB ? eta -:--:--\n\n\n\n\n\n\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/64.9 kB ? eta -:--:--\n\n\u001b[?25hDownloading tabledata-1.3.3-py3-none-any.whl (11 kB)\nDownloading tcolorpy-0.1.6-py3-none-any.whl (8.1 kB)\nDownloading typepy-1.3.2-py3-none-any.whl (31 kB)\nDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\nDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/134.8 kB ? eta -:--:--\n\n\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\nDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/194.1 kB ? eta -:--:--\n\n\u001b[?25hDownloading chardet-5.2.0-py3-none-any.whl (199 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/199.4 kB ? eta -:--:--\n\n\u001b[?25hBuilding wheels for collected packages: lm_eval, antlr4-python3-runtime, rouge-score, sqlitedict\n  Building editable for lm_eval (pyproject.toml) ... \u001b[?25l-\b \bdone\n\u001b[?25h  Created wheel for lm_eval: filename=lm_eval-0.3.0-0.editable-py3-none-any.whl size=5375 sha256=4775069cb435eddd1f8caf4094d7183feffe7bbe9b3b5265f216173790497cb2\n  Stored in directory: /tmp/pip-ephem-wheel-cache-hq83c65x/wheels/d5/07/68/d233f5a039882bb6e5a25b5849d05577ab2db5321acad8a026\n  Building wheel for antlr4-python3-runtime (pyproject.toml) ... \u001b[?25l-\b \b\\\b \bdone\n\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144552 sha256=0a3c29533f9daaaf25ad2d696193300886364a11b61048d8012f94afcd176a1b\n  Stored in directory: /tmp/pip-ephem-wheel-cache-hq83c65x/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n  Building wheel for rouge-score (pyproject.toml) ... \u001b[?25l-\b \bdone\n\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=9b7d4f38ddd8399715e79213715d7cf793b6711a8cd6a73504b52eac2f858e87\n  Stored in directory: /tmp/pip-ephem-wheel-cache-hq83c65x/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n  Building wheel for sqlitedict (pyproject.toml) ... \u001b[?25l-\b \bdone\n\u001b[?25h  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16862 sha256=989bf47b76b869b8a7a03ac3f3eb451680c0c9ec2ff5e1f62db2fb0aea030dd2\n  Stored in directory: /tmp/pip-ephem-wheel-cache-hq83c65x/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\nSuccessfully built lm_eval antlr4-python3-runtime rouge-score sqlitedict\nInstalling collected packages: sqlitedict, antlr4-python3-runtime, zstandard, xxhash, tcolorpy, requests, pycountry, pybind11, pyarrow, portalocker, pathvalidate, omegaconf, numexpr, jsonlines, jiter, einops, distro, dill, colorama, chardet, tqdm-multiprocess, sacrebleu, rouge-score, multiprocess, mbstrdecoder, typepy, openai, accelerate, datasets, peft, DataProperty, tabledata, pytablewriter, lm_eval\n  Attempting uninstall: requests\n    Found existing installation: requests 2.31.0\n    Uninstalling requests-2.31.0:\n      Successfully uninstalled requests-2.31.0\nSuccessfully installed DataProperty-1.0.1 accelerate-0.33.0 antlr4-python3-runtime-4.9.3 chardet-5.2.0 colorama-0.4.6 datasets-2.21.0 dill-0.3.8 distro-1.9.0 einops-0.8.0 jiter-0.5.0 jsonlines-4.0.0 lm_eval-0.3.0 mbstrdecoder-1.1.3 multiprocess-0.70.16 numexpr-2.10.1 omegaconf-2.3.0 openai-1.41.0 pathvalidate-3.2.0 peft-0.12.0 portalocker-2.10.1 pyarrow-17.0.0 pybind11-2.13.4 pycountry-24.6.1 pytablewriter-1.2.0 requests-2.32.3 rouge-score-0.1.2 sacrebleu-1.5.0 sqlitedict-2.1.0 tabledata-1.3.3 tcolorpy-0.1.6 tqdm-multiprocess-0.0.11 typepy-1.3.2 xxhash-3.5.0 zstandard-0.23.0\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install -e MERA/lm-evaluation-harness/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beef49fb",
   "metadata": {},
   "source": [
    "### Step 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b368f8a9",
   "metadata": {},
   "source": [
    "Execution Start time: 19:42:11, End time: 19:42:20 (duration: 9 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91302d1a",
   "metadata": {},
   "source": [
    "**Thought**: The dependencies for the lm-evaluation-harness have been installed successfully. Now, I need to run the evaluation using the `main.py` script with the specified model `ai-forever/rugpt3small_based_on_gpt2`, task `chegeka`, and hyperparameters `batch_size=1` and `num_fewshot=4`. Additionally, I need to ensure that only the first 10 rows of the dataset are loaded. I will proceed with running the evaluation script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61298466",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5d5d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "usage: main.py [-h] --model\n               {hf,hf-causal,hf-causal-experimental,hf-seq2seq,gpt2,gpt3,anthropic,dummy,hf-gigachat}\n               [--model_args MODEL_ARGS]\n               [--tasks {bps,chegeka,lcs,mathlogicqa,multiq,parus,rcb,rudetox,ruethics,ruhatespeech,ruhhh,ruhumaneval,rummlu,rumodar,rumultiar,ruopenbookqa,rutie,ruworldtree,rwsd,simplear,use}]\n               [--num_fewshot NUM_FEWSHOT] [--batch_size BATCH_SIZE]\n               [--max_batch_size MAX_BATCH_SIZE] [--device DEVICE]\n               [--output_path OUTPUT_PATH] [--limit LIMIT] [--no_cache]\n               [--decontamination_ngrams_path DECONTAMINATION_NGRAMS_PATH]\n               [--description_dict_path DESCRIPTION_DICT_PATH]\n               [--check_integrity] [--write_out]\n               [--output_base_path OUTPUT_BASE_PATH] [--inference]\nmain.py: error: argument --model: invalid choice: 'ai-forever/rugpt3small_based_on_gpt2' (choose from 'hf', 'hf-causal', 'hf-causal-experimental', 'hf-seq2seq', 'gpt2', 'gpt3', 'anthropic', 'dummy', 'hf-gigachat')\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python MERA/lm-evaluation-harness/main.py --model ai-forever/rugpt3small_based_on_gpt2 --tasks chegeka --num_fewshot 4 --batch_size 1 --limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05163722",
   "metadata": {},
   "source": [
    "### Step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356513a0",
   "metadata": {},
   "source": [
    "Execution Start time: 19:42:23, End time: 19:43:12 (duration: 49 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7132adbb",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the `--model` argument expects a model type rather than a specific model name. The model type should be one of the listed options, such as `hf` for Hugging Face models. Since `ai-forever/rugpt3small_based_on_gpt2` is a Hugging Face model, I should use `hf` as the model type and specify the model name in the `--model_args` parameter. I will adjust the command accordingly and rerun it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a759c478",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a8ab08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WARNING: --limit SHOULD ONLY BE USED FOR TESTING. REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\nSelected Tasks: ['chegeka']\nUsing device 'cuda'\n/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\nconfig.json:   0%|                                    | 0.00/720 [00:00<?, ?B/s]\n\n\npytorch_model.bin:   0%|                             | 0.00/551M [00:00<?, ?B/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraceback (most recent call last):\n  File \"/content/MERA/lm-evaluation-harness/main.py\", line 141, in <module>\n    main()\n  File \"/content/MERA/lm-evaluation-harness/main.py\", line 98, in main\n    results = evaluator.simple_evaluate(\n  File \"/content/MERA/lm-evaluation-harness/lm_eval/utils.py\", line 238, in _wrapper\n    return fn(*args, **kwargs)\n  File \"/content/MERA/lm-evaluation-harness/lm_eval/evaluator.py\", line 85, in simple_evaluate\n    lm = lm_eval.models.get_model(model).create_from_arg_string(\n  File \"/content/MERA/lm-evaluation-harness/lm_eval/base.py\", line 119, in create_from_arg_string\n    return cls(**args, **args2)\n  File \"/content/MERA/lm-evaluation-harness/lm_eval/models/gpt2.py\", line 80, in __init__\n    ).to(self.device)\n  File \"/usr/local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 2692, in to\n    return super().to(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1152, in to\n    return self._apply(convert)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 802, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 802, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 825, in _apply\n    param_applied = fn(param)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1150, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n  File \"/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 293, in _lazy_init\n    raise AssertionError(\"Torch not compiled with CUDA enabled\")\nAssertionError: Torch not compiled with CUDA enabled\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python MERA/lm-evaluation-harness/main.py --model hf --model_args pretrained=ai-forever/rugpt3small_based_on_gpt2 --tasks chegeka --num_fewshot 4 --batch_size 1 --limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ab15a7",
   "metadata": {},
   "source": [
    "### Step 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77956671",
   "metadata": {},
   "source": [
    "Execution Start time: 19:43:14, End time: 19:47:04 (duration: 229 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c068fa8a",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the current environment does not have CUDA enabled, which is required for running the model on a GPU. I need to modify the command to run the evaluation on the CPU instead. I will update the command to specify the device as 'cpu' and rerun it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bd65e8",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7c9927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WARNING: --limit SHOULD ONLY BE USED FOR TESTING. REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\nSelected Tasks: ['chegeka']\nUsing device 'cpu'\n/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\ntokenizer_config.json:   0%|                        | 0.00/1.25k [00:00<?, ?B/s]\n\n\nvocab.json:   0%|                                   | 0.00/1.71M [00:00<?, ?B/s]\n\n\n\nmerges.txt:   0%|                                   | 0.00/1.27M [00:00<?, ?B/s]\n\n\nspecial_tokens_map.json:   0%|                        | 0.00/574 [00:00<?, ?B/s]\n\nUsing `model.prepare_inputs_for_generation` method for `model.forward`.\n\nDownloading readme:   0%|                            | 0.00/137k [00:00<?, ?B/s]\n\n\nDownloading data:   0%|                             | 0.00/26.5M [00:00<?, ?B/s]\n\n\n\nDownloading data:   0%|                              | 0.00/385k [00:00<?, ?B/s]\n\n\nGenerating train split:   0%|                  | 0/29376 [00:00<?, ? examples/s]\nGenerating train split:  79%|█▌| 23270/29376 [00:00<00:00, 171815.70 examples/s]\nGenerating train split: 100%|██| 29376/29376 [00:00<00:00, 179371.99 examples/s]\n\nGenerating test split:   0%|                     | 0/416 [00:00<?, ? examples/s]\nGenerating test split: 100%|████████| 416/416 [00:00<00:00, 93017.94 examples/s]\nTask: chegeka; number of docs: 416\nRunning greedy_until requests\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\n 10%|████▍                                       | 1/10 [00:14<02:14, 14.91s/it]\n 20%|████████▊                                   | 2/10 [00:29<01:56, 14.62s/it]\n 30%|█████████████▏                              | 3/10 [00:43<01:42, 14.59s/it]\n 40%|█████████████████▌                          | 4/10 [00:58<01:27, 14.60s/it]\n 50%|██████████████████████                      | 5/10 [01:13<01:13, 14.74s/it]\n 60%|██████████████████████████▍                 | 6/10 [01:27<00:58, 14.65s/it]\n 70%|██████████████████████████████▊             | 7/10 [01:42<00:44, 14.70s/it]\n 80%|███████████████████████████████████▏        | 8/10 [01:57<00:29, 14.60s/it]\n 90%|███████████████████████████████████████▌    | 9/10 [02:11<00:14, 14.60s/it]\n100%|███████████████████████████████████████████| 10/10 [02:25<00:00, 14.35s/it]\n100%|███████████████████████████████████████████| 10/10 [02:25<00:00, 14.55s/it]\n{\n  \"results\": {\n    \"chegeka\": {\n      \"f1\": 0.0,\n      \"f1_stderr\": 0.0,\n      \"em\": 0.0,\n      \"em_stderr\": 0.0\n    }\n  },\n  \"versions\": {\n    \"chegeka\": 0\n  },\n  \"tasks\": {\n    \"chegeka\": 416\n  },\n  \"config\": {\n    \"model\": \"hf\",\n    \"model_args\": \"pretrained=ai-forever/rugpt3small_based_on_gpt2\",\n    \"num_fewshot\": 4,\n    \"batch_size\": \"1\",\n    \"batch_sizes\": [],\n    \"device\": \"cpu\",\n    \"no_cache\": false,\n    \"limit\": 10.0,\n    \"bootstrap_iters\": 100000,\n    \"description_dict\": {},\n    \"pretty_env_info\": \"PyTorch version: 2.2.1+cpu\\nIs debug build: False\\nCUDA used to build PyTorch: None\\nROCM used to build PyTorch: N/A\\n\\nOS: Ubuntu 22.04.4 LTS (x86_64)\\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\\nClang version: 14.0.0-1ubuntu1.1\\nCMake version: version 3.27.9\\nLibc version: glibc-2.35\\n\\nPython version: 3.10.13 (main, Aug 26 2023, 07:12:19) [Clang 16.0.3 ] (64-bit runtime)\\nPython platform: Linux-4.4.0-x86_64-with-glibc2.35\\nIs CUDA available: False\\nCUDA runtime version: No CUDA\\nCUDA_MODULE_LOADING set to: N/A\\nGPU models and configuration: No CUDA\\nNvidia driver version: No CUDA\\ncuDNN version: No CUDA\\nHIP runtime version: N/A\\nMIOpen runtime version: N/A\\nIs XNNPACK available: True\\n\\nCPU:\\nArchitecture:        x86_64\\nCPU op-mode(s):      32-bit, 64-bit\\nAddress sizes:       46 bits physical, 48 bits virtual\\nByte Order:          Little Endian\\nCPU(s):              5\\nOn-line CPU(s) list: 0-4\\nVendor ID:           GenuineIntel\\nModel name:          unknown\\nCPU family:          6\\nModel:               106\\nThread(s) per core:  0\\nCore(s) per socket:  0\\nSocket(s):           0\\nStepping:            unknown\\nBogoMIPS:            3300.00\\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\\nVirtualization:      VT-x\\nHypervisor vendor:   KVM\\nVirtualization type: full\\n\\nVersions of relevant libraries:\\n[pip3] numpy==1.25.2\\n[pip3] torch==2.2.1+cpu\\n[conda] Could not collect\",\n    \"transformers_version\": \"Transformers: 4.40.2\",\n    \"current_dir_commit\": \"\",\n    \"upper_dir_commit\": \"\"\n  }\n}\nhf (pretrained=ai-forever/rugpt3small_based_on_gpt2), limit: 10.0, num_fewshot: 4, batch_size: 1\n| Task  |Version|Metric|Value|   |Stderr|\n|-------|------:|------|----:|---|-----:|\n|chegeka|      0|f1    |    0|±  |     0|\n|       |       |em    |    0|±  |     0|\n\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python MERA/lm-evaluation-harness/main.py --model hf --model_args pretrained=ai-forever/rugpt3small_based_on_gpt2 --tasks chegeka --num_fewshot 4 --batch_size 1 --limit 10 --device cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a67e3d",
   "metadata": {},
   "source": [
    "### Step 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05e082a",
   "metadata": {},
   "source": [
    "Execution Start time: 19:47:06, End time: 19:47:06 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7556540",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947201d8",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c6df5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70a8c7f5",
   "metadata": {},
   "source": [
    "### Step 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71879eef",
   "metadata": {},
   "source": [
    "Execution Start time: 19:47:07, End time: 19:47:07 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de301ef",
   "metadata": {},
   "source": [
    "**Thought**: The evaluation has been completed, and the results for the `chegeka` task are displayed. The metrics `f1` and `em` both have values of 0 with a standard error of 0. I will now format these results into the required JSON structure and submit the final answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f958fd8a",
   "metadata": {},
   "source": [
    "**Action (submit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b860cdd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'metric': 0.0, 'metric_stderr': 0.0}"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
