{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3442c88",
   "metadata": {},
   "source": [
    "### Query\n",
    "Train a roberta-base model on the RTE dataset using the UPET method with the `run.py` script. Report eval accuracy as a json structured as follows: {\"eval_accuracy\": 0.0} (replace 0.0 with the actual value).\n",
    "\n",
    "Additional instructions:\n",
    "1. Run for only 1 epoch. 2. Set 5 examples per label, seed=42, and other hyperparameters taken from the example in the repository readme file.\n",
    "\n",
    "Git repository: https://github.com/wjn1996/UPET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d60771",
   "metadata": {},
   "source": [
    "### Prompt\n",
    "<details>\n",
    "[{'role': 'system', 'content': 'Interact in a jupyter notebook to solve the request.\\nOutput a thought and an action.\\nThought can reason about the current situation, and action allows you to interact with the Jupyter notebook. There are three types of actions:\\n(1) execute, which executes a notebook cell with the command under the \"content\" field. This could be a bash command (beginning with !), a python command, or magic command (beginning with %).\\n(2) edit, which allows you to replace existing lines in a file with new lines. To replace lines, you will need to provide the lines as they appear before the replacement, and the lines to replace them with.\\nIn the \"content\" field, format should be ```{filename}\\n<<<BEFORE_EDIT>>>\\nlines before edit\\nlines before edit\\n<<<AFTER_EDIT>>>\\nlines after edit\\nlines after edit```\\n  - Include the entire lines WITH indentation. After editing, make sure indentation is correct.\\n(3) submit, which submits the final answer (or null if there\\'s no answer to submit), formatted as JSON (make sure to use double quotes for keys and values).\\nAlways output exactly one \"thought\" and one \"action\", and encapsulate the content with <content>...</content> tags.\\nI will provide the cell output (observation) for you.\\n\\nTips:\\n- When a github repository is given, you should start by cloning it. You can read the readme file and browse the repo for installation and usage instructions.\\n- Avoid using conda, instead use pip for package management.\\n- Each execution has a 5-minute timeout. After this period, you can decide whether to interrupt the kernel or continue waiting.\\n  You should interrupt if: (1) you are prompted with an interactive message (e.g., \"Do you want to continue? [Y/n]\"). In that case, you can bypass it by executing echo \"y\" | command, or (2) the execution is taking longer than expected because the wrong task is being executed, such as a different model or incorrect number of epochs.\\n  You should continue to wait if the current execution is correct and proceeding as expected.\\n\\nHere are some examples.\\nGit repository: EleutherAI/lm-evaluation-harness\\nQuery: Evaluate EleutherAI/gpt-j-6B on hellaswag. What\\'s the score?\\nThought: First, I need to clone the GitHub repository \\'EleutherAI/lm-evaluation-harness\\'.\\nAction: execute\\n<content>\\n!git clone https://github.com/EleutherAI/lm-evaluation-harness.git\\n</content>\\nObservation: ```\\nCloning into \\'lm-evaluation-harness\\'...\\n```\\nThought: The repository has been cloned. Next, I should check the README file for installation and usage instructions to proceed with evaluating EleutherAI/gpt-j-6B on hellaswag.\\nAction: execute\\n<content>\\n!cat lm-evaluation-harness/README.md\\n</content>\\nObservation: ```# Language Model Evaluation Harness ... truncated ...\\n```\\nThought: The README provides detailed instructions for installing the `lm-eval` package and evaluating models like GPT-J-6B on tasks like hellaswag. I need to install the package and its dependencies first.\\nAction: execute\\n<content>\\n!pip install -e lm-evaluation-harness/.\\n</content>\\nObservation: ```\\nObtaining file:///home/nora/lm-evaluation-harness\\n  Installing build dependencies ... truncated ...\\n```\\nThought: The `lm-eval` package and its dependencies have been installed successfully. Now, I can proceed with evaluating the EleutherAI/gpt-j-6B model on the hellaswag task as per the instructions in the README.\\nAction: execute\\n<content>\\n!lm_eval --model hf --model_args pretrained=EleutherAI/gpt-j-6B --tasks hellaswag --device cuda:0 --batch_size 8\\n</content>\\nObservation: ```\\n{\\n  \"hellaswag\": {\\n    \"acc\": 0.24603658536585363,\\n    \"acc_norm\": 0.34436059706446426,\\n  },  ... truncated ...\\n```\\nThought: The `lm-eval` was executed successfully, providing the metrics I needed. The `acc` metric is 0.2460. The `acc_norm` metric is 0.3444\\nAction: submit\\n<content>\\n{\"acc\": 0.2460, \"acc_norm\": 0.3444}\\n</content>\\n\\nQuery: use pandas to read the file \\'data.csv\\' and display the first 2 rows.\\nThought: First, I need to read the file \\'data.csv\\' using the pandas library.\\nAction: execute\\n<content>\\nimport pandas as pd\\npd.read_csv(\\'data.csv\\').head()\\n</content>\\nObservation: ```\\n    name\\n  0  Alice\\n   1    Bob\\n  2  Charlie\\n```\\nThought: The file \\'data.csv\\' has been read successfully and the first 2 rows have been displayed.\\nAction: submit\\n<content>\\n[\"Alice\", \"Bob\"]\\n</content>\\n\\nQuery: Change the model name in the \\'config.json\\' file from \\'roberta-base\\' to \\'roberta-large\\'.\\nThought: First, I need to read the \\'config.json\\' file to find the location where I can replace the model name.\\nAction: execute\\n<content>\\n!cat config.json\\n</content>\\nObservation: ```\\n{\\n    \"model\": \"roberta-base\",\\n    \"batch_size\": 8,\\n    \"epochs\": 10\\n}\\n```\\nThought: The file has been read successfully. Now, I can replace the model name in the \\'config.json\\' file.\\nAction: edit\\n<content>\\nconfig.json\\n<<<BEFORE_EDIT>>>\\n{\\n    \"model\": \"roberta-base\",\\n    \"batch_size\": 8,\\n<<<AFTER_EDIT>>>\\n{\\n    \"model\": \"roberta-large\",\\n    \"batch_size\": 8,\\n</content>\\nObservation: \"editing file config.json\"\\nThought: The model name has been replaced in the \\'config.json\\' file.\\nAction: submit\\n<content>\\nnull\\n</content>\\n'}, {'role': 'user', 'content': 'Query: Train a roberta-base model on the RTE dataset using the UPET method with the `run.py` script. Report eval accuracy as a json structured as follows: {\"eval_accuracy\": 0.0} (replace 0.0 with the actual value).\\n\\nAdditional instructions:\\n1. Run for only 1 epoch. 2. Set 5 examples per label, seed=42, and other hyperparameters taken from the example in the repository readme file.\\n\\nGit repository: https://github.com/wjn1996/UPET'}]\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ef782c",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63ed22f",
   "metadata": {},
   "source": [
    "Execution Start time: 22:11:12, End time: 22:11:12 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b650f96",
   "metadata": {},
   "source": [
    "**Thought**: First, I need to clone the GitHub repository 'wjn1996/UPET' to access the `run.py` script and other necessary files for training the model using the UPET method. After cloning, I will check the README file for instructions on how to train the model and set the required hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41626874",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50b07ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cloning into 'UPET'...\nremote: Enumerating objects: 384, done.\u001b[K\nremote: Counting objects: 100% (19/19)\u001b[K\nremote: Counting objects: 100% (19/19), done.\u001b[K\nremote: Compressing objects: 100% (17/17)\u001b[K\nremote: Compressing objects: 100% (17/17), done.\u001b[K\nremote: Total 384 (delta 5), reused 2 (delta 2), pack-reused 365 (from 1)\u001b[K\nReceiving objects: 100% (384/384)\nReceiving objects: 100% (384/384), 576.99 KiB | 23.08 MiB/s, done.\nResolving deltas: 100% (72/72)\nResolving deltas: 100% (72/72), done.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!git clone https://github.com/wjn1996/UPET.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3314f6",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047a616b",
   "metadata": {},
   "source": [
    "Execution Start time: 22:11:14, End time: 22:11:14 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c3cfff",
   "metadata": {},
   "source": [
    "**Thought**: The repository has been cloned successfully. Next, I will check the README file for instructions on how to train the model using the `run.py` script and set the required hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c452be6",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d4174e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# UPET: Uncertainty-aware Parameter-Efficient Tuning for Semi-supervised Language Understanding\n\n\nHead Tuning: Training the model with CLS head, whith or whitout prefix / adapter\nPrompt Tuning: Training the model with prompt and verbalizer (MLM head), whith or whitout prefix / adapter\n\naugment definition：\ne.g.,\n--prefix -> --head-prefix or --prompt-prefix\n--prompt -> --head-ptuning or --prompt-ptuning\n\n### Setup\nWe conduct our experiment with Anaconda3. If you have installed Anaconda3, then create the environment for P-tuning v2:\n\n```shell\nconda create -n pt2 python=3.8.5\nconda activate pt2\n```\n\nAfter we setup basic conda environment, install pytorch related packages via:\n\n```shell\nconda install -n pt2 pytorch==1.7.1 torchvision==0.8.2 torchaudio==0.7.2 cudatoolkit=11.0 -c pytorch\n```\n\nFinally, install other python packages we need:\n\n```shell\npip install -r requirements.txt\n```\n\n### Data\nFor SuperGLUE and SQuAD datasets, we download them from the Huggingface Datasets APIs (embedded in our codes).\n\nFor sequence tagging (NER, SRL) datasets, we prepare a non-official packup [here](https://zenodo.org/record/6318701/files/P-tuning-v2_data.tar.gz?download=1). \nAfter downloading, unzip the packup to the project root.\nPlease use at your own risk.\n\n### Training\nRun training scripts in [run_script](run_script) (e.g., RoBERTa for RTE):\n\nYou can change the augments and run:\n```shell\nbash run_script/run_rte_roberta.sh\n```\nor\n\n```shell\nexport TASK_NAME=superglue\nexport DATASET_NAME=rte\nexport CUDA_VISIBLE_DEVICES=0\nbs=32\nlr=5e-3\ndropout=0.1\npsl=128\nepoch=100\npython3 run.py \\\n  --model_name_or_path /wjn/pre-trained-lm/roberta-large \\\n  --task_name $TASK_NAME \\\n  --dataset_name $DATASET_NAME \\\n  --do_train \\\n  --do_eval \\\n  --max_seq_length 128 \\\n  --per_device_train_batch_size $bs \\\n  --learning_rate $lr \\\n  --num_train_epochs $epoch \\\n  --output_dir checkpoints/$DATASET_NAME-roberta/ \\\n  --overwrite_output_dir \\\n  --hidden_dropout_prob $dropout \\\n  --seed 11 \\\n  --save_strategy no \\\n  --evaluation_strategy epoch \\\n  --prefix\n```\nThis script is run for Full-data Full-supervised Pre-fix Tuning.\n\nWe provide the following kinds of settings:\n- Full-data v.s. Few-shot: The training data is full / few-shot\n- Full-supervised v.s. Semi-supervised: We use full-supervised / self-training\n- Full-Tuning v.s. Patameter-efficient Tuning: Only tuning the full parameters / Tuning the few parameters\n- One-stage v.s. Two-stage: directly tuning / tuning the few paraemters and then tuning the full\n- Head-Tuning v.s. Prompt-Tuning: Prefix/Adapter + CLS head / Prefix/Adapter + Prompt + Vaberlizer\n\nThe specific augments for different scenarios:\n\n\n#### Few-shot Head Tuning\n\n### Implemented Results\nCurrently we have released our reimplementation on following tasks and datasets. More implementation will be released soon.\n\nReleased results on BERT-large\n\n|              | BoolQ | COPA | RTE  | WiC  | WSC  | CoNLL04 | OntoNotes 5.0 | CoNLL12 |\n|--------------|-------|------|------|------|------|---------|---------------|---------|\n| Result       | 74.3  | 77.0 | 80.1 | 75.1 | 68.3 | 84.5    | 86.4          | 85.3    |\n| Total Epochs | 100   | 80   | 60   | 80   | 80   | 40      | 30            | 45      |\n| Best Epoch   | 58    | 12   | 30   | 56   | 17   | 33      | 24            | 43      |\n\nReleased results on RoBERTa-large\n\n|              | BoolQ | COPA | RTE  | WiC  | WSC  | CoNLL03 | CoNLL04 | OntoNotes 5.0 | CoNLL12 | CoNLL05 WSJ | CoNLL05 Brown | SQuAD 1.1 | SQuAD 2.0 |\n|--------------|-------|------|------|------|------|---------|---------|---------------|---------|-------------|---------------|-----------|-----------|\n| Results      | 84.0  | 92.0 | 86.6 | 73.7 | 64.4 | 91.8    | 88.4    | 90.1          | 84.7    | 89.4        | 83.9          | 88.1/94.2 | 81.3/84.7 |\n| Total Epochs | 100   | 120  | 100  | 50   | 10   | 30      | 80      | 60            | 45      | 15          | -             | 30        | 10        |\n| Best Epoch   | 86    | 78   | 65   | 31   | 3    | 28      | 45      | 59            | 37      | 13          | -             | 24        | 9         |\n\nFor other hyper-parameters, please refer to the training scripts. \nIf you can not achieve the reported results at the best epoch, there is probably an environmental mismatch and hyper-parameter search is needed.\n\n## Citation\n\nIf you find our work useful, please kindly cite our paper:\n\n\n\n\n\n\n---\n\nuse_prompt\nsst-2,mr,cr,mnli,snli,qnli,rte,mrpc,qqp,cola,trec\n\n\n\nsuperglue\nBoolQ\tCB\tCOPA\tMultiRC\tReCoRD\tRTE\tWiC\tWSC\tAX-b\tAX-g\nglue\nCoLA\tSST-2\tMRPC\tSTS-B\tQQP\tMNLI-m\tMNLI-mm\tQNLI\tRTE\tWNLI\tAX\n\n\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!cat UPET/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c03a452",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b099689",
   "metadata": {},
   "source": [
    "Execution Start time: 22:11:17, End time: 22:11:25 (duration: 8 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad85e706",
   "metadata": {},
   "source": [
    "**Thought**: The README file provides instructions for setting up the environment and running the training script. I need to install the required Python packages using the `requirements.txt` file. After that, I will modify the training command to use the `roberta-base` model, set the number of epochs to 1, and configure the other hyperparameters as specified in the query (5 examples per label, seed=42). Finally, I will execute the training script and report the evaluation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bc89a4",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0128efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting datasets==1.15.1 (from -r UPET/requirements.txt (line 1))\n  Downloading datasets-1.15.1-py3-none-any.whl.metadata (19 kB)\nCollecting numpy==1.19.2 (from -r UPET/requirements.txt (line 2))\n  Downloading numpy-1.19.2.zip (7.3 MB)\n\u001b[?25l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/7.3 MB ? eta -:--:--\n\n\u001b[?25h  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \b\\\b \bdone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \b\\\b \berror\n  error: subprocess-exited-with-error\n  \n  × Preparing metadata (pyproject.toml) did not run successfully.\n  │ exit code: 1\n  ╰─> [54 lines of output]\n      Running from numpy source directory.\n      setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates\n        run_build = parse_setuppy_commands()\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n              for i in range(1, RK_STATE_LEN):\n                  self.rng_state.key[i] = val[i]\n              self.rng_state.pos = i\n      \n              self._bitgen.state = &self.rng_state\n              self._bitgen.next_uint64 = &mt19937_uint64\n                                         ^\n      ------------------------------------------------------------\n      \n      _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to the type of the value being assigned.\n      Processing numpy/random/_bounded_integers.pxd.in\n      Processing numpy/random/_mt19937.pyx\n      Traceback (most recent call last):\n        File \"/tmp/pip-install-z8m21d4r/numpy_6898bd2863694dccafd0cbd292af7608/tools/cythonize.py\", line 235, in <module>\n          main()\n        File \"/tmp/pip-install-z8m21d4r/numpy_6898bd2863694dccafd0cbd292af7608/tools/cythonize.py\", line 231, in main\n          find_process_files(root_dir)\n        File \"/tmp/pip-install-z8m21d4r/numpy_6898bd2863694dccafd0cbd292af7608/tools/cythonize.py\", line 222, in find_process_files\n          process(root_dir, fromfile, tofile, function, hash_db)\n        File \"/tmp/pip-install-z8m21d4r/numpy_6898bd2863694dccafd0cbd292af7608/tools/cythonize.py\", line 188, in process\n          processor_function(fromfile, tofile)\n        File \"/tmp/pip-install-z8m21d4r/numpy_6898bd2863694dccafd0cbd292af7608/tools/cythonize.py\", line 77, in process_pyx\n          subprocess.check_call(\n        File \"/usr/local/lib/python3.10/subprocess.py\", line 369, in check_call\n          raise CalledProcessError(retcode, cmd)\n      subprocess.CalledProcessError: Command '['/usr/local/bin/python', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1.\n      Cythonizing sources\n      Traceback (most recent call last):\n        File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\n          main()\n        File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\n          json_out['return_val'] = hook(**hook_input['kwargs'])\n        File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 149, in prepare_metadata_for_build_wheel\n          return hook(metadata_directory, config_settings)\n        File \"/tmp/pip-build-env-w03nep0y/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 157, in prepare_metadata_for_build_wheel\n          self.run_setup()\n        File \"/tmp/pip-build-env-w03nep0y/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 248, in run_setup\n          super(_BuildMetaLegacyBackend,\n        File \"/tmp/pip-build-env-w03nep0y/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 142, in run_setup\n          exec(compile(code, __file__, 'exec'), locals())\n        File \"setup.py\", line 499, in <module>\n          setup_package()\n        File \"setup.py\", line 479, in setup_package\n          generate_cython()\n        File \"setup.py\", line 274, in generate_cython\n          raise RuntimeError(\"Running cythonize failed!\")\n      RuntimeError: Running cythonize failed!\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: metadata-generation-failed\n\n× Encountered error while generating package metadata.\n╰─> See above for output.\n\nnote: This is an issue with the package mentioned above, not pip.\nhint: See above for details.\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n\u001b[?25h"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install -r UPET/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de88af5f",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4e1850",
   "metadata": {},
   "source": [
    "Execution Start time: 22:11:26, End time: 22:11:34 (duration: 7 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40366f2f",
   "metadata": {},
   "source": [
    "**Thought**: It seems there is an issue with installing the specific version of numpy mentioned in the `requirements.txt` file. I will try to install the latest version of numpy separately and then proceed with installing the other requirements. This might resolve the issue with the installation process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf05f996",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42840462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Requirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (1.25.2)\nCollecting numpy\n  Downloading numpy-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[?25l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/60.9 kB ? eta -:--:--\n\n\u001b[?25hDownloading numpy-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/16.3 MB ? eta -:--:--\n\n\n\n\u001b[?25hInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.25.2\n    Uninstalling numpy-1.25.2:\n      Successfully uninstalled numpy-1.25.2\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nscipy 1.11.4 requires numpy<1.28.0,>=1.21.6, but you have numpy 2.1.0 which is incompatible.\nSuccessfully installed numpy-2.1.0\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install numpy --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02c216c",
   "metadata": {},
   "source": [
    "### Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa6166b",
   "metadata": {},
   "source": [
    "Execution Start time: 22:11:35, End time: 22:11:45 (duration: 9 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8252524",
   "metadata": {},
   "source": [
    "**Thought**: The numpy package has been upgraded successfully. Now, I will attempt to install the remaining requirements from the `requirements.txt` file, excluding numpy, to see if the installation issue is resolved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02ba658",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4171c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting datasets==1.15.1 (from -r UPET/requirements.txt (line 1))\n  Downloading datasets-1.15.1-py3-none-any.whl.metadata (19 kB)\nCollecting numpy==1.19.2 (from -r UPET/requirements.txt (line 2))\n  Downloading numpy-1.19.2.zip (7.3 MB)\n\u001b[?25l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/7.3 MB ? eta -:--:--\n\n\n\u001b[?25h  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \b\\\b \bdone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \b\\\b \berror\n  error: subprocess-exited-with-error\n  \n  × Preparing metadata (pyproject.toml) did not run successfully.\n  │ exit code: 1\n  ╰─> [54 lines of output]\n      Running from numpy source directory.\n      setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates\n        run_build = parse_setuppy_commands()\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n              for i in range(1, RK_STATE_LEN):\n                  self.rng_state.key[i] = val[i]\n              self.rng_state.pos = i\n      \n              self._bitgen.state = &self.rng_state\n              self._bitgen.next_uint64 = &mt19937_uint64\n                                         ^\n      ------------------------------------------------------------\n      \n      _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to the type of the value being assigned.\n      Processing numpy/random/_bounded_integers.pxd.in\n      Processing numpy/random/_mt19937.pyx\n      Traceback (most recent call last):\n        File \"/tmp/pip-install-qkdgoyoh/numpy_df7ce3daad984ea0af7c9316bbcaf1cf/tools/cythonize.py\", line 235, in <module>\n          main()\n        File \"/tmp/pip-install-qkdgoyoh/numpy_df7ce3daad984ea0af7c9316bbcaf1cf/tools/cythonize.py\", line 231, in main\n          find_process_files(root_dir)\n        File \"/tmp/pip-install-qkdgoyoh/numpy_df7ce3daad984ea0af7c9316bbcaf1cf/tools/cythonize.py\", line 222, in find_process_files\n          process(root_dir, fromfile, tofile, function, hash_db)\n        File \"/tmp/pip-install-qkdgoyoh/numpy_df7ce3daad984ea0af7c9316bbcaf1cf/tools/cythonize.py\", line 188, in process\n          processor_function(fromfile, tofile)\n        File \"/tmp/pip-install-qkdgoyoh/numpy_df7ce3daad984ea0af7c9316bbcaf1cf/tools/cythonize.py\", line 77, in process_pyx\n          subprocess.check_call(\n        File \"/usr/local/lib/python3.10/subprocess.py\", line 369, in check_call\n          raise CalledProcessError(retcode, cmd)\n      subprocess.CalledProcessError: Command '['/usr/local/bin/python', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1.\n      Cythonizing sources\n      Traceback (most recent call last):\n        File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\n          main()\n        File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\n          json_out['return_val'] = hook(**hook_input['kwargs'])\n        File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 149, in prepare_metadata_for_build_wheel\n          return hook(metadata_directory, config_settings)\n        File \"/tmp/pip-build-env-4c3ldmbx/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 157, in prepare_metadata_for_build_wheel\n          self.run_setup()\n        File \"/tmp/pip-build-env-4c3ldmbx/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 248, in run_setup\n          super(_BuildMetaLegacyBackend,\n        File \"/tmp/pip-build-env-4c3ldmbx/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 142, in run_setup\n          exec(compile(code, __file__, 'exec'), locals())\n        File \"setup.py\", line 499, in <module>\n          setup_package()\n        File \"setup.py\", line 479, in setup_package\n          generate_cython()\n        File \"setup.py\", line 274, in generate_cython\n          raise RuntimeError(\"Running cythonize failed!\")\n      RuntimeError: Running cythonize failed!\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: metadata-generation-failed\n\n× Encountered error while generating package metadata.\n╰─> See above for output.\n\nnote: This is an issue with the package mentioned above, not pip.\nhint: See above for details.\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n\u001b[?25h"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install -r UPET/requirements.txt --no-deps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089874e6",
   "metadata": {},
   "source": [
    "### Step 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0ba5b2",
   "metadata": {},
   "source": [
    "Execution Start time: 22:11:47, End time: 22:11:48 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2299b033",
   "metadata": {},
   "source": [
    "**Thought**: The installation of the specific version of numpy is still causing issues. I will try to install the other packages individually from the `requirements.txt` file, excluding numpy, to see if that resolves the issue. This approach might help identify if there are any other problematic packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d595417",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ecc3c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting datasets==1.15.1\n  Downloading datasets-1.15.1-py3-none-any.whl.metadata (19 kB)\nCollecting transformers==4.11.3\n  Downloading transformers-4.11.3-py3-none-any.whl.metadata (53 kB)\n\u001b[?25l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/53.7 kB ? eta -:--:--\n\n\u001b[?25hERROR: Could not find a version that satisfies the requirement torch==1.10.0 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0)\nERROR: No matching distribution found for torch==1.10.0\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install datasets==1.15.1 transformers==4.11.3 torch==1.10.0 scikit-learn==0.24.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14604b9e",
   "metadata": {},
   "source": [
    "### Step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e3afaa",
   "metadata": {},
   "source": [
    "Execution Start time: 22:11:50, End time: 22:13:25 (duration: 95 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344b992e",
   "metadata": {},
   "source": [
    "**Thought**: It seems that the specific version of `torch` (1.10.0) is not available for installation. I will try installing a compatible version of `torch` that is available, and then proceed with installing the other packages. This should help in setting up the environment correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e94d50",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9726dd62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting torch==2.0.0\n  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from torch==2.0.0) (3.15.1)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/site-packages (from torch==2.0.0) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/site-packages (from torch==2.0.0) (1.12)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch==2.0.0) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch==2.0.0) (3.1.4)\nCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.0)\n  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.0)\n  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.0)\n  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.0)\n  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.0)\n  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.0)\n  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.0)\n  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.0)\n  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.0)\n  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.0)\n  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.0)\n  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.0.0 (from torch==2.0.0)\n  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (68.1.2)\nCollecting wheel (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0)\n  Downloading wheel-0.44.0-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: cmake in /usr/local/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.0) (3.27.9)\nCollecting lit (from triton==2.0.0->torch==2.0.0)\n  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch==2.0.0) (2.1.5)\nRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/site-packages (from sympy->torch==2.0.0) (1.3.0)\nDownloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/619.9 MB ? eta -:--:--\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/317.1 MB ? eta -:--:--\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/11.8 MB ? eta -:--:--\n\n\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/21.0 MB ? eta -:--:--\n\n\n\n\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/849.3 kB ? eta -:--:--\n\n\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/557.1 MB ? eta -:--:--\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/168.4 MB ? eta -:--:--\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/54.6 MB ? eta -:--:--\n\n\n\n\n\n\n\n\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/102.6 MB ? eta -:--:--\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/173.2 MB ? eta -:--:--\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/177.1 MB ? eta -:--:--\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/98.6 kB ? eta -:--:--\n\n\u001b[?25hDownloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/63.3 MB ? eta -:--:--\n\n\n\n\n\n\n\n\n\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/96.4 kB ? eta -:--:--\n\n\u001b[?25hDownloading wheel-0.44.0-py3-none-any.whl (67 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/67.1 kB ? eta -:--:--\n\n\u001b[?25hInstalling collected packages: lit, wheel, nvidia-nccl-cu11, nvidia-cufft-cu11, nvidia-cuda-nvrtc-cu11, nvidia-nvtx-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch\n  Attempting uninstall: torch\n    Found existing installation: torch 2.2.1+cpu\n    Uninstalling torch-2.2.1+cpu:\n      Successfully uninstalled torch-2.2.1+cpu\nSuccessfully installed lit-18.1.8 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.0 triton-2.0.0 wheel-0.44.0\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install torch==2.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b8145e",
   "metadata": {},
   "source": [
    "### Step 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06d35e7",
   "metadata": {},
   "source": [
    "Execution Start time: 22:13:27, End time: 22:14:23 (duration: 56 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711c18c4",
   "metadata": {},
   "source": [
    "**Thought**: The `torch` package has been installed successfully. Now, I will proceed with installing the remaining packages (`datasets`, `transformers`, and `scikit-learn`) to complete the environment setup. After that, I will configure the training script to use the `roberta-base` model, set the number of epochs to 1, and configure the other hyperparameters as specified in the query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d65b04b",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49093459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting datasets==1.15.1\n  Downloading datasets-1.15.1-py3-none-any.whl.metadata (19 kB)\nCollecting transformers==4.11.3\n  Downloading transformers-4.11.3-py3-none-any.whl.metadata (53 kB)\n\u001b[?25l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/53.7 kB ? eta -:--:--\n\n\u001b[?25hCollecting scikit-learn==0.24.2\n  Downloading scikit-learn-0.24.2.tar.gz (7.5 MB)\n\u001b[?25l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/7.5 MB ? eta -:--:--\n\n\u001b[?25h  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \bdone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \berror\n  error: subprocess-exited-with-error\n  \n  × Preparing metadata (pyproject.toml) did not run successfully.\n  │ exit code: 1\n  ╰─> [1534 lines of output]\n      <string>:17: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n      Partial import of sklearn during the build process.\n      <string>:116: DeprecationWarning:\n      \n        `numpy.distutils` is deprecated since NumPy 1.23.0, as a result\n        of the deprecation of `distutils` itself. It will be removed for\n        Python >= 3.12. For older Python versions it will remain present.\n        It is recommended to use `setuptools < 60.0` for those Python versions.\n        For more details, see:\n          https://numpy.org/devdocs/reference/distutils_status_migration.html\n      \n      \n      INFO: C compiler: clang -pthread -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/tools/deps/include -I/tools/deps/include/ncursesw -I/tools/deps/libedit/include -g0 -fPIC\n      \n      INFO: compile options: '-c'\n      INFO: clang: test_program.c\n      INFO: clang -pthread objects/test_program.o -o test_program\n      INFO: C compiler: clang -pthread -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/tools/deps/include -I/tools/deps/include/ncursesw -I/tools/deps/libedit/include -g0 -fPIC\n      \n      INFO: compile options: '-c'\n      extra options: '-fopenmp'\n      INFO: clang: test_program.c\n      test_program.c:1:10: fatal error: 'omp.h' file not found\n      #include <omp.h>\n               ^~~~~~~\n      1 error generated.\n      /tmp/pip-install-4ahn4q0h/scikit-learn_f4da9738e7a942ed9ff84f784cf5041b/sklearn/_build_utils/openmp_helpers.py:120: UserWarning:\n      \n                      ***********\n                      * WARNING *\n                      ***********\n      \n      It seems that scikit-learn cannot be built with OpenMP.\n      \n      - Make sure you have followed the installation instructions:\n      \n          https://scikit-learn.org/dev/developers/advanced_installation.html\n      \n      - If your compiler supports OpenMP but you still see this\n        message, please submit a bug report at:\n      \n          https://github.com/scikit-learn/scikit-learn/issues\n      \n      - The build will continue with OpenMP-based parallelism\n        disabled. Note however that some estimators will run in\n        sequential mode instead of leveraging thread-based\n        parallelism.\n      \n                          ***\n      \n        warnings.warn(message)\n      Compiling sklearn/__check_build/_check_build.pyx because it changed.\n      Compiling sklearn/preprocessing/_csr_polynomial_expansion.pyx because it changed.\n      Compiling sklearn/cluster/_dbscan_inner.pyx because it changed.\n      Compiling sklearn/cluster/_hierarchical_fast.pyx because it changed.\n      Compiling sklearn/cluster/_k_means_fast.pyx because it changed.\n      Compiling sklearn/cluster/_k_means_lloyd.pyx because it changed.\n      Compiling sklearn/cluster/_k_means_elkan.pyx because it changed.\n      Compiling sklearn/datasets/_svmlight_format_fast.pyx because it changed.\n      Compiling sklearn/decomposition/_online_lda_fast.pyx because it changed.\n      Compiling sklearn/decomposition/_cdnmf_fast.pyx because it changed.\n      Compiling sklearn/ensemble/_gradient_boosting.pyx because it changed.\n      Compiling sklearn/ensemble/_hist_gradient_boosting/_gradient_boosting.pyx because it changed.\n      Compiling sklearn/ensemble/_hist_gradient_boosting/histogram.pyx because it changed.\n      Compiling sklearn/ensemble/_hist_gradient_boosting/splitting.pyx because it changed.\n      Compiling sklearn/ensemble/_hist_gradient_boosting/_binning.pyx because it changed.\n      Compiling sklearn/ensemble/_hist_gradient_boosting/_predictor.pyx because it changed.\n      Compiling sklearn/ensemble/_hist_gradient_boosting/_loss.pyx because it changed.\n      Compiling sklearn/ensemble/_hist_gradient_boosting/_bitset.pyx because it changed.\n      Compiling sklearn/ensemble/_hist_gradient_boosting/common.pyx because it changed.\n      Compiling sklearn/ensemble/_hist_gradient_boosting/utils.pyx because it changed.\n      Compiling sklearn/feature_extraction/_hashing_fast.pyx because it changed.\n      Compiling sklearn/manifold/_utils.pyx because it changed.\n      Compiling sklearn/manifold/_barnes_hut_tsne.pyx because it changed.\n      Compiling sklearn/metrics/cluster/_expected_mutual_info_fast.pyx because it changed.\n      Compiling sklearn/metrics/_pairwise_fast.pyx because it changed.\n      Compiling sklearn/neighbors/_ball_tree.pyx because it changed.\n      Compiling sklearn/neighbors/_kd_tree.pyx because it changed.\n      Compiling sklearn/neighbors/_dist_metrics.pyx because it changed.\n      Compiling sklearn/neighbors/_typedefs.pyx because it changed.\n      Compiling sklearn/neighbors/_quad_tree.pyx because it changed.\n      Compiling sklearn/tree/_tree.pyx because it changed.\n      Compiling sklearn/tree/_splitter.pyx because it changed.\n      Compiling sklearn/tree/_criterion.pyx because it changed.\n      Compiling sklearn/tree/_utils.pyx because it changed.\n      Compiling sklearn/utils/sparsefuncs_fast.pyx because it changed.\n      Compiling sklearn/utils/_cython_blas.pyx because it changed.\n      Compiling sklearn/utils/arrayfuncs.pyx because it changed.\n      Compiling sklearn/utils/murmurhash.pyx because it changed.\n      Compiling sklearn/utils/graph_shortest_path.pyx because it changed.\n      Compiling sklearn/utils/_fast_dict.pyx because it changed.\n      Compiling sklearn/utils/_openmp_helpers.pyx because it changed.\n      Compiling sklearn/utils/_seq_dataset.pyx because it changed.\n      Compiling sklearn/utils/_weight_vector.pyx because it changed.\n      Compiling sklearn/utils/_random.pyx because it changed.\n      Compiling sklearn/utils/_logistic_sigmoid.pyx because it changed.\n      Compiling sklearn/svm/_newrand.pyx because it changed.\n      Compiling sklearn/svm/_libsvm.pyx because it changed.\n      Compiling sklearn/svm/_liblinear.pyx because it changed.\n      Compiling sklearn/svm/_libsvm_sparse.pyx because it changed.\n      Compiling sklearn/linear_model/_cd_fast.pyx because it changed.\n      Compiling sklearn/linear_model/_sgd_fast.pyx because it changed.\n      Compiling sklearn/linear_model/_sag_fast.pyx because it changed.\n      Compiling sklearn/_isotonic.pyx because it changed.\n      warning: sklearn/cluster/_dbscan_inner.pyx:17:5: Only extern functions can throw C++ exceptions.\n      warning: sklearn/neighbors/_dist_metrics.pxd:19:64: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pxd:29:65: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pxd:38:73: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pxd:42:73: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pxd:65:51: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pxd:68:52: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pxd:75:68: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pxd:77:67: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      performance hint: sklearn/cluster/_k_means_elkan.pyx:336:5: Exception check on '_update_chunk_dense' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '_update_chunk_dense' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '_update_chunk_dense' to allow an error code to be returned.\n      performance hint: sklearn/cluster/_k_means_elkan.pyx:574:5: Exception check on '_update_chunk_sparse' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '_update_chunk_sparse' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '_update_chunk_sparse' to allow an error code to be returned.\n      performance hint: sklearn/cluster/_k_means_elkan.pyx:294:31: Exception check after calling '__pyx_fuse_0_update_chunk_dense' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '__pyx_fuse_0_update_chunk_dense' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '__pyx_fuse_0_update_chunk_dense' to allow an error code to be returned.\n      performance hint: sklearn/cluster/_k_means_elkan.pyx:294:31: Exception check after calling '__pyx_fuse_1_update_chunk_dense' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '__pyx_fuse_1_update_chunk_dense' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '__pyx_fuse_1_update_chunk_dense' to allow an error code to be returned.\n      performance hint: sklearn/cluster/_k_means_elkan.pyx:385:60: Exception check after calling '__pyx_fuse_0_euclidean_dense_dense' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '__pyx_fuse_0_euclidean_dense_dense' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Declare any exception value explicitly for functions in pxd files.\n      performance hint: sklearn/cluster/_k_means_elkan.pyx:396:57: Exception check after calling '__pyx_fuse_0_euclidean_dense_dense' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '__pyx_fuse_0_euclidean_dense_dense' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Declare any exception value explicitly for functions in pxd files.\n      performance hint: sklearn/cluster/_k_means_elkan.pyx:385:60: Exception check after calling '__pyx_fuse_1_euclidean_dense_dense' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '__pyx_fuse_1_euclidean_dense_dense' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Declare any exception value explicitly for functions in pxd files.\n      performance hint: sklearn/cluster/_k_means_elkan.pyx:396:57: Exception check after calling '__pyx_fuse_1_euclidean_dense_dense' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '__pyx_fuse_1_euclidean_dense_dense' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Declare any exception value explicitly for functions in pxd files.\n      performance hint: sklearn/cluster/_k_means_elkan.pyx:528:32: Exception check after calling '__pyx_fuse_0_update_chunk_sparse' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '__pyx_fuse_0_update_chunk_sparse' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '__pyx_fuse_0_update_chunk_sparse' to allow an error code to be returned.\n      performance hint: sklearn/cluster/_k_means_elkan.pyx:528:32: Exception check after calling '__pyx_fuse_1_update_chunk_sparse' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '__pyx_fuse_1_update_chunk_sparse' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '__pyx_fuse_1_update_chunk_sparse' to allow an error code to be returned.\n      performance hint: sklearn/cluster/_k_means_elkan.pyx:624:61: Exception check after calling '__pyx_fuse_0_euclidean_sparse_dense' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '__pyx_fuse_0_euclidean_sparse_dense' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Declare any exception value explicitly for functions in pxd files.\n      performance hint: sklearn/cluster/_k_means_elkan.pyx:636:58: Exception check after calling '__pyx_fuse_0_euclidean_sparse_dense' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '__pyx_fuse_0_euclidean_sparse_dense' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Declare any exception value explicitly for functions in pxd files.\n      performance hint: sklearn/cluster/_k_means_elkan.pyx:624:61: Exception check after calling '__pyx_fuse_1_euclidean_sparse_dense' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '__pyx_fuse_1_euclidean_sparse_dense' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Declare any exception value explicitly for functions in pxd files.\n      performance hint: sklearn/cluster/_k_means_elkan.pyx:636:58: Exception check after calling '__pyx_fuse_1_euclidean_sparse_dense' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '__pyx_fuse_1_euclidean_sparse_dense' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Declare any exception value explicitly for functions in pxd files.\n      performance hint: sklearn/cluster/_k_means_fast.pyx:34:5: Exception check on '_euclidean_dense_dense' will always require the GIL to be acquired. Declare '_euclidean_dense_dense' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n      performance hint: sklearn/cluster/_k_means_fast.pyx:66:5: Exception check on '_euclidean_sparse_dense' will always require the GIL to be acquired. Declare '_euclidean_sparse_dense' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n      performance hint: sklearn/cluster/_k_means_lloyd.pyx:164:5: Exception check on '_update_chunk_dense' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '_update_chunk_dense' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '_update_chunk_dense' to allow an error code to be returned.\n      performance hint: sklearn/cluster/_k_means_lloyd.pyx:361:5: Exception check on '_update_chunk_sparse' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '_update_chunk_sparse' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '_update_chunk_sparse' to allow an error code to be returned.\n      performance hint: sklearn/cluster/_k_means_lloyd.pyx:131:31: Exception check after calling '__pyx_fuse_0_update_chunk_dense' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '__pyx_fuse_0_update_chunk_dense' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '__pyx_fuse_0_update_chunk_dense' to allow an error code to be returned.\n      performance hint: sklearn/cluster/_k_means_lloyd.pyx:131:31: Exception check after calling '__pyx_fuse_1_update_chunk_dense' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '__pyx_fuse_1_update_chunk_dense' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '__pyx_fuse_1_update_chunk_dense' to allow an error code to be returned.\n      performance hint: sklearn/cluster/_k_means_lloyd.pyx:201:9: Exception check after calling '__pyx_fuse_0_gemm' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '__pyx_fuse_0_gemm' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '__pyx_fuse_0_gemm' to allow an error code to be returned.\n      performance hint: sklearn/cluster/_k_means_lloyd.pyx:201:9: Exception check after calling '__pyx_fuse_1_gemm' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '__pyx_fuse_1_gemm' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '__pyx_fuse_1_gemm' to allow an error code to be returned.\n      performance hint: sklearn/cluster/_k_means_lloyd.pyx:327:32: Exception check after calling '__pyx_fuse_0_update_chunk_sparse' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '__pyx_fuse_0_update_chunk_sparse' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '__pyx_fuse_0_update_chunk_sparse' to allow an error code to be returned.\n      performance hint: sklearn/cluster/_k_means_lloyd.pyx:327:32: Exception check after calling '__pyx_fuse_1_update_chunk_sparse' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '__pyx_fuse_1_update_chunk_sparse' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '__pyx_fuse_1_update_chunk_sparse' to allow an error code to be returned.\n      warning: sklearn/tree/_tree.pxd:61:68: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_tree.pxd:62:59: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_tree.pxd:63:63: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_splitter.pxd:84:72: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_splitter.pxd:89:68: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pxd:57:45: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pxd:58:40: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pxd:59:48: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pxd:60:57: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:49:75: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:87:61: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:119:56: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:137:40: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:139:71: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:160:71: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:161:40: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pxd:76:59: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pxd:95:51: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pxd:98:59: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pxd:99:63: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pxd:100:80: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      performance hint: sklearn/ensemble/_hist_gradient_boosting/_bitset.pyx:19:5: Exception check on 'init_bitset' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'init_bitset' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on 'init_bitset' to allow an error code to be returned.\n      performance hint: sklearn/ensemble/_hist_gradient_boosting/_bitset.pyx:27:5: Exception check on 'set_bitset' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'set_bitset' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on 'set_bitset' to allow an error code to be returned.\n      performance hint: sklearn/ensemble/_hist_gradient_boosting/_bitset.pxd:11:28: No exception value declared for 'in_bitset' in pxd file.\n      Users cimporting this function and calling it without the gil will always require an exception check.\n      Suggest adding an explicit exception value.\n      performance hint: sklearn/ensemble/_hist_gradient_boosting/_bitset.pxd:13:40: No exception value declared for 'in_bitset_memoryview' in pxd file.\n      Users cimporting this function and calling it without the gil will always require an exception check.\n      Suggest adding an explicit exception value.\n      performance hint: sklearn/ensemble/_hist_gradient_boosting/_bitset.pxd:16:42: No exception value declared for 'in_bitset_2d_memoryview' in pxd file.\n      Users cimporting this function and calling it without the gil will always require an exception check.\n      Suggest adding an explicit exception value.\n      performance hint: sklearn/ensemble/_hist_gradient_boosting/_loss.pyx:187:5: Exception check on '_compute_softmax' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '_compute_softmax' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '_compute_softmax' to allow an error code to be returned.\n      performance hint: sklearn/ensemble/_hist_gradient_boosting/_loss.pyx:167:28: Exception check after calling '_compute_softmax' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '_compute_softmax' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '_compute_softmax' to allow an error code to be returned.\n      performance hint: sklearn/ensemble/_hist_gradient_boosting/_loss.pyx:178:28: Exception check after calling '_compute_softmax' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '_compute_softmax' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '_compute_softmax' to allow an error code to be returned.\n      performance hint: sklearn/ensemble/_hist_gradient_boosting/_predictor.pyx:72:38: Exception check after calling 'in_bitset_2d_memoryview' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'in_bitset_2d_memoryview' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Declare any exception value explicitly for functions in pxd files.\n      performance hint: sklearn/ensemble/_hist_gradient_boosting/_predictor.pyx:77:40: Exception check after calling 'in_bitset_2d_memoryview' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'in_bitset_2d_memoryview' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Declare any exception value explicitly for functions in pxd files.\n      performance hint: sklearn/ensemble/_hist_gradient_boosting/_predictor.pyx:136:38: Exception check after calling 'in_bitset_2d_memoryview' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'in_bitset_2d_memoryview' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Declare any exception value explicitly for functions in pxd files.\n      performance hint: sklearn/ensemble/_hist_gradient_boosting/histogram.pyx:252:6: Exception check on '_build_histogram_naive' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '_build_histogram_naive' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '_build_histogram_naive' to allow an error code to be returned.\n      performance hint: sklearn/ensemble/_hist_gradient_boosting/histogram.pyx:276:6: Exception check on '_subtract_histograms' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '_subtract_histograms' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '_subtract_histograms' to allow an error code to be returned.\n      performance hint: sklearn/ensemble/_hist_gradient_boosting/histogram.pyx:300:6: Exception check on '_build_histogram' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '_build_histogram' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '_build_histogram' to allow an error code to be returned.\n      performance hint: sklearn/ensemble/_hist_gradient_boosting/histogram.pyx:347:6: Exception check on '_build_histogram_no_hessian' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '_build_histogram_no_hessian' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '_build_histogram_no_hessian' to allow an error code to be returned.\n      performance hint: sklearn/ensemble/_hist_gradient_boosting/histogram.pyx:391:6: Exception check on '_build_histogram_root' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '_build_histogram_root' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '_build_histogram_root' to allow an error code to be returned.\n      performance hint: sklearn/ensemble/_hist_gradient_boosting/histogram.pyx:444:6: Exception check on '_build_histogram_root_no_hessian' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '_build_histogram_root_no_hessian' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '_build_histogram_root_no_hessian' to allow an error code to be returned.\n      performance hint: sklearn/ensemble/_hist_gradient_boosting/histogram.pyx:158:60: Exception check after calling '_compute_histogram_brute_single_feature' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '_compute_histogram_brute_single_feature' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '_compute_histogram_brute_single_feature' to allow an error code to be returned.\n      performance hint: sklearn/ensemble/_hist_gradient_boosting/histogram.pyx:190:48: Exception check after calling '_build_histogram_root_no_hessian' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '_build_histogram_root_no_hessian' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '_build_histogram_root_no_hessian' to allow an error code to be returned.\n      performance hint: sklearn/ensemble/_hist_gradient_boosting/histogram.pyx:194:37: Exception check after calling '_build_histogram_root' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '_build_histogram_root' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '_build_histogram_root' to allow an error code to be returned.\n      performance hint: sklearn/ensemble/_hist_gradient_boosting/histogram.pyx:199:43: Exception check after calling '_build_histogram_no_hessian' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '_build_histogram_no_hessian' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '_build_histogram_no_hessian' to allow an error code to be returned.\n      performance hint: sklearn/ensemble/_hist_gradient_boosting/histogram.pyx:203:32: Exception check after calling '_build_histogram' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '_build_histogram' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '_build_histogram' to allow an error code to be returned.\n      performance hint: sklearn/ensemble/_hist_gradient_boosting/histogram.pyx:244:32: Exception check after calling '_subtract_histograms' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '_subtract_histograms' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '_subtract_histograms' to allow an error code to be returned.\n      warning: sklearn/ensemble/_hist_gradient_boosting/splitting.pyx:19:0: The 'IF' statement is deprecated and will be removed in a future Cython version. Consider using runtime conditions or C macros instead. See https://github.com/cython/cython/issues/4310\n      warning: sklearn/ensemble/_hist_gradient_boosting/splitting.pyx:309:12: The 'IF' statement is deprecated and will be removed in a future Cython version. Consider using runtime conditions or C macros instead. See https://github.com/cython/cython/issues/4310\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n              if n_used_bins <= 1:\n                  free(cat_infos)\n                  return\n      \n              qsort(cat_infos, n_used_bins, sizeof(categorical_info),\n                    compare_cat_infos)\n                    ^\n      ------------------------------------------------------------\n      \n      sklearn/ensemble/_hist_gradient_boosting/splitting.pyx:912:14: Cannot assign type 'int (const void *, const void *) except? -1 nogil' to 'int (*)(const void *, const void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to the type of 'compare_cat_infos'.\n      Traceback (most recent call last):\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/Cython/Build/Dependencies.py\", line 1345, in cythonize_one_helper\n          return cythonize_one(*m)\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/Cython/Build/Dependencies.py\", line 1321, in cythonize_one\n          raise CompileError(None, pyx_file)\n      Cython.Compiler.Errors.CompileError: sklearn/ensemble/_hist_gradient_boosting/splitting.pyx\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n          # Max value for our rand_r replacement (near the bottom).\n          # We don't use RAND_MAX because it's different across platforms and\n          # particularly tiny on Windows/MSVC.\n          RAND_R_MAX = 0x7FFFFFFF\n      \n      cpdef sample_without_replacement(np.int_t n_population,\n                                       ^\n      ------------------------------------------------------------\n      \n      sklearn/utils/_random.pxd:18:33: 'int_t' is not a type identifier\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n          # We don't use RAND_MAX because it's different across platforms and\n          # particularly tiny on Windows/MSVC.\n          RAND_R_MAX = 0x7FFFFFFF\n      \n      cpdef sample_without_replacement(np.int_t n_population,\n                                       np.int_t n_samples,\n                                       ^\n      ------------------------------------------------------------\n      \n      sklearn/utils/_random.pxd:19:33: 'int_t' is not a type identifier\n      performance hint: sklearn/linear_model/_cd_fast.pyx:48:5: Exception check on 'fmax' will always require the GIL to be acquired. Declare 'fmax' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n      performance hint: sklearn/linear_model/_cd_fast.pyx:54:5: Exception check on 'fsign' will always require the GIL to be acquired. Declare 'fsign' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n      performance hint: sklearn/linear_model/_cd_fast.pyx:63:5: Exception check on 'abs_max' will always require the GIL to be acquired. Declare 'abs_max' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n      performance hint: sklearn/linear_model/_cd_fast.pyx:75:5: Exception check on 'max' will always require the GIL to be acquired. Declare 'max' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n      performance hint: sklearn/linear_model/_cd_fast.pyx:87:5: Exception check on 'diff_abs_max' will always require the GIL to be acquired. Declare 'diff_abs_max' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n      Traceback (most recent call last):\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/Cython/Build/Dependencies.py\", line 1345, in cythonize_one_helper\n          return cythonize_one(*m)\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/Cython/Build/Dependencies.py\", line 1321, in cythonize_one\n          raise CompileError(None, pyx_file)\n      Cython.Compiler.Errors.CompileError: sklearn/linear_model/_cd_fast.pyx\n      performance hint: sklearn/linear_model/_sag_fast.pyx:1202:5: Exception check on 'predict_sample64' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'predict_sample64' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on 'predict_sample64' to allow an error code to be returned.\n      performance hint: sklearn/linear_model/_sag_fast.pyx:1249:5: Exception check on 'predict_sample32' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'predict_sample32' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on 'predict_sample32' to allow an error code to be returned.\n      performance hint: sklearn/linear_model/_sag_fast.pyx:467:43: Exception check after calling 'random' will always require the GIL to be acquired. Declare 'random' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n      performance hint: sklearn/linear_model/_sag_fast.pyx:495:32: Exception check after calling 'predict_sample64' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'predict_sample64' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on 'predict_sample64' to allow an error code to be returned.\n      performance hint: sklearn/linear_model/_sag_fast.pyx:500:35: Exception check after calling 'dloss' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'dloss' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on 'dloss' to allow an error code to be returned.\n      performance hint: sklearn/linear_model/_sag_fast.pyx:503:44: Exception check after calling 'dloss' will always require the GIL to be acquired. Declare 'dloss' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n      performance hint: sklearn/linear_model/_sag_fast.pyx:798:43: Exception check after calling 'random' will always require the GIL to be acquired. Declare 'random' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n      performance hint: sklearn/linear_model/_sag_fast.pyx:826:32: Exception check after calling 'predict_sample32' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'predict_sample32' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on 'predict_sample32' to allow an error code to be returned.\n      performance hint: sklearn/linear_model/_sag_fast.pyx:831:35: Exception check after calling 'dloss' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'dloss' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on 'dloss' to allow an error code to be returned.\n      performance hint: sklearn/linear_model/_sag_fast.pyx:834:44: Exception check after calling 'dloss' will always require the GIL to be acquired. Declare 'dloss' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n      performance hint: sklearn/linear_model/_sag_fast.pyx:1337:24: Exception check after calling 'next' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'next' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on 'next' to allow an error code to be returned.\n      performance hint: sklearn/linear_model/_sag_fast.pyx:1341:28: Exception check after calling 'predict_sample64' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'predict_sample64' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on 'predict_sample64' to allow an error code to be returned.\n      performance hint: sklearn/linear_model/_sag_fast.pyx:1345:27: Exception check after calling 'dloss' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'dloss' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on 'dloss' to allow an error code to be returned.\n      performance hint: sklearn/linear_model/_sag_fast.pyx:1348:39: Exception check after calling '_loss' will always require the GIL to be acquired. Declare '_loss' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n      performance hint: sklearn/linear_model/_sgd_fast.pyx:666:5: Exception check on 'l1penalty' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'l1penalty' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on 'l1penalty' to allow an error code to be returned.\n      performance hint: sklearn/linear_model/_sgd_fast.pyx:514:31: Exception check after calling 'shuffle' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'shuffle' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on 'shuffle' to allow an error code to be returned.\n      performance hint: sklearn/linear_model/_sgd_fast.pyx:516:28: Exception check after calling 'next' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'next' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on 'next' to allow an error code to be returned.\n      performance hint: sklearn/linear_model/_sgd_fast.pyx:524:25: Exception check after calling 'dot' will always require the GIL to be acquired. Declare 'dot' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n      performance hint: sklearn/linear_model/_sgd_fast.pyx:531:40: Exception check after calling 'loss' will always require the GIL to be acquired. Declare 'loss' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n      performance hint: sklearn/linear_model/_sgd_fast.pyx:542:45: Exception check after calling 'loss' will always require the GIL to be acquired. Declare 'loss' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n      performance hint: sklearn/linear_model/_sgd_fast.pyx:545:38: Exception check after calling 'loss' will always require the GIL to be acquired. Declare 'loss' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n      performance hint: sklearn/linear_model/_sgd_fast.pyx:547:38: Exception check after calling 'dloss' will always require the GIL to be acquired. Declare 'dloss' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n      performance hint: sklearn/linear_model/_sgd_fast.pyx:569:27: Exception check after calling 'scale' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'scale' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on 'scale' to allow an error code to be returned.\n      performance hint: sklearn/linear_model/_sgd_fast.pyx:571:25: Exception check after calling 'add' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'add' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on 'add' to allow an error code to be returned.\n      performance hint: sklearn/linear_model/_sgd_fast.pyx:580:33: Exception check after calling 'add_average' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'add_average' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on 'add_average' to allow an error code to be returned.\n      performance hint: sklearn/linear_model/_sgd_fast.pyx:587:29: Exception check after calling 'l1penalty' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'l1penalty' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on 'l1penalty' to allow an error code to be returned.\n      warning: sklearn/neighbors/_quad_tree.pxd:76:59: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pxd:95:51: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pxd:98:59: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pxd:99:63: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pxd:100:80: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      performance hint: sklearn/manifold/_barnes_hut_tsne.pyx:224:30: Exception check after calling 'summarize' will always require the GIL to be acquired. Declare 'summarize' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n      warning: sklearn/neighbors/_binary_tree.pxi:595:66: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_binary_tree.pxi:603:49: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_binary_tree.pxi:665:57: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_binary_tree.pxi:1215:58: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_binary_tree.pxi:1224:59: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_binary_tree.pxi:1805:78: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_ball_tree.pyx:109:57: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_ball_tree.pyx:125:82: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_ball_tree.pyx:136:58: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pxd:19:64: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pxd:29:65: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pxd:38:73: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pxd:42:73: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pxd:65:51: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pxd:68:52: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pxd:75:68: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pxd:77:67: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      performance hint: sklearn/neighbors/_binary_tree.pxi:539:5: Exception check on 'dual_swap' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'dual_swap' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on 'dual_swap' to allow an error code to be returned.\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n              # determine number of levels in the tree, and from this\n              # the number of nodes in the tree.  This results in leaf nodes\n              # with numbers of points between leaf_size and 2 * leaf_size\n              self.n_levels = int(\n                  np.log2(fmax(1, (n_samples - 1) / self.leaf_size)) + 1)\n              self.n_nodes = (2 ** self.n_levels) - 1\n                                                  ^\n      ------------------------------------------------------------\n      \n      sklearn/neighbors/_binary_tree.pxi:1079:44: Cannot assign type 'double' to 'ITYPE_t' (alias of 'long')\n      Traceback (most recent call last):\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/Cython/Build/Dependencies.py\", line 1345, in cythonize_one_helper\n          return cythonize_one(*m)\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/Cython/Build/Dependencies.py\", line 1321, in cythonize_one\n          raise CompileError(None, pyx_file)\n      Cython.Compiler.Errors.CompileError: sklearn/neighbors/_ball_tree.pyx\n      warning: sklearn/neighbors/_dist_metrics.pxd:19:64: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pxd:29:65: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pxd:38:73: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pxd:42:73: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pxd:65:51: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pxd:68:52: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pxd:75:68: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pxd:77:67: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:304:51: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:312:52: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:344:68: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:348:67: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:427:58: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:431:59: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:434:75: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:437:74: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:467:59: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:476:58: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:479:75: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:482:74: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:505:58: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:538:58: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:569:59: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:577:58: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:580:75: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:583:74: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:629:59: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:637:58: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:640:75: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:643:74: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:694:59: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:710:58: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:713:75: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:716:74: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:739:58: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:761:58: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:784:58: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:810:58: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:840:58: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:864:58: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:889:58: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:914:58: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:938:58: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:962:58: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:986:58: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:1020:59: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:1026:58: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:1029:75: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:1032:74: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pyx:1129:58: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_binary_tree.pxi:595:66: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_binary_tree.pxi:603:49: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_binary_tree.pxi:665:57: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_binary_tree.pxi:1215:58: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_binary_tree.pxi:1224:59: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_binary_tree.pxi:1805:78: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_kd_tree.pyx:91:51: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_kd_tree.pyx:152:82: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pxd:19:64: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pxd:29:65: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pxd:38:73: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pxd:42:73: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pxd:65:51: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pxd:68:52: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pxd:75:68: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_dist_metrics.pxd:77:67: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      performance hint: sklearn/neighbors/_binary_tree.pxi:539:5: Exception check on 'dual_swap' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'dual_swap' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on 'dual_swap' to allow an error code to be returned.\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n              # determine number of levels in the tree, and from this\n              # the number of nodes in the tree.  This results in leaf nodes\n              # with numbers of points between leaf_size and 2 * leaf_size\n              self.n_levels = int(\n                  np.log2(fmax(1, (n_samples - 1) / self.leaf_size)) + 1)\n              self.n_nodes = (2 ** self.n_levels) - 1\n                                                  ^\n      ------------------------------------------------------------\n      \n      sklearn/neighbors/_binary_tree.pxi:1079:44: Cannot assign type 'double' to 'ITYPE_t' (alias of 'long')\n      Traceback (most recent call last):\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/Cython/Build/Dependencies.py\", line 1345, in cythonize_one_helper\n          return cythonize_one(*m)\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/Cython/Build/Dependencies.py\", line 1321, in cythonize_one\n          raise CompileError(None, pyx_file)\n      Cython.Compiler.Errors.CompileError: sklearn/neighbors/_kd_tree.pyx\n      warning: sklearn/neighbors/_quad_tree.pxd:76:59: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pxd:95:51: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pxd:98:59: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pxd:99:63: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pxd:100:80: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pyx:120:59: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pyx:309:51: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pyx:468:40: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pyx:563:59: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pyx:575:70: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:49:75: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:87:61: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:119:56: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:137:40: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:139:71: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:160:71: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:161:40: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_tree.pxd:61:68: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_tree.pxd:62:59: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_tree.pxd:63:63: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_splitter.pxd:84:72: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_splitter.pxd:89:68: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pxd:57:45: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pxd:58:40: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pxd:59:48: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pxd:60:57: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n          def __cinit__(self, int n_dimensions, int verbose):\n              \"\"\"Constructor.\"\"\"\n              # Parameters of the tree\n              self.n_dimensions = n_dimensions\n              self.verbose = verbose\n              self.n_cells_per_cell = 2 ** self.n_dimensions\n                                        ^\n      ------------------------------------------------------------\n      \n      sklearn/neighbors/_quad_tree.pyx:60:34: Cannot assign type 'double' to 'SIZE_t' (alias of 'long')\n      Traceback (most recent call last):\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/Cython/Build/Dependencies.py\", line 1345, in cythonize_one_helper\n          return cythonize_one(*m)\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/Cython/Build/Dependencies.py\", line 1321, in cythonize_one\n          raise CompileError(None, pyx_file)\n      Cython.Compiler.Errors.CompileError: sklearn/neighbors/_quad_tree.pyx\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n              free_problem(problem)\n              free_parameter(param)\n              raise ValueError(error_msg)\n      \n          cdef BlasFunctions blas_functions\n          blas_functions.dot = _dot[double]\n                                   ^\n      ------------------------------------------------------------\n      \n      sklearn/svm/_liblinear.pyx:55:29: Cannot assign type 'double (int, double *, int, double *, int) except * nogil' to 'dot_func' (alias of 'double (*)(int, double *, int, double *, int) noexcept'). Exception values are incompatible. Suggest adding 'noexcept' to the type of the value being assigned.\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n              free_parameter(param)\n              raise ValueError(error_msg)\n      \n          cdef BlasFunctions blas_functions\n          blas_functions.dot = _dot[double]\n          blas_functions.axpy = _axpy[double]\n                                     ^\n      ------------------------------------------------------------\n      \n      sklearn/svm/_liblinear.pyx:56:31: Cannot assign type 'void (int, double, double *, int, double *, int) except * nogil' to 'axpy_func' (alias of 'void (*)(int, double, double *, int, double *, int) noexcept'). Exception values are incompatible. Suggest adding 'noexcept' to the type of the value being assigned.\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n              raise ValueError(error_msg)\n      \n          cdef BlasFunctions blas_functions\n          blas_functions.dot = _dot[double]\n          blas_functions.axpy = _axpy[double]\n          blas_functions.scal = _scal[double]\n                                     ^\n      ------------------------------------------------------------\n      \n      sklearn/svm/_liblinear.pyx:57:31: Cannot assign type 'void (int, double, double *, int) except * nogil' to 'scal_func' (alias of 'void (*)(int, double, double *, int) noexcept'). Exception values are incompatible. Suggest adding 'noexcept' to the type of the value being assigned.\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n      \n          cdef BlasFunctions blas_functions\n          blas_functions.dot = _dot[double]\n          blas_functions.axpy = _axpy[double]\n          blas_functions.scal = _scal[double]\n          blas_functions.nrm2 = _nrm2[double]\n                                     ^\n      ------------------------------------------------------------\n      \n      sklearn/svm/_liblinear.pyx:58:31: Cannot assign type 'double (int, double *, int) except * nogil' to 'nrm2_func' (alias of 'double (*)(int, double *, int) noexcept'). Exception values are incompatible. Suggest adding 'noexcept' to the type of the value being assigned.\n      Traceback (most recent call last):\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/Cython/Build/Dependencies.py\", line 1345, in cythonize_one_helper\n          return cythonize_one(*m)\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/Cython/Build/Dependencies.py\", line 1321, in cythonize_one\n          raise CompileError(None, pyx_file)\n      Cython.Compiler.Errors.CompileError: sklearn/svm/_liblinear.pyx\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n          if error_msg:\n              # for SVR: epsilon is called p in libsvm\n              error_repl = error_msg.decode('utf-8').replace(\"p < 0\", \"epsilon < 0\")\n              raise ValueError(error_repl)\n          cdef BlasFunctions blas_functions\n          blas_functions.dot = _dot[double]\n                                   ^\n      ------------------------------------------------------------\n      \n      sklearn/svm/_libsvm.pyx:194:29: Cannot assign type 'double (int, double *, int, double *, int) except * nogil' to 'dot_func' (alias of 'double (*)(int, double *, int, double *, int) noexcept'). Exception values are incompatible. Suggest adding 'noexcept' to the type of the value being assigned.\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n                             class_weight_label.data, class_weight.data)\n          model = set_model(&param, <int> nSV.shape[0], SV.data, SV.shape,\n                            support.data, support.shape, sv_coef.strides,\n                            sv_coef.data, intercept.data, nSV.data, probA.data, probB.data)\n          cdef BlasFunctions blas_functions\n          blas_functions.dot = _dot[double]\n                                   ^\n      ------------------------------------------------------------\n      \n      sklearn/svm/_libsvm.pyx:358:29: Cannot assign type 'double (int, double *, int, double *, int) except * nogil' to 'dot_func' (alias of 'double (*)(int, double *, int, double *, int) noexcept'). Exception values are incompatible. Suggest adding 'noexcept' to the type of the value being assigned.\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n                            sv_coef.data, intercept.data, nSV.data,\n                            probA.data, probB.data)\n      \n          cdef np.npy_intp n_class = get_nr(model)\n          cdef BlasFunctions blas_functions\n          blas_functions.dot = _dot[double]\n                                   ^\n      ------------------------------------------------------------\n      \n      sklearn/svm/_libsvm.pyx:464:29: Cannot assign type 'double (int, double *, int, double *, int) except * nogil' to 'dot_func' (alias of 'double (*)(int, double *, int, double *, int) noexcept'). Exception values are incompatible. Suggest adding 'noexcept' to the type of the value being assigned.\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n              n_class = 1\n          else:\n              n_class = get_nr(model)\n              n_class = n_class * (n_class - 1) // 2\n          cdef BlasFunctions blas_functions\n          blas_functions.dot = _dot[double]\n                                   ^\n      ------------------------------------------------------------\n      \n      sklearn/svm/_libsvm.pyx:570:29: Cannot assign type 'double (int, double *, int, double *, int) except * nogil' to 'dot_func' (alias of 'double (*)(int, double *, int, double *, int) noexcept'). Exception values are incompatible. Suggest adding 'noexcept' to the type of the value being assigned.\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n          if error_msg:\n              raise ValueError(error_msg)\n      \n          cdef np.ndarray[np.float64_t, ndim=1, mode='c'] target\n          cdef BlasFunctions blas_functions\n          blas_functions.dot = _dot[double]\n                                   ^\n      ------------------------------------------------------------\n      \n      sklearn/svm/_libsvm.pyx:714:29: Cannot assign type 'double (int, double *, int, double *, int) except * nogil' to 'dot_func' (alias of 'double (*)(int, double *, int, double *, int) noexcept'). Exception values are incompatible. Suggest adding 'noexcept' to the type of the value being assigned.\n      Traceback (most recent call last):\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/Cython/Build/Dependencies.py\", line 1345, in cythonize_one_helper\n          return cythonize_one(*m)\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/Cython/Build/Dependencies.py\", line 1321, in cythonize_one\n          raise CompileError(None, pyx_file)\n      Cython.Compiler.Errors.CompileError: sklearn/svm/_libsvm.pyx\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n          if error_msg:\n              free_problem(problem)\n              free_param(param)\n              raise ValueError(error_msg)\n          cdef BlasFunctions blas_functions\n          blas_functions.dot = _dot[double]\n                                   ^\n      ------------------------------------------------------------\n      \n      sklearn/svm/_libsvm_sparse.pyx:153:29: Cannot assign type 'double (int, double *, int, double *, int) except * nogil' to 'dot_func' (alias of 'double (*)(int, double *, int, double *, int) noexcept'). Exception values are incompatible. Suggest adding 'noexcept' to the type of the value being assigned.\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n                                sv_coef.data, intercept.data,\n                                nSV.data, probA.data, probB.data)\n          #TODO: use check_model\n          dec_values = np.empty(T_indptr.shape[0]-1)\n          cdef BlasFunctions blas_functions\n          blas_functions.dot = _dot[double]\n                                   ^\n      ------------------------------------------------------------\n      \n      sklearn/svm/_libsvm_sparse.pyx:284:29: Cannot assign type 'double (int, double *, int, double *, int) except * nogil' to 'dot_func' (alias of 'double (*)(int, double *, int, double *, int) noexcept'). Exception values are incompatible. Suggest adding 'noexcept' to the type of the value being assigned.\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n          #TODO: use check_model\n          cdef np.npy_intp n_class = get_nr(model)\n          cdef int rv\n          dec_values = np.empty((T_indptr.shape[0]-1, n_class), dtype=np.float64)\n          cdef BlasFunctions blas_functions\n          blas_functions.dot = _dot[double]\n                                   ^\n      ------------------------------------------------------------\n      \n      sklearn/svm/_libsvm_sparse.pyx:343:29: Cannot assign type 'double (int, double *, int, double *, int) except * nogil' to 'dot_func' (alias of 'double (*)(int, double *, int, double *, int) noexcept'). Exception values are incompatible. Suggest adding 'noexcept' to the type of the value being assigned.\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n              n_class = get_nr(model)\n              n_class = n_class * (n_class - 1) // 2\n      \n          dec_values = np.empty((T_indptr.shape[0] - 1, n_class), dtype=np.float64)\n          cdef BlasFunctions blas_functions\n          blas_functions.dot = _dot[double]\n                                   ^\n      ------------------------------------------------------------\n      \n      sklearn/svm/_libsvm_sparse.pyx:412:29: Cannot assign type 'double (int, double *, int, double *, int) except * nogil' to 'dot_func' (alias of 'double (*)(int, double *, int, double *, int) noexcept'). Exception values are incompatible. Suggest adding 'noexcept' to the type of the value being assigned.\n      Traceback (most recent call last):\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/Cython/Build/Dependencies.py\", line 1345, in cythonize_one_helper\n          return cythonize_one(*m)\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/Cython/Build/Dependencies.py\", line 1321, in cythonize_one\n          raise CompileError(None, pyx_file)\n      Cython.Compiler.Errors.CompileError: sklearn/svm/_libsvm_sparse.pyx\n      warning: sklearn/tree/_criterion.pxd:57:45: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pxd:58:40: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pxd:59:48: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pxd:60:57: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_tree.pxd:61:68: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_tree.pxd:62:59: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_tree.pxd:63:63: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_splitter.pxd:84:72: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_splitter.pxd:89:68: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pyx:61:45: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pyx:86:40: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pyx:93:48: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pyx:100:57: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pyx:282:76: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pyx:348:40: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pyx:375:48: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pyx:402:57: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pyx:744:45: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pyx:788:40: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pyx:799:48: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pyx:810:57: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pyx:1032:45: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pyx:1082:40: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pyx:1113:48: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pyx:1141:57: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:49:75: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:87:61: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:119:56: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:137:40: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:139:71: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:160:71: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:161:40: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pxd:76:59: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pxd:95:51: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pxd:98:59: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pxd:99:63: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pxd:100:80: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      performance hint: sklearn/tree/_criterion.pyx:169:30: Exception check after calling 'children_impurity' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'children_impurity' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on 'children_impurity' to allow an error code to be returned.\n      performance hint: sklearn/tree/_criterion.pyx:540:44: Exception check after calling 'log' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'log' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Declare any exception value explicitly for functions in pxd files.\n      performance hint: sklearn/tree/_criterion.pyx:574:49: Exception check after calling 'log' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'log' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Declare any exception value explicitly for functions in pxd files.\n      performance hint: sklearn/tree/_criterion.pyx:579:50: Exception check after calling 'log' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'log' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Declare any exception value explicitly for functions in pxd files.\n      performance hint: sklearn/tree/_criterion.pyx:1076:89: Exception check after calling 'get_median' will always require the GIL to be acquired. Declare 'get_median' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n      performance hint: sklearn/tree/_criterion.pyx:1104:74: Exception check after calling 'size' will always require the GIL to be acquired. Declare 'size' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n      performance hint: sklearn/tree/_criterion.pyx:1106:62: Exception check after calling 'pop' will always require the GIL to be acquired. Declare 'pop' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n      performance hint: sklearn/tree/_criterion.pyx:1132:75: Exception check after calling 'size' will always require the GIL to be acquired. Declare 'size' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n      performance hint: sklearn/tree/_criterion.pyx:1134:63: Exception check after calling 'pop' will always require the GIL to be acquired. Declare 'pop' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n      performance hint: sklearn/tree/_criterion.pyx:1172:70: Exception check after calling 'remove' will always require the GIL to be acquired. Declare 'remove' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n      performance hint: sklearn/tree/_criterion.pyx:1188:69: Exception check after calling 'remove' will always require the GIL to be acquired. Declare 'remove' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n      performance hint: sklearn/tree/_criterion.pyx:1252:74: Exception check after calling 'get_median' will always require the GIL to be acquired. Declare 'get_median' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n      performance hint: sklearn/tree/_criterion.pyx:1264:75: Exception check after calling 'get_median' will always require the GIL to be acquired. Declare 'get_median' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n      performance hint: sklearn/tree/_criterion.pyx:1365:32: Exception check after calling 'poisson_loss' will always require the GIL to be acquired. Declare 'poisson_loss' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n      performance hint: sklearn/tree/_criterion.pyx:1400:56: Exception check after calling 'log' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'log' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Declare any exception value explicitly for functions in pxd files.\n      performance hint: sklearn/tree/_criterion.pyx:1401:58: Exception check after calling 'log' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'log' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Declare any exception value explicitly for functions in pxd files.\n      performance hint: sklearn/tree/_criterion.pyx:1422:44: Exception check after calling 'poisson_loss' will always require the GIL to be acquired. Declare 'poisson_loss' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n      performance hint: sklearn/tree/_criterion.pyx:1425:45: Exception check after calling 'poisson_loss' will always require the GIL to be acquired. Declare 'poisson_loss' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n      warning: sklearn/tree/_splitter.pxd:84:72: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_splitter.pxd:89:68: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pxd:57:45: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pxd:58:40: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pxd:59:48: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pxd:60:57: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_tree.pxd:61:68: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_tree.pxd:62:59: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_tree.pxd:63:63: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_splitter.pyx:184:72: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_splitter.pyx:214:68: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_splitter.pyx:268:68: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_splitter.pyx:582:68: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_splitter.pyx:1100:68: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_splitter.pyx:1330:68: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:49:75: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:87:61: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:119:56: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:137:40: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:139:71: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:160:71: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:161:40: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pxd:76:59: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pxd:95:51: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pxd:98:59: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pxd:99:63: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pxd:100:80: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      performance hint: sklearn/tree/_splitter.pyx:46:5: Exception check on '_init_split' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '_init_split' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '_init_split' to allow an error code to be returned.\n      performance hint: sklearn/tree/_splitter.pyx:460:5: Exception check on 'sort' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'sort' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on 'sort' to allow an error code to be returned.\n      performance hint: sklearn/tree/_splitter.pyx:467:5: Exception check on 'swap' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'swap' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on 'swap' to allow an error code to be returned.\n      performance hint: sklearn/tree/_splitter.pyx:496:5: Exception check on 'introsort' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'introsort' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on 'introsort' to allow an error code to be returned.\n      performance hint: sklearn/tree/_splitter.pyx:529:5: Exception check on 'sift_down' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'sift_down' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on 'sift_down' to allow an error code to be returned.\n      performance hint: sklearn/tree/_splitter.pyx:552:5: Exception check on 'heapsort' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'heapsort' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on 'heapsort' to allow an error code to be returned.\n      performance hint: sklearn/tree/_splitter.pyx:944:5: Exception check on 'binary_search' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'binary_search' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on 'binary_search' to allow an error code to be returned.\n      performance hint: sklearn/tree/_splitter.pyx:969:5: Exception check on 'extract_nnz_index_to_samples' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'extract_nnz_index_to_samples' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on 'extract_nnz_index_to_samples' to allow an error code to be returned.\n      performance hint: sklearn/tree/_splitter.pyx:1009:5: Exception check on 'extract_nnz_binary_search' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'extract_nnz_binary_search' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on 'extract_nnz_binary_search' to allow an error code to be returned.\n      performance hint: sklearn/tree/_splitter.pyx:1081:5: Exception check on 'sparse_swap' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'sparse_swap' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on 'sparse_swap' to allow an error code to be returned.\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n          if not is_samples_sorted[0]:\n              n_samples = end - start\n              memcpy(sorted_samples + start, samples + start,\n                     n_samples * sizeof(SIZE_t))\n              qsort(sorted_samples + start, n_samples, sizeof(SIZE_t),\n                    compare_SIZE_t)\n                    ^\n      ------------------------------------------------------------\n      \n      sklearn/tree/_splitter.pyx:1037:14: Cannot assign type 'int (const void *, const void *) except? -1 nogil' to 'int (*)(const void *, const void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to the type of 'compare_SIZE_t'.\n      Traceback (most recent call last):\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/Cython/Build/Dependencies.py\", line 1345, in cythonize_one_helper\n          return cythonize_one(*m)\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/Cython/Build/Dependencies.py\", line 1321, in cythonize_one\n          raise CompileError(None, pyx_file)\n      Cython.Compiler.Errors.CompileError: sklearn/tree/_splitter.pyx\n      warning: sklearn/tree/_tree.pxd:61:68: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_tree.pxd:62:59: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_tree.pxd:63:63: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_splitter.pxd:84:72: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_splitter.pxd:89:68: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pxd:57:45: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pxd:58:40: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pxd:59:48: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pxd:60:57: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_tree.pyx:273:71: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_tree.pyx:421:76: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_tree.pyx:678:59: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_tree.pyx:690:70: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_tree.pyx:724:73: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:49:75: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:87:61: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:119:56: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:137:40: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:139:71: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:160:71: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:161:40: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pxd:76:59: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pxd:95:51: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pxd:98:59: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pxd:99:63: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pxd:100:80: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n      \n              # Initial capacity\n              cdef int init_capacity\n      \n              if tree.max_depth <= 10:\n                  init_capacity = (2 ** (tree.max_depth + 1)) - 1\n                                                              ^\n      ------------------------------------------------------------\n      \n      sklearn/tree/_tree.pyx:151:56: Cannot assign type 'double' to 'int'\n      Traceback (most recent call last):\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/Cython/Build/Dependencies.py\", line 1345, in cythonize_one_helper\n          return cythonize_one(*m)\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/Cython/Build/Dependencies.py\", line 1321, in cythonize_one\n          raise CompileError(None, pyx_file)\n      Cython.Compiler.Errors.CompileError: sklearn/tree/_tree.pyx\n      warning: sklearn/tree/_utils.pxd:49:75: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:87:61: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:119:56: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:137:40: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:139:71: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:160:71: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pxd:161:40: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_tree.pxd:61:68: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_tree.pxd:62:59: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_tree.pxd:63:63: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_splitter.pxd:84:72: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_splitter.pxd:89:68: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pxd:57:45: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pxd:58:40: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pxd:59:48: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_criterion.pxd:60:57: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pxd:76:59: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pxd:95:51: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pxd:98:59: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pxd:99:63: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/neighbors/_quad_tree.pxd:100:80: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pyx:29:75: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pyx:114:61: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pyx:230:56: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pyx:318:40: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pyx:335:71: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pyx:493:40: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      warning: sklearn/tree/_utils.pyx:507:71: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' will be disallowed in a future version of Cython.\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n          # Max value for our rand_r replacement (near the bottom).\n          # We don't use RAND_MAX because it's different across platforms and\n          # particularly tiny on Windows/MSVC.\n          RAND_R_MAX = 0x7FFFFFFF\n      \n      cpdef sample_without_replacement(np.int_t n_population,\n                                       ^\n      ------------------------------------------------------------\n      \n      sklearn/utils/_random.pxd:18:33: 'int_t' is not a type identifier\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n          # We don't use RAND_MAX because it's different across platforms and\n          # particularly tiny on Windows/MSVC.\n          RAND_R_MAX = 0x7FFFFFFF\n      \n      cpdef sample_without_replacement(np.int_t n_population,\n                                       np.int_t n_samples,\n                                       ^\n      ------------------------------------------------------------\n      \n      sklearn/utils/_random.pxd:19:33: 'int_t' is not a type identifier\n      performance hint: sklearn/tree/_utils.pyx:29:5: Exception check on 'safe_realloc' will always require the GIL to be acquired. Declare 'safe_realloc' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n      performance hint: sklearn/tree/_utils.pxd:55:20: No exception value declared for 'rand_int' in pxd file.\n      Users cimporting this function and calling it without the gil will always require an exception check.\n      Suggest adding an explicit exception value.\n      performance hint: sklearn/tree/_utils.pxd:59:24: No exception value declared for 'rand_uniform' in pxd file.\n      Users cimporting this function and calling it without the gil will always require an exception check.\n      Suggest adding an explicit exception value.\n      performance hint: sklearn/tree/_utils.pxd:63:15: No exception value declared for 'log' in pxd file.\n      Users cimporting this function and calling it without the gil will always require an exception check.\n      Suggest adding an explicit exception value.\n      Traceback (most recent call last):\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/Cython/Build/Dependencies.py\", line 1345, in cythonize_one_helper\n          return cythonize_one(*m)\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/Cython/Build/Dependencies.py\", line 1321, in cythonize_one\n          raise CompileError(None, pyx_file)\n      Cython.Compiler.Errors.CompileError: sklearn/tree/_utils.pyx\n      performance hint: sklearn/utils/_cython_blas.pyx:20:5: Exception check on '_dot' will always require the GIL to be acquired. Declare '_dot' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n      performance hint: sklearn/utils/_cython_blas.pyx:33:5: Exception check on '_asum' will always require the GIL to be acquired. Declare '_asum' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n      performance hint: sklearn/utils/_cython_blas.pyx:45:5: Exception check on '_axpy' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '_axpy' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '_axpy' to allow an error code to be returned.\n      performance hint: sklearn/utils/_cython_blas.pyx:58:5: Exception check on '_nrm2' will always require the GIL to be acquired. Declare '_nrm2' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n      performance hint: sklearn/utils/_cython_blas.pyx:70:5: Exception check on '_copy' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '_copy' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '_copy' to allow an error code to be returned.\n      performance hint: sklearn/utils/_cython_blas.pyx:82:5: Exception check on '_scal' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '_scal' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '_scal' to allow an error code to be returned.\n      performance hint: sklearn/utils/_cython_blas.pyx:94:5: Exception check on '_rotg' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '_rotg' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '_rotg' to allow an error code to be returned.\n      performance hint: sklearn/utils/_cython_blas.pyx:107:5: Exception check on '_rot' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '_rot' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '_rot' to allow an error code to be returned.\n      performance hint: sklearn/utils/_cython_blas.pyx:124:5: Exception check on '_gemv' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '_gemv' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '_gemv' to allow an error code to be returned.\n      performance hint: sklearn/utils/_cython_blas.pyx:153:5: Exception check on '_ger' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '_ger' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '_ger' to allow an error code to be returned.\n      performance hint: sklearn/utils/_cython_blas.pyx:183:5: Exception check on '_gemm' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '_gemm' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '_gemm' to allow an error code to be returned.\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n              dec(end)\n              # Construct our arguments\n              cdef pair[ITYPE_t, DTYPE_t] args\n              args.first = key\n              args.second = value\n              self.my_map.insert(end, args)\n                                 ^\n      ------------------------------------------------------------\n      \n      sklearn/utils/_fast_dict.pyx:138:27: Cannot assign type 'iterator' to 'const_iterator'\n      Traceback (most recent call last):\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/Cython/Build/Dependencies.py\", line 1345, in cythonize_one_helper\n          return cythonize_one(*m)\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/Cython/Build/Dependencies.py\", line 1321, in cythonize_one\n          raise CompileError(None, pyx_file)\n      Cython.Compiler.Errors.CompileError: sklearn/utils/_fast_dict.pyx\n      warning: sklearn/utils/_openmp_helpers.pyx:1:0: The 'IF' statement is deprecated and will be removed in a future Cython version. Consider using runtime conditions or C macros instead. See https://github.com/cython/cython/issues/4310\n      warning: sklearn/utils/_openmp_helpers.pyx:44:4: The 'IF' statement is deprecated and will be removed in a future Cython version. Consider using runtime conditions or C macros instead. See https://github.com/cython/cython/issues/4310\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n          # Max value for our rand_r replacement (near the bottom).\n          # We don't use RAND_MAX because it's different across platforms and\n          # particularly tiny on Windows/MSVC.\n          RAND_R_MAX = 0x7FFFFFFF\n      \n      cpdef sample_without_replacement(np.int_t n_population,\n                                       ^\n      ------------------------------------------------------------\n      \n      sklearn/utils/_random.pxd:18:33: 'int_t' is not a type identifier\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n          # We don't use RAND_MAX because it's different across platforms and\n          # particularly tiny on Windows/MSVC.\n          RAND_R_MAX = 0x7FFFFFFF\n      \n      cpdef sample_without_replacement(np.int_t n_population,\n                                       np.int_t n_samples,\n                                       ^\n      ------------------------------------------------------------\n      \n      sklearn/utils/_random.pxd:19:33: 'int_t' is not a type identifier\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n      from . import check_random_state\n      \n      cdef UINT32_t DEFAULT_SEED = 1\n      \n      \n      cpdef _sample_without_replacement_check_input(np.int_t n_population,\n                                                    ^\n      ------------------------------------------------------------\n      \n      sklearn/utils/_random.pyx:27:46: 'int_t' is not a type identifier\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n      \n      cdef UINT32_t DEFAULT_SEED = 1\n      \n      \n      cpdef _sample_without_replacement_check_input(np.int_t n_population,\n                                                    np.int_t n_samples):\n                                                    ^\n      ------------------------------------------------------------\n      \n      sklearn/utils/_random.pyx:28:46: 'int_t' is not a type identifier\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n                               'n_samples, got n_samples > n_population (%s > %s)'\n                               % (n_samples, n_population))\n      \n      \n      cpdef _sample_without_replacement_with_tracking_selection(\n              np.int_t n_population,\n              ^\n      ------------------------------------------------------------\n      \n      sklearn/utils/_random.pyx:41:8: 'int_t' is not a type identifier\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n                               % (n_samples, n_population))\n      \n      \n      cpdef _sample_without_replacement_with_tracking_selection(\n              np.int_t n_population,\n              np.int_t n_samples,\n              ^\n      ------------------------------------------------------------\n      \n      sklearn/utils/_random.pyx:42:8: 'int_t' is not a type identifier\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n              out[i] = j\n      \n          return out\n      \n      \n      cpdef _sample_without_replacement_with_pool(np.int_t n_population,\n                                                  ^\n      ------------------------------------------------------------\n      \n      sklearn/utils/_random.pyx:105:44: 'int_t' is not a type identifier\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n      \n          return out\n      \n      \n      cpdef _sample_without_replacement_with_pool(np.int_t n_population,\n                                                  np.int_t n_samples,\n                                                  ^\n      ------------------------------------------------------------\n      \n      sklearn/utils/_random.pyx:106:44: 'int_t' is not a type identifier\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n      \n          return out\n      \n      \n      cpdef _sample_without_replacement_with_reservoir_sampling(\n          np.int_t n_population,\n          ^\n      ------------------------------------------------------------\n      \n      sklearn/utils/_random.pyx:165:4: 'int_t' is not a type identifier\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n          return out\n      \n      \n      cpdef _sample_without_replacement_with_reservoir_sampling(\n          np.int_t n_population,\n          np.int_t n_samples,\n          ^\n      ------------------------------------------------------------\n      \n      sklearn/utils/_random.pyx:166:4: 'int_t' is not a type identifier\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n                  out[j] = i\n      \n          return out\n      \n      \n      cpdef sample_without_replacement(np.int_t n_population,\n                                       ^\n      ------------------------------------------------------------\n      \n      sklearn/utils/_random.pyx:223:33: 'int_t' is not a type identifier\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n      \n          return out\n      \n      \n      cpdef sample_without_replacement(np.int_t n_population,\n                                       np.int_t n_samples,\n                                       ^\n      ------------------------------------------------------------\n      \n      sklearn/utils/_random.pyx:224:33: 'int_t' is not a type identifier\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n          out : ndarray of shape (n_samples,)\n              The sampled subsets of integer.\n          \"\"\"\n          _sample_without_replacement_check_input(n_population, n_samples)\n      \n          cdef np.int_t i\n               ^\n      ------------------------------------------------------------\n      \n      sklearn/utils/_random.pyx:84:9: 'int_t' is not a type identifier\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n              The sampled subsets of integer.\n          \"\"\"\n          _sample_without_replacement_check_input(n_population, n_samples)\n      \n          cdef np.int_t i\n          cdef np.int_t j\n               ^\n      ------------------------------------------------------------\n      \n      sklearn/utils/_random.pyx:85:9: 'int_t' is not a type identifier\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n          \"\"\"\n          _sample_without_replacement_check_input(n_population, n_samples)\n      \n          cdef np.int_t i\n          cdef np.int_t j\n          cdef np.ndarray[np.int_t, ndim=1] out = np.empty((n_samples, ), dtype=int)\n                             ^\n      ------------------------------------------------------------\n      \n      sklearn/utils/_random.pyx:86:23: Invalid type.\n      Traceback (most recent call last):\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/Cython/Build/Dependencies.py\", line 1345, in cythonize_one_helper\n          return cythonize_one(*m)\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/Cython/Build/Dependencies.py\", line 1321, in cythonize_one\n          raise CompileError(None, pyx_file)\n      Cython.Compiler.Errors.CompileError: sklearn/utils/_random.pyx\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n          # Max value for our rand_r replacement (near the bottom).\n          # We don't use RAND_MAX because it's different across platforms and\n          # particularly tiny on Windows/MSVC.\n          RAND_R_MAX = 0x7FFFFFFF\n      \n      cpdef sample_without_replacement(np.int_t n_population,\n                                       ^\n      ------------------------------------------------------------\n      \n      sklearn/utils/_random.pxd:18:33: 'int_t' is not a type identifier\n      \n      Error compiling Cython file:\n      ------------------------------------------------------------\n      ...\n          # We don't use RAND_MAX because it's different across platforms and\n          # particularly tiny on Windows/MSVC.\n          RAND_R_MAX = 0x7FFFFFFF\n      \n      cpdef sample_without_replacement(np.int_t n_population,\n                                       np.int_t n_samples,\n                                       ^\n      ------------------------------------------------------------\n      \n      sklearn/utils/_random.pxd:19:33: 'int_t' is not a type identifier\n      Traceback (most recent call last):\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/Cython/Build/Dependencies.py\", line 1345, in cythonize_one_helper\n          return cythonize_one(*m)\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/Cython/Build/Dependencies.py\", line 1321, in cythonize_one\n          raise CompileError(None, pyx_file)\n      Cython.Compiler.Errors.CompileError: sklearn/utils/_seq_dataset.pyx\n      performance hint: sklearn/utils/_weight_vector.pyx:169:29: Exception check after calling 'reset_wscale' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare 'reset_wscale' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on 'reset_wscale' to allow an error code to be returned.\n      performance hint: sklearn/utils/_weight_vector.pyx:174:17: Exception check after calling '__pyx_fuse_1_axpy' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '__pyx_fuse_1_axpy' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '__pyx_fuse_1_axpy' to allow an error code to be returned.\n      performance hint: sklearn/utils/_weight_vector.pyx:176:17: Exception check after calling '__pyx_fuse_1_scal' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '__pyx_fuse_1_scal' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '__pyx_fuse_1_scal' to allow an error code to be returned.\n      performance hint: sklearn/utils/_weight_vector.pyx:180:13: Exception check after calling '__pyx_fuse_1_scal' will always require the GIL to be acquired.\n      Possible solutions:\n          1. Declare '__pyx_fuse_1_scal' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n          2. Use an 'int' return type on '__pyx_fuse_1_scal' to allow an error code to be returned.\n      multiprocessing.pool.RemoteTraceback:\n      \"\"\"\n      Traceback (most recent call last):\n        File \"/usr/local/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n          result = (True, func(*args, **kwds))\n        File \"/usr/local/lib/python3.10/multiprocessing/pool.py\", line 48, in mapstar\n          return list(map(*args))\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/Cython/Build/Dependencies.py\", line 1345, in cythonize_one_helper\n          return cythonize_one(*m)\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/Cython/Build/Dependencies.py\", line 1321, in cythonize_one\n          raise CompileError(None, pyx_file)\n      Cython.Compiler.Errors.CompileError: sklearn/ensemble/_hist_gradient_boosting/splitting.pyx\n      \"\"\"\n      \n      The above exception was the direct cause of the following exception:\n      \n      Traceback (most recent call last):\n        File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\n      [ 1/53] Cythonizing sklearn/__check_build/_check_build.pyx\n      [ 2/53] Cythonizing sklearn/_isotonic.pyx\n      [ 3/53] Cythonizing sklearn/cluster/_dbscan_inner.pyx\n      [ 4/53] Cythonizing sklearn/cluster/_hierarchical_fast.pyx\n      [ 5/53] Cythonizing sklearn/cluster/_k_means_elkan.pyx\n      [ 6/53] Cythonizing sklearn/cluster/_k_means_fast.pyx\n      [ 7/53] Cythonizing sklearn/cluster/_k_means_lloyd.pyx\n      [ 8/53] Cythonizing sklearn/datasets/_svmlight_format_fast.pyx\n      [ 9/53] Cythonizing sklearn/decomposition/_cdnmf_fast.pyx\n      [10/53] Cythonizing sklearn/decomposition/_online_lda_fast.pyx\n      [11/53] Cythonizing sklearn/ensemble/_gradient_boosting.pyx\n      [12/53] Cythonizing sklearn/ensemble/_hist_gradient_boosting/_binning.pyx\n      [13/53] Cythonizing sklearn/ensemble/_hist_gradient_boosting/_bitset.pyx\n      [14/53] Cythonizing sklearn/ensemble/_hist_gradient_boosting/_gradient_boosting.pyx\n      [15/53] Cythonizing sklearn/ensemble/_hist_gradient_boosting/_loss.pyx\n      [16/53] Cythonizing sklearn/ensemble/_hist_gradient_boosting/_predictor.pyx\n      [17/53] Cythonizing sklearn/ensemble/_hist_gradient_boosting/common.pyx\n      [18/53] Cythonizing sklearn/ensemble/_hist_gradient_boosting/histogram.pyx\n      [19/53] Cythonizing sklearn/ensemble/_hist_gradient_boosting/splitting.pyx\n      [20/53] Cythonizing sklearn/ensemble/_hist_gradient_boosting/utils.pyx\n      [21/53] Cythonizing sklearn/feature_extraction/_hashing_fast.pyx\n      [22/53] Cythonizing sklearn/linear_model/_cd_fast.pyx\n      [23/53] Cythonizing sklearn/linear_model/_sag_fast.pyx\n      [24/53] Cythonizing sklearn/linear_model/_sgd_fast.pyx\n      [25/53] Cythonizing sklearn/manifold/_barnes_hut_tsne.pyx\n      [26/53] Cythonizing sklearn/manifold/_utils.pyx\n      [27/53] Cythonizing sklearn/metrics/_pairwise_fast.pyx\n      [28/53] Cythonizing sklearn/metrics/cluster/_expected_mutual_info_fast.pyx\n      [29/53] Cythonizing sklearn/neighbors/_ball_tree.pyx\n      [30/53] Cythonizing sklearn/neighbors/_dist_metrics.pyx\n      [31/53] Cythonizing sklearn/neighbors/_kd_tree.pyx\n      [32/53] Cythonizing sklearn/neighbors/_quad_tree.pyx\n      [33/53] Cythonizing sklearn/neighbors/_typedefs.pyx\n      [34/53] Cythonizing sklearn/preprocessing/_csr_polynomial_expansion.pyx\n      [35/53] Cythonizing sklearn/svm/_liblinear.pyx\n      [36/53] Cythonizing sklearn/svm/_libsvm.pyx\n      [37/53] Cythonizing sklearn/svm/_libsvm_sparse.pyx\n      [38/53] Cythonizing sklearn/svm/_newrand.pyx\n      [39/53] Cythonizing sklearn/tree/_criterion.pyx\n      [40/53] Cythonizing sklearn/tree/_splitter.pyx\n      [41/53] Cythonizing sklearn/tree/_tree.pyx\n      [42/53] Cythonizing sklearn/tree/_utils.pyx\n      [43/53] Cythonizing sklearn/utils/_cython_blas.pyx\n      [44/53] Cythonizing sklearn/utils/_fast_dict.pyx\n      [45/53] Cythonizing sklearn/utils/_logistic_sigmoid.pyx\n      [46/53] Cythonizing sklearn/utils/_openmp_helpers.pyx\n      [47/53] Cythonizing sklearn/utils/_random.pyx\n      [48/53] Cythonizing sklearn/utils/_seq_dataset.pyx\n      [49/53] Cythonizing sklearn/utils/_weight_vector.pyx\n      [50/53] Cythonizing sklearn/utils/arrayfuncs.pyx\n      [51/53] Cythonizing sklearn/utils/graph_shortest_path.pyx\n      [52/53] Cythonizing sklearn/utils/murmurhash.pyx\n      [53/53] Cythonizing sklearn/utils/sparsefuncs_fast.pyx\n          main()\n        File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\n          json_out['return_val'] = hook(**hook_input['kwargs'])\n        File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 149, in prepare_metadata_for_build_wheel\n          return hook(metadata_directory, config_settings)\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 373, in prepare_metadata_for_build_wheel\n          self.run_setup()\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 502, in run_setup\n          super().run_setup(setup_script=setup_script)\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 318, in run_setup\n          exec(code, locals())\n        File \"<string>\", line 301, in <module>\n        File \"<string>\", line 297, in setup_package\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/numpy/distutils/core.py\", line 135, in setup\n          config = configuration()\n        File \"<string>\", line 188, in configuration\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/numpy/distutils/misc_util.py\", line 1041, in add_subpackage\n          config_list = self.get_subpackage(subpackage_name, subpackage_path,\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/numpy/distutils/misc_util.py\", line 1007, in get_subpackage\n          config = self._get_configuration_from_setup_py(\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/numpy/distutils/misc_util.py\", line 949, in _get_configuration_from_setup_py\n          config = setup_module.configuration(*args)\n        File \"/tmp/pip-install-4ahn4q0h/scikit-learn_f4da9738e7a942ed9ff84f784cf5041b/sklearn/setup.py\", line 83, in configuration\n          cythonize_extensions(top_path, config)\n        File \"/tmp/pip-install-4ahn4q0h/scikit-learn_f4da9738e7a942ed9ff84f784cf5041b/sklearn/_build_utils/__init__.py\", line 70, in cythonize_extensions\n          config.ext_modules = cythonize(\n        File \"/tmp/pip-build-env-van343ha/overlay/lib/python3.10/site-packages/Cython/Build/Dependencies.py\", line 1145, in cythonize\n          result.get(99999)  # seconds\n        File \"/usr/local/lib/python3.10/multiprocessing/pool.py\", line 774, in get\n          raise self._value\n      Cython.Compiler.Errors.CompileError: sklearn/ensemble/_hist_gradient_boosting/splitting.pyx\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: metadata-generation-failed\n\n× Encountered error while generating package metadata.\n╰─> See above for output.\n\nnote: This is an issue with the package mentioned above, not pip.\nhint: See above for details.\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n\u001b[?25h"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install datasets==1.15.1 transformers==4.11.3 scikit-learn==0.24.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf535044",
   "metadata": {},
   "source": [
    "### Step 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8489501d",
   "metadata": {},
   "source": [
    "Execution Start time: 22:14:25, End time: 22:14:35 (duration: 9 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bddd71",
   "metadata": {},
   "source": [
    "**Thought**: It seems there are compatibility issues with the specific versions of the packages mentioned in the `requirements.txt` file, particularly with `scikit-learn`. I will try installing the latest versions of these packages to see if that resolves the issue. This approach might help in setting up the environment correctly without version conflicts. After that, I will proceed with configuring the training script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aabcd3e",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8855015d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting datasets\n  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/site-packages (4.40.2)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/site-packages (1.5.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from datasets) (2.1.0)\nCollecting pyarrow>=15.0.0 (from datasets)\n  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\nCollecting dill<0.3.9,>=0.3.0 (from datasets)\n  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from datasets) (2.0.3)\nCollecting requests>=2.32.2 (from datasets)\n  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/site-packages (from datasets) (4.66.4)\nCollecting xxhash (from datasets)\n  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nCollecting multiprocess (from datasets)\n  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2023.6.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/site-packages (from datasets) (3.8.3)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/site-packages (from datasets) (0.23.3)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from datasets) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn) (1.11.4)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (2.1.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.0.7)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.6.2)\nCollecting numpy>=1.17 (from datasets)\n  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n\u001b[?25l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/61.0 kB ? eta -:--:--\n\n\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDownloading datasets-2.21.0-py3-none-any.whl (527 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/527.3 kB ? eta -:--:--\n\n\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/116.3 kB ? eta -:--:--\n\n\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/39.9 MB ? eta -:--:--\n\n\n\n\n\n\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/64.9 kB ? eta -:--:--\n\n\u001b[?25hDownloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/18.2 MB ? eta -:--:--\n\n\n\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/134.8 kB ? eta -:--:--\n\n\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/194.1 kB ? eta -:--:--\n\n\u001b[?25hInstalling collected packages: xxhash, requests, numpy, dill, pyarrow, multiprocess, datasets\n  Attempting uninstall: requests\n    Found existing installation: requests 2.31.0\n    Uninstalling requests-2.31.0:\n      Successfully uninstalled requests-2.31.0\n  Attempting uninstall: numpy\n    Found existing installation: numpy 2.1.0\n    Uninstalling numpy-2.1.0:\n      Successfully uninstalled numpy-2.1.0\nSuccessfully installed datasets-2.21.0 dill-0.3.8 multiprocess-0.70.16 numpy-1.26.4 pyarrow-17.0.0 requests-2.32.3 xxhash-3.5.0\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install datasets transformers scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501332cd",
   "metadata": {},
   "source": [
    "### Step 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906912d1",
   "metadata": {},
   "source": [
    "Execution Start time: 22:14:37, End time: 22:14:42 (duration: 4 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244c27c9",
   "metadata": {},
   "source": [
    "**Thought**: The required packages have been installed successfully. Now, I will proceed to configure the training script to use the `roberta-base` model, set the number of epochs to 1, and configure the other hyperparameters as specified in the query. I will then execute the training script and report the evaluation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f2d7e7",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa646f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "usage: run.py [-h] --model_name_or_path MODEL_NAME_OR_PATH\n              [--config_name CONFIG_NAME] [--tokenizer_name TOKENIZER_NAME]\n              [--cache_dir CACHE_DIR]\n              [--use_fast_tokenizer [USE_FAST_TOKENIZER]]\n              [--no_use_fast_tokenizer] [--model_revision MODEL_REVISION]\n              [--use_auth_token [USE_AUTH_TOKEN]] [--use_pe [USE_PE]]\n              [--head_prefix [HEAD_PREFIX]] [--prompt_prefix [PROMPT_PREFIX]]\n              [--head_only [HEAD_ONLY]] [--prompt_only [PROMPT_ONLY]]\n              [--head_ptuning [HEAD_PTUNING]]\n              [--prompt_ptuning [PROMPT_PTUNING]]\n              [--head_adapter [HEAD_ADAPTER]]\n              [--prompt_adapter [PROMPT_ADAPTER]]\n              [--adapter_choice ADAPTER_CHOICE] [--adapter_dim ADAPTER_DIM]\n              [--pre_seq_len PRE_SEQ_LEN]\n              [--prefix_projection [PREFIX_PROJECTION]]\n              [--prefix_hidden_size PREFIX_HIDDEN_SIZE]\n              [--hidden_dropout_prob HIDDEN_DROPOUT_PROB] --task_name\n              {glue,superglue,ner,srl,qa,other_cls} --dataset_name\n              {cola,mnli,mrpc,qnli,qqp,rte,sst2,stsb,wnli,boolq,cb,rte,wic,wsc,copa,record,multirc,conll2003,conll2004,ontonotes,conll2005,conll2012,squad,squad_v2,movie_rationales,cr,snli,trec,ag_news,yelp_polarity}\n              [--num_examples_per_label NUM_EXAMPLES_PER_LABEL]\n              [--dataset_config_name DATASET_CONFIG_NAME]\n              [--max_seq_length MAX_SEQ_LENGTH]\n              [--overwrite_cache [OVERWRITE_CACHE]]\n              [--pad_to_max_length [PAD_TO_MAX_LENGTH]]\n              [--no_pad_to_max_length] [--max_train_samples MAX_TRAIN_SAMPLES]\n              [--max_eval_samples MAX_EVAL_SAMPLES]\n              [--max_predict_samples MAX_PREDICT_SAMPLES]\n              [--train_file TRAIN_FILE] [--validation_file VALIDATION_FILE]\n              [--test_file TEST_FILE] [--template_id TEMPLATE_ID] --output_dir\n              OUTPUT_DIR [--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]]\n              [--do_train [DO_TRAIN]] [--do_eval [DO_EVAL]]\n              [--do_predict [DO_PREDICT]]\n              [--evaluation_strategy {no,steps,epoch}]\n              [--prediction_loss_only [PREDICTION_LOSS_ONLY]]\n              [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\n              [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\n              [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n              [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n              [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n              [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]\n              [--eval_delay EVAL_DELAY] [--learning_rate LEARNING_RATE]\n              [--weight_decay WEIGHT_DECAY] [--adam_beta1 ADAM_BETA1]\n              [--adam_beta2 ADAM_BETA2] [--adam_epsilon ADAM_EPSILON]\n              [--max_grad_norm MAX_GRAD_NORM]\n              [--num_train_epochs NUM_TRAIN_EPOCHS] [--max_steps MAX_STEPS]\n              [--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup,inverse_sqrt,reduce_lr_on_plateau,cosine_with_min_lr}]\n              [--lr_scheduler_kwargs LR_SCHEDULER_KWARGS]\n              [--warmup_ratio WARMUP_RATIO] [--warmup_steps WARMUP_STEPS]\n              [--log_level {detail,debug,info,warning,error,critical,passive}]\n              [--log_level_replica {detail,debug,info,warning,error,critical,passive}]\n              [--log_on_each_node [LOG_ON_EACH_NODE]] [--no_log_on_each_node]\n              [--logging_dir LOGGING_DIR]\n              [--logging_strategy {no,steps,epoch}]\n              [--logging_first_step [LOGGING_FIRST_STEP]]\n              [--logging_steps LOGGING_STEPS]\n              [--logging_nan_inf_filter [LOGGING_NAN_INF_FILTER]]\n              [--no_logging_nan_inf_filter] [--save_strategy {no,steps,epoch}]\n              [--save_steps SAVE_STEPS] [--save_total_limit SAVE_TOTAL_LIMIT]\n              [--save_safetensors [SAVE_SAFETENSORS]] [--no_save_safetensors]\n              [--save_on_each_node [SAVE_ON_EACH_NODE]]\n              [--save_only_model [SAVE_ONLY_MODEL]] [--no_cuda [NO_CUDA]]\n              [--use_cpu [USE_CPU]] [--use_mps_device [USE_MPS_DEVICE]]\n              [--seed SEED] [--data_seed DATA_SEED]\n              [--jit_mode_eval [JIT_MODE_EVAL]] [--use_ipex [USE_IPEX]]\n              [--bf16 [BF16]] [--fp16 [FP16]]\n              [--fp16_opt_level FP16_OPT_LEVEL]\n              [--half_precision_backend {auto,apex,cpu_amp}]\n              [--bf16_full_eval [BF16_FULL_EVAL]]\n              [--fp16_full_eval [FP16_FULL_EVAL]] [--tf32 TF32]\n              [--local_rank LOCAL_RANK]\n              [--ddp_backend {nccl,gloo,mpi,ccl,hccl,cncl}]\n              [--tpu_num_cores TPU_NUM_CORES]\n              [--tpu_metrics_debug [TPU_METRICS_DEBUG]]\n              [--debug DEBUG [DEBUG ...]]\n              [--dataloader_drop_last [DATALOADER_DROP_LAST]]\n              [--eval_steps EVAL_STEPS]\n              [--dataloader_num_workers DATALOADER_NUM_WORKERS]\n              [--dataloader_prefetch_factor DATALOADER_PREFETCH_FACTOR]\n              [--past_index PAST_INDEX] [--run_name RUN_NAME]\n              [--disable_tqdm DISABLE_TQDM]\n              [--remove_unused_columns [REMOVE_UNUSED_COLUMNS]]\n              [--no_remove_unused_columns]\n              [--label_names LABEL_NAMES [LABEL_NAMES ...]]\n              [--load_best_model_at_end [LOAD_BEST_MODEL_AT_END]]\n              [--metric_for_best_model METRIC_FOR_BEST_MODEL]\n              [--greater_is_better GREATER_IS_BETTER]\n              [--ignore_data_skip [IGNORE_DATA_SKIP]] [--fsdp FSDP]\n              [--fsdp_min_num_params FSDP_MIN_NUM_PARAMS]\n              [--fsdp_config FSDP_CONFIG]\n              [--fsdp_transformer_layer_cls_to_wrap FSDP_TRANSFORMER_LAYER_CLS_TO_WRAP]\n              [--accelerator_config ACCELERATOR_CONFIG]\n              [--deepspeed DEEPSPEED]\n              [--label_smoothing_factor LABEL_SMOOTHING_FACTOR]\n              [--optim {adamw_hf,adamw_torch,adamw_torch_fused,adamw_torch_xla,adamw_torch_npu_fused,adamw_apex_fused,adafactor,adamw_anyprecision,sgd,adagrad,adamw_bnb_8bit,adamw_8bit,lion_8bit,lion_32bit,paged_adamw_32bit,paged_adamw_8bit,paged_lion_32bit,paged_lion_8bit,rmsprop,rmsprop_bnb,rmsprop_bnb_8bit,rmsprop_bnb_32bit,galore_adamw,galore_adamw_8bit,galore_adafactor,galore_adamw_layerwise,galore_adamw_8bit_layerwise,galore_adafactor_layerwise}]\n              [--optim_args OPTIM_ARGS] [--adafactor [ADAFACTOR]]\n              [--group_by_length [GROUP_BY_LENGTH]]\n              [--length_column_name LENGTH_COLUMN_NAME]\n              [--report_to REPORT_TO]\n              [--ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS]\n              [--ddp_bucket_cap_mb DDP_BUCKET_CAP_MB]\n              [--ddp_broadcast_buffers DDP_BROADCAST_BUFFERS]\n              [--dataloader_pin_memory [DATALOADER_PIN_MEMORY]]\n              [--no_dataloader_pin_memory]\n              [--dataloader_persistent_workers [DATALOADER_PERSISTENT_WORKERS]]\n              [--skip_memory_metrics [SKIP_MEMORY_METRICS]]\n              [--no_skip_memory_metrics]\n              [--use_legacy_prediction_loop [USE_LEGACY_PREDICTION_LOOP]]\n              [--push_to_hub [PUSH_TO_HUB]]\n              [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n              [--hub_model_id HUB_MODEL_ID]\n              [--hub_strategy {end,every_save,checkpoint,all_checkpoints}]\n              [--hub_token HUB_TOKEN] [--hub_private_repo [HUB_PRIVATE_REPO]]\n              [--hub_always_push [HUB_ALWAYS_PUSH]]\n              [--gradient_checkpointing [GRADIENT_CHECKPOINTING]]\n              [--gradient_checkpointing_kwargs GRADIENT_CHECKPOINTING_KWARGS]\n              [--include_inputs_for_metrics [INCLUDE_INPUTS_FOR_METRICS]]\n              [--eval_do_concat_batches [EVAL_DO_CONCAT_BATCHES]]\n              [--no_eval_do_concat_batches]\n              [--fp16_backend {auto,apex,cpu_amp}]\n              [--push_to_hub_model_id PUSH_TO_HUB_MODEL_ID]\n              [--push_to_hub_organization PUSH_TO_HUB_ORGANIZATION]\n              [--push_to_hub_token PUSH_TO_HUB_TOKEN]\n              [--mp_parameters MP_PARAMETERS]\n              [--auto_find_batch_size [AUTO_FIND_BATCH_SIZE]]\n              [--full_determinism [FULL_DETERMINISM]]\n              [--torchdynamo TORCHDYNAMO] [--ray_scope RAY_SCOPE]\n              [--ddp_timeout DDP_TIMEOUT] [--torch_compile [TORCH_COMPILE]]\n              [--torch_compile_backend TORCH_COMPILE_BACKEND]\n              [--torch_compile_mode TORCH_COMPILE_MODE]\n              [--dispatch_batches DISPATCH_BATCHES]\n              [--split_batches SPLIT_BATCHES]\n              [--include_tokens_per_second [INCLUDE_TOKENS_PER_SECOND]]\n              [--include_num_input_tokens_seen [INCLUDE_NUM_INPUT_TOKENS_SEEN]]\n              [--neftune_noise_alpha NEFTUNE_NOISE_ALPHA]\n              [--optim_target_modules OPTIM_TARGET_MODULES]\n              [--use_semi [USE_SEMI]]\n              [--unlabeled_data_num UNLABELED_DATA_NUM]\n              [--unlabeled_data_batch_size UNLABELED_DATA_BATCH_SIZE]\n              [--pseudo_sample_num_or_ratio PSEUDO_SAMPLE_NUM_OR_RATIO]\n              [--teacher_training_epoch TEACHER_TRAINING_EPOCH]\n              [--teacher_tuning_epoch TEACHER_TUNING_EPOCH]\n              [--student_training_epoch STUDENT_TRAINING_EPOCH]\n              [--student_learning_rate STUDENT_LEARNING_RATE]\n              [--self_training_epoch SELF_TRAINING_EPOCH]\n              [--post_student_train [POST_STUDENT_TRAIN]]\n              [--student_pre_seq_len STUDENT_PRE_SEQ_LEN]\n              [--n_best_size N_BEST_SIZE]\n              [--max_answer_length MAX_ANSWER_LENGTH]\n              [--version_2_with_negative [VERSION_2_WITH_NEGATIVE]]\n              [--null_score_diff_threshold NULL_SCORE_DIFF_THRESHOLD]\nrun.py: error: argument --task_name: invalid choice: 'rte' (choose from 'glue', 'superglue', 'ner', 'srl', 'qa', 'other_cls')\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python UPET/run.py --model_name_or_path roberta-base --task_name rte --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 8 --learning_rate 2e-5 --num_train_epochs 1 --output_dir /tmp/rte/ --overwrite_output_dir --seed 42 --examples_per_label 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57529b76",
   "metadata": {},
   "source": [
    "### Step 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603decd2",
   "metadata": {},
   "source": [
    "Execution Start time: 22:14:46, End time: 22:14:50 (duration: 4 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6466df5",
   "metadata": {},
   "source": [
    "**Thought**: It seems there is an issue with the `--task_name` argument. The script expects a task name from a predefined set of choices, and 'rte' is not directly recognized as a valid choice. I need to specify the task as part of the `superglue` tasks, which includes 'rte'. I will adjust the command accordingly and rerun the script. Additionally, I will ensure that the `--dataset_name` argument is set to 'rte' to specify the dataset correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca26c8ee",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a43920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Traceback (most recent call last):\n  File \"/content/UPET/run.py\", line 71, in <module>\n    args = get_args()\n  File \"/content/UPET/arguments.py\", line 326, in get_args\n    args = parser.parse_args_into_dataclasses()\n  File \"/usr/local/lib/python3.10/site-packages/transformers/hf_argparser.py\", line 339, in parse_args_into_dataclasses\n    obj = dtype(**inputs)\n  File \"<string>\", line 125, in __init__\n  File \"/usr/local/lib/python3.10/site-packages/transformers/training_args.py\", line 1605, in __post_init__\n    and (self.device.type != \"cuda\")\n  File \"/usr/local/lib/python3.10/site-packages/transformers/training_args.py\", line 2094, in device\n    return self._setup_devices\n  File \"/usr/local/lib/python3.10/site-packages/transformers/utils/generic.py\", line 63, in __get__\n    cached = self.fget(obj)\n  File \"/usr/local/lib/python3.10/site-packages/transformers/training_args.py\", line 2000, in _setup_devices\n    raise ImportError(\nImportError: Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python UPET/run.py --model_name_or_path roberta-base --task_name superglue --dataset_name rte --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 8 --learning_rate 2e-5 --num_train_epochs 1 --output_dir /tmp/rte/ --overwrite_output_dir --seed 42 --num_examples_per_label 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a783cf",
   "metadata": {},
   "source": [
    "### Step 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abff1c3",
   "metadata": {},
   "source": [
    "Execution Start time: 22:14:52, End time: 22:14:55 (duration: 3 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e0b861",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the `accelerate` package is required for using the `Trainer` with `PyTorch`. I need to install the `accelerate` package to resolve this issue. After installing the package, I will rerun the training script with the correct task and dataset configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e786c9e",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94047683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting accelerate\n  Downloading accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from accelerate) (24.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/site-packages (from accelerate) (5.9.8)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/site-packages (from accelerate) (2.0.0)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/site-packages (from accelerate) (0.23.3)\nRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/site-packages (from accelerate) (0.4.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2023.6.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\nRequirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.7.101)\nRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.5.0.96)\nRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.10.3.66)\nRequirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.9.0.58)\nRequirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.2.10.91)\nRequirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.0.1)\nRequirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.7.4.91)\nRequirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.14.3)\nRequirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.7.91)\nRequirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.0.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (68.1.2)\nRequirement already satisfied: wheel in /usr/local/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (0.44.0)\nRequirement already satisfied: cmake in /usr/local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.9)\nRequirement already satisfied: lit in /usr/local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (18.1.8)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.7)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.6.2)\nRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading accelerate-0.33.0-py3-none-any.whl (315 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/315.1 kB ? eta -:--:--\n\n\u001b[?25hInstalling collected packages: accelerate\nSuccessfully installed accelerate-0.33.0\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950c9fed",
   "metadata": {},
   "source": [
    "### Step 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0891a0",
   "metadata": {},
   "source": [
    "Execution Start time: 22:14:58, End time: 22:15:03 (duration: 4 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30561a4b",
   "metadata": {},
   "source": [
    "**Thought**: The `accelerate` package has been installed successfully. Now, I will rerun the training script with the correct task and dataset configuration to train the `roberta-base` model on the RTE dataset using the UPET method. After the training completes, I will report the evaluation accuracy as requested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d0f705",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1494c0c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n08/19/2024 22:15:02 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0distributed training: True, 16-bits training: False\nTraceback (most recent call last):\n  File \"/content/UPET/run.py\", line 101, in <module>\n    from tasks.superglue.get_trainer import get_trainer\n  File \"/content/UPET/tasks/superglue/get_trainer.py\", line 14, in <module>\n    from training.trainer_exp import ExponentialTrainer\n  File \"/content/UPET/training/trainer_exp.py\", line 16, in <module>\n    from transformers.trainer_utils import (\nImportError: cannot import name 'ShardedDDPOption' from 'transformers.trainer_utils' (/usr/local/lib/python3.10/site-packages/transformers/trainer_utils.py)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python UPET/run.py --model_name_or_path roberta-base --task_name superglue --dataset_name rte --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 8 --learning_rate 2e-5 --num_train_epochs 1 --output_dir /tmp/rte/ --overwrite_output_dir --seed 42 --num_examples_per_label 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372f9f92",
   "metadata": {},
   "source": [
    "### Step 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c64ed5",
   "metadata": {},
   "source": [
    "Execution Start time: 22:15:04, End time: 22:15:18 (duration: 13 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6e4d27",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that there is an import issue with the `ShardedDDPOption` from the `transformers` library. This might be due to a version mismatch between the `transformers` library and the code in the repository. I will try updating the `transformers` library to the latest version to see if that resolves the issue. After updating, I will rerun the training script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc39ab9",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eadcac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Requirement already satisfied: transformers in /usr/local/lib/python3.10/site-packages (4.40.2)\nCollecting transformers\n  Downloading transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\n\u001b[?25l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/43.7 kB ? eta -:--:--\n\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/site-packages (from transformers) (0.23.3)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (2.0.7)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\nDownloading transformers-4.44.0-py3-none-any.whl (9.5 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/9.5 MB ? eta -:--:--\n\n\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.40.2\n    Uninstalling transformers-4.40.2:\n      Successfully uninstalled transformers-4.40.2\nSuccessfully installed transformers-4.44.0\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install transformers --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d691470b",
   "metadata": {},
   "source": [
    "### Step 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb083728",
   "metadata": {},
   "source": [
    "Execution Start time: 22:15:21, End time: 22:15:25 (duration: 4 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f421bc2e",
   "metadata": {},
   "source": [
    "**Thought**: The `transformers` library has been updated successfully. Now, I will rerun the training script to train the `roberta-base` model on the RTE dataset using the UPET method. After the training completes, I will report the evaluation accuracy as requested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9433ad8",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415da578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n08/19/2024 22:15:24 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0distributed training: True, 16-bits training: False\nTraceback (most recent call last):\n  File \"/content/UPET/run.py\", line 101, in <module>\n    from tasks.superglue.get_trainer import get_trainer\n  File \"/content/UPET/tasks/superglue/get_trainer.py\", line 14, in <module>\n    from training.trainer_exp import ExponentialTrainer\n  File \"/content/UPET/training/trainer_exp.py\", line 16, in <module>\n    from transformers.trainer_utils import (\nImportError: cannot import name 'ShardedDDPOption' from 'transformers.trainer_utils' (/usr/local/lib/python3.10/site-packages/transformers/trainer_utils.py)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python UPET/run.py --model_name_or_path roberta-base --task_name superglue --dataset_name rte --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 8 --learning_rate 2e-5 --num_train_epochs 1 --output_dir /tmp/rte/ --overwrite_output_dir --seed 42 --num_examples_per_label 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c912b2",
   "metadata": {},
   "source": [
    "### Step 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082ab900",
   "metadata": {},
   "source": [
    "Execution Start time: 22:15:27, End time: 22:15:28 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b23aa1",
   "metadata": {},
   "source": [
    "**Thought**: The import error persists even after updating the `transformers` library. This suggests that the code in the repository might be incompatible with the latest versions of the `transformers` library. To resolve this, I will check the repository for any specific version requirements for the `transformers` library or any known issues related to this import error. If no specific version is mentioned, I will attempt to modify the code to remove or replace the problematic import. Let's first check if there are any specific version requirements or known issues in the repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cd33c3",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8427a921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets==1.15.1\nnumpy==1.19.2\ntqdm==4.62.3\ntransformers==4.11.3\nseqeval==1.2.2\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!cat UPET/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4760383",
   "metadata": {},
   "source": [
    "### Step 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86e9372",
   "metadata": {},
   "source": [
    "Execution Start time: 22:15:30, End time: 22:15:37 (duration: 7 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7b63dc",
   "metadata": {},
   "source": [
    "**Thought**: The `requirements.txt` file specifies `transformers==4.11.3`, which is an older version than the one currently installed. The import error might be due to changes in the `transformers` library between versions. I will downgrade the `transformers` library to version 4.11.3 as specified in the `requirements.txt` file and then rerun the training script. This should resolve the import error and allow the script to run successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2607be33",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10507b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting transformers==4.11.3\n  Downloading transformers-4.11.3-py3-none-any.whl.metadata (53 kB)\n\u001b[?25l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/53.7 kB ? eta -:--:--\n\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (3.15.1)\nRequirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (0.23.3)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (2023.12.25)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (2.32.3)\nCollecting sacremoses (from transformers==4.11.3)\n  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\nCollecting tokenizers<0.11,>=0.10.1 (from transformers==4.11.3)\n  Downloading tokenizers-0.10.3.tar.gz (212 kB)\n\u001b[?25l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/212.7 kB ? eta -:--:--\n\n\u001b[?25h  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\n\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.0.17->transformers==4.11.3) (2023.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.0.17->transformers==4.11.3) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.11.3) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.11.3) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.11.3) (2.0.7)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.11.3) (2024.6.2)\nRequirement already satisfied: click in /usr/local/lib/python3.10/site-packages (from sacremoses->transformers==4.11.3) (8.1.7)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/site-packages (from sacremoses->transformers==4.11.3) (1.4.2)\nDownloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/2.9 MB ? eta -:--:--\n\n\n\n\n\n\n\n\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/897.5 kB ? eta -:--:--\n\n\u001b[?25hBuilding wheels for collected packages: tokenizers\n  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l-\b \berror\n  error: subprocess-exited-with-error\n  \n  × Building wheel for tokenizers (pyproject.toml) did not run successfully.\n  │ exit code: 1\n  ╰─> [51 lines of output]\n      running bdist_wheel\n      running build\n      running build_py\n      creating build\n      creating build/lib.linux-x86_64-cpython-310\n      creating build/lib.linux-x86_64-cpython-310/tokenizers\n      copying py_src/tokenizers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/models\n      copying py_src/tokenizers/models/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/models\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/decoders\n      copying py_src/tokenizers/decoders/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/decoders\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/normalizers\n      copying py_src/tokenizers/normalizers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/normalizers\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/pre_tokenizers\n      copying py_src/tokenizers/pre_tokenizers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/pre_tokenizers\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/processors\n      copying py_src/tokenizers/processors/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/processors\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/trainers\n      copying py_src/tokenizers/trainers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/trainers\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/base_tokenizer.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/bert_wordpiece.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/byte_level_bpe.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/char_level_bpe.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/sentencepiece_bpe.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/sentencepiece_unigram.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/tools\n      copying py_src/tokenizers/tools/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/tools\n      copying py_src/tokenizers/tools/visualizer.py -> build/lib.linux-x86_64-cpython-310/tokenizers/tools\n      copying py_src/tokenizers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers\n      copying py_src/tokenizers/models/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/models\n      copying py_src/tokenizers/decoders/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/decoders\n      copying py_src/tokenizers/normalizers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/normalizers\n      copying py_src/tokenizers/pre_tokenizers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/pre_tokenizers\n      copying py_src/tokenizers/processors/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/processors\n      copying py_src/tokenizers/trainers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/trainers\n      copying py_src/tokenizers/tools/visualizer-styles.css -> build/lib.linux-x86_64-cpython-310/tokenizers/tools\n      running build_ext\n      running build_rust\n      error: can't find Rust compiler\n      \n      If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n      \n      To update pip, run:\n      \n          pip install --upgrade pip\n      \n      and then retry package installation.\n      \n      If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\n  ERROR: Failed building wheel for tokenizers\n\u001b[?25hFailed to build tokenizers\nERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install transformers==4.11.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c9740d",
   "metadata": {},
   "source": [
    "### Step 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd06a302",
   "metadata": {},
   "source": [
    "Execution Start time: 22:15:39, End time: 22:15:52 (duration: 12 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f585ccb",
   "metadata": {},
   "source": [
    "**Thought**: The installation of the `tokenizers` package failed because it requires a Rust compiler to build from source. To resolve this, I will install the Rust compiler using `rustup`, which is the recommended way to download and update the Rust compiler toolchain. After installing Rust, I will attempt to install the `transformers` library again with the specified version. This should resolve the issue and allow the script to run successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af20042",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fea4889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "info: downloading installer\ninfo: profile set to 'default'\ninfo: default host triple is x86_64-unknown-linux-gnu\ninfo: syncing channel updates for 'stable-x86_64-unknown-linux-gnu'\ninfo: latest update on 2024-08-08, rust version 1.80.1 (3f5fd8dd4 2024-08-06)\ninfo: downloading component 'cargo'\ninfo: downloading component 'clippy'\ninfo: downloading component 'rust-docs'\ninfo: downloading component 'rust-std'\ninfo: downloading component 'rustc'\ninfo: downloading component 'rustfmt'\ninfo: installing component 'cargo'\ninfo: installing component 'clippy'\ninfo: installing component 'rust-docs'\n\n  4.8 MiB /  15.8 MiB ( 30 %)   0 B/s in  1s ETA: Unknown\n                                                         \n  5.5 MiB /  15.8 MiB ( 35 %)   4.8 MiB/s in  2s ETA:  2s\n                                                         \n  8.9 MiB /  15.8 MiB ( 56 %)   2.7 MiB/s in  3s ETA:  2s\n                                                         \n 15.8 MiB /  15.8 MiB (100 %)   3.0 MiB/s in  3s ETA:  0s\ninfo: installing component 'rust-std'\n\n 14.3 MiB /  26.7 MiB ( 54 %)   0 B/s in  1s ETA: Unknown\n                                                         \n 26.7 MiB /  26.7 MiB (100 %)  14.3 MiB/s in  1s ETA:  0s\ninfo: installing component 'rustc'\n\n 15.1 MiB /  65.0 MiB ( 23 %)   0 B/s in  1s ETA: Unknown\n                                                         \n 30.6 MiB /  65.0 MiB ( 47 %)  15.1 MiB/s in  2s ETA:  2s\n                                                         \n 45.1 MiB /  65.0 MiB ( 69 %)  15.3 MiB/s in  3s ETA:  1s\n                                                         \n 60.1 MiB /  65.0 MiB ( 92 %)  15.0 MiB/s in  4s ETA:  0s\n                                                         \n 65.0 MiB /  65.0 MiB (100 %)  15.0 MiB/s in  4s ETA:  0s\ninfo: installing component 'rustfmt'\ninfo: default toolchain set to 'stable-x86_64-unknown-linux-gnu'\n\n  stable-x86_64-unknown-linux-gnu installed - rustc 1.80.1 (3f5fd8dd4 2024-08-06)\n\n\nRust is installed now. Great!\n\nTo get started you may need to restart your current shell.\nThis would reload your PATH environment variable to include\nCargo's bin directory ($HOME/.cargo/bin).\n\nTo configure your current shell, you need to source\nthe corresponding env file under $HOME/.cargo.\n\nThis is usually done by running one of the following (note the leading DOT):\n. \"$HOME/.cargo/env\"            # For sh/bash/zsh/ash/dash/pdksh\nsource \"$HOME/.cargo/env.fish\"  # For fish\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08e348a",
   "metadata": {},
   "source": [
    "### Step 19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a15cb84",
   "metadata": {},
   "source": [
    "Execution Start time: 22:15:56, End time: 22:15:56 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c293d65e",
   "metadata": {},
   "source": [
    "**Thought**: The Rust compiler has been installed successfully. I will now configure the current shell to include Cargo's bin directory in the PATH environment variable. After that, I will attempt to install the `transformers` library again with the specified version (4.11.3). This should resolve the issue with the `tokenizers` package and allow the script to run successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50137155",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75877624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/usr/bin/sh: 1: source: not found\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!source $HOME/.cargo/env && pip install transformers==4.11.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36044233",
   "metadata": {},
   "source": [
    "### Step 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace260e7",
   "metadata": {},
   "source": [
    "Execution Start time: 22:15:59, End time: 22:16:48 (duration: 49 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e8abc7",
   "metadata": {},
   "source": [
    "**Thought**: The `source` command is a shell built-in and cannot be executed directly in a non-interactive shell environment like this one. Instead, I will use the `.` command to source the environment file. After that, I will attempt to install the `transformers` library again with the specified version (4.11.3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c108ac5",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc5aefd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting transformers==4.11.3\n  Downloading transformers-4.11.3-py3-none-any.whl.metadata (53 kB)\n\u001b[?25l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/53.7 kB ? eta -:--:--\n\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (3.15.1)\nRequirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (0.23.3)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (2023.12.25)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (2.32.3)\nCollecting sacremoses (from transformers==4.11.3)\n  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\nCollecting tokenizers<0.11,>=0.10.1 (from transformers==4.11.3)\n  Downloading tokenizers-0.10.3.tar.gz (212 kB)\n\u001b[?25l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/212.7 kB ? eta -:--:--\n\n\u001b[?25h  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\n\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.0.17->transformers==4.11.3) (2023.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.0.17->transformers==4.11.3) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.11.3) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.11.3) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.11.3) (2.0.7)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.11.3) (2024.6.2)\nRequirement already satisfied: click in /usr/local/lib/python3.10/site-packages (from sacremoses->transformers==4.11.3) (8.1.7)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/site-packages (from sacremoses->transformers==4.11.3) (1.4.2)\nDownloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/2.9 MB ? eta -:--:--\n\n\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/897.5 kB ? eta -:--:--\n\n\u001b[?25hBuilding wheels for collected packages: tokenizers\n  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \berror\n  error: subprocess-exited-with-error\n  \n  × Building wheel for tokenizers (pyproject.toml) did not run successfully.\n  │ exit code: 1\n  ╰─> [550 lines of output]\n      running bdist_wheel\n      running build\n      running build_py\n      creating build\n      creating build/lib.linux-x86_64-cpython-310\n      creating build/lib.linux-x86_64-cpython-310/tokenizers\n      copying py_src/tokenizers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/models\n      copying py_src/tokenizers/models/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/models\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/decoders\n      copying py_src/tokenizers/decoders/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/decoders\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/normalizers\n      copying py_src/tokenizers/normalizers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/normalizers\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/pre_tokenizers\n      copying py_src/tokenizers/pre_tokenizers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/pre_tokenizers\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/processors\n      copying py_src/tokenizers/processors/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/processors\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/trainers\n      copying py_src/tokenizers/trainers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/trainers\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/base_tokenizer.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/bert_wordpiece.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/byte_level_bpe.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/char_level_bpe.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/sentencepiece_bpe.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/sentencepiece_unigram.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/tools\n      copying py_src/tokenizers/tools/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/tools\n      copying py_src/tokenizers/tools/visualizer.py -> build/lib.linux-x86_64-cpython-310/tokenizers/tools\n      copying py_src/tokenizers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers\n      copying py_src/tokenizers/models/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/models\n      copying py_src/tokenizers/decoders/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/decoders\n      copying py_src/tokenizers/normalizers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/normalizers\n      copying py_src/tokenizers/pre_tokenizers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/pre_tokenizers\n      copying py_src/tokenizers/processors/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/processors\n      copying py_src/tokenizers/trainers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/trainers\n      copying py_src/tokenizers/tools/visualizer-styles.css -> build/lib.linux-x86_64-cpython-310/tokenizers/tools\n      running build_ext\n      running build_rust\n          Updating crates.io index\n          Updating git repository `https://github.com/pyo3/rust-numpy/`\n          Updating git repository `https://github.com/n1t0/rayon-cond`\n           Locking 135 packages to latest compatible versions\n            Adding base64 v0.13.1 (latest: v0.22.1)\n            Adding bitflags v1.3.2 (latest: v2.6.0)\n            Adding cfg-if v0.1.10 (latest: v1.0.0)\n            Adding clap v2.34.0 (latest: v4.5.16)\n            Adding ctor v0.1.26 (latest: v0.2.8)\n            Adding darling v0.10.2 (latest: v0.20.10)\n            Adding darling_core v0.10.2 (latest: v0.20.10)\n            Adding darling_macro v0.10.2 (latest: v0.20.10)\n            Adding derive_builder v0.9.0 (latest: v0.20.0)\n            Adding derive_builder_core v0.9.0 (latest: v0.20.0)\n            Adding encode_unicode v0.3.6 (latest: v1.0.0)\n            Adding env_logger v0.7.1 (latest: v0.11.5)\n            Adding getrandom v0.1.16 (latest: v0.2.15)\n            Adding hermit-abi v0.1.19 (latest: v0.4.0)\n            Adding humantime v1.3.0 (latest: v2.1.0)\n            Adding indicatif v0.15.0 (latest: v0.17.8)\n            Adding indoc v0.3.6 (latest: v2.0.5)\n            Adding indoc-impl v0.3.6 (latest: v0.3.7)\n            Adding inventory v0.1.11 (latest: v0.3.15)\n            Adding itertools v0.8.2 (latest: v0.13.0)\n            Adding itertools v0.9.0 (latest: v0.13.0)\n            Adding linux-raw-sys v0.4.14 (latest: v0.6.5)\n            Adding matrixmultiply v0.2.4 (latest: v0.3.9)\n            Adding ndarray v0.13.1 (latest: v0.16.1)\n            Adding num-complex v0.2.4 (latest: v0.4.6)\n            Adding number_prefix v0.3.0 (latest: v0.4.0)\n            Adding parking_lot v0.11.2 (latest: v0.12.3)\n            Adding parking_lot_core v0.8.6 (latest: v0.9.10)\n            Adding paste v0.1.18 (latest: v1.0.15)\n            Adding pyo3 v0.12.4 (latest: v0.22.2)\n            Adding pyo3-derive-backend v0.12.4 (latest: v0.12.5)\n            Adding pyo3cls v0.12.4 (latest: v0.12.5)\n            Adding quick-error v1.2.3 (latest: v2.0.1)\n            Adding rand v0.7.3 (latest: v0.8.5)\n            Adding rand_chacha v0.2.2 (latest: v0.3.1)\n            Adding rand_core v0.5.1 (latest: v0.6.4)\n            Adding rand_hc v0.2.0 (latest: v0.3.2)\n            Adding redox_syscall v0.2.16 (latest: v0.5.3)\n            Adding regex-syntax v0.6.29 (latest: v0.8.4)\n            Adding strsim v0.8.0 (latest: v0.11.1)\n            Adding strsim v0.9.3 (latest: v0.11.1)\n            Adding syn v1.0.109 (latest: v2.0.75)\n            Adding textwrap v0.11.0 (latest: v0.16.1)\n            Adding unindent v0.1.11 (latest: v0.2.3)\n            Adding wasi v0.9.0+wasi-snapshot-preview1 (latest: v0.13.2+wasi-0.2.1)\n            Adding windows-sys v0.52.0 (latest: v0.59.0)\n       Downloading crates ...\n        Downloaded atty v0.2.14\n        Downloaded unindent v0.1.11\n        Downloaded shlex v1.3.0\n        Downloaded windows-targets v0.52.6\n        Downloaded regex-syntax v0.6.29\n        Downloaded spm_precompiled v0.1.4\n        Downloaded libc v0.2.158\n        Downloaded onig_sys v69.8.1\n        Downloaded zerocopy-derive v0.7.35\n        Downloaded windows_x86_64_gnullvm v0.52.6\n        Downloaded windows_i686_gnullvm v0.52.6\n        Downloaded unicode-width v0.1.13\n        Downloaded windows_x86_64_msvc v0.52.6\n        Downloaded windows_x86_64_gnu v0.52.6\n        Downloaded winapi v0.3.9\n        Downloaded linux-raw-sys v0.4.14\n        Downloaded windows_i686_msvc v0.52.6\n        Downloaded windows_i686_gnu v0.52.6\n        Downloaded windows_aarch64_msvc v0.52.6\n        Downloaded windows_aarch64_gnullvm v0.52.6\n        Downloaded windows-sys v0.59.0\n        Downloaded windows-sys v0.52.0\n        Downloaded syn v2.0.75\n        Downloaded zerocopy v0.7.35\n        Downloaded winapi-x86_64-pc-windows-gnu v0.4.0\n        Downloaded winapi-i686-pc-windows-gnu v0.4.0\n        Downloaded winapi-util v0.1.9\n        Downloaded wasi v0.9.0+wasi-snapshot-preview1\n        Downloaded vec_map v0.8.2\n        Downloaded unicode_categories v0.1.1\n        Downloaded unicode-segmentation v1.11.0\n        Downloaded unicode-ident v1.0.12\n        Downloaded syn v1.0.109\n        Downloaded rustix v0.38.34\n        Downloaded regex-automata v0.4.7\n        Downloaded regex v1.10.6\n        Downloaded ndarray v0.13.1\n        Downloaded inventory v0.1.11\n        Downloaded textwrap v0.11.0\n        Downloaded termcolor v1.4.1\n        Downloaded tempfile v3.12.0\n        Downloaded strsim v0.9.3\n        Downloaded strsim v0.8.0\n        Downloaded smallvec v1.13.2\n        Downloaded serde_json v1.0.125\n        Downloaded serde_derive v1.0.208\n        Downloaded serde v1.0.208\n        Downloaded scopeguard v1.2.0\n        Downloaded ryu v1.0.18\n        Downloaded regex-syntax v0.8.4\n        Downloaded redox_syscall v0.2.16\n        Downloaded rayon-core v1.12.1\n        Downloaded rayon v1.10.0\n        Downloaded rand_hc v0.2.0\n        Downloaded rand_core v0.5.1\n        Downloaded rand_chacha v0.2.2\n        Downloaded rand v0.7.3\n        Downloaded quote v1.0.36\n        Downloaded quick-error v1.2.3\n        Downloaded pyo3cls v0.12.4\n        Downloaded pyo3-derive-backend v0.12.4\n        Downloaded pyo3 v0.12.4\n        Downloaded proc-macro2 v1.0.86\n        Downloaded proc-macro-hack v0.5.20+deprecated\n        Downloaded ppv-lite86 v0.2.20\n        Downloaded portable-atomic-util v0.2.2\n        Downloaded portable-atomic v1.7.0\n        Downloaded pkg-config v0.3.30\n        Downloaded paste-impl v0.1.18\n        Downloaded paste v0.1.18\n        Downloaded parking_lot_core v0.8.6\n        Downloaded parking_lot v0.11.2\n        Downloaded onig v6.4.0\n        Downloaded once_cell v1.19.0\n        Downloaded number_prefix v0.3.0\n        Downloaded num-traits v0.2.19\n        Downloaded num-integer v0.1.46\n        Downloaded num-complex v0.4.6\n        Downloaded num-complex v0.2.4\n        Downloaded nom v7.1.3\n        Downloaded ndarray v0.16.1\n        Downloaded minimal-lexical v0.2.1\n        Downloaded memchr v2.7.4\n        Downloaded matrixmultiply v0.3.9\n        Downloaded matrixmultiply v0.2.4\n        Downloaded log v0.4.22\n        Downloaded lock_api v0.4.12\n        Downloaded lazy_static v1.5.0\n        Downloaded itoa v1.0.11\n        Downloaded itertools v0.9.0\n        Downloaded itertools v0.8.2\n        Downloaded esaxx-rs v0.1.10\n        Downloaded env_logger v0.7.1\n        Downloaded encode_unicode v0.3.6\n        Downloaded either v1.13.0\n        Downloaded derive_builder v0.9.0\n        Downloaded ctor v0.1.26\n        Downloaded console v0.15.8\n        Downloaded clap v2.34.0\n        Downloaded cfg-if v0.1.10\n        Downloaded aho-corasick v1.1.3\n        Downloaded inventory-impl v0.1.11\n        Downloaded instant v0.1.13\n        Downloaded indoc v0.3.6\n        Downloaded ident_case v1.0.1\n        Downloaded humantime v1.3.0\n        Downloaded hermit-abi v0.1.19\n        Downloaded ghost v0.1.17\n        Downloaded getrandom v0.1.16\n        Downloaded fastrand v2.1.0\n        Downloaded darling_core v0.10.2\n        Downloaded darling v0.10.2\n        Downloaded crossbeam-utils v0.8.20\n        Downloaded crossbeam-epoch v0.9.18\n        Downloaded cfg-if v1.0.0\n        Downloaded cc v1.1.13\n        Downloaded bitflags v2.6.0\n        Downloaded base64 v0.13.1\n        Downloaded ansi_term v0.12.1\n        Downloaded unicode-normalization-alignments v0.1.12\n        Downloaded rawpointer v0.2.1\n        Downloaded indoc-impl v0.3.6\n        Downloaded indicatif v0.15.0\n        Downloaded fnv v1.0.7\n        Downloaded errno v0.3.9\n        Downloaded derive_builder_core v0.9.0\n        Downloaded darling_macro v0.10.2\n        Downloaded crossbeam-deque v0.8.5\n        Downloaded byteorder v1.5.0\n        Downloaded bitflags v1.3.2\n        Downloaded autocfg v1.3.0\n      cargo rustc --lib --message-format=json-render-diagnostics --manifest-path Cargo.toml --release -v --features pyo3/extension-module --crate-type cdylib --\n      warning: unused manifest key: target.x86_64-apple-darwin.rustflags\n         Compiling proc-macro2 v1.0.86\n         Compiling unicode-ident v1.0.12\n         Compiling autocfg v1.3.0\n         Compiling syn v1.0.109\n         Compiling libc v0.2.158\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/proc-macro2-1.0.86/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --cfg 'feature=\"proc-macro\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"nightly\", \"proc-macro\", \"span-locations\"))' -C metadata=dce65f3895ff3b4f -C extra-filename=-dce65f3895ff3b4f --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/proc-macro2-dce65f3895ff3b4f -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name unicode_ident --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/unicode-ident-1.0.12/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=64225dcd6d908b0e -C extra-filename=-64225dcd6d908b0e --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name autocfg --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/autocfg-1.3.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=7883da62ebd09624 -C extra-filename=-7883da62ebd09624 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/syn-1.0.109/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"clone-impls\"' --cfg 'feature=\"default\"' --cfg 'feature=\"derive\"' --cfg 'feature=\"extra-traits\"' --cfg 'feature=\"full\"' --cfg 'feature=\"parsing\"' --cfg 'feature=\"printing\"' --cfg 'feature=\"proc-macro\"' --cfg 'feature=\"quote\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"clone-impls\", \"default\", \"derive\", \"extra-traits\", \"fold\", \"full\", \"parsing\", \"printing\", \"proc-macro\", \"quote\", \"test\", \"visit\", \"visit-mut\"))' -C metadata=4cf12c30d28f810d -C extra-filename=-4cf12c30d28f810d --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/syn-4cf12c30d28f810d -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/libc-0.2.158/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --cfg 'feature=\"extra_traits\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"align\", \"const-extern-fn\", \"default\", \"extra_traits\", \"rustc-dep-of-std\", \"rustc-std-workspace-core\", \"std\", \"use_std\"))' -C metadata=4b261129cdecb68b -C extra-filename=-4b261129cdecb68b --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/libc-4b261129cdecb68b -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling cfg-if v1.0.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name cfg_if --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/cfg-if-1.0.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"compiler_builtins\", \"core\", \"rustc-dep-of-std\"))' -C metadata=069aa8d3b6fffb61 -C extra-filename=-069aa8d3b6fffb61 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling shlex v1.3.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name shlex --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/shlex-1.3.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"std\"))' -C metadata=29f68b1250937afa -C extra-filename=-29f68b1250937afa --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling memchr v2.7.4\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name memchr --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/memchr-2.7.4/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"alloc\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"alloc\", \"compiler_builtins\", \"core\", \"default\", \"libc\", \"logging\", \"rustc-dep-of-std\", \"std\", \"use_std\"))' -C metadata=98aad6ebcffcf120 -C extra-filename=-98aad6ebcffcf120 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling num-traits v0.2.19\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/num-traits-0.2.19/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --cfg 'feature=\"i128\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"i128\", \"libm\", \"std\"))' -C metadata=48a8ac98b389200f -C extra-filename=-48a8ac98b389200f --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/num-traits-48a8ac98b389200f -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern autocfg=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libautocfg-7883da62ebd09624.rlib --cap-lints allow`\n           Running `/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/syn-4cf12c30d28f810d/build-script-build`\n           Running `/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/proc-macro2-dce65f3895ff3b4f/build-script-build`\n         Compiling cc v1.1.13\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name cc --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/cc-1.1.13/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"jobserver\", \"parallel\"))' -C metadata=94c042aa09c605a0 -C extra-filename=-94c042aa09c605a0 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern shlex=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libshlex-29f68b1250937afa.rmeta --cap-lints allow`\n           Running `/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/libc-4b261129cdecb68b/build-script-build`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name libc --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/libc-0.2.158/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"extra_traits\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"align\", \"const-extern-fn\", \"default\", \"extra_traits\", \"rustc-dep-of-std\", \"rustc-std-workspace-core\", \"std\", \"use_std\"))' -C metadata=8262032f1e05c089 -C extra-filename=-8262032f1e05c089 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow --cfg freebsd11 --cfg libc_priv_mod_use --cfg libc_union --cfg libc_const_size_of --cfg libc_align --cfg libc_int128 --cfg libc_core_cvoid --cfg libc_packedN --cfg libc_cfg_target_vendor --cfg libc_non_exhaustive --cfg libc_long_array --cfg libc_ptr_addr_of --cfg libc_underscore_const_names --cfg libc_const_extern_fn --check-cfg 'cfg(emscripten_new_stat_abi)' --check-cfg 'cfg(espidf_time64)' --check-cfg 'cfg(freebsd10)' --check-cfg 'cfg(freebsd11)' --check-cfg 'cfg(freebsd12)' --check-cfg 'cfg(freebsd13)' --check-cfg 'cfg(freebsd14)' --check-cfg 'cfg(freebsd15)' --check-cfg 'cfg(libc_align)' --check-cfg 'cfg(libc_cfg_target_vendor)' --check-cfg 'cfg(libc_const_extern_fn)' --check-cfg 'cfg(libc_const_extern_fn_unstable)' --check-cfg 'cfg(libc_const_size_of)' --check-cfg 'cfg(libc_core_cvoid)' --check-cfg 'cfg(libc_deny_warnings)' --check-cfg 'cfg(libc_int128)' --check-cfg 'cfg(libc_long_array)' --check-cfg 'cfg(libc_non_exhaustive)' --check-cfg 'cfg(libc_packedN)' --check-cfg 'cfg(libc_priv_mod_use)' --check-cfg 'cfg(libc_ptr_addr_of)' --check-cfg 'cfg(libc_thread_local)' --check-cfg 'cfg(libc_underscore_const_names)' --check-cfg 'cfg(libc_union)' --check-cfg 'cfg(target_os,values(\"switch\",\"aix\",\"ohos\",\"hurd\",\"visionos\"))' --check-cfg 'cfg(target_env,values(\"illumos\",\"wasi\",\"aix\",\"ohos\"))' --check-cfg 'cfg(target_arch,values(\"loongarch64\",\"mips32r6\",\"mips64r6\",\"csky\"))'`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name proc_macro2 --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/proc-macro2-1.0.86/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --cfg 'feature=\"proc-macro\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"nightly\", \"proc-macro\", \"span-locations\"))' -C metadata=a53443ded1dfcef2 -C extra-filename=-a53443ded1dfcef2 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern unicode_ident=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libunicode_ident-64225dcd6d908b0e.rmeta --cap-lints allow --cfg wrap_proc_macro --check-cfg 'cfg(fuzzing)' --check-cfg 'cfg(no_is_available)' --check-cfg 'cfg(no_literal_byte_character)' --check-cfg 'cfg(no_literal_c_string)' --check-cfg 'cfg(no_source_text)' --check-cfg 'cfg(proc_macro_span)' --check-cfg 'cfg(procmacro2_backtrace)' --check-cfg 'cfg(procmacro2_nightly_testing)' --check-cfg 'cfg(procmacro2_semver_exempt)' --check-cfg 'cfg(randomize_layout)' --check-cfg 'cfg(span_locations)' --check-cfg 'cfg(super_unstable)' --check-cfg 'cfg(wrap_proc_macro)'`\n         Compiling crossbeam-utils v0.8.20\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/crossbeam-utils-0.8.20/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no --warn=unexpected_cfgs -C debug-assertions=off --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"loom\", \"nightly\", \"std\"))' -C metadata=83731834fd1d61fd -C extra-filename=-83731834fd1d61fd --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/crossbeam-utils-83731834fd1d61fd -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling quote v1.0.36\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name quote --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/quote-1.0.36/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --cfg 'feature=\"proc-macro\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"proc-macro\"))' -C metadata=4c220be45cf63bba -C extra-filename=-4c220be45cf63bba --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern proc_macro2=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libproc_macro2-a53443ded1dfcef2.rmeta --cap-lints allow`\n         Compiling proc-macro-hack v0.5.20+deprecated\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/proc-macro-hack-0.5.20+deprecated/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=431824022aa8ad7f -C extra-filename=-431824022aa8ad7f --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/proc-macro-hack-431824022aa8ad7f -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name syn --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/syn-1.0.109/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"clone-impls\"' --cfg 'feature=\"default\"' --cfg 'feature=\"derive\"' --cfg 'feature=\"extra-traits\"' --cfg 'feature=\"full\"' --cfg 'feature=\"parsing\"' --cfg 'feature=\"printing\"' --cfg 'feature=\"proc-macro\"' --cfg 'feature=\"quote\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"clone-impls\", \"default\", \"derive\", \"extra-traits\", \"fold\", \"full\", \"parsing\", \"printing\", \"proc-macro\", \"quote\", \"test\", \"visit\", \"visit-mut\"))' -C metadata=3a67f5675a1f15d1 -C extra-filename=-3a67f5675a1f15d1 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern proc_macro2=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libproc_macro2-a53443ded1dfcef2.rmeta --extern quote=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libquote-4c220be45cf63bba.rmeta --extern unicode_ident=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libunicode_ident-64225dcd6d908b0e.rmeta --cap-lints allow --cfg syn_disable_nightly_tests`\n         Compiling syn v2.0.75\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name syn --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/syn-2.0.75/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"clone-impls\"' --cfg 'feature=\"default\"' --cfg 'feature=\"derive\"' --cfg 'feature=\"parsing\"' --cfg 'feature=\"printing\"' --cfg 'feature=\"proc-macro\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"clone-impls\", \"default\", \"derive\", \"extra-traits\", \"fold\", \"full\", \"parsing\", \"printing\", \"proc-macro\", \"test\", \"visit\", \"visit-mut\"))' -C metadata=a7527cdbfb662196 -C extra-filename=-a7527cdbfb662196 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern proc_macro2=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libproc_macro2-a53443ded1dfcef2.rmeta --extern quote=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libquote-4c220be45cf63bba.rmeta --extern unicode_ident=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libunicode_ident-64225dcd6d908b0e.rmeta --cap-lints allow`\n           Running `/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/proc-macro-hack-431824022aa8ad7f/build-script-build`\n           Running `/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/crossbeam-utils-83731834fd1d61fd/build-script-build`\n           Running `/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/num-traits-48a8ac98b389200f/build-script-build`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name crossbeam_utils --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/crossbeam-utils-0.8.20/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --warn=unexpected_cfgs --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"loom\", \"nightly\", \"std\"))' -C metadata=5fe1a42cb8431226 -C extra-filename=-5fe1a42cb8431226 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow --check-cfg 'cfg(crossbeam_no_atomic,crossbeam_sanitize_thread)'`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name num_traits --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/num-traits-0.2.19/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"i128\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"i128\", \"libm\", \"std\"))' -C metadata=04f24cb06d3c2fbe -C extra-filename=-04f24cb06d3c2fbe --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow --cfg has_total_cmp`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name proc_macro_hack --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/proc-macro-hack-0.5.20+deprecated/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type proc-macro --emit=dep-info,link -C prefer-dynamic -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=b49030fd20457a17 -C extra-filename=-b49030fd20457a17 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern proc_macro --cap-lints allow`\n         Compiling getrandom v0.1.16\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/getrandom-0.1.16/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"bindgen\", \"compiler_builtins\", \"core\", \"dummy\", \"js-sys\", \"log\", \"rustc-dep-of-std\", \"std\", \"stdweb\", \"test-in-browser\", \"wasm-bindgen\"))' -C metadata=df0ecdf682f41f96 -C extra-filename=-df0ecdf682f41f96 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/getrandom-df0ecdf682f41f96 -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling smallvec v1.13.2\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name smallvec --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/smallvec-1.13.2/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"arbitrary\", \"const_generics\", \"const_new\", \"debugger_visualizer\", \"drain_filter\", \"drain_keep_rest\", \"may_dangle\", \"serde\", \"specialization\", \"union\", \"write\"))' -C metadata=54b60278d2fd4af7 -C extra-filename=-54b60278d2fd4af7 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling strsim v0.9.3\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name strsim --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/strsim-0.9.3/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=c5ed5f0f3eec67ae -C extra-filename=-c5ed5f0f3eec67ae --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling fnv v1.0.7\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name fnv --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/fnv-1.0.7/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"std\"))' -C metadata=444c02e4b5242b37 -C extra-filename=-444c02e4b5242b37 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling ident_case v1.0.1\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name ident_case --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/ident_case-1.0.1/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=f9ebc3e0d26d6510 -C extra-filename=-f9ebc3e0d26d6510 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n           Running `/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/getrandom-df0ecdf682f41f96/build-script-build`\n         Compiling crossbeam-epoch v0.9.18\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name crossbeam_epoch --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/crossbeam-epoch-0.9.18/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"alloc\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"alloc\", \"default\", \"loom\", \"loom-crate\", \"nightly\", \"std\"))' -C metadata=1162c98467d8fcf6 -C extra-filename=-1162c98467d8fcf6 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern crossbeam_utils=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libcrossbeam_utils-5fe1a42cb8431226.rmeta --cap-lints allow`\n         Compiling aho-corasick v1.1.3\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name aho_corasick --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/aho-corasick-1.1.3/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"perf-literal\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"logging\", \"perf-literal\", \"std\"))' -C metadata=3ab56ade417ee174 -C extra-filename=-3ab56ade417ee174 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern memchr=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libmemchr-98aad6ebcffcf120.rmeta --cap-lints allow`\n         Compiling lock_api v0.4.12\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/lock_api-0.4.12/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"atomic_usize\"' --cfg 'feature=\"default\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"arc_lock\", \"atomic_usize\", \"default\", \"nightly\", \"owning_ref\", \"serde\"))' -C metadata=73b0bf1b17677ff7 -C extra-filename=-73b0bf1b17677ff7 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/lock_api-73b0bf1b17677ff7 -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern autocfg=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libautocfg-7883da62ebd09624.rlib --cap-lints allow`\n         Compiling unicode-width v0.1.13\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name unicode_width --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/unicode-width-0.1.13/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"compiler_builtins\", \"core\", \"default\", \"no_std\", \"rustc-dep-of-std\", \"std\"))' -C metadata=147e4b4395044b58 -C extra-filename=-147e4b4395044b58 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling regex-syntax v0.8.4\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name regex_syntax --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/regex-syntax-0.8.4/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --cfg 'feature=\"unicode\"' --cfg 'feature=\"unicode-age\"' --cfg 'feature=\"unicode-bool\"' --cfg 'feature=\"unicode-case\"' --cfg 'feature=\"unicode-gencat\"' --cfg 'feature=\"unicode-perl\"' --cfg 'feature=\"unicode-script\"' --cfg 'feature=\"unicode-segment\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"arbitrary\", \"default\", \"std\", \"unicode\", \"unicode-age\", \"unicode-bool\", \"unicode-case\", \"unicode-gencat\", \"unicode-perl\", \"unicode-script\", \"unicode-segment\"))' -C metadata=189987509e6b620a -C extra-filename=-189987509e6b620a --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling either v1.13.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name either --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/either-1.13.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"use_std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"serde\", \"use_std\"))' -C metadata=cbb2d4c5b75a4107 -C extra-filename=-cbb2d4c5b75a4107 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling byteorder v1.5.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name byteorder --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/byteorder-1.5.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"i128\", \"std\"))' -C metadata=701a861a4d770fe6 -C extra-filename=-701a861a4d770fe6 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling darling_core v0.10.2\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name darling_core --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/darling_core-0.10.2/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"strsim\"' --cfg 'feature=\"suggestions\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"diagnostics\", \"strsim\", \"suggestions\"))' -C metadata=79999c50edd86e3a -C extra-filename=-79999c50edd86e3a --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern fnv=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libfnv-444c02e4b5242b37.rmeta --extern ident_case=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libident_case-f9ebc3e0d26d6510.rmeta --extern proc_macro2=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libproc_macro2-a53443ded1dfcef2.rmeta --extern quote=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libquote-4c220be45cf63bba.rmeta --extern strsim=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libstrsim-c5ed5f0f3eec67ae.rmeta --extern syn=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libsyn-3a67f5675a1f15d1.rmeta --cap-lints allow`\n         Compiling zerocopy-derive v0.7.35\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name zerocopy_derive --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/zerocopy-derive-0.7.35/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type proc-macro --emit=dep-info,link -C prefer-dynamic -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=c32dc9592cf4c364 -C extra-filename=-c32dc9592cf4c364 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern proc_macro2=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libproc_macro2-a53443ded1dfcef2.rlib --extern quote=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libquote-4c220be45cf63bba.rlib --extern syn=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libsyn-a7527cdbfb662196.rlib --extern proc_macro --cap-lints allow`\n         Compiling pkg-config v0.3.30\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name pkg_config --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/pkg-config-0.3.30/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=e2aa6d146c930417 -C extra-filename=-e2aa6d146c930417 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling rayon-core v1.12.1\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rayon-core-1.12.1/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"web_spin_lock\"))' -C metadata=2202607f5aa42298 -C extra-filename=-2202607f5aa42298 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/rayon-core-2202607f5aa42298 -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling rawpointer v0.2.1\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name rawpointer --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rawpointer-0.2.1/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=25344ca069a916c0 -C extra-filename=-25344ca069a916c0 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling serde v1.0.208\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/serde-1.0.208/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --cfg 'feature=\"derive\"' --cfg 'feature=\"rc\"' --cfg 'feature=\"serde_derive\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"alloc\", \"default\", \"derive\", \"rc\", \"serde_derive\", \"std\", \"unstable\"))' -C metadata=54947cb5889e94c1 -C extra-filename=-54947cb5889e94c1 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/serde-54947cb5889e94c1 -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling parking_lot_core v0.8.6\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/parking_lot_core-0.8.6/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"backtrace\", \"deadlock_detection\", \"nightly\", \"petgraph\", \"thread-id\"))' -C metadata=e36b8b1d7602edbf -C extra-filename=-e36b8b1d7602edbf --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/parking_lot_core-e36b8b1d7602edbf -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling onig_sys v69.8.1\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/onig_sys-69.8.1/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"bindgen\", \"default\", \"generate\", \"posix-api\", \"print-debug\"))' -C metadata=178fcc49a4a36512 -C extra-filename=-178fcc49a4a36512 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/onig_sys-178fcc49a4a36512 -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern cc=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libcc-94c042aa09c605a0.rlib --extern pkg_config=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libpkg_config-e2aa6d146c930417.rlib --cap-lints allow`\n           Running `/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/serde-54947cb5889e94c1/build-script-build`\n           Running `/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/rayon-core-2202607f5aa42298/build-script-build`\n         Compiling darling_macro v0.10.2\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name darling_macro --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/darling_macro-0.10.2/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type proc-macro --emit=dep-info,link -C prefer-dynamic -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=7c5d1759444ecb34 -C extra-filename=-7c5d1759444ecb34 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern darling_core=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libdarling_core-79999c50edd86e3a.rlib --extern quote=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libquote-4c220be45cf63bba.rlib --extern syn=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libsyn-3a67f5675a1f15d1.rlib --extern proc_macro --cap-lints allow`\n           Running `/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/parking_lot_core-e36b8b1d7602edbf/build-script-build`\n         Compiling zerocopy v0.7.35\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name zerocopy --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/zerocopy-0.7.35/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"byteorder\"' --cfg 'feature=\"default\"' --cfg 'feature=\"derive\"' --cfg 'feature=\"simd\"' --cfg 'feature=\"zerocopy-derive\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"__internal_use_only_features_that_work_on_stable\", \"alloc\", \"byteorder\", \"default\", \"derive\", \"simd\", \"simd-nightly\", \"zerocopy-derive\"))' -C metadata=038baa6cbd8eb075 -C extra-filename=-038baa6cbd8eb075 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern byteorder=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libbyteorder-701a861a4d770fe6.rmeta --extern zerocopy_derive=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libzerocopy_derive-c32dc9592cf4c364.so --cap-lints allow`\n         Compiling regex-automata v0.4.7\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name regex_automata --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/regex-automata-0.4.7/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"alloc\"' --cfg 'feature=\"dfa-onepass\"' --cfg 'feature=\"hybrid\"' --cfg 'feature=\"meta\"' --cfg 'feature=\"nfa-backtrack\"' --cfg 'feature=\"nfa-pikevm\"' --cfg 'feature=\"nfa-thompson\"' --cfg 'feature=\"perf-inline\"' --cfg 'feature=\"perf-literal\"' --cfg 'feature=\"perf-literal-multisubstring\"' --cfg 'feature=\"perf-literal-substring\"' --cfg 'feature=\"std\"' --cfg 'feature=\"syntax\"' --cfg 'feature=\"unicode\"' --cfg 'feature=\"unicode-age\"' --cfg 'feature=\"unicode-bool\"' --cfg 'feature=\"unicode-case\"' --cfg 'feature=\"unicode-gencat\"' --cfg 'feature=\"unicode-perl\"' --cfg 'feature=\"unicode-script\"' --cfg 'feature=\"unicode-segment\"' --cfg 'feature=\"unicode-word-boundary\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"alloc\", \"default\", \"dfa\", \"dfa-build\", \"dfa-onepass\", \"dfa-search\", \"hybrid\", \"internal-instrument\", \"internal-instrument-pikevm\", \"logging\", \"meta\", \"nfa\", \"nfa-backtrack\", \"nfa-pikevm\", \"nfa-thompson\", \"perf\", \"perf-inline\", \"perf-literal\", \"perf-literal-multisubstring\", \"perf-literal-substring\", \"std\", \"syntax\", \"unicode\", \"unicode-age\", \"unicode-bool\", \"unicode-case\", \"unicode-gencat\", \"unicode-perl\", \"unicode-script\", \"unicode-segment\", \"unicode-word-boundary\"))' -C metadata=ed7cef13f17ea6d9 -C extra-filename=-ed7cef13f17ea6d9 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern aho_corasick=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libaho_corasick-3ab56ade417ee174.rmeta --extern memchr=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libmemchr-98aad6ebcffcf120.rmeta --extern regex_syntax=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libregex_syntax-189987509e6b620a.rmeta --cap-lints allow`\n         Compiling serde_derive v1.0.208\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name serde_derive --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/serde_derive-1.0.208/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type proc-macro --emit=dep-info,link -C prefer-dynamic -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"deserialize_in_place\"))' -C metadata=cdab2c8bee3f7a78 -C extra-filename=-cdab2c8bee3f7a78 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern proc_macro2=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libproc_macro2-a53443ded1dfcef2.rlib --extern quote=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libquote-4c220be45cf63bba.rlib --extern syn=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libsyn-a7527cdbfb662196.rlib --extern proc_macro --cap-lints allow`\n           Running `/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/lock_api-73b0bf1b17677ff7/build-script-build`\n         Compiling crossbeam-deque v0.8.5\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name crossbeam_deque --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/crossbeam-deque-0.8.5/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"std\"))' -C metadata=743c0f6ed6fe6fe8 -C extra-filename=-743c0f6ed6fe6fe8 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern crossbeam_epoch=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libcrossbeam_epoch-1162c98467d8fcf6.rmeta --extern crossbeam_utils=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libcrossbeam_utils-5fe1a42cb8431226.rmeta --cap-lints allow`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name getrandom --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/getrandom-0.1.16/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"bindgen\", \"compiler_builtins\", \"core\", \"dummy\", \"js-sys\", \"log\", \"rustc-dep-of-std\", \"std\", \"stdweb\", \"test-in-browser\", \"wasm-bindgen\"))' -C metadata=8d6eb4053cea3593 -C extra-filename=-8d6eb4053cea3593 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern cfg_if=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libcfg_if-069aa8d3b6fffb61.rmeta --extern libc=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/liblibc-8262032f1e05c089.rmeta --cap-lints allow`\n         Compiling num-complex v0.2.4\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/num-complex-0.2.4/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"i128\", \"rand\", \"serde\", \"std\"))' -C metadata=23abeec020ef583f -C extra-filename=-23abeec020ef583f --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/num-complex-23abeec020ef583f -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern autocfg=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libautocfg-7883da62ebd09624.rlib --cap-lints allow`\n         Compiling matrixmultiply v0.3.9\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/matrixmultiply-0.3.9/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"cgemm\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"cgemm\", \"constconf\", \"default\", \"num_cpus\", \"once_cell\", \"std\", \"thread-tree\", \"threading\"))' -C metadata=e7d43178843fe7b3 -C extra-filename=-e7d43178843fe7b3 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/matrixmultiply-e7d43178843fe7b3 -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern autocfg=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libautocfg-7883da62ebd09624.rlib --cap-lints allow`\n         Compiling instant v0.1.13\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name instant --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/instant-0.1.13/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"inaccurate\", \"js-sys\", \"now\", \"stdweb\", \"wasm-bindgen\", \"wasm-bindgen_rs\", \"web-sys\"))' -C metadata=30811dddb1d7c14f -C extra-filename=-30811dddb1d7c14f --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern cfg_if=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libcfg_if-069aa8d3b6fffb61.rmeta --cap-lints allow`\n         Compiling inventory v0.1.11\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/inventory-0.1.11/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=1d913761fc71377a -C extra-filename=-1d913761fc71377a --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/inventory-1d913761fc71377a -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling unindent v0.1.11\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name unindent --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/unindent-0.1.11/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=c0ca27a732d0d4f2 -C extra-filename=-c0ca27a732d0d4f2 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling scopeguard v1.2.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name scopeguard --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/scopeguard-1.2.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"use_std\"))' -C metadata=ac8773d0c8be3a88 -C extra-filename=-ac8773d0c8be3a88 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name lock_api --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/lock_api-0.4.12/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"atomic_usize\"' --cfg 'feature=\"default\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"arc_lock\", \"atomic_usize\", \"default\", \"nightly\", \"owning_ref\", \"serde\"))' -C metadata=f83b6d9ef7c7f49f -C extra-filename=-f83b6d9ef7c7f49f --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern scopeguard=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libscopeguard-ac8773d0c8be3a88.rmeta --cap-lints allow --cfg has_const_fn_trait_bound`\n         Compiling indoc-impl v0.3.6\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name indoc_impl --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/indoc-impl-0.3.6/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type proc-macro --emit=dep-info,link -C prefer-dynamic -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"unstable\"))' -C metadata=31120b60d9d8ff7a -C extra-filename=-31120b60d9d8ff7a --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern proc_macro_hack=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libproc_macro_hack-b49030fd20457a17.so --extern proc_macro2=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libproc_macro2-a53443ded1dfcef2.rlib --extern quote=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libquote-4c220be45cf63bba.rlib --extern syn=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libsyn-3a67f5675a1f15d1.rlib --extern unindent=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libunindent-c0ca27a732d0d4f2.rlib --extern proc_macro --cap-lints allow`\n           Running `/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/inventory-1d913761fc71377a/build-script-build`\n         Compiling regex v1.10.6\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name regex --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/regex-1.10.6/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"perf\"' --cfg 'feature=\"perf-backtrack\"' --cfg 'feature=\"perf-cache\"' --cfg 'feature=\"perf-dfa\"' --cfg 'feature=\"perf-inline\"' --cfg 'feature=\"perf-literal\"' --cfg 'feature=\"perf-onepass\"' --cfg 'feature=\"std\"' --cfg 'feature=\"unicode\"' --cfg 'feature=\"unicode-age\"' --cfg 'feature=\"unicode-bool\"' --cfg 'feature=\"unicode-case\"' --cfg 'feature=\"unicode-gencat\"' --cfg 'feature=\"unicode-perl\"' --cfg 'feature=\"unicode-script\"' --cfg 'feature=\"unicode-segment\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"logging\", \"pattern\", \"perf\", \"perf-backtrack\", \"perf-cache\", \"perf-dfa\", \"perf-dfa-full\", \"perf-inline\", \"perf-literal\", \"perf-onepass\", \"std\", \"unicode\", \"unicode-age\", \"unicode-bool\", \"unicode-case\", \"unicode-gencat\", \"unicode-perl\", \"unicode-script\", \"unicode-segment\", \"unstable\", \"use_std\"))' -C metadata=883f132f16a8c2d0 -C extra-filename=-883f132f16a8c2d0 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern aho_corasick=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libaho_corasick-3ab56ade417ee174.rmeta --extern memchr=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libmemchr-98aad6ebcffcf120.rmeta --extern regex_automata=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libregex_automata-ed7cef13f17ea6d9.rmeta --extern regex_syntax=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libregex_syntax-189987509e6b620a.rmeta --cap-lints allow`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name parking_lot_core --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/parking_lot_core-0.8.6/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"backtrace\", \"deadlock_detection\", \"nightly\", \"petgraph\", \"thread-id\"))' -C metadata=36d039a4665525ee -C extra-filename=-36d039a4665525ee --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern cfg_if=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libcfg_if-069aa8d3b6fffb61.rmeta --extern instant=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libinstant-30811dddb1d7c14f.rmeta --extern libc=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/liblibc-8262032f1e05c089.rmeta --extern smallvec=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libsmallvec-54b60278d2fd4af7.rmeta --cap-lints allow`\n           Running `/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/matrixmultiply-e7d43178843fe7b3/build-script-build`\n           Running `/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/num-complex-23abeec020ef583f/build-script-build`\n         Compiling rand_core v0.5.1\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name rand_core --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rand_core-0.5.1/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"alloc\"' --cfg 'feature=\"getrandom\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"alloc\", \"getrandom\", \"serde\", \"serde1\", \"std\"))' -C metadata=eb45dce8fe3856ac -C extra-filename=-eb45dce8fe3856ac --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern getrandom=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libgetrandom-8d6eb4053cea3593.rmeta --cap-lints allow`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name rayon_core --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rayon-core-1.12.1/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"web_spin_lock\"))' -C metadata=42d3d4e7963916c1 -C extra-filename=-42d3d4e7963916c1 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern crossbeam_deque=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libcrossbeam_deque-743c0f6ed6fe6fe8.rmeta --extern crossbeam_utils=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libcrossbeam_utils-5fe1a42cb8431226.rmeta --cap-lints allow`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name serde --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/serde-1.0.208/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"derive\"' --cfg 'feature=\"rc\"' --cfg 'feature=\"serde_derive\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"alloc\", \"default\", \"derive\", \"rc\", \"serde_derive\", \"std\", \"unstable\"))' -C metadata=05c41278f1822ffa -C extra-filename=-05c41278f1822ffa --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern serde_derive=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libserde_derive-cdab2c8bee3f7a78.so --cap-lints allow --check-cfg 'cfg(no_core_cstr)' --check-cfg 'cfg(no_core_num_saturating)' --check-cfg 'cfg(no_core_try_from)' --check-cfg 'cfg(no_diagnostic_namespace)' --check-cfg 'cfg(no_float_copysign)' --check-cfg 'cfg(no_num_nonzero_signed)' --check-cfg 'cfg(no_relaxed_trait_bounds)' --check-cfg 'cfg(no_serde_derive)' --check-cfg 'cfg(no_std_atomic)' --check-cfg 'cfg(no_std_atomic64)' --check-cfg 'cfg(no_systemtime_checked_add)' --check-cfg 'cfg(no_target_has_atomic)'`\n         Compiling darling v0.10.2\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name darling --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/darling-0.10.2/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --cfg 'feature=\"suggestions\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"diagnostics\", \"suggestions\"))' -C metadata=ad121ed9a36f1792 -C extra-filename=-ad121ed9a36f1792 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern darling_core=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libdarling_core-79999c50edd86e3a.rmeta --extern darling_macro=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libdarling_macro-7c5d1759444ecb34.so --cap-lints allow`\n         Compiling ppv-lite86 v0.2.20\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name ppv_lite86 --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/ppv-lite86-0.2.20/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"simd\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"no_simd\", \"simd\", \"std\"))' -C metadata=2c7f0af64e74045b -C extra-filename=-2c7f0af64e74045b --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern zerocopy=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libzerocopy-038baa6cbd8eb075.rmeta --cap-lints allow`\n           Running `/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/onig_sys-178fcc49a4a36512/build-script-build`\n         Compiling inventory-impl v0.1.11\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name inventory_impl --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/inventory-impl-0.1.11/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type proc-macro --emit=dep-info,link -C prefer-dynamic -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=f31c599e4feebe61 -C extra-filename=-f31c599e4feebe61 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern proc_macro2=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libproc_macro2-a53443ded1dfcef2.rlib --extern quote=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libquote-4c220be45cf63bba.rlib --extern syn=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libsyn-3a67f5675a1f15d1.rlib --extern proc_macro --cap-lints allow`\n         Compiling ctor v0.1.26\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name ctor --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/ctor-0.1.26/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type proc-macro --emit=dep-info,link -C prefer-dynamic -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=957f2898f4676888 -C extra-filename=-957f2898f4676888 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern quote=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libquote-4c220be45cf63bba.rlib --extern syn=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libsyn-3a67f5675a1f15d1.rlib --extern proc_macro --cap-lints allow`\n         Compiling ghost v0.1.17\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name ghost --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/ghost-0.1.17/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type proc-macro --emit=dep-info,link -C prefer-dynamic -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=ef121d9eb7fe0345 -C extra-filename=-ef121d9eb7fe0345 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern proc_macro2=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libproc_macro2-a53443ded1dfcef2.rlib --extern quote=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libquote-4c220be45cf63bba.rlib --extern syn=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libsyn-a7527cdbfb662196.rlib --extern proc_macro --cap-lints allow`\n         Compiling pyo3-derive-backend v0.12.4\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name pyo3_derive_backend --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/pyo3-derive-backend-0.12.4/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=fb5705e5a6f8418d -C extra-filename=-fb5705e5a6f8418d --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern proc_macro2=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libproc_macro2-a53443ded1dfcef2.rmeta --extern quote=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libquote-4c220be45cf63bba.rmeta --extern syn=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libsyn-3a67f5675a1f15d1.rmeta --cap-lints allow`\n         Compiling paste-impl v0.1.18\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name paste_impl --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/paste-impl-0.1.18/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type proc-macro --emit=dep-info,link -C prefer-dynamic -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=af92603fbf9be662 -C extra-filename=-af92603fbf9be662 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern proc_macro_hack=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libproc_macro_hack-b49030fd20457a17.so --extern proc_macro --cap-lints allow`\n         Compiling num-integer v0.1.46\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name num_integer --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/num-integer-0.1.46/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"i128\", \"std\"))' -C metadata=6b9813fd539b3b7a -C extra-filename=-6b9813fd539b3b7a --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern num_traits=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libnum_traits-04f24cb06d3c2fbe.rmeta --cap-lints allow`\n         Compiling esaxx-rs v0.1.10\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/esaxx-rs-0.1.10/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"cc\"' --cfg 'feature=\"cpp\"' --cfg 'feature=\"default\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"cc\", \"cpp\", \"default\"))' -C metadata=c586f188cc80cffc -C extra-filename=-c586f188cc80cffc --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/esaxx-rs-c586f188cc80cffc -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern cc=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libcc-94c042aa09c605a0.rlib --cap-lints allow`\n         Compiling atty v0.2.14\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name atty --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/atty-0.2.14/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=824de6acfdffc9b1 -C extra-filename=-824de6acfdffc9b1 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern libc=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/liblibc-8262032f1e05c089.rmeta --cap-lints allow`\n         Compiling derive_builder v0.9.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_mod --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/derive_builder-0.9.0/build/mod.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"compiletest_rs\", \"env_logger\", \"log\", \"logging\", \"nightlytests\", \"skeptic\", \"skeptic_tests\"))' -C metadata=f6af66a5f4111def -C extra-filename=-f6af66a5f4111def --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/derive_builder-f6af66a5f4111def -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling pyo3 v0.12.4\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/pyo3-0.12.4/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"ctor\"' --cfg 'feature=\"default\"' --cfg 'feature=\"extension-module\"' --cfg 'feature=\"indoc\"' --cfg 'feature=\"inventory\"' --cfg 'feature=\"macros\"' --cfg 'feature=\"paste\"' --cfg 'feature=\"pyo3cls\"' --cfg 'feature=\"unindent\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"ctor\", \"default\", \"extension-module\", \"hashbrown\", \"indoc\", \"inventory\", \"macros\", \"nightly\", \"num-bigint\", \"num-complex\", \"paste\", \"pyo3cls\", \"python3\", \"unindent\"))' -C metadata=34b558ef4d8979ef -C extra-filename=-34b558ef4d8979ef --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/pyo3-34b558ef4d8979ef -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling lazy_static v1.5.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name lazy_static --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/lazy_static-1.5.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"spin\", \"spin_no_std\"))' -C metadata=b8cd5ec493744c98 -C extra-filename=-b8cd5ec493744c98 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling bitflags v1.3.2\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name bitflags --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/bitflags-1.3.2/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"compiler_builtins\", \"core\", \"default\", \"example_generated\", \"rustc-dep-of-std\"))' -C metadata=572d549a09e7b042 -C extra-filename=-572d549a09e7b042 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling serde_json v1.0.125\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/serde_json-1.0.125/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"alloc\", \"arbitrary_precision\", \"default\", \"float_roundtrip\", \"indexmap\", \"preserve_order\", \"raw_value\", \"std\", \"unbounded_depth\"))' -C metadata=72c53176e9434372 -C extra-filename=-72c53176e9434372 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/serde_json-72c53176e9434372 -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling minimal-lexical v0.2.1\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name minimal_lexical --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/minimal-lexical-0.2.1/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"alloc\", \"compact\", \"default\", \"lint\", \"nightly\", \"std\"))' -C metadata=88cdc20278564cf1 -C extra-filename=-88cdc20278564cf1 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling pyo3cls v0.12.4\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name pyo3cls --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/pyo3cls-0.12.4/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type proc-macro --emit=dep-info,link -C prefer-dynamic -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=5a5ad2657c3500fc -C extra-filename=-5a5ad2657c3500fc --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern pyo3_derive_backend=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libpyo3_derive_backend-fb5705e5a6f8418d.rlib --extern quote=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libquote-4c220be45cf63bba.rlib --extern syn=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libsyn-3a67f5675a1f15d1.rlib --extern proc_macro --cap-lints allow`\n         Compiling nom v7.1.3\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name nom --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/nom-7.1.3/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"alloc\"' --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"alloc\", \"default\", \"docsrs\", \"std\"))' -C metadata=8a084cf9e98aef2c -C extra-filename=-8a084cf9e98aef2c --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern memchr=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libmemchr-98aad6ebcffcf120.rmeta --extern minimal_lexical=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libminimal_lexical-88cdc20278564cf1.rmeta --cap-lints allow`\n           Running `/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/pyo3-34b558ef4d8979ef/build-script-build`\n           Running `/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/serde_json-72c53176e9434372/build-script-build`\n         Compiling console v0.15.8\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name console --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/console-0.15.8/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"ansi-parsing\"' --cfg 'feature=\"default\"' --cfg 'feature=\"unicode-width\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"ansi-parsing\", \"default\", \"unicode-width\", \"windows-console-colors\"))' -C metadata=7389811a5688094b -C extra-filename=-7389811a5688094b --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern lazy_static=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/liblazy_static-b8cd5ec493744c98.rmeta --extern libc=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/liblibc-8262032f1e05c089.rmeta --extern unicode_width=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libunicode_width-147e4b4395044b58.rmeta --cap-lints allow`\n           Running `/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/esaxx-rs-c586f188cc80cffc/build-script-build`\n           Running `/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/derive_builder-f6af66a5f4111def/build-script-mod`\n         Compiling paste v0.1.18\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name paste --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/paste-0.1.18/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=77cfc92c5c84a892 -C extra-filename=-77cfc92c5c84a892 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern paste_impl=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libpaste_impl-af92603fbf9be662.so --extern proc_macro_hack=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libproc_macro_hack-b49030fd20457a17.so --cap-lints allow`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name inventory --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/inventory-0.1.11/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=01732795dcd7bb16 -C extra-filename=-01732795dcd7bb16 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern ctor=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libctor-957f2898f4676888.so --extern ghost=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libghost-ef121d9eb7fe0345.so --extern inventory_impl=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libinventory_impl-f31c599e4feebe61.so --cap-lints allow`\n         Compiling rand_chacha v0.2.2\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name rand_chacha --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rand_chacha-0.2.2/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"simd\", \"std\"))' -C metadata=d82021d0089f938a -C extra-filename=-d82021d0089f938a --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern ppv_lite86=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libppv_lite86-2c7f0af64e74045b.rmeta --extern rand_core=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/librand_core-eb45dce8fe3856ac.rmeta --cap-lints allow`\n         Compiling derive_builder_core v0.9.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name derive_builder_core --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/derive_builder_core-0.9.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"log\", \"logging\"))' -C metadata=e7db7296aa9f92b1 -C extra-filename=-e7db7296aa9f92b1 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern darling=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libdarling-ad121ed9a36f1792.rmeta --extern proc_macro2=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libproc_macro2-a53443ded1dfcef2.rmeta --extern quote=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libquote-4c220be45cf63bba.rmeta --extern syn=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libsyn-3a67f5675a1f15d1.rmeta --cap-lints allow`\n         Compiling rayon v1.10.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name rayon --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rayon-1.10.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"web_spin_lock\"))' -C metadata=8704d374a66af352 -C extra-filename=-8704d374a66af352 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern either=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libeither-cbb2d4c5b75a4107.rmeta --extern rayon_core=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/librayon_core-42d3d4e7963916c1.rmeta --cap-lints allow`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name num_complex --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/num-complex-0.2.4/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"i128\", \"rand\", \"serde\", \"std\"))' -C metadata=88ee02742315b6d2 -C extra-filename=-88ee02742315b6d2 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern num_traits=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libnum_traits-04f24cb06d3c2fbe.rmeta --cap-lints allow --cfg has_i128 --cfg has_const_fn`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name matrixmultiply --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/matrixmultiply-0.3.9/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"cgemm\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"cgemm\", \"constconf\", \"default\", \"num_cpus\", \"once_cell\", \"std\", \"thread-tree\", \"threading\"))' -C metadata=fef05d356b78315f -C extra-filename=-fef05d356b78315f --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern rawpointer=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/librawpointer-25344ca069a916c0.rmeta --cap-lints allow`\n         Compiling parking_lot v0.11.2\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name parking_lot --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/parking_lot-0.11.2/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"arc_lock\", \"deadlock_detection\", \"default\", \"nightly\", \"owning_ref\", \"send_guard\", \"serde\", \"stdweb\", \"wasm-bindgen\"))' -C metadata=6389d506bd23dbb3 -C extra-filename=-6389d506bd23dbb3 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern instant=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libinstant-30811dddb1d7c14f.rmeta --extern lock_api=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/liblock_api-f83b6d9ef7c7f49f.rmeta --extern parking_lot_core=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libparking_lot_core-36d039a4665525ee.rmeta --cap-lints allow`\n         Compiling indoc v0.3.6\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name indoc --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/indoc-0.3.6/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"unstable\"))' -C metadata=5ae5844505251d95 -C extra-filename=-5ae5844505251d95 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern indoc_impl=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libindoc_impl-31120b60d9d8ff7a.so --extern proc_macro_hack=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libproc_macro_hack-b49030fd20457a17.so --cap-lints allow`\n         Compiling itertools v0.8.2\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name itertools --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/itertools-0.8.2/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"use_std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"use_std\"))' -C metadata=23c94ea62554b785 -C extra-filename=-23c94ea62554b785 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern either=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libeither-cbb2d4c5b75a4107.rmeta --cap-lints allow`\n         Compiling textwrap v0.11.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name textwrap --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/textwrap-0.11.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"hyphenation\", \"term_size\"))' -C metadata=b47182d7f07a0e83 -C extra-filename=-b47182d7f07a0e83 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern unicode_width=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libunicode_width-147e4b4395044b58.rmeta --cap-lints allow`\n         Compiling num-complex v0.4.6\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name num_complex --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/num-complex-0.4.6/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"bytecheck\", \"bytemuck\", \"default\", \"libm\", \"rand\", \"rkyv\", \"serde\", \"std\"))' -C metadata=4bce8ee361c8cbb5 -C extra-filename=-4bce8ee361c8cbb5 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern num_traits=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libnum_traits-04f24cb06d3c2fbe.rmeta --cap-lints allow`\n         Compiling base64 v0.13.1\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name base64 --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/base64-0.13.1/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"alloc\", \"default\", \"std\"))' -C metadata=835cd68726307e54 -C extra-filename=-835cd68726307e54 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling vec_map v0.8.2\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name vec_map --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/vec_map-0.8.2/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"eders\", \"serde\"))' -C metadata=fa32a1cd80d7fcbc -C extra-filename=-fa32a1cd80d7fcbc --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling number_prefix v0.3.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name number_prefix --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/number_prefix-0.3.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"std\"))' -C metadata=47262f61f1c6a366 -C extra-filename=-47262f61f1c6a366 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling unicode-segmentation v1.11.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name unicode_segmentation --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/unicode-segmentation-1.11.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"no_std\"))' -C metadata=f9cb403ccf9aafe3 -C extra-filename=-f9cb403ccf9aafe3 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling ryu v1.0.18\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name ryu --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/ryu-1.0.18/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"no-panic\", \"small\"))' -C metadata=9fe2ffa6563da72a -C extra-filename=-9fe2ffa6563da72a --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling strsim v0.8.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name strsim --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/strsim-0.8.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=25f5b22ddc7add9b -C extra-filename=-25f5b22ddc7add9b --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling ndarray v0.13.1\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/ndarray-0.13.1/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"approx\", \"blas\", \"blas-src\", \"cblas-sys\", \"docs\", \"rayon\", \"serde\", \"serde-1\", \"test\", \"test-blas-openblas-sys\"))' -C metadata=ffd10b2b084df0ee -C extra-filename=-ffd10b2b084df0ee --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/ndarray-ffd10b2b084df0ee -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling itoa v1.0.11\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name itoa --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/itoa-1.0.11/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"no-panic\"))' -C metadata=5ca595a7c9604cbf -C extra-filename=-5ca595a7c9604cbf --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling once_cell v1.19.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name once_cell --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/once_cell-1.19.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"alloc\"' --cfg 'feature=\"default\"' --cfg 'feature=\"race\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"alloc\", \"atomic-polyfill\", \"critical-section\", \"default\", \"parking_lot\", \"portable-atomic\", \"race\", \"std\", \"unstable\"))' -C metadata=9e854eed23737148 -C extra-filename=-9e854eed23737148 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling log v0.4.22\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name log --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/log-0.4.22/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"kv\", \"kv_serde\", \"kv_std\", \"kv_sval\", \"kv_unstable\", \"kv_unstable_serde\", \"kv_unstable_std\", \"kv_unstable_sval\", \"max_level_debug\", \"max_level_error\", \"max_level_info\", \"max_level_off\", \"max_level_trace\", \"max_level_warn\", \"release_max_level_debug\", \"release_max_level_error\", \"release_max_level_info\", \"release_max_level_off\", \"release_max_level_trace\", \"release_max_level_warn\", \"serde\", \"std\", \"sval\", \"sval_ref\", \"value-bag\"))' -C metadata=fee03072b48908b6 -C extra-filename=-fee03072b48908b6 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name unindent --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/unindent-0.1.11/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=78354fc3d1163902 -C extra-filename=-78354fc3d1163902 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling quick-error v1.2.3\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name quick_error --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/quick-error-1.2.3/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=83540d84ed6b4b87 -C extra-filename=-83540d84ed6b4b87 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling ansi_term v0.12.1\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name ansi_term --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/ansi_term-0.12.1/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"derive_serde_style\", \"serde\"))' -C metadata=9697d021dfc96b25 -C extra-filename=-9697d021dfc96b25 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling humantime v1.3.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name humantime --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/humantime-1.3.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=b17c493756e9b200 -C extra-filename=-b17c493756e9b200 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern quick_error=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libquick_error-83540d84ed6b4b87.rmeta --cap-lints allow`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name pyo3 --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/pyo3-0.12.4/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"ctor\"' --cfg 'feature=\"default\"' --cfg 'feature=\"extension-module\"' --cfg 'feature=\"indoc\"' --cfg 'feature=\"inventory\"' --cfg 'feature=\"macros\"' --cfg 'feature=\"paste\"' --cfg 'feature=\"pyo3cls\"' --cfg 'feature=\"unindent\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"ctor\", \"default\", \"extension-module\", \"hashbrown\", \"indoc\", \"inventory\", \"macros\", \"nightly\", \"num-bigint\", \"num-complex\", \"paste\", \"pyo3cls\", \"python3\", \"unindent\"))' -C metadata=95ae9d9bf4e7ce9e -C extra-filename=-95ae9d9bf4e7ce9e --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern ctor=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libctor-957f2898f4676888.so --extern indoc=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libindoc-5ae5844505251d95.rmeta --extern inventory=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libinventory-01732795dcd7bb16.rmeta --extern libc=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/liblibc-8262032f1e05c089.rmeta --extern parking_lot=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libparking_lot-6389d506bd23dbb3.rmeta --extern paste=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libpaste-77cfc92c5c84a892.rmeta --extern pyo3cls=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libpyo3cls-5a5ad2657c3500fc.so --extern unindent=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libunindent-78354fc3d1163902.rmeta --cap-lints allow --cfg Py_3_5 --cfg Py_3_6 --cfg Py_3_7 --cfg Py_3_8 --cfg Py_3_9 --cfg Py_3_10 --cfg Py_3 --cfg 'py_sys_config=\"WITH_THREAD\"'`\n         Compiling clap v2.34.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name clap --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/clap-2.34.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"ansi_term\"' --cfg 'feature=\"atty\"' --cfg 'feature=\"color\"' --cfg 'feature=\"default\"' --cfg 'feature=\"strsim\"' --cfg 'feature=\"suggestions\"' --cfg 'feature=\"vec_map\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"ansi_term\", \"atty\", \"clippy\", \"color\", \"debug\", \"default\", \"doc\", \"nightly\", \"no_cargo\", \"strsim\", \"suggestions\", \"term_size\", \"unstable\", \"vec_map\", \"wrap_help\", \"yaml\", \"yaml-rust\"))' -C metadata=8b73c3ff02b2061a -C extra-filename=-8b73c3ff02b2061a --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern ansi_term=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libansi_term-9697d021dfc96b25.rmeta --extern atty=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libatty-824de6acfdffc9b1.rmeta --extern bitflags=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libbitflags-572d549a09e7b042.rmeta --extern strsim=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libstrsim-25f5b22ddc7add9b.rmeta --extern textwrap=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libtextwrap-b47182d7f07a0e83.rmeta --extern unicode_width=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libunicode_width-147e4b4395044b58.rmeta --extern vec_map=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libvec_map-fa32a1cd80d7fcbc.rmeta --cap-lints allow`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name serde_json --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/serde_json-1.0.125/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"alloc\", \"arbitrary_precision\", \"default\", \"float_roundtrip\", \"indexmap\", \"preserve_order\", \"raw_value\", \"std\", \"unbounded_depth\"))' -C metadata=fa4c0abbd1ca3397 -C extra-filename=-fa4c0abbd1ca3397 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern itoa=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libitoa-5ca595a7c9604cbf.rmeta --extern memchr=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libmemchr-98aad6ebcffcf120.rmeta --extern ryu=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libryu-9fe2ffa6563da72a.rmeta --extern serde=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libserde-05c41278f1822ffa.rmeta --cap-lints allow --cfg limb_width_64 --check-cfg 'cfg(limb_width_32)' --check-cfg 'cfg(limb_width_64)'`\n           Running `/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/ndarray-ffd10b2b084df0ee/build-script-build`\n         Compiling spm_precompiled v0.1.4\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name spm_precompiled --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/spm_precompiled-0.1.4/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=43a0b58c3a4cde0e -C extra-filename=-43a0b58c3a4cde0e --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern base64=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libbase64-835cd68726307e54.rmeta --extern nom=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libnom-8a084cf9e98aef2c.rmeta --extern serde=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libserde-05c41278f1822ffa.rmeta --extern unicode_segmentation=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libunicode_segmentation-f9cb403ccf9aafe3.rmeta --cap-lints allow`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name onig_sys --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/onig_sys-69.8.1/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"bindgen\", \"default\", \"generate\", \"posix-api\", \"print-debug\"))' -C metadata=b402d193bca184db -C extra-filename=-b402d193bca184db --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow -L native=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/onig_sys-1f90ae5af86047a8/out -l static=onig`\n         Compiling onig v6.4.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name onig --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/onig-6.4.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"generate\", \"posix-api\", \"print-debug\", \"std-pattern\"))' -C metadata=ac548520ce4f12f7 -C extra-filename=-ac548520ce4f12f7 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern bitflags=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libbitflags-572d549a09e7b042.rmeta --extern once_cell=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libonce_cell-9e854eed23737148.rmeta --extern onig_sys=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libonig_sys-b402d193bca184db.rmeta --cap-lints allow -L native=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/onig_sys-1f90ae5af86047a8/out`\n         Compiling ndarray v0.16.1\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name ndarray --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/ndarray-0.16.1/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"approx\", \"blas\", \"default\", \"docs\", \"matrixmultiply-threading\", \"portable-atomic-critical-section\", \"rayon\", \"serde\", \"serde-1\", \"std\", \"test\"))' -C metadata=9cdbb7e178a9dab9 -C extra-filename=-9cdbb7e178a9dab9 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern matrixmultiply=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libmatrixmultiply-fef05d356b78315f.rmeta --extern num_complex=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libnum_complex-4bce8ee361c8cbb5.rmeta --extern num_integer=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libnum_integer-6b9813fd539b3b7a.rmeta --extern num_traits=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libnum_traits-04f24cb06d3c2fbe.rmeta --extern rawpointer=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/librawpointer-25344ca069a916c0.rmeta --cap-lints allow`\n         Compiling indicatif v0.15.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name indicatif --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/indicatif-0.15.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"improved_unicode\", \"rayon\", \"unicode-segmentation\", \"unicode-width\", \"with_rayon\"))' -C metadata=92e5708bfdd21479 -C extra-filename=-92e5708bfdd21479 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern console=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libconsole-7389811a5688094b.rmeta --extern lazy_static=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/liblazy_static-b8cd5ec493744c98.rmeta --extern number_prefix=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libnumber_prefix-47262f61f1c6a366.rmeta --extern regex=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libregex-883f132f16a8c2d0.rmeta --cap-lints allow`\n         Compiling rayon-cond v0.1.0 (https://github.com/n1t0/rayon-cond#c56e4f1d)\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name rayon_cond --edition=2015 /root/.cargo/git/checkouts/rayon-cond-7ecee7da88eb486f/c56e4f1/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=825f7e3b4ea61273 -C extra-filename=-825f7e3b4ea61273 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern either=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libeither-cbb2d4c5b75a4107.rmeta --extern itertools=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libitertools-23c94ea62554b785.rmeta --extern rayon=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/librayon-8704d374a66af352.rmeta --cap-lints allow`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name esaxx_rs --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/esaxx-rs-0.1.10/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"cc\"' --cfg 'feature=\"cpp\"' --cfg 'feature=\"default\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"cc\", \"cpp\", \"default\"))' -C metadata=37214f5b5ebdfb66 -C extra-filename=-37214f5b5ebdfb66 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow -L native=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/esaxx-rs-f18adfc51d566de3/out -l static=esaxx -l stdc++`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name derive_builder --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/derive_builder-0.9.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type proc-macro --emit=dep-info,link -C prefer-dynamic -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"compiletest_rs\", \"env_logger\", \"log\", \"logging\", \"nightlytests\", \"skeptic\", \"skeptic_tests\"))' -C metadata=1858b2f9319ccc12 -C extra-filename=-1858b2f9319ccc12 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern darling=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libdarling-ad121ed9a36f1792.rlib --extern derive_builder_core=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libderive_builder_core-e7db7296aa9f92b1.rlib --extern proc_macro2=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libproc_macro2-a53443ded1dfcef2.rlib --extern quote=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libquote-4c220be45cf63bba.rlib --extern syn=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libsyn-3a67f5675a1f15d1.rlib --extern proc_macro --cap-lints allow`\n         Compiling rand v0.7.3\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name rand --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rand-0.7.3/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"alloc\"' --cfg 'feature=\"default\"' --cfg 'feature=\"getrandom\"' --cfg 'feature=\"getrandom_package\"' --cfg 'feature=\"libc\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"alloc\", \"default\", \"getrandom\", \"getrandom_package\", \"libc\", \"log\", \"nightly\", \"packed_simd\", \"rand_pcg\", \"serde1\", \"simd_support\", \"small_rng\", \"std\", \"stdweb\", \"wasm-bindgen\"))' -C metadata=d987cd4e82b0315f -C extra-filename=-d987cd4e82b0315f --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern getrandom_package=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libgetrandom-8d6eb4053cea3593.rmeta --extern libc=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/liblibc-8262032f1e05c089.rmeta --extern rand_chacha=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/librand_chacha-d82021d0089f938a.rmeta --extern rand_core=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/librand_core-eb45dce8fe3856ac.rmeta --cap-lints allow`\n         Compiling matrixmultiply v0.2.4\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name matrixmultiply --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/matrixmultiply-0.2.4/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"std\"))' -C metadata=e81e1bb8e77793e1 -C extra-filename=-e81e1bb8e77793e1 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern rawpointer=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/librawpointer-25344ca069a916c0.rmeta --cap-lints allow`\n         Compiling itertools v0.9.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name itertools --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/itertools-0.9.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"use_std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"use_std\"))' -C metadata=120219380b521206 -C extra-filename=-120219380b521206 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern either=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libeither-cbb2d4c5b75a4107.rmeta --cap-lints allow`\n         Compiling unicode-normalization-alignments v0.1.12\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name unicode_normalization_alignments --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/unicode-normalization-alignments-0.1.12/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=63bf7301ba1e71a4 -C extra-filename=-63bf7301ba1e71a4 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern smallvec=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libsmallvec-54b60278d2fd4af7.rmeta --cap-lints allow`\n         Compiling regex-syntax v0.6.29\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name regex_syntax --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/regex-syntax-0.6.29/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"unicode\"' --cfg 'feature=\"unicode-age\"' --cfg 'feature=\"unicode-bool\"' --cfg 'feature=\"unicode-case\"' --cfg 'feature=\"unicode-gencat\"' --cfg 'feature=\"unicode-perl\"' --cfg 'feature=\"unicode-script\"' --cfg 'feature=\"unicode-segment\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"unicode\", \"unicode-age\", \"unicode-bool\", \"unicode-case\", \"unicode-gencat\", \"unicode-perl\", \"unicode-script\", \"unicode-segment\"))' -C metadata=436fa53568c5ecc4 -C extra-filename=-436fa53568c5ecc4 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling unicode_categories v0.1.1\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name unicode_categories --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/unicode_categories-0.1.1/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=6cc42eb7547cbd30 -C extra-filename=-6cc42eb7547cbd30 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling termcolor v1.4.1\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name termcolor --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/termcolor-1.4.1/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=91817b919abf241e -C extra-filename=-91817b919abf241e --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n         Compiling cfg-if v0.1.10\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name cfg_if --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/cfg-if-0.1.10/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"compiler_builtins\", \"core\", \"rustc-dep-of-std\"))' -C metadata=61526e3ac0a5d11a -C extra-filename=-61526e3ac0a5d11a --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --cap-lints allow`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name ndarray --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/ndarray-0.13.1/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"approx\", \"blas\", \"blas-src\", \"cblas-sys\", \"docs\", \"rayon\", \"serde\", \"serde-1\", \"test\", \"test-blas-openblas-sys\"))' -C metadata=8406a1ca7eb20aaa -C extra-filename=-8406a1ca7eb20aaa --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern matrixmultiply=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libmatrixmultiply-e81e1bb8e77793e1.rmeta --extern num_complex=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libnum_complex-88ee02742315b6d2.rmeta --extern num_integer=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libnum_integer-6b9813fd539b3b7a.rmeta --extern num_traits=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libnum_traits-04f24cb06d3c2fbe.rmeta --extern rawpointer=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/librawpointer-25344ca069a916c0.rmeta --cap-lints allow`\n         Compiling numpy v0.11.0 (https://github.com/pyo3/rust-numpy/?rev=e331befa27fede78d4662edf08fa0508db39be01#e331befa)\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name numpy --edition=2018 /root/.cargo/git/checkouts/rust-numpy-ea13d29ac0b89ab6/e331bef/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"python3\"))' -C metadata=8dbec0b557a24b63 -C extra-filename=-8dbec0b557a24b63 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern cfg_if=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libcfg_if-61526e3ac0a5d11a.rmeta --extern libc=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/liblibc-8262032f1e05c089.rmeta --extern ndarray=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libndarray-9cdbb7e178a9dab9.rmeta --extern num_complex=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libnum_complex-88ee02742315b6d2.rmeta --extern num_traits=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libnum_traits-04f24cb06d3c2fbe.rmeta --extern pyo3=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libpyo3-95ae9d9bf4e7ce9e.rmeta --cap-lints allow`\n         Compiling env_logger v0.7.1\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name env_logger --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/env_logger-0.7.1/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"atty\"' --cfg 'feature=\"default\"' --cfg 'feature=\"humantime\"' --cfg 'feature=\"regex\"' --cfg 'feature=\"termcolor\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"atty\", \"default\", \"humantime\", \"regex\", \"termcolor\"))' -C metadata=25532a30b2e55d0b -C extra-filename=-25532a30b2e55d0b --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern atty=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libatty-824de6acfdffc9b1.rmeta --extern humantime=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libhumantime-b17c493756e9b200.rmeta --extern log=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/liblog-fee03072b48908b6.rmeta --extern regex=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libregex-883f132f16a8c2d0.rmeta --extern termcolor=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libtermcolor-91817b919abf241e.rmeta --cap-lints allow`\n         Compiling tokenizers v0.10.1 (/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/tokenizers-lib)\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name tokenizers --edition=2018 tokenizers-lib/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"indicatif\"' --cfg 'feature=\"progressbar\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"indicatif\", \"progressbar\"))' -C metadata=b8be9f1c19456a03 -C extra-filename=-b8be9f1c19456a03 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern clap=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libclap-8b73c3ff02b2061a.rmeta --extern derive_builder=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libderive_builder-1858b2f9319ccc12.so --extern esaxx_rs=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libesaxx_rs-37214f5b5ebdfb66.rmeta --extern indicatif=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libindicatif-92e5708bfdd21479.rmeta --extern itertools=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libitertools-120219380b521206.rmeta --extern lazy_static=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/liblazy_static-b8cd5ec493744c98.rmeta --extern log=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/liblog-fee03072b48908b6.rmeta --extern onig=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libonig-ac548520ce4f12f7.rmeta --extern rand=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/librand-d987cd4e82b0315f.rmeta --extern rayon=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/librayon-8704d374a66af352.rmeta --extern rayon_cond=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/librayon_cond-825f7e3b4ea61273.rmeta --extern regex=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libregex-883f132f16a8c2d0.rmeta --extern regex_syntax=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libregex_syntax-436fa53568c5ecc4.rmeta --extern serde=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libserde-05c41278f1822ffa.rmeta --extern serde_json=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libserde_json-fa4c0abbd1ca3397.rmeta --extern spm_precompiled=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libspm_precompiled-43a0b58c3a4cde0e.rmeta --extern unicode_normalization_alignments=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libunicode_normalization_alignments-63bf7301ba1e71a4.rmeta --extern unicode_segmentation=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libunicode_segmentation-f9cb403ccf9aafe3.rmeta --extern unicode_categories=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libunicode_categories-6cc42eb7547cbd30.rmeta -L native=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/esaxx-rs-f18adfc51d566de3/out -L native=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/onig_sys-1f90ae5af86047a8/out`\n      warning: `#[macro_use]` only has an effect on `extern crate` and modules\n        --> tokenizers-lib/src/utils/mod.rs:24:1\n         |\n      24 | #[macro_use]\n         | ^^^^^^^^^^^^\n         |\n         = note: `#[warn(unused_attributes)]` on by default\n      \n      warning: `#[macro_use]` only has an effect on `extern crate` and modules\n        --> tokenizers-lib/src/utils/mod.rs:35:1\n         |\n      35 | #[macro_use]\n         | ^^^^^^^^^^^^\n      \n      warning: variable does not need to be mutable\n         --> tokenizers-lib/src/models/unigram/model.rs:280:21\n          |\n      280 |                 let mut target_node = &mut best_path_ends_at[key_pos];\n          |                     ----^^^^^^^^^^^\n          |                     |\n          |                     help: remove this `mut`\n          |\n          = note: `#[warn(unused_mut)]` on by default\n      \n      warning: variable does not need to be mutable\n         --> tokenizers-lib/src/models/unigram/model.rs:297:21\n          |\n      297 |                 let mut target_node = &mut best_path_ends_at[starts_at + mblen];\n          |                     ----^^^^^^^^^^^\n          |                     |\n          |                     help: remove this `mut`\n      \n      warning: variable does not need to be mutable\n         --> tokenizers-lib/src/pre_tokenizers/byte_level.rs:175:59\n          |\n      175 |     encoding.process_tokens_with_offsets_mut(|(i, (token, mut offsets))| {\n          |                                                           ----^^^^^^^\n          |                                                           |\n          |                                                           help: remove this `mut`\n      \n      warning: fields `bos_id` and `eos_id` are never read\n        --> tokenizers-lib/src/models/unigram/lattice.rs:59:5\n         |\n      53 | pub struct Lattice<'a> {\n         |            ------- fields in this struct\n      ...\n      59 |     bos_id: usize,\n         |     ^^^^^^\n      60 |     eos_id: usize,\n         |     ^^^^^^\n         |\n         = note: `Lattice` has a derived impl for the trait `Debug`, but this is intentionally ignored during dead code analysis\n         = note: `#[warn(dead_code)]` on by default\n      \n      error: casting `&T` to `&mut T` is undefined behavior, even if the reference is unused, consider instead using an `UnsafeCell`\n         --> tokenizers-lib/src/models/bpe/trainer.rs:517:47\n          |\n      513 |                     let w = &words[*i] as *const _ as *mut _;\n          |                             -------------------------------- casting happend here\n      ...\n      517 |                         let word: &mut Word = &mut (*w);\n          |                                               ^^^^^^^^^\n          |\n          = note: for more information, visit <https://doc.rust-lang.org/book/ch15-05-interior-mutability.html>\n          = note: `#[deny(invalid_reference_casting)]` on by default\n      \n      warning: `tokenizers` (lib) generated 6 warnings\n      error: could not compile `tokenizers` (lib) due to 1 previous error; 6 warnings emitted\n      \n      Caused by:\n        process didn't exit successfully: `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name tokenizers --edition=2018 tokenizers-lib/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"indicatif\"' --cfg 'feature=\"progressbar\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"indicatif\", \"progressbar\"))' -C metadata=b8be9f1c19456a03 -C extra-filename=-b8be9f1c19456a03 --out-dir /tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps --extern clap=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libclap-8b73c3ff02b2061a.rmeta --extern derive_builder=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libderive_builder-1858b2f9319ccc12.so --extern esaxx_rs=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libesaxx_rs-37214f5b5ebdfb66.rmeta --extern indicatif=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libindicatif-92e5708bfdd21479.rmeta --extern itertools=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libitertools-120219380b521206.rmeta --extern lazy_static=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/liblazy_static-b8cd5ec493744c98.rmeta --extern log=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/liblog-fee03072b48908b6.rmeta --extern onig=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libonig-ac548520ce4f12f7.rmeta --extern rand=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/librand-d987cd4e82b0315f.rmeta --extern rayon=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/librayon-8704d374a66af352.rmeta --extern rayon_cond=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/librayon_cond-825f7e3b4ea61273.rmeta --extern regex=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libregex-883f132f16a8c2d0.rmeta --extern regex_syntax=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libregex_syntax-436fa53568c5ecc4.rmeta --extern serde=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libserde-05c41278f1822ffa.rmeta --extern serde_json=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libserde_json-fa4c0abbd1ca3397.rmeta --extern spm_precompiled=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libspm_precompiled-43a0b58c3a4cde0e.rmeta --extern unicode_normalization_alignments=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libunicode_normalization_alignments-63bf7301ba1e71a4.rmeta --extern unicode_segmentation=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libunicode_segmentation-f9cb403ccf9aafe3.rmeta --extern unicode_categories=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/deps/libunicode_categories-6cc42eb7547cbd30.rmeta -L native=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/esaxx-rs-f18adfc51d566de3/out -L native=/tmp/pip-install-3sth2ear/tokenizers_53ce053c21704350afdb25ed46eb6aab/target/release/build/onig_sys-1f90ae5af86047a8/out` (exit status: 1)\n      warning: build failed, waiting for other jobs to finish...\n      error: `cargo rustc --lib --message-format=json-render-diagnostics --manifest-path Cargo.toml --release -v --features pyo3/extension-module --crate-type cdylib --` failed with code 101\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\n  ERROR: Failed building wheel for tokenizers\n\u001b[?25hFailed to build tokenizers\nERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!. $HOME/.cargo/env && pip install transformers==4.11.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac5fc12",
   "metadata": {},
   "source": [
    "### Step 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9125fcd8",
   "metadata": {},
   "source": [
    "Execution Start time: 22:17:00, End time: 22:17:03 (duration: 3 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8ba43e",
   "metadata": {},
   "source": [
    "**Thought**: The installation of the `tokenizers` package is failing due to a compilation error in the Rust code. This issue is likely due to a version mismatch or an incompatibility with the current environment. Since the `tokenizers` package is a dependency of the `transformers` library, I will try installing a compatible version of the `transformers` library that does not require building `tokenizers` from source. This might involve using a more recent version of `transformers` that includes pre-built wheels for `tokenizers`. Let's try installing the latest version of `transformers` again to see if it resolves the issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5bded8",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd73956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Requirement already satisfied: transformers in /usr/local/lib/python3.10/site-packages (4.44.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/site-packages (from transformers) (0.23.3)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (2.0.7)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install transformers --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1b8d98",
   "metadata": {},
   "source": [
    "### Step 22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30284fdd",
   "metadata": {},
   "source": [
    "Execution Start time: 22:17:11, End time: 22:17:16 (duration: 4 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9208a2a",
   "metadata": {},
   "source": [
    "**Thought**: The `transformers` library has been upgraded to the latest version, and the `tokenizers` package is now installed successfully. I will now rerun the training script to train the `roberta-base` model on the RTE dataset using the UPET method. After the training completes, I will report the evaluation accuracy as requested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589987af",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb25bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n08/19/2024 22:17:15 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0distributed training: True, 16-bits training: False\nTraceback (most recent call last):\n  File \"/content/UPET/run.py\", line 101, in <module>\n    from tasks.superglue.get_trainer import get_trainer\n  File \"/content/UPET/tasks/superglue/get_trainer.py\", line 14, in <module>\n    from training.trainer_exp import ExponentialTrainer\n  File \"/content/UPET/training/trainer_exp.py\", line 16, in <module>\n    from transformers.trainer_utils import (\nImportError: cannot import name 'ShardedDDPOption' from 'transformers.trainer_utils' (/usr/local/lib/python3.10/site-packages/transformers/trainer_utils.py)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python UPET/run.py --model_name_or_path roberta-base --task_name superglue --dataset_name rte --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 8 --learning_rate 2e-5 --num_train_epochs 1 --output_dir /tmp/rte/ --overwrite_output_dir --seed 42 --num_examples_per_label 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82a17b9",
   "metadata": {},
   "source": [
    "### Step 23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6851dd4",
   "metadata": {},
   "source": [
    "Execution Start time: 22:17:18, End time: 22:17:18 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c0b074",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea792c6",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3d39a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73406fb9",
   "metadata": {},
   "source": [
    "### Step 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf29c217",
   "metadata": {},
   "source": [
    "Execution Start time: 22:17:20, End time: 22:17:20 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d686fe09",
   "metadata": {},
   "source": [
    "**Thought**: The import error persists, indicating that the code in the repository is incompatible with the current version of the `transformers` library. Since downgrading to the specified version (4.11.3) failed due to issues with the `tokenizers` package, I will attempt to modify the code to remove or replace the problematic import. Specifically, I will check the `trainer_exp.py` file and see if I can comment out or replace the import of `ShardedDDPOption` to allow the script to run successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2995e0e8",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f0bc8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import logging\nimport os\nimport random\nimport sys\n\nfrom typing import Any, Dict, List, Optional, OrderedDict, Tuple, Union\nimport math\nimport random\nimport time\nimport warnings\nimport collections\n\nfrom transformers.debug_utils import DebugOption, DebugUnderflowOverflow\nfrom transformers.trainer_callback import TrainerState\nfrom transformers.trainer_pt_utils import IterableDatasetShard\nfrom transformers.trainer_utils import (\n    HPSearchBackend,\n    ShardedDDPOption,\n    TrainOutput,\n    get_last_checkpoint,\n    set_seed,\n    speed_metrics,\n)\nfrom transformers.file_utils import (\n    CONFIG_NAME,\n    WEIGHTS_NAME,\n    is_torch_tpu_available,\n)\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.distributed import DistributedSampler\n\nfrom training.trainer_base import BaseTrainer, logger\n\n\nclass ExponentialTrainer(BaseTrainer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def create_scheduler(self, num_training_steps: int, optimizer: torch.optim.Optimizer = None):\n        if self.lr_scheduler is None:\n            self.lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=0.95, verbose=True)\n        return self.lr_scheduler\n\n\n    def train(\n        self,\n        resume_from_checkpoint: Optional[Union[str, bool]] = None,\n        trial: Union[\"optuna.Trial\", Dict[str, Any]] = None,\n        ignore_keys_for_eval: Optional[List[str]] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Main training entry point.\n        Args:\n            resume_from_checkpoint (:obj:`str` or :obj:`bool`, `optional`):\n                If a :obj:`str`, local path to a saved checkpoint as saved by a previous instance of\n                :class:`~transformers.Trainer`. If a :obj:`bool` and equals `True`, load the last checkpoint in\n                `args.output_dir` as saved by a previous instance of :class:`~transformers.Trainer`. If present,\n                training will resume from the model/optimizer/scheduler states loaded here.\n            trial (:obj:`optuna.Trial` or :obj:`Dict[str, Any]`, `optional`):\n                The trial run or the hyperparameter dictionary for hyperparameter search.\n            ignore_keys_for_eval (:obj:`List[str]`, `optional`)\n                A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n                gathering predictions for evaluation during the training.\n            kwargs:\n                Additional keyword arguments used to hide deprecated arguments\n        \"\"\"\n        resume_from_checkpoint = None if not resume_from_checkpoint else resume_from_checkpoint\n\n        # memory metrics - must set up as early as possible\n        self._memory_tracker.start()\n\n        args = self.args\n\n        self.is_in_train = True\n\n        # do_train is not a reliable argument, as it might not be set and .train() still called, so\n        # the following is a workaround:\n        if args.fp16_full_eval and not args.do_train:\n            self._move_model_to_device(self.model, args.device)\n\n        if \"model_path\" in kwargs:\n            resume_from_checkpoint = kwargs.pop(\"model_path\")\n            warnings.warn(\n                \"`model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` \"\n                \"instead.\",\n                FutureWarning,\n            )\n        if len(kwargs) > 0:\n            raise TypeError(f\"train() received got unexpected keyword arguments: {', '.join(list(kwargs.keys()))}.\")\n        # This might change the seed so needs to run first.\n        self._hp_search_setup(trial)\n\n        # Model re-init\n        model_reloaded = False\n        if self.model_init is not None:\n            # Seed must be set before instantiating the model when using model_init.\n            set_seed(args.seed)\n            self.model = self.call_model_init(trial)\n            model_reloaded = True\n            # Reinitializes optimizer and scheduler\n            self.optimizer, self.lr_scheduler = None, None\n\n        # Load potential model checkpoint\n        if isinstance(resume_from_checkpoint, bool) and resume_from_checkpoint:\n            resume_from_checkpoint = get_last_checkpoint(args.output_dir)\n            if resume_from_checkpoint is None:\n                raise ValueError(f\"No valid checkpoint found in output directory ({args.output_dir})\")\n\n        if resume_from_checkpoint is not None:\n            if not os.path.isfile(os.path.join(resume_from_checkpoint, WEIGHTS_NAME)):\n                raise ValueError(f\"Can't find a valid checkpoint at {resume_from_checkpoint}\")\n\n            logger.info(f\"Loading model from {resume_from_checkpoint}).\")\n\n            if os.path.isfile(os.path.join(resume_from_checkpoint, CONFIG_NAME)):\n                config = PretrainedConfig.from_json_file(os.path.join(resume_from_checkpoint, CONFIG_NAME))\n                checkpoint_version = config.transformers_version\n                if checkpoint_version is not None and checkpoint_version != __version__:\n                    logger.warn(\n                        f\"You are resuming training from a checkpoint trained with {checkpoint_version} of \"\n                        f\"Transformers but your current version is {__version__}. This is not recommended and could \"\n                        \"yield to errors or unwanted behaviors.\"\n                    )\n\n            if args.deepspeed:\n                # will be resumed in deepspeed_init\n                pass\n            else:\n                # We load the model state dict on the CPU to avoid an OOM error.\n                state_dict = torch.load(os.path.join(resume_from_checkpoint, WEIGHTS_NAME), map_location=\"cpu\")\n                # If the model is on the GPU, it still works!\n                self._load_state_dict_in_model(state_dict)\n\n                # release memory\n                del state_dict\n\n        # If model was re-initialized, put it on the right device and update self.model_wrapped\n        if model_reloaded:\n            if self.place_model_on_device:\n                self._move_model_to_device(self.model, args.device)\n            self.model_wrapped = self.model\n\n        # Keeping track whether we can can len() on the dataset or not\n        train_dataset_is_sized = isinstance(self.train_dataset, collections.abc.Sized)\n\n        # Data loader and number of training steps\n        train_dataloader = self.get_train_dataloader()\n\n        # Setting up training control variables:\n        # number of training epochs: num_train_epochs\n        # number of training steps per epoch: num_update_steps_per_epoch\n        # total number of training steps to execute: max_steps\n        total_train_batch_size = args.train_batch_size * args.gradient_accumulation_steps * args.world_size\n        if train_dataset_is_sized:\n            num_update_steps_per_epoch = len(train_dataloader) // args.gradient_accumulation_steps\n            num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)\n            if args.max_steps > 0:\n                max_steps = args.max_steps\n                num_train_epochs = args.max_steps // num_update_steps_per_epoch + int(\n                    args.max_steps % num_update_steps_per_epoch > 0\n                )\n                # May be slightly incorrect if the last batch in the training datalaoder has a smaller size but it's\n                # the best we can do.\n                num_train_samples = args.max_steps * total_train_batch_size\n            else:\n                max_steps = math.ceil(args.num_train_epochs * num_update_steps_per_epoch)\n                num_train_epochs = math.ceil(args.num_train_epochs)\n                num_train_samples = len(self.train_dataset) * args.num_train_epochs\n        else:\n            # see __init__. max_steps is set when the dataset has no __len__\n            max_steps = args.max_steps\n            # Setting a very large number of epochs so we go as many times as necessary over the iterator.\n            num_train_epochs = sys.maxsize\n            num_update_steps_per_epoch = max_steps\n            num_train_samples = args.max_steps * total_train_batch_size\n\n        if DebugOption.UNDERFLOW_OVERFLOW in self.args.debug:\n            if self.args.n_gpu > 1:\n                # nn.DataParallel(model) replicates the model, creating new variables and module\n                # references registered here no longer work on other gpus, breaking the module\n                raise ValueError(\n                    \"Currently --debug underflow_overflow is not supported under DP. Please use DDP (torch.distributed.launch).\"\n                )\n            else:\n                debug_overflow = DebugUnderflowOverflow(self.model)  # noqa\n\n        delay_optimizer_creation = self.sharded_ddp is not None and self.sharded_ddp != ShardedDDPOption.SIMPLE\n        if args.deepspeed:\n            deepspeed_engine, optimizer, lr_scheduler = deepspeed_init(\n                self, num_training_steps=max_steps, resume_from_checkpoint=resume_from_checkpoint\n            )\n            self.model = deepspeed_engine.module\n            self.model_wrapped = deepspeed_engine\n            self.deepspeed = deepspeed_engine\n            self.optimizer = optimizer\n            self.lr_scheduler = lr_scheduler\n        elif not delay_optimizer_creation:\n            self.create_optimizer_and_scheduler(num_training_steps=max_steps)\n\n        self.state = TrainerState()\n        self.state.is_hyper_param_search = trial is not None\n\n        # Activate gradient checkpointing if needed\n        if args.gradient_checkpointing:\n            self.model.gradient_checkpointing_enable()\n\n        model = self._wrap_model(self.model_wrapped)\n\n        # for the rest of this function `model` is the outside model, whether it was wrapped or not\n        if model is not self.model:\n            self.model_wrapped = model\n\n        if delay_optimizer_creation:\n            self.create_optimizer_and_scheduler(num_training_steps=max_steps)\n\n        # Check if saved optimizer or scheduler states exist\n        self._load_optimizer_and_scheduler(resume_from_checkpoint)\n\n        # important: at this point:\n        # self.model         is the Transformers Model\n        # self.model_wrapped is DDP(Transformers Model), Deepspeed(Transformers Model), etc.\n\n        # Train!\n        num_examples = (\n            self.num_examples(train_dataloader) if train_dataset_is_sized else total_train_batch_size * args.max_steps\n        )\n\n        logger.info(\"***** Running training *****\")\n        logger.info(f\"  Num examples = {num_examples}\")\n        logger.info(f\"  Num Epochs = {num_train_epochs}\")\n        logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n        logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_train_batch_size}\")\n        logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n        logger.info(f\"  Total optimization steps = {max_steps}\")\n\n        self.state.epoch = 0\n        start_time = time.time()\n        epochs_trained = 0\n        steps_trained_in_current_epoch = 0\n        steps_trained_progress_bar = None\n\n        # Check if continuing training from a checkpoint\n        if resume_from_checkpoint is not None and os.path.isfile(\n            os.path.join(resume_from_checkpoint, TRAINER_STATE_NAME)\n        ):\n            self.state = TrainerState.load_from_json(os.path.join(resume_from_checkpoint, TRAINER_STATE_NAME))\n            epochs_trained = self.state.global_step // num_update_steps_per_epoch\n            if not args.ignore_data_skip:\n                steps_trained_in_current_epoch = self.state.global_step % (num_update_steps_per_epoch)\n                steps_trained_in_current_epoch *= args.gradient_accumulation_steps\n            else:\n                steps_trained_in_current_epoch = 0\n\n            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n            logger.info(f\"  Continuing training from epoch {epochs_trained}\")\n            logger.info(f\"  Continuing training from global step {self.state.global_step}\")\n            if not args.ignore_data_skip:\n                logger.info(\n                    f\"  Will skip the first {epochs_trained} epochs then the first {steps_trained_in_current_epoch} \"\n                    \"batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` \"\n                    \"flag to your launch command, but you will resume the training on data already seen by your model.\"\n                )\n                if self.is_local_process_zero() and not args.disable_tqdm:\n                    steps_trained_progress_bar = tqdm(total=steps_trained_in_current_epoch)\n                    steps_trained_progress_bar.set_description(\"Skipping the first batches\")\n\n        # Update the references\n        self.callback_handler.model = self.model\n        self.callback_handler.optimizer = self.optimizer\n        self.callback_handler.lr_scheduler = self.lr_scheduler\n        self.callback_handler.train_dataloader = train_dataloader\n        self.state.trial_name = self.hp_name(trial) if self.hp_name is not None else None\n        if trial is not None:\n            assignments = trial.assignments if self.hp_search_backend == HPSearchBackend.SIGOPT else trial\n            self.state.trial_params = hp_params(assignments)\n        else:\n            self.state.trial_params = None\n        # This should be the same if the state has been saved but in case the training arguments changed, it's safer\n        # to set this after the load.\n        self.state.max_steps = max_steps\n        self.state.num_train_epochs = num_train_epochs\n        self.state.is_local_process_zero = self.is_local_process_zero()\n        self.state.is_world_process_zero = self.is_world_process_zero()\n\n        # tr_loss is a tensor to avoid synchronization of TPUs through .item()\n        tr_loss = torch.tensor(0.0).to(args.device)\n        # _total_loss_scalar is updated everytime .item() has to be called on tr_loss and stores the sum of all losses\n        self._total_loss_scalar = 0.0\n        self._globalstep_last_logged = self.state.global_step\n        model.zero_grad()\n\n        self.control = self.callback_handler.on_train_begin(args, self.state, self.control)\n\n        # Skip the first epochs_trained epochs to get the random state of the dataloader at the right point.\n        if not args.ignore_data_skip:\n            for epoch in range(epochs_trained):\n                # We just need to begin an iteration to create the randomization of the sampler.\n                for _ in train_dataloader:\n                    break\n\n\n        for epoch in range(epochs_trained, num_train_epochs):\n            if isinstance(train_dataloader, DataLoader) and isinstance(train_dataloader.sampler, DistributedSampler):\n                train_dataloader.sampler.set_epoch(epoch)\n            elif isinstance(train_dataloader.dataset, IterableDatasetShard):\n                train_dataloader.dataset.set_epoch(epoch)\n\n            if is_torch_tpu_available():\n                parallel_loader = pl.ParallelLoader(train_dataloader, [args.device]).per_device_loader(args.device)\n                epoch_iterator = parallel_loader\n            else:\n                epoch_iterator = train_dataloader\n\n            # Reset the past mems state at the beginning of each epoch if necessary.\n            if args.past_index >= 0:\n                self._past = None\n\n            steps_in_epoch = (\n                len(epoch_iterator) if train_dataset_is_sized else args.max_steps * args.gradient_accumulation_steps\n            )\n            self.control = self.callback_handler.on_epoch_begin(args, self.state, self.control)\n\n            step = -1\n            for step, inputs in enumerate(epoch_iterator):\n\n                # Skip past any already trained steps if resuming training\n                if steps_trained_in_current_epoch > 0:\n                    steps_trained_in_current_epoch -= 1\n                    if steps_trained_progress_bar is not None:\n                        steps_trained_progress_bar.update(1)\n                    if steps_trained_in_current_epoch == 0:\n                        self._load_rng_state(resume_from_checkpoint)\n                    continue\n                elif steps_trained_progress_bar is not None:\n                    steps_trained_progress_bar.close()\n                    steps_trained_progress_bar = None\n\n                if step % args.gradient_accumulation_steps == 0:\n                    self.control = self.callback_handler.on_step_begin(args, self.state, self.control)\n\n                if (\n                    ((step + 1) % args.gradient_accumulation_steps != 0)\n                    and args.local_rank != -1\n                    and args._no_sync_in_gradient_accumulation\n                ):\n                    # Avoid unnecessary DDP synchronization since there will be no backward pass on this example.\n                    with model.no_sync():\n                        tr_loss_step = self.training_step(model, inputs)\n                else:\n                    tr_loss_step = self.training_step(model, inputs)\n\n                if (\n                    args.logging_nan_inf_filter\n                    and not is_torch_tpu_available()\n                    and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))\n                ):\n                    # if loss is nan or inf simply add the average of previous logged losses\n                    tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)\n                else:\n                    tr_loss += tr_loss_step\n\n                self.current_flos += float(self.floating_point_ops(inputs))\n\n                # Optimizer step for deepspeed must be called on every step regardless of the value of gradient_accumulation_steps\n                if self.deepspeed:\n                    self.deepspeed.step()\n\n                if (step + 1) % args.gradient_accumulation_steps == 0 or (\n                    # last step in epoch but step is always smaller than gradient_accumulation_steps\n                    steps_in_epoch <= args.gradient_accumulation_steps\n                    and (step + 1) == steps_in_epoch\n                ):\n                    # Gradient clipping\n                    if args.max_grad_norm is not None and args.max_grad_norm > 0 and not self.deepspeed:\n                        # deepspeed does its own clipping\n\n                        if self.do_grad_scaling:\n                            # AMP: gradients need unscaling\n                            self.scaler.unscale_(self.optimizer)\n\n                        if hasattr(self.optimizer, \"clip_grad_norm\"):\n                            # Some optimizers (like the sharded optimizer) have a specific way to do gradient clipping\n                            self.optimizer.clip_grad_norm(args.max_grad_norm)\n                        elif hasattr(model, \"clip_grad_norm_\"):\n                            # Some models (like FullyShardedDDP) have a specific way to do gradient clipping\n                            model.clip_grad_norm_(args.max_grad_norm)\n                        else:\n                            # Revert to normal clipping otherwise, handling Apex or full precision\n                            nn.utils.clip_grad_norm_(\n                                amp.master_params(self.optimizer) if self.use_apex else model.parameters(),\n                                args.max_grad_norm,\n                            )\n\n                    # Optimizer step\n                    optimizer_was_run = True\n                    if self.deepspeed:\n                        pass  # called outside the loop\n                    elif is_torch_tpu_available():\n                        xm.optimizer_step(self.optimizer)\n                    elif self.do_grad_scaling:\n                        scale_before = self.scaler.get_scale()\n                        self.scaler.step(self.optimizer)\n                        self.scaler.update()\n                        scale_after = self.scaler.get_scale()\n                        optimizer_was_run = scale_before <= scale_after\n                    else:\n                        self.optimizer.step()\n\n                    if optimizer_was_run and not self.deepspeed and (step + 1) == steps_in_epoch:\n                        self.lr_scheduler.step()\n\n                    model.zero_grad()\n                    self.state.global_step += 1\n                    self.state.epoch = epoch + (step + 1) / steps_in_epoch\n                    self.control = self.callback_handler.on_step_end(args, self.state, self.control)\n\n                    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n                else:\n                    self.control = self.callback_handler.on_substep_end(args, self.state, self.control)\n\n                if self.control.should_epoch_stop or self.control.should_training_stop:\n                    break\n            if step < 0:\n                logger.warning(\n                    f\"There seems to be not a single sample in your epoch_iterator, stopping training at step\"\n                    f\" {self.state.global_step}! This is expected if you're using an IterableDataset and set\"\n                    f\" num_steps ({max_steps}) higher than the number of available samples.\"\n                )\n                self.control.should_training_stop = True\n\n            self.control = self.callback_handler.on_epoch_end(args, self.state, self.control)\n            self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\n            if DebugOption.TPU_METRICS_DEBUG in self.args.debug:\n                if is_torch_tpu_available():\n                    # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\n                    xm.master_print(met.metrics_report())\n                else:\n                    logger.warning(\n                        \"You enabled PyTorch/XLA debug metrics but you don't have a TPU \"\n                        \"configured. Check your training configuration if this is unexpected.\"\n                    )\n            if self.control.should_training_stop:\n                break\n\n\n        if args.past_index and hasattr(self, \"_past\"):\n            # Clean the state at the end of training\n            delattr(self, \"_past\")\n\n        logger.info(\"\\n\\nTraining completed. Do not forget to share your model on huggingface.co/models =)\\n\\n\")\n        if args.load_best_model_at_end and self.state.best_model_checkpoint is not None:\n            # Wait for everyone to get here so we are sur the model has been saved by process 0.\n            if is_torch_tpu_available():\n                xm.rendezvous(\"load_best_model_at_end\")\n            elif args.local_rank != -1:\n                dist.barrier()\n\n            logger.info(\n                f\"Loading best model from {self.state.best_model_checkpoint} (score: {self.state.best_metric}).\"\n            )\n\n            best_model_path = os.path.join(self.state.best_model_checkpoint, WEIGHTS_NAME)\n            if os.path.exists(best_model_path):\n                # We load the model state dict on the CPU to avoid an OOM error.\n                state_dict = torch.load(best_model_path, map_location=\"cpu\")\n                # If the model is on the GPU, it still works!\n                self._load_state_dict_in_model(state_dict)\n            else:\n                logger.warn(\n                    f\"Could not locate the best model at {best_model_path}, if you are running a distributed training \"\n                    \"on multiple nodes, you should activate `--save_on_each_node`.\"\n                )\n\n            if self.deepspeed:\n                self.deepspeed.load_checkpoint(\n                    self.state.best_model_checkpoint, load_optimizer_states=False, load_lr_scheduler_states=False\n                )\n\n        # add remaining tr_loss\n        self._total_loss_scalar += tr_loss.item()\n        train_loss = self._total_loss_scalar / self.state.global_step\n\n        metrics = speed_metrics(\"train\", start_time, num_samples=num_train_samples, num_steps=self.state.max_steps)\n        self.store_flos()\n        metrics[\"total_flos\"] = self.state.total_flos\n        metrics[\"train_loss\"] = train_loss\n\n        self.is_in_train = False\n\n        self._memory_tracker.stop_and_update_metrics(metrics)\n        \n        self.log(metrics)\n\n        self.control = self.callback_handler.on_train_end(args, self.state, self.control)\n\n        \n        return TrainOutput(self.state.global_step, train_loss, metrics)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!cat UPET/training/trainer_exp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e82ff3",
   "metadata": {},
   "source": [
    "### Step 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6b969f",
   "metadata": {},
   "source": [
    "Execution Start time: 22:17:23, End time: 22:17:24 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7791f6",
   "metadata": {},
   "source": [
    "**Thought**: The `trainer_exp.py` file imports `ShardedDDPOption` from `transformers.trainer_utils`, which is causing the import error. Since this import is not used directly in the code, I can try commenting it out to see if the script can run without it. This might resolve the issue and allow the script to execute successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1499fe11",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b46405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\nimport collections\n\nfrom transformers.debug_utils import DebugOption, DebugUnderflowOverflow\nfrom transformers.trainer_callback import TrainerState\nfrom transformers.trainer_pt_utils import IterableDatasetShard\nfrom transformers.trainer_utils import (\n    HPSearchBackend,\n    # ShardedDDPOption,\n    TrainOutput,\n    get_last_checkpoint,\n    set_seed,\n    speed_metrics,\n)\nfrom transformers.file_utils import ("
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UPET/training/trainer_exp.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "    ShardedDDPOption,\n",
    "<<<AFTER_EDIT>>>\n",
    "    # ShardedDDPOption,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c3e074",
   "metadata": {},
   "source": [
    "### Step 26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476536c3",
   "metadata": {},
   "source": [
    "Execution Start time: 22:17:27, End time: 22:17:32 (duration: 4 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61635080",
   "metadata": {},
   "source": [
    "**Thought**: I have successfully commented out the import of `ShardedDDPOption` in the `trainer_exp.py` file. Now, I will rerun the training script to train the `roberta-base` model on the RTE dataset using the UPET method. After the training completes, I will report the evaluation accuracy as requested. This modification should allow the script to execute without encountering the import error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff82989e",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96901f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n08/19/2024 22:17:31 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0distributed training: True, 16-bits training: False\nTraceback (most recent call last):\n  File \"/content/UPET/run.py\", line 101, in <module>\n    from tasks.superglue.get_trainer import get_trainer\n  File \"/content/UPET/tasks/superglue/get_trainer.py\", line 14, in <module>\n    from training.trainer_exp import ExponentialTrainer\n  File \"/content/UPET/training/trainer_exp.py\", line 24, in <module>\n    from transformers.file_utils import (\nImportError: cannot import name 'is_torch_tpu_available' from 'transformers.file_utils' (/usr/local/lib/python3.10/site-packages/transformers/file_utils.py)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python UPET/run.py --model_name_or_path roberta-base --task_name superglue --dataset_name rte --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 8 --learning_rate 2e-5 --num_train_epochs 1 --output_dir /tmp/rte/ --overwrite_output_dir --seed 42 --num_examples_per_label 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b2baca",
   "metadata": {},
   "source": [
    "### Step 27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f4f57e",
   "metadata": {},
   "source": [
    "Execution Start time: 22:17:34, End time: 22:17:34 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb7ca1e",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d922033b",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cf9201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "903104ae",
   "metadata": {},
   "source": [
    "### Step 28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c996602",
   "metadata": {},
   "source": [
    "Execution Start time: 22:17:37, End time: 22:17:38 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796bf86b",
   "metadata": {},
   "source": [
    "**Thought**: The import error persists, indicating that the code in the repository is incompatible with the current version of the `transformers` library. Since downgrading to the specified version (4.11.3) failed due to issues with the `tokenizers` package, I will attempt to modify the code to remove or replace the problematic import. Specifically, I will check the `trainer_exp.py` file and see if I can comment out or replace the import of `is_torch_tpu_available` to allow the script to run successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e419ea6",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bdcf94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n    get_last_checkpoint,\n    set_seed,\n    speed_metrics,\n)\nfrom transformers.file_utils import (\n    CONFIG_NAME,\n    WEIGHTS_NAME,\n    # is_torch_tpu_available,\n)\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.distributed import DistributedSampler"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UPET/training/trainer_exp.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "    is_torch_tpu_available,\n",
    "<<<AFTER_EDIT>>>\n",
    "    # is_torch_tpu_available,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26572400",
   "metadata": {},
   "source": [
    "### Step 29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0ae127",
   "metadata": {},
   "source": [
    "Execution Start time: 22:17:39, End time: 22:17:39 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2d2fa0",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fcdfd5",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b45667f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b1b17a8",
   "metadata": {},
   "source": [
    "### Step 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe2b131",
   "metadata": {},
   "source": [
    "Execution Start time: 22:17:42, End time: 22:18:03 (duration: 20 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e61fa7",
   "metadata": {},
   "source": [
    "**Thought**: I have successfully commented out the import of `is_torch_tpu_available` in the `trainer_exp.py` file. Now, I will rerun the training script to train the `roberta-base` model on the RTE dataset using the UPET method. After the training completes, I will report the evaluation accuracy as requested. This modification should allow the script to execute without encountering the import error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194ff44b",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0ab7ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n08/19/2024 22:17:46 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0distributed training: True, 16-bits training: False\n\ntokenizer_config.json:   0%|                         | 0.00/25.0 [00:00<?, ?B/s]\ntokenizer_config.json: 100%|██████████████████| 25.0/25.0 [00:00<00:00, 217kB/s]\n\nconfig.json:   0%|                                    | 0.00/481 [00:00<?, ?B/s]\n\n\nvocab.json:   0%|                                    | 0.00/899k [00:00<?, ?B/s]\n\n\nmerges.txt:   0%|                                    | 0.00/456k [00:00<?, ?B/s]\n\n\ntokenizer.json:   0%|                               | 0.00/1.36M [00:00<?, ?B/s]\n\n/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n\nDownloading builder script:   0%|                   | 0.00/30.7k [00:00<?, ?B/s]\nDownloading builder script: 100%|███████████| 30.7k/30.7k [00:00<00:00, 924kB/s]\n\nDownloading readme:   0%|                           | 0.00/18.2k [00:00<?, ?B/s]\nDownloading readme: 100%|███████████████████| 18.2k/18.2k [00:00<00:00, 689kB/s]\nThe repository for super_glue contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/super_glue.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N] Traceback (most recent call last):\n  File \"/content/UPET/run.py\", line 128, in <module>\n    trainer, predict_dataset = get_trainer(args)\n  File \"/content/UPET/tasks/superglue/get_trainer.py\", line 36, in get_trainer\n    dataset = SuperGlueDataset(tokenizer, data_args, training_args, semi_training_args=semi_training_args, use_prompt=use_prompt)\n  File \"/content/UPET/tasks/superglue/dataset.py\", line 90, in __init__\n    raw_datasets = load_dataset(\"super_glue\", data_args.dataset_name)\n  File \"/usr/local/lib/python3.10/site-packages/datasets/load.py\", line 2606, in load_dataset\n    builder_instance = load_dataset_builder(\n  File \"/usr/local/lib/python3.10/site-packages/datasets/load.py\", line 2277, in load_dataset_builder\n    dataset_module = dataset_module_factory(\n  File \"/usr/local/lib/python3.10/site-packages/datasets/load.py\", line 1923, in dataset_module_factory\n    raise e1 from None\n  File \"/usr/local/lib/python3.10/site-packages/datasets/load.py\", line 1896, in dataset_module_factory\n    ).get_module()\n  File \"/usr/local/lib/python3.10/site-packages/datasets/load.py\", line 1532, in get_module\n    trust_remote_code = resolve_trust_remote_code(self.trust_remote_code, self.name)\n  File \"/usr/local/lib/python3.10/site-packages/datasets/load.py\", line 134, in resolve_trust_remote_code\n    raise ValueError(\nValueError: The repository for super_glue contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/super_glue.\nPlease pass the argument `trust_remote_code=True` to allow custom code to be run.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python UPET/run.py --model_name_or_path roberta-base --task_name superglue --dataset_name rte --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 8 --learning_rate 2e-5 --num_train_epochs 1 --output_dir /tmp/rte/ --overwrite_output_dir --seed 42 --num_examples_per_label 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c25ad39",
   "metadata": {},
   "source": [
    "### Step 31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e419f8",
   "metadata": {},
   "source": [
    "Execution Start time: 22:18:06, End time: 22:18:06 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a819cc1b",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37edc340",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14364cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7b66a17",
   "metadata": {},
   "source": [
    "### Step 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a99599",
   "metadata": {},
   "source": [
    "Execution Start time: 22:18:14, End time: 22:18:19 (duration: 4 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0288e3e8",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the dataset repository for `super_glue` contains custom code that needs to be executed to load the dataset correctly. To resolve this, I need to pass the argument `trust_remote_code=True` when loading the dataset. I will modify the command to include this argument and rerun the training script. This should allow the script to execute successfully and report the evaluation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47670f1a",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc5924c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nTraceback (most recent call last):\n  File \"/content/UPET/run.py\", line 71, in <module>\n    args = get_args()\n  File \"/content/UPET/arguments.py\", line 326, in get_args\n    args = parser.parse_args_into_dataclasses()\n  File \"/usr/local/lib/python3.10/site-packages/transformers/hf_argparser.py\", line 348, in parse_args_into_dataclasses\n    raise ValueError(f\"Some specified arguments are not used by the HfArgumentParser: {remaining_args}\")\nValueError: Some specified arguments are not used by the HfArgumentParser: ['--trust_remote_code', 'True']\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python UPET/run.py --model_name_or_path roberta-base --task_name superglue --dataset_name rte --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 8 --learning_rate 2e-5 --num_train_epochs 1 --output_dir /tmp/rte/ --overwrite_output_dir --seed 42 --num_examples_per_label 5 --trust_remote_code True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df378fe6",
   "metadata": {},
   "source": [
    "### Step 33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a127c5f",
   "metadata": {},
   "source": [
    "Execution Start time: 22:18:20, End time: 22:18:20 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bafb8fe",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fa12cf",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64632fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f7b2177",
   "metadata": {},
   "source": [
    "### Step 34"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d4ac93",
   "metadata": {},
   "source": [
    "Execution Start time: 22:18:24, End time: 22:18:25 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102fd340",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the `--trust_remote_code` argument is not recognized by the `HfArgumentParser`. This suggests that the argument needs to be passed directly to the dataset loading function rather than as a command-line argument. I will need to modify the code in the `run.py` script to include this argument when loading the dataset. Let's first inspect the `run.py` script to identify where the dataset is being loaded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0585be1",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6ca606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import logging\nimport os\nimport sys\nimport numpy as np\nfrom typing import Dict\n\nimport datasets\nimport transformers\nfrom transformers import set_seed, Trainer\nfrom transformers.trainer_utils import get_last_checkpoint\n\nfrom arguments import get_args\n\nfrom tasks.utils import *\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\nlogger = logging.getLogger(__name__)\n\ndef train(trainer, resume_from_checkpoint=None, last_checkpoint=None):\n    checkpoint = None\n    if resume_from_checkpoint is not None:\n        checkpoint = resume_from_checkpoint\n    elif last_checkpoint is not None:\n        checkpoint = last_checkpoint\n    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n    # trainer.save_model()，\n    \n    try:\n        metrics = train_result.metrics\n\n        trainer.log_metrics(\"train\", metrics)\n        trainer.save_metrics(\"train\", metrics)\n        trainer.save_state()\n\n        trainer.log_best_metrics()\n    except:\n        pass\n\ndef evaluate(trainer):\n    logger.info(\"*** Evaluate ***\")\n    metrics = trainer.evaluate()\n\n    trainer.log_metrics(\"eval\", metrics)\n    trainer.save_metrics(\"eval\", metrics)\n\ndef predict(trainer, predict_dataset=None):\n    if predict_dataset is None:\n        logger.info(\"No dataset is available for testing\")\n\n    elif isinstance(predict_dataset, dict):\n        \n        for dataset_name, d in predict_dataset.items():\n            logger.info(\"*** Predict: %s ***\" % dataset_name)\n            predictions, labels, metrics = trainer.predict(d, metric_key_prefix=\"predict\")\n            predictions = np.argmax(predictions, axis=2)\n\n            trainer.log_metrics(\"predict\", metrics)\n            trainer.save_metrics(\"predict\", metrics)\n\n    else:\n        logger.info(\"*** Predict ***\")\n        predictions, labels, metrics = trainer.predict(predict_dataset, metric_key_prefix=\"predict\")\n        predictions = np.argmax(predictions, axis=2)\n\n        trainer.log_metrics(\"predict\", metrics)\n        trainer.save_metrics(\"predict\", metrics)\n\nif __name__ == '__main__':\n\n    args = get_args()\n\n    _, data_args, training_args, semi_training_args, _ = args\n\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        handlers=[logging.StreamHandler(sys.stdout)],\n    )\n\n    log_level = training_args.get_process_log_level()\n    logger.setLevel(log_level)\n    datasets.utils.logging.set_verbosity(log_level)\n    transformers.utils.logging.set_verbosity(log_level)\n    transformers.utils.logging.enable_default_handler()\n    transformers.utils.logging.enable_explicit_format()\n\n    # Log on each process the small summary:\n    logger.warning(\n        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n    )\n    logger.info(f\"Training/evaluation parameters {training_args}\")\n    \n\n    if not os.path.isdir(\"checkpoints\") or not os.path.exists(\"checkpoints\"):\n        os.mkdir(\"checkpoints\")\n\n    if data_args.task_name.lower() == \"superglue\":\n        assert data_args.dataset_name.lower() in SUPERGLUE_DATASETS\n        from tasks.superglue.get_trainer import get_trainer\n\n    elif data_args.task_name.lower() == \"glue\":\n        assert data_args.dataset_name.lower() in GLUE_DATASETS\n        from tasks.glue.get_trainer import get_trainer\n\n    elif data_args.task_name.lower() == \"ner\":\n        assert data_args.dataset_name.lower() in NER_DATASETS\n        from tasks.ner.get_trainer import get_trainer\n\n    elif data_args.task_name.lower() == \"srl\":\n        assert data_args.dataset_name.lower() in SRL_DATASETS\n        from tasks.srl.get_trainer import get_trainer\n    \n    elif data_args.task_name.lower() == \"qa\":\n        assert data_args.dataset_name.lower() in QA_DATASETS\n        from tasks.qa.get_trainer import get_trainer\n    \n    elif data_args.task_name.lower() == \"other_cls\":\n        assert data_args.dataset_name.lower() in OTHER_DATASETS\n        from tasks.other_cls.get_trainer import get_trainer\n        \n    else:\n        raise NotImplementedError('Task {} is not implemented. Please choose a task from: {}'.format(data_args.task_name, \", \".join(TASKS)))\n\n    set_seed(training_args.seed)\n\n    trainer, predict_dataset = get_trainer(args)\n\n    last_checkpoint = None\n    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n            raise ValueError(\n                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n                \"Use --overwrite_output_dir to overcome.\"\n            )\n        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n            logger.info(\n                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n            )\n\n\n    if training_args.do_train:\n        train(trainer, training_args.resume_from_checkpoint, last_checkpoint)\n    \n    # if training_args.do_eval:\n    #     evaluate(trainer)\n\n    # if training_args.do_predict:\n    #     predict(trainer, predict_dataset)\n\n   "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!cat UPET/run.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4ecbb3",
   "metadata": {},
   "source": [
    "### Step 35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01548e5",
   "metadata": {},
   "source": [
    "Execution Start time: 22:18:28, End time: 22:18:28 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a0630d",
   "metadata": {},
   "source": [
    "**Thought**: The `run.py` script imports the `get_trainer` function from `tasks.superglue.get_trainer`. This is likely where the dataset is being loaded. I need to inspect the `get_trainer` function to modify it to include the `trust_remote_code=True` argument when loading the dataset. This should resolve the issue and allow the script to execute successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9f420d",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6034dc22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import logging\nimport os\nimport random\nimport sys\n\nfrom transformers import (\n    AutoConfig,\n    AutoTokenizer,\n)\n\nfrom model.utils import get_model, TaskType\nfrom tasks.superglue.dataset import SuperGlueDataset\nfrom training.trainer_base import BaseTrainer\nfrom training.trainer_exp import ExponentialTrainer\nfrom training.self_trainer import SelfTrainer\n\nlogger = logging.getLogger(__name__)\n\ndef get_trainer(args):\n    model_args, data_args, training_args, semi_training_args, _ = args\n\n    log_level = training_args.get_process_log_level()\n    logger.setLevel(log_level)\n\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_args.model_name_or_path,\n        use_fast=model_args.use_fast_tokenizer,\n        revision=model_args.model_revision,\n    )\n\n    # add by wjn check if use prompt template\n    use_prompt = False\n    if model_args.prompt_prefix or model_args.prompt_ptuning or model_args.prompt_adapter or model_args.prompt_only:\n        use_prompt = True\n\n    dataset = SuperGlueDataset(tokenizer, data_args, training_args, semi_training_args=semi_training_args, use_prompt=use_prompt)\n\n    data_args.label_word_list = None # add by wjn\n    if use_prompt:\n        data_args.label_word_list = dataset.label_word_list # add by wjn\n\n    if training_args.do_train:\n        for index in random.sample(range(len(dataset.train_dataset)), 3):\n            logger.info(f\"Sample {index} of the training set: {dataset.train_dataset[index]}.\")\n\n    if not dataset.multiple_choice:\n        config = AutoConfig.from_pretrained(\n            model_args.model_name_or_path,\n            num_labels=dataset.num_labels,\n            label2id=dataset.label2id,\n            id2label=dataset.id2label,\n            finetuning_task=data_args.dataset_name,\n            revision=model_args.model_revision,\n        )\n    else:\n        config = AutoConfig.from_pretrained(\n            model_args.model_name_or_path,\n            num_labels=dataset.num_labels,\n            finetuning_task=data_args.dataset_name,\n            revision=model_args.model_revision,\n        )\n\n    if not dataset.multiple_choice:\n        model = get_model(data_args, model_args, TaskType.SEQUENCE_CLASSIFICATION, config)\n    else:\n        model = get_model(data_args, model_args, TaskType.MULTIPLE_CHOICE, config)\n\n\n    # Initialize our Trainer\n\n    if semi_training_args.use_semi:\n        model_args.pre_seq_len = semi_training_args.student_pre_seq_len\n        student_model = get_model(data_args, model_args, TaskType.SEQUENCE_CLASSIFICATION, config)\n        trainer = SelfTrainer(\n            teacher_base_model=model,\n            student_base_model=student_model,\n            training_args=training_args,\n            semi_training_args=semi_training_args,\n            train_dataset=dataset.train_dataset if training_args.do_train else None,\n            unlabeled_dataset=dataset.unlabeled_dataset,\n            eval_dataset=dataset.eval_dataset if training_args.do_eval else None,\n            compute_metrics=dataset.compute_metrics,\n            tokenizer=tokenizer,\n            teacher_data_collator=dataset.data_collator,\n            student_data_collator=dataset.data_collator,\n            test_key=dataset.test_key,\n            task_type=\"cls\",\n            num_classes=len(dataset.label2id),\n        )\n\n        return trainer, None\n\n    trainer = BaseTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=dataset.train_dataset if training_args.do_train else None,\n        eval_dataset=dataset.eval_dataset if training_args.do_eval else None,\n        compute_metrics=dataset.compute_metrics,\n        tokenizer=tokenizer,\n        data_collator=dataset.data_collator,\n        test_key=dataset.test_key\n    )\n\n    return trainer, None\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!cat UPET/tasks/superglue/get_trainer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4267b0",
   "metadata": {},
   "source": [
    "### Step 36"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb91d14d",
   "metadata": {},
   "source": [
    "Execution Start time: 22:18:31, End time: 22:18:31 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cd3de0",
   "metadata": {},
   "source": [
    "**Thought**: The `get_trainer` function in `get_trainer.py` creates an instance of `SuperGlueDataset`, which is likely where the dataset is being loaded. I need to inspect the `SuperGlueDataset` class in `dataset.py` to modify it to include the `trust_remote_code=True` argument when loading the dataset. This should resolve the issue and allow the script to execute successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d5b16b",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c08832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "from datasets.load import load_dataset, load_metric\nfrom transformers import (\n    AutoTokenizer,\n    DataCollatorWithPadding,\n    EvalPrediction,\n    default_data_collator,\n)\nimport numpy as np\nimport logging\nfrom collections import defaultdict\nfrom typing import Optional\n\n# add by wjn\ndef random_sampling(raw_datasets: load_dataset, data_type: str=\"train\", num_examples_per_label: Optional[int]=16):\n    assert data_type in [\"train\", \"validation\", \"test\"]\n    label_list = raw_datasets[data_type][\"label\"] # [0, 1, 0, 0, ...]\n    label_dict = dict()\n    # 记录每个label对应的样本索引\n    for ei, label in enumerate(label_list):\n        if label not in label_dict.keys():\n            label_dict[label] = list()\n        label_dict[label].append(ei)\n    # 对于每个类别，随机采样k个样本\n    few_example_ids = list()\n    for label, eid_list in label_dict.items():\n        # examples = deepcopy(eid_list)\n        # shuffle(examples)\n        idxs = np.random.choice(len(eid_list), size=num_examples_per_label, replace=False)\n        selected_eids = [eid_list[i] for i in idxs]\n        few_example_ids.extend(selected_eids)\n    # 保存没有被选中的example id\n    num_examples = len(label_list)\n    un_selected_examples_ids = [idx for idx in range(num_examples) if idx not in few_example_ids]\n    return few_example_ids, un_selected_examples_ids\n\ntask_to_test_key = {\n    \"boolq\": \"accuracy\",\n    \"cb\": \"accuracy\",\n    \"rte\": \"accuracy\",\n    \"wic\": \"accuracy\",\n    \"wsc\": \"accuracy\",\n    \"copa\": \"accuracy\",\n    \"record\": \"f1\",\n    \"multirc\": \"f1\"\n}\n\ntask_to_keys = {\n    \"boolq\": (\"question\", \"passage\"),\n    \"cb\": (\"premise\", \"hypothesis\"),\n    \"rte\": (\"premise\", \"hypothesis\"),\n    \"wic\": (\"processed_sentence1\", None),\n    \"wsc\": (\"span2_word_text\", \"span1_text\"),\n    \"copa\": (None, None),\n    \"record\": (None, None),\n    \"multirc\": (\"paragraph\", \"question_answer\")\n}\n\ntask_to_template = {\n    \"boolq\" : [{\"prefix_template\": \"Question: \", \"suffix_template\": \"\"}, {\"prefix_template\": \"Passage: \", \"suffix_template\": \" Answer: <mask> .\"}],\n    \"cb\": [None, {\"prefix_template\": \"? <mask> ,\", \"suffix_template\": \"\"}],\n    \"rte\": [None, {\"prefix_template\": \"? <mask> ,\", \"suffix_template\": \"\"}], # prefix / suffix template in each segment.\n    # \"wic\": [None, None],\n    # \"wsc\": [None, None],\n    # \"copa\": [None, None],\n    # \"record\": [None, None],\n    # \"multirc\": [None, None],\n}\n\n# add by wjn\nlabel_words_mapping = {\n    \"boolq\": {\"True\": [\"Yes\"], \"False\": [\"No\"]},\n    \"cb\": {\"contradiction\": [\"No\"], \"neutral\": [\"Maybe\"], \"entailment\": [\"Yes\"]},\n    \"rte\": {\"not_entailment\": [\"No\"], \"entailment\": [\"Yes\"]}\n}\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass SuperGlueDataset():\n    def __init__(\n        self, \n        tokenizer: AutoTokenizer, \n        data_args, \n        training_args, \n        semi_training_args=None,\n        use_prompt=False\n    ) -> None:\n        super().__init__()\n        raw_datasets = load_dataset(\"super_glue\", data_args.dataset_name)\n        self.tokenizer = tokenizer\n        self.data_args = data_args\n        \n        self.multiple_choice = data_args.dataset_name in [\"copa\"]\n\n        if data_args.dataset_name == \"record\":\n            self.num_labels = 2\n            self.label_list = [\"0\", \"1\"]\n        elif not self.multiple_choice:\n            self.label_list = raw_datasets[\"train\"].features[\"label\"].names\n            self.num_labels = len(self.label_list)\n        else:\n            self.num_labels = 1\n\n        # === generate template ===== add by wjn\n        self.use_prompt = False\n        if data_args.dataset_name in task_to_template.keys():\n            self.use_prompt = use_prompt\n        \n        if self.use_prompt:\n            if 't5' in type(tokenizer).__name__.lower():\n                self.special_token_mapping = {\n                    'cls': 3, 'mask': 32099, 'sep': tokenizer.eos_token_id,\n                    'sep+': tokenizer.eos_token_id,\n                    'pseudo_token': tokenizer.unk_token_id\n                }\n            else:\n                self.special_token_mapping = {\n                    'cls': tokenizer.cls_token_id, 'mask': tokenizer.mask_token_id, 'sep': tokenizer.sep_token_id,\n                    'sep+': tokenizer.sep_token_id,\n                    'pseudo_token': tokenizer.unk_token_id\n                }\n            self.template = task_to_template[data_args.dataset_name] # dict\n\n        \n        # === generate label word mapping ===== add by wjn\n        if self.use_prompt:\n            assert data_args.dataset_name in label_words_mapping.keys(), \"You must define label word mapping for the task {}\".format(data_args.dataset_name)\n            self.label_to_word = label_words_mapping[data_args.dataset_name] # e.g., {\"0\": [\"great\"], \"1\": [bad]}\n            self.label_to_word = {label: label_word[0] if type(label_word) == list else label_word for label, label_word in self.label_to_word.items()}\n\n            for key in self.label_to_word:\n                # For RoBERTa/BART/T5, tokenization also considers space, so we use space+word as label words.\n                if self.label_to_word[key][0] not in ['<', '[', '.', ',']:\n                    # Make sure space+word is in the vocabulary\n                    assert len(tokenizer.tokenize(' ' + self.label_to_word[key])) == 1\n                    self.label_to_word[key] = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(' ' + self.label_to_word[key])[0])\n                else:\n                    self.label_to_word[key] = tokenizer.convert_tokens_to_ids(self.label_to_word[key])\n                logger.info(\"Label {} to word {} ({})\".format(key, tokenizer._convert_id_to_token(self.label_to_word[key]), self.label_to_word[key]))\n\n            if len(self.label_list) > 1:\n                self.label_word_list = [self.label_to_word[label] for label in self.label_list]\n            else:\n                # Regression task\n                # '0' represents low polarity and '1' represents high polarity.\n                self.label_word_list = [self.label_to_word[label] for label in ['0', '1']]\n        # =============\n\n        # Preprocessing the raw_datasets\n        self.sentence1_key, self.sentence2_key = task_to_keys[data_args.dataset_name]\n\n        # Padding strategy\n        if data_args.pad_to_max_length:\n            self.padding = \"max_length\"\n        else:\n            # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n            self.padding = False\n\n        if not self.multiple_choice:\n            self.label2id = {l: i for i, l in enumerate(self.label_list)}\n            self.id2label = {id: label for label, id in self.label2id.items()}\n            print(f\"{self.label2id}\")\n            print(f\"{self.id2label}\")\n\n        if data_args.max_seq_length > tokenizer.model_max_length:\n            logger.warning(\n                f\"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the\"\n                f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n            )\n        \n        self.max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n\n        if data_args.dataset_name == \"record\":\n            raw_datasets = raw_datasets.map(\n                self.record_preprocess_function,\n                batched=True,\n                load_from_cache_file=not data_args.overwrite_cache,\n                remove_columns=raw_datasets[\"train\"].column_names,\n                desc=\"Running tokenizer on dataset\",\n            )\n        else:\n            raw_datasets = raw_datasets.map(\n                self.preprocess_function,\n                batched=True,\n                load_from_cache_file=not data_args.overwrite_cache,\n                desc=\"Running tokenizer on dataset\",\n            )\n\n        if training_args.do_train:\n            self.train_dataset = raw_datasets[\"train\"]\n            if data_args.max_train_samples is not None:\n                self.train_dataset = self.train_dataset.select(range(data_args.max_train_samples))\n\n        if training_args.do_eval:\n            self.eval_dataset = raw_datasets[\"validation\"]\n            if data_args.max_eval_samples is not None:\n                self.eval_dataset = self.eval_dataset.select(range(data_args.max_eval_samples))\n\n        if training_args.do_predict or data_args.dataset_name is not None or data_args.test_file is not None:\n            self.predict_dataset = raw_datasets[\"test\"]\n            if data_args.max_predict_samples is not None:\n                self.predict_dataset = self.predict_dataset.select(range(data_args.max_predict_samples))\n        \n\n        # add by wjn \n        self.unlabeled_dataset = None\n        if semi_training_args.use_semi is True:\n            assert data_args.num_examples_per_label is not None and data_args.num_examples_per_label != -1\n        \n        # 随机采样few-shot training / dev data（传入label_list，对每个label进行采样，最后得到索引列表）\n        if data_args.num_examples_per_label is not None and data_args.num_examples_per_label != -1:\n            train_examples_idx_list, un_selected_idx_list = random_sampling(\n                raw_datasets=raw_datasets, \n                data_type=\"train\", \n                num_examples_per_label=data_args.num_examples_per_label\n            )\n            self.all_train_dataset = self.train_dataset\n            self.train_dataset = self.all_train_dataset.select(train_examples_idx_list)\n            print(\"Randomly sampling {}-shot training examples for each label. Total examples number is {}\".format(\n                data_args.num_examples_per_label, \n                len(self.train_dataset)\n                ))\n            \n            if semi_training_args.use_semi is True:\n                self.unlabeled_dataset = self.all_train_dataset.select(un_selected_idx_list)\n                print(\"The number of unlabeled data is {}\".format(len(self.unlabeled_dataset)))\n\n        self.metric = load_metric(\"./metrics/super_glue\", data_args.dataset_name)\n\n        if data_args.pad_to_max_length:\n            self.data_collator = default_data_collator\n        elif training_args.fp16:\n            self.data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n\n        # self.test_key = \"accuracy\" if data_args.dataset_name not in [\"record\", \"multirc\"] else \"f1\"\n\n        # ==== define test key ====== add by wjn\n        self.test_key = task_to_test_key[data_args.dataset_name]\n\n    def preprocess_function(self, examples):\n\n        # print(\"examples[self.sentence1_key]=\", examples[self.sentence1_key][0])\n\n        # WSC\n        if self.data_args.dataset_name == \"wsc\":\n            examples[\"span2_word_text\"] = []\n            for text, span2_index, span2_word in zip(examples[\"text\"], examples[\"span2_index\"], examples[\"span2_text\"]):\n                if self.data_args.template_id == 0:\n                    examples[\"span2_word_text\"].append(span2_word + \": \" + text)\n                elif self.data_args.template_id == 1:\n                    words_a = text.split()\n                    words_a[span2_index] = \"*\" + words_a[span2_index] + \"*\"\n                    examples[\"span2_word_text\"].append(' '.join(words_a))\n\n        # WiC\n        if self.data_args.dataset_name == \"wic\":\n            examples[\"processed_sentence1\"] = []\n            if self.data_args.template_id == 1:\n                self.sentence2_key = \"processed_sentence2\"\n                examples[\"processed_sentence2\"] = []\n            for sentence1, sentence2, word, start1, end1, start2, end2 in zip(examples[\"sentence1\"], examples[\"sentence2\"], examples[\"word\"], examples[\"start1\"], examples[\"end1\"], examples[\"start2\"], examples[\"end2\"]):\n                if self.data_args.template_id == 0: #ROBERTA\n                    examples[\"processed_sentence1\"].append(f\"{sentence1} {sentence2} Does {word} have the same meaning in both sentences?\")\n                elif self.data_args.template_id == 1: #BERT\n                    examples[\"processed_sentence1\"].append(word + \": \" + sentence1)\n                    examples[\"processed_sentence2\"].append(word + \": \" + sentence2)\n\n        # MultiRC\n        if self.data_args.dataset_name == \"multirc\":\n            examples[\"question_answer\"] = []\n            for question, answer in zip(examples[\"question\"], examples[\"answer\"]):\n                examples[\"question_answer\"].append(f\"{question} {answer}\")\n\n        # COPA\n        if self.data_args.dataset_name == \"copa\":\n            examples[\"text_a\"] = []\n            for premise, question in zip(examples[\"premise\"], examples[\"question\"]):\n                joiner = \"because\" if question == \"cause\" else \"so\"\n                text_a = f\"{premise} {joiner}\"                    \n                examples[\"text_a\"].append(text_a)\n\n            result1 = self.tokenizer(examples[\"text_a\"], examples[\"choice1\"], padding=self.padding, max_length=self.max_seq_length, truncation=True) \n            result2 = self.tokenizer(examples[\"text_a\"], examples[\"choice2\"], padding=self.padding, max_length=self.max_seq_length, truncation=True)\n            result = {}  \n            for key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]:\n                if key in result1 and key in result2:\n                    result[key] = []\n                    for value1, value2 in zip(result1[key], result2[key]):\n                        result[key].append([value1, value2])\n            return result\n\n        # add by wjn\n        # adding prompt into each example\n        if self.use_prompt:\n            # if use prompt, insert template into example\n            examples = self.prompt_preprocess_function(examples)\n        \n\n        args = (\n            (examples[self.sentence1_key],) if self.sentence2_key is None else (examples[self.sentence1_key], examples[self.sentence2_key])\n        )\n        result = self.tokenizer(*args, padding=self.padding, max_length=self.max_seq_length, truncation=True)\n\n        # add by wjn\n        # adding mask pos feature\n        \n        # print(\"self.use_prompt=\", self.use_prompt)\n        if self.use_prompt:\n            mask_pos = []\n            for input_ids in result[\"input_ids\"]:\n                mask_pos.append(input_ids.index(self.special_token_mapping[\"mask\"]))\n            result[\"mask_pos\"] = mask_pos\n            # print(\"result=\", result)\n        # print(\"*\"*100)\n        # print(\"result.keys()=\", result.keys())\n        # print(\"result['token_type_ids']=\", result[\"token_type_ids\"][0])\n        return result\n    \n    # add by wjn\n    # process data for prompt (add template)\n    def prompt_preprocess_function(self, examples):\n        \n        def replace_mask_token(template):\n            return template.replace(\"<mask>\", self.tokenizer.convert_ids_to_tokens(self.special_token_mapping[\"mask\"]))\n        \n        sequence1_prefix_template = replace_mask_token(self.template[0][\"prefix_template\"] if self.template[0] is not None else \"\")\n        sequence1_suffix_template = replace_mask_token(self.template[0][\"suffix_template\"] if self.template[0] is not None else \"\")\n        sequence2_prefix_template = replace_mask_token(self.template[1][\"prefix_template\"] if self.template[1] is not None else \"\")\n        sequence2_suffix_template = replace_mask_token(self.template[1][\"suffix_template\"] if self.template[1] is not None else \"\")\n        example_num = len(examples[self.sentence1_key])\n        for example_id in range(example_num):\n            sequence1 = examples[self.sentence1_key][example_id]\n            if self.sentence2_key is None:\n                sequence1 = sequence1[:self.data_args.max_seq_length - len(sequence1_suffix_template) - 10]\n            examples[self.sentence1_key][example_id] = \"{}{}{}\".format(\n                sequence1_prefix_template, sequence1, sequence1_suffix_template)\n\n            if self.sentence2_key is not None:\n                sequence2 = examples[self.sentence2_key][example_id]\n                sequence2 = sequence2[:self.data_args.max_seq_length - len(sequence1) - len(sequence1_prefix_template) - len(sequence1_suffix_template) - len(sequence2_prefix_template)- 10]\n                examples[self.sentence2_key][example_id] = \"{}{}{}\".format(\n                    sequence2_prefix_template, sequence2, sequence2_suffix_template)\n        return examples\n\n\n    def compute_metrics(self, p: EvalPrediction):\n        preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n        preds = np.argmax(preds, axis=1)\n\n        if self.data_args.dataset_name == \"record\":\n            return self.reocrd_compute_metrics(p)\n\n        if self.data_args.dataset_name == \"multirc\":\n            from sklearn.metrics import f1_score\n            return {\"f1\": f1_score(preds, p.label_ids)}\n\n        if self.data_args.dataset_name is not None:\n            result = self.metric.compute(predictions=preds, references=p.label_ids)\n            if len(result) > 1:\n                result[\"combined_score\"] = np.mean(list(result.values())).item()\n            return result\n        elif self.is_regression:\n            return {\"mse\": ((preds - p.label_ids) ** 2).mean().item()}\n        else:\n            return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}\n\n    def reocrd_compute_metrics(self, p: EvalPrediction):\n        from tasks.superglue.utils import f1_score, exact_match_score, metric_max_over_ground_truths\n        probs = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n        examples = self.eval_dataset\n        qid2pred = defaultdict(list)\n        qid2ans = {}\n        for prob, example in zip(probs, examples):\n            qid = example['question_id']\n            qid2pred[qid].append((prob[1], example['entity']))\n            if qid not in qid2ans:\n                qid2ans[qid] = example['answers']\n        n_correct, n_total = 0, 0\n        f1, em = 0, 0\n        for qid in qid2pred:\n            preds = sorted(qid2pred[qid], reverse=True)\n            entity = preds[0][1]\n            n_total += 1\n            n_correct += (entity in qid2ans[qid])\n            f1 += metric_max_over_ground_truths(f1_score, entity, qid2ans[qid])\n            em += metric_max_over_ground_truths(exact_match_score, entity, qid2ans[qid])\n        acc = n_correct / n_total\n        f1 = f1 / n_total\n        em = em / n_total\n        return {'f1': f1, 'exact_match': em}\n\n    def record_preprocess_function(self, examples, split=\"train\"):\n        results = {\n            \"index\": list(),\n            \"question_id\": list(),\n            \"input_ids\": list(),\n            \"attention_mask\": list(),\n            \"token_type_ids\": list(),\n            \"label\": list(),\n            \"entity\": list(),\n            \"answers\": list()\n        }\n        for idx, passage in enumerate(examples[\"passage\"]):\n            query, entities, answers =  examples[\"query\"][idx], examples[\"entities\"][idx], examples[\"answers\"][idx]\n            index = examples[\"idx\"][idx]\n            passage = passage.replace(\"@highlight\\n\", \"- \")\n            \n            for ent_idx, ent in enumerate(entities):\n                question = query.replace(\"@placeholder\", ent)\n                result = self.tokenizer(passage, question, padding=self.padding, max_length=self.max_seq_length, truncation=True)\n                label = 1 if ent in answers else 0\n\n                results[\"input_ids\"].append(result[\"input_ids\"])\n                results[\"attention_mask\"].append(result[\"attention_mask\"])\n                if \"token_type_ids\" in result: results[\"token_type_ids\"].append(result[\"token_type_ids\"])\n                results[\"label\"].append(label)\n                results[\"index\"].append(index)\n                results[\"question_id\"].append(index[\"query\"])\n                results[\"entity\"].append(ent)\n                results[\"answers\"].append(answers)\n\n        return results\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!cat UPET/tasks/superglue/dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2a1e13",
   "metadata": {},
   "source": [
    "### Step 37"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcc0b4f",
   "metadata": {},
   "source": [
    "Execution Start time: 22:18:34, End time: 22:18:34 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d29a564",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed42c5cb",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e1591b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "446ff9b0",
   "metadata": {},
   "source": [
    "### Step 38"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f65110",
   "metadata": {},
   "source": [
    "Execution Start time: 22:18:35, End time: 22:18:36 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517b0beb",
   "metadata": {},
   "source": [
    "**Thought**: The `SuperGlueDataset` class in `dataset.py` is responsible for loading the dataset. To resolve the issue, I need to modify the dataset loading function to include the `trust_remote_code=True` argument. This will allow the custom code in the dataset repository to be executed, resolving the error. Let's identify the specific function responsible for loading the dataset and modify it accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2ce4ce",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12df404f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1:from datasets.load import load_dataset, load_metric\n14:def random_sampling(raw_datasets: load_dataset, data_type: str=\"train\", num_examples_per_label: Optional[int]=16):\n90:        raw_datasets = load_dataset(\"super_glue\", data_args.dataset_name)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!grep -n \"load_dataset\" UPET/tasks/superglue/dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec7e0db",
   "metadata": {},
   "source": [
    "### Step 39"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f90c48",
   "metadata": {},
   "source": [
    "Execution Start time: 22:18:41, End time: 22:18:41 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e562a6",
   "metadata": {},
   "source": [
    "**Thought**: The `load_dataset` function is called on line 90 in the `dataset.py` file to load the \"super_glue\" dataset. I need to modify this line to include the `trust_remote_code=True` argument, which will allow the custom code in the dataset repository to be executed. This should resolve the error and allow the script to execute successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361213de",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb67b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        tokenizer: AutoTokenizer, \n        data_args, \n        training_args, \n        semi_training_args=None,\n        use_prompt=False\n    ) -> None:\n        super().__init__()\n        raw_datasets = load_dataset(\"super_glue\", data_args.dataset_name, trust_remote_code=True)\n        self.tokenizer = tokenizer\n        self.data_args = data_args\n        \n        self.multiple_choice = data_args.dataset_name in [\"copa\"]\n\n        if data_args.dataset_name == \"record\":"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UPET/tasks/superglue/dataset.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "        raw_datasets = load_dataset(\"super_glue\", data_args.dataset_name)\n",
    "<<<AFTER_EDIT>>>\n",
    "        raw_datasets = load_dataset(\"super_glue\", data_args.dataset_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fe9e7d",
   "metadata": {},
   "source": [
    "### Step 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f132e61e",
   "metadata": {},
   "source": [
    "Execution Start time: 22:18:44, End time: 22:18:51 (duration: 7 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97096750",
   "metadata": {},
   "source": [
    "**Thought**: I have successfully modified the `dataset.py` file to include the `trust_remote_code=True` argument when loading the dataset. Now, I will rerun the training script to train the `roberta-base` model on the RTE dataset using the UPET method. After the training completes, I will report the evaluation accuracy as requested. This modification should allow the script to execute without encountering the previous error related to custom code execution in the dataset repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df68c59",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc2d7fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n08/19/2024 22:18:48 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0distributed training: True, 16-bits training: False\n/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n\nDownloading data:   0%|                              | 0.00/751k [00:00<?, ?B/s]\nDownloading data:   8%|█▊                    | 60.4k/751k [00:00<00:01, 490kB/s]\n\n\n\nGenerating train split:   0%|                   | 0/2490 [00:00<?, ? examples/s]\nGenerating train split: 100%|█████| 2490/2490 [00:00<00:00, 30468.43 examples/s]\n\nGenerating validation split:   0%|               | 0/277 [00:00<?, ? examples/s]\nGenerating validation split: 100%|██| 277/277 [00:00<00:00, 29496.86 examples/s]\n\nGenerating test split:   0%|                    | 0/3000 [00:00<?, ? examples/s]\nGenerating test split: 100%|██████| 3000/3000 [00:00<00:00, 37306.36 examples/s]\n{'entailment': 0, 'not_entailment': 1}\n{0: 'entailment', 1: 'not_entailment'}\n\nRunning tokenizer on dataset:   0%|             | 0/2490 [00:00<?, ? examples/s]\nRunning tokenizer on dataset:  40%|▍| 1000/2490 [00:00<00:00, 9112.43 examples/s\nRunning tokenizer on dataset: 100%|█| 2490/2490 [00:00<00:00, 11986.73 examples/\nRunning tokenizer on dataset: 100%|█| 2490/2490 [00:00<00:00, 11401.73 examples/\n\nRunning tokenizer on dataset:   0%|              | 0/277 [00:00<?, ? examples/s]\nRunning tokenizer on dataset: 100%|█| 277/277 [00:00<00:00, 11894.53 examples/s]\n\nRunning tokenizer on dataset:   0%|             | 0/3000 [00:00<?, ? examples/s]\nRunning tokenizer on dataset:  67%|▋| 2000/3000 [00:00<00:00, 16950.38 examples/\nRunning tokenizer on dataset: 100%|█| 3000/3000 [00:00<00:00, 14454.99 examples/\nRandomly sampling 5-shot training examples for each label. Total examples number is 10\n/content/UPET/tasks/superglue/dataset.py:229: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n  self.metric = load_metric(\"./metrics/super_glue\", data_args.dataset_name)\nTraceback (most recent call last):\n  File \"/content/UPET/run.py\", line 128, in <module>\n    trainer, predict_dataset = get_trainer(args)\n  File \"/content/UPET/tasks/superglue/get_trainer.py\", line 36, in get_trainer\n    dataset = SuperGlueDataset(tokenizer, data_args, training_args, semi_training_args=semi_training_args, use_prompt=use_prompt)\n  File \"/content/UPET/tasks/superglue/dataset.py\", line 229, in __init__\n    self.metric = load_metric(\"./metrics/super_glue\", data_args.dataset_name)\n  File \"/usr/local/lib/python3.10/site-packages/datasets/utils/deprecation_utils.py\", line 46, in wrapper\n    return deprecated_function(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/datasets/load.py\", line 2121, in load_metric\n    metric_module = metric_module_factory(\n  File \"/usr/local/lib/python3.10/site-packages/datasets/utils/deprecation_utils.py\", line 46, in wrapper\n    return deprecated_function(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/datasets/load.py\", line 2045, in metric_module_factory\n    raise FileNotFoundError(f\"Couldn't find a metric script at {relative_to_absolute_path(combined_path)}.\")\nFileNotFoundError: Couldn't find a metric script at /content/metrics/super_glue/super_glue.py.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python UPET/run.py --model_name_or_path roberta-base --task_name superglue --dataset_name rte --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 8 --learning_rate 2e-5 --num_train_epochs 1 --output_dir /tmp/rte/ --overwrite_output_dir --seed 42 --num_examples_per_label 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442bc670",
   "metadata": {},
   "source": [
    "### Step 41"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5b1483",
   "metadata": {},
   "source": [
    "Execution Start time: 22:18:56, End time: 22:18:56 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18cde0e",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de89808",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecc211f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0abe093",
   "metadata": {},
   "source": [
    "### Step 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a70a676",
   "metadata": {},
   "source": [
    "Execution Start time: 22:18:58, End time: 22:18:58 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080b61b2",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759be176",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdbc270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "605ced1e",
   "metadata": {},
   "source": [
    "### Step 43"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cef15f6",
   "metadata": {},
   "source": [
    "Execution Start time: 22:19:00, End time: 22:19:00 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b785e765",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4e0294",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc2c1c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00830704",
   "metadata": {},
   "source": [
    "### Step 44"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86a36da",
   "metadata": {},
   "source": [
    "Execution Start time: 22:19:03, End time: 22:19:03 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d342db",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a60d73",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62246f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2614c1c9",
   "metadata": {},
   "source": [
    "### Step 45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2d26d2",
   "metadata": {},
   "source": [
    "Execution Start time: 22:19:13, End time: 22:19:13 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786d4755",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the metric script for the \"super_glue\" dataset could not be found. This might be due to an incorrect path or a missing metric script in the repository. To resolve this, I will check the `load_metric` function call in the `dataset.py` file to ensure that the correct metric is being loaded. If necessary, I will modify the code to specify the correct path or metric name. Let's first inspect the `load_metric` function call in the `dataset.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f24495d",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7775ec1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1:from datasets.load import load_dataset, load_metric\n229:        self.metric = load_metric(\"./metrics/super_glue\", data_args.dataset_name)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!grep -n \"load_metric\" UPET/tasks/superglue/dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d493cae",
   "metadata": {},
   "source": [
    "### Step 46"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f53c82",
   "metadata": {},
   "source": [
    "Execution Start time: 22:19:18, End time: 22:19:18 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a327f8b",
   "metadata": {},
   "source": [
    "**Thought**: The `load_metric` function is called on line 229 in the `dataset.py` file to load the metric for the \"super_glue\" dataset. The path \"./metrics/super_glue\" might be incorrect or the metric script might be missing. I will modify this line to use the `load_metric` function with the correct metric name for the \"super_glue\" dataset, which should be available in the `datasets` library. This should resolve the error and allow the script to execute successfully. Let's modify the code to use the correct metric name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd84faf5",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a317a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n                len(self.train_dataset)\n                ))\n            \n            if semi_training_args.use_semi is True:\n                self.unlabeled_dataset = self.all_train_dataset.select(un_selected_idx_list)\n                print(\"The number of unlabeled data is {}\".format(len(self.unlabeled_dataset)))\n\n        self.metric = load_metric(\"super_glue\", data_args.dataset_name)\n\n        if data_args.pad_to_max_length:\n            self.data_collator = default_data_collator\n        elif training_args.fp16:\n            self.data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UPET/tasks/superglue/dataset.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "        self.metric = load_metric(\"./metrics/super_glue\", data_args.dataset_name)\n",
    "<<<AFTER_EDIT>>>\n",
    "        self.metric = load_metric(\"super_glue\", data_args.dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b8d47e",
   "metadata": {},
   "source": [
    "### Step 47"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6657dc25",
   "metadata": {},
   "source": [
    "Execution Start time: 22:19:23, End time: 22:19:43 (duration: 20 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc01f22",
   "metadata": {},
   "source": [
    "**Thought**: I have successfully modified the `dataset.py` file to use the correct metric name for the \"super_glue\" dataset. Now, I will rerun the training script to train the `roberta-base` model on the RTE dataset using the UPET method. After the training completes, I will report the evaluation accuracy as requested. This modification should resolve the previous error related to the missing metric script and allow the script to execute successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73306e21",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15fb495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n08/19/2024 22:19:27 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0distributed training: True, 16-bits training: False\n/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n{'entailment': 0, 'not_entailment': 1}\n{0: 'entailment', 1: 'not_entailment'}\n\nRunning tokenizer on dataset:   0%|              | 0/277 [00:00<?, ? examples/s]\nRunning tokenizer on dataset: 100%|██| 277/277 [00:00<00:00, 6685.67 examples/s]\nRandomly sampling 5-shot training examples for each label. Total examples number is 10\n/content/UPET/tasks/superglue/dataset.py:229: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n  self.metric = load_metric(\"super_glue\", data_args.dataset_name)\n\nDownloading builder script:   0%|                   | 0.00/2.63k [00:00<?, ?B/s]\n\n\nDownloading extra modules:   0%|                    | 0.00/1.24k [00:00<?, ?B/s]\n\nThe repository for super_glue contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/super_glue.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N] Traceback (most recent call last):\n  File \"/content/UPET/run.py\", line 128, in <module>\n    trainer, predict_dataset = get_trainer(args)\n  File \"/content/UPET/tasks/superglue/get_trainer.py\", line 36, in get_trainer\n    dataset = SuperGlueDataset(tokenizer, data_args, training_args, semi_training_args=semi_training_args, use_prompt=use_prompt)\n  File \"/content/UPET/tasks/superglue/dataset.py\", line 229, in __init__\n    self.metric = load_metric(\"super_glue\", data_args.dataset_name)\n  File \"/usr/local/lib/python3.10/site-packages/datasets/utils/deprecation_utils.py\", line 46, in wrapper\n    return deprecated_function(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/datasets/load.py\", line 2121, in load_metric\n    metric_module = metric_module_factory(\n  File \"/usr/local/lib/python3.10/site-packages/datasets/utils/deprecation_utils.py\", line 46, in wrapper\n    return deprecated_function(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/datasets/load.py\", line 2039, in metric_module_factory\n    raise e1 from None\n  File \"/usr/local/lib/python3.10/site-packages/datasets/load.py\", line 2033, in metric_module_factory\n    ).get_module()\n  File \"/usr/local/lib/python3.10/site-packages/datasets/load.py\", line 803, in get_module\n    trust_remote_code = resolve_trust_remote_code(self.trust_remote_code, self.name)\n  File \"/usr/local/lib/python3.10/site-packages/datasets/load.py\", line 134, in resolve_trust_remote_code\n    raise ValueError(\nValueError: The repository for super_glue contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/super_glue.\nPlease pass the argument `trust_remote_code=True` to allow custom code to be run.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python UPET/run.py --model_name_or_path roberta-base --task_name superglue --dataset_name rte --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 8 --learning_rate 2e-5 --num_train_epochs 1 --output_dir /tmp/rte/ --overwrite_output_dir --seed 42 --num_examples_per_label 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a059514",
   "metadata": {},
   "source": [
    "### Step 48"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a022722",
   "metadata": {},
   "source": [
    "Execution Start time: 22:19:46, End time: 22:19:46 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17d1f7b",
   "metadata": {},
   "source": [
    "**Thought**: The error persists because the `trust_remote_code=True` argument needs to be passed when loading the metric as well. I will modify the `dataset.py` file to include this argument in the `load_metric` function call. This should resolve the issue and allow the script to execute successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d30d229",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae0c865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n                len(self.train_dataset)\n                ))\n            \n            if semi_training_args.use_semi is True:\n                self.unlabeled_dataset = self.all_train_dataset.select(un_selected_idx_list)\n                print(\"The number of unlabeled data is {}\".format(len(self.unlabeled_dataset)))\n\n        self.metric = load_metric(\"super_glue\", data_args.dataset_name, trust_remote_code=True)\n\n        if data_args.pad_to_max_length:\n            self.data_collator = default_data_collator\n        elif training_args.fp16:\n            self.data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UPET/tasks/superglue/dataset.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "        self.metric = load_metric(\"super_glue\", data_args.dataset_name)\n",
    "<<<AFTER_EDIT>>>\n",
    "        self.metric = load_metric(\"super_glue\", data_args.dataset_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d83ea45",
   "metadata": {},
   "source": [
    "### Step 49"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfa4a7c",
   "metadata": {},
   "source": [
    "Execution Start time: 22:19:51, End time: 22:20:06 (duration: 15 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487624da",
   "metadata": {},
   "source": [
    "**Thought**: I have successfully modified the `dataset.py` file to include the `trust_remote_code=True` argument in the `load_metric` function call. Now, I will rerun the training script to train the `roberta-base` model on the RTE dataset using the UPET method. After the training completes, I will report the evaluation accuracy as requested. This modification should resolve the previous error related to custom code execution in the metric repository and allow the script to execute successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fd1b7d",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23c253c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n08/19/2024 22:19:55 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0distributed training: True, 16-bits training: False\n/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n{'entailment': 0, 'not_entailment': 1}\n{0: 'entailment', 1: 'not_entailment'}\n\nRunning tokenizer on dataset:   0%|             | 0/3000 [00:00<?, ? examples/s]\nRunning tokenizer on dataset:  33%|▎| 1000/3000 [00:00<00:00, 9767.60 examples/s\nRunning tokenizer on dataset: 100%|█| 3000/3000 [00:00<00:00, 12139.17 examples/\nRunning tokenizer on dataset: 100%|█| 3000/3000 [00:00<00:00, 11667.30 examples/\nRandomly sampling 5-shot training examples for each label. Total examples number is 10\n/content/UPET/tasks/superglue/dataset.py:229: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n  self.metric = load_metric(\"super_glue\", data_args.dataset_name, trust_remote_code=True)\n\nmodel.safetensors:   0%|                             | 0.00/499M [00:00<?, ?B/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[WARNING|modeling_utils.py:4490] 2024-08-19 22:19:59,744 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n***** total param is 124647170 *****\n08/19/2024 22:19:59 - WARNING - accelerate.utils.other - Detected kernel version 4.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n\n  0%|                                                     | 0/2 [00:00<?, ?it/s]\n 50%|██████████████████████▌                      | 1/2 [00:05<00:05,  5.68s/it]Traceback (most recent call last):\n  File \"/content/UPET/run.py\", line 146, in <module>\n    train(trainer, training_args.resume_from_checkpoint, last_checkpoint)\n  File \"/content/UPET/run.py\", line 26, in train\n    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n  File \"/usr/local/lib/python3.10/site-packages/transformers/trainer.py\", line 1948, in train\n    return inner_training_loop(\n  File \"/usr/local/lib/python3.10/site-packages/transformers/trainer.py\", line 2366, in _inner_training_loop\n    self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\nTypeError: BaseTrainer._maybe_log_save_evaluate() takes 6 positional arguments but 7 were given\n\n 50%|█████     | 1/2 [00:05<00:05,  5.90s/it]                                   \n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python UPET/run.py --model_name_or_path roberta-base --task_name superglue --dataset_name rte --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 8 --learning_rate 2e-5 --num_train_epochs 1 --output_dir /tmp/rte/ --overwrite_output_dir --seed 42 --num_examples_per_label 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5080ec4",
   "metadata": {},
   "source": [
    "### Step 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047f384c",
   "metadata": {},
   "source": [
    "Execution Start time: 22:20:11, End time: 22:20:11 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75efdfbd",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that there is a mismatch in the number of arguments expected by the `_maybe_log_save_evaluate` method in the `BaseTrainer` class. This suggests that the code in the repository is not compatible with the current version of the `transformers` library. To resolve this, I need to modify the `_maybe_log_save_evaluate` method in the `BaseTrainer` class to accept the correct number of arguments. Let's first inspect the `BaseTrainer` class to identify the `_maybe_log_save_evaluate` method and modify it accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406b4b3d",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4786cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!grep -n \"def _maybe_log_save_evaluate\" UPET/training/trainer_exp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f46ea5",
   "metadata": {},
   "source": [
    "### Step 51"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c25317",
   "metadata": {},
   "source": [
    "Execution Start time: 22:20:14, End time: 22:20:14 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83f130d",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bae54aa",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c87d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cf0ca38",
   "metadata": {},
   "source": [
    "### Step 52"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcc47bd",
   "metadata": {},
   "source": [
    "Execution Start time: 22:20:16, End time: 22:20:16 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5e3da6",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54396bc",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e982248a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30ad9147",
   "metadata": {},
   "source": [
    "### Step 53"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e36a27",
   "metadata": {},
   "source": [
    "Execution Start time: 22:20:19, End time: 22:20:19 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98eee0b",
   "metadata": {},
   "source": [
    "**Thought**: It seems that the `_maybe_log_save_evaluate` method is not directly defined in the `trainer_exp.py` file. This method might be inherited from a parent class in the `transformers` library. To resolve the issue, I need to check the `transformers` library documentation or source code to understand the correct number of arguments for this method. Alternatively, I can modify the call to this method in the `trainer_exp.py` file to match the expected number of arguments. Let's first inspect the `trainer_exp.py` file to identify where this method is being called and adjust the arguments accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f701c81",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b556b560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "421:                    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n436:            self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!grep -n \"_maybe_log_save_evaluate\" UPET/training/trainer_exp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce077ce7",
   "metadata": {},
   "source": [
    "### Step 54"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21fcf90",
   "metadata": {},
   "source": [
    "Execution Start time: 22:20:21, End time: 22:20:21 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01c2626",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54bdfad",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f73fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db57ebcb",
   "metadata": {},
   "source": [
    "### Step 55"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122ef6e5",
   "metadata": {},
   "source": [
    "Execution Start time: 22:20:25, End time: 22:20:25 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dd25b0",
   "metadata": {},
   "source": [
    "**Thought**: The `_maybe_log_save_evaluate` method is called on lines 421 and 436 in the `trainer_exp.py` file. The error suggests that the method is being called with 6 arguments, but it expects 7. To resolve this, I need to modify these calls to include the correct number of arguments. I will check the `transformers` library documentation or source code to determine the correct arguments for this method and adjust the calls accordingly. Let's first inspect the `transformers` library source code to understand the correct number of arguments for this method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb3627b",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28024f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2788:    def _maybe_log_save_evaluate(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval):\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!grep -n \"def _maybe_log_save_evaluate\" /usr/local/lib/python3.10/site-packages/transformers/trainer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daedef4e",
   "metadata": {},
   "source": [
    "### Step 56"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97852ab7",
   "metadata": {},
   "source": [
    "Execution Start time: 22:20:28, End time: 22:20:28 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7863fdb8",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe02f134",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e6fa81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b4a341c",
   "metadata": {},
   "source": [
    "### Step 57"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06833229",
   "metadata": {},
   "source": [
    "Execution Start time: 22:20:34, End time: 22:20:34 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bc1ee5",
   "metadata": {},
   "source": [
    "**Thought**: The `_maybe_log_save_evaluate` method in the `transformers` library expects 7 arguments: `tr_loss`, `grad_norm`, `model`, `trial`, `epoch`, and `ignore_keys_for_eval`. The calls in the `trainer_exp.py` file are missing the `grad_norm` argument. I will modify the calls to include a placeholder value for `grad_norm`, such as `None`, to match the expected number of arguments. This should resolve the error and allow the script to execute successfully. Let's proceed with editing the `trainer_exp.py` file to include the correct number of arguments in the method calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e9aaf7",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5537c833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Found multiple BEFORE_EDIT in the patch. Line 5: <<<BEFORE_EDIT>>>\nFormat of edit is:```\n{filename}\n[optional] preceding lines\n<<<BEFORE_EDIT>>>\nline to remove\nline to remove\n<<<AFTER_EDIT>>>\nline to add\nline to add\nline to add\n```"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UPET/training/trainer_exp.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "                    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n",
    "<<<AFTER_EDIT>>>\n",
    "                    self._maybe_log_save_evaluate(tr_loss, None, model, trial, epoch, ignore_keys_for_eval)\n",
    "<<<BEFORE_EDIT>>>\n",
    "            self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n",
    "<<<AFTER_EDIT>>>\n",
    "            self._maybe_log_save_evaluate(tr_loss, None, model, trial, epoch, ignore_keys_for_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff2da20",
   "metadata": {},
   "source": [
    "### Step 58"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb23056d",
   "metadata": {},
   "source": [
    "Execution Start time: 22:20:41, End time: 22:20:41 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba6b4da",
   "metadata": {},
   "source": [
    "**Thought**: I mistakenly included multiple `BEFORE_EDIT` sections in the edit action. I need to combine the changes into a single edit action to ensure the correct format. Let's proceed with editing the `trainer_exp.py` file again to include the correct number of arguments in the method calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad31b71e",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a090a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Found multiple BEFORE_EDIT in the patch. Line 9: <<<BEFORE_EDIT>>>\nFormat of edit is:```\n{filename}\n[optional] preceding lines\n<<<BEFORE_EDIT>>>\nline to remove\nline to remove\n<<<AFTER_EDIT>>>\nline to add\nline to add\nline to add\n```"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UPET/training/trainer_exp.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "                    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n",
    "                else:\n",
    "                    self.control = self.callback_handler.on_step_end(self.args, self.state, self.control)\n",
    "<<<AFTER_EDIT>>>\n",
    "                    self._maybe_log_save_evaluate(tr_loss, None, model, trial, epoch, ignore_keys_for_eval)\n",
    "                else:\n",
    "                    self.control = self.callback_handler.on_step_end(self.args, self.state, self.control)\n",
    "<<<BEFORE_EDIT>>>\n",
    "            self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n",
    "<<<AFTER_EDIT>>>\n",
    "            self._maybe_log_save_evaluate(tr_loss, None, model, trial, epoch, ignore_keys_for_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1ebd04",
   "metadata": {},
   "source": [
    "### Step 59"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47b62d6",
   "metadata": {},
   "source": [
    "Execution Start time: 22:20:45, End time: 22:20:45 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa64f75",
   "metadata": {},
   "source": [
    "**Thought**: I need to combine the changes into a single edit action to ensure the correct format. Let's proceed with editing the `trainer_exp.py` file again to include the correct number of arguments in the method calls. I will make sure to format the edit action correctly this time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40cca42",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2293383d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Found multiple BEFORE_EDIT in the patch. Line 5: <<<BEFORE_EDIT>>>\nFormat of edit is:```\n{filename}\n[optional] preceding lines\n<<<BEFORE_EDIT>>>\nline to remove\nline to remove\n<<<AFTER_EDIT>>>\nline to add\nline to add\nline to add\n```"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UPET/training/trainer_exp.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "                    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n",
    "<<<AFTER_EDIT>>>\n",
    "                    self._maybe_log_save_evaluate(tr_loss, None, model, trial, epoch, ignore_keys_for_eval)\n",
    "<<<BEFORE_EDIT>>>\n",
    "            self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n",
    "<<<AFTER_EDIT>>>\n",
    "            self._maybe_log_save_evaluate(tr_loss, None, model, trial, epoch, ignore_keys_for_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3db28c",
   "metadata": {},
   "source": [
    "### Step 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6997b0a9",
   "metadata": {},
   "source": [
    "Execution Start time: 22:20:47, End time: 22:20:48 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346a4fb5",
   "metadata": {},
   "source": [
    "**Thought**: I need to combine the changes into a single edit action to ensure the correct format. Let's proceed with editing the `trainer_exp.py` file again to include the correct number of arguments in the method calls. I will make sure to format the edit action correctly this time by editing each section separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf4c026",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d12850e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n                        self.lr_scheduler.step()\n\n                    model.zero_grad()\n                    self.state.global_step += 1\n                    self.state.epoch = epoch + (step + 1) / steps_in_epoch\n                    self.control = self.callback_handler.on_step_end(args, self.state, self.control)\n\n                    self._maybe_log_save_evaluate(tr_loss, None, model, trial, epoch, ignore_keys_for_eval)\n                else:\n                    self.control = self.callback_handler.on_substep_end(args, self.state, self.control)\n\n                if self.control.should_epoch_stop or self.control.should_training_stop:\n                    break\n            if step < 0:"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UPET/training/trainer_exp.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "                    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n",
    "<<<AFTER_EDIT>>>\n",
    "                    self._maybe_log_save_evaluate(tr_loss, None, model, trial, epoch, ignore_keys_for_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f197ecc",
   "metadata": {},
   "source": [
    "### Step 61"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9905b1a",
   "metadata": {},
   "source": [
    "Execution Start time: 22:20:51, End time: 22:20:51 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544530fa",
   "metadata": {},
   "source": [
    "**Thought**: I have successfully modified the first call to `_maybe_log_save_evaluate` to include the correct number of arguments. Now, I will proceed to edit the second call to this method in the `trainer_exp.py` file to ensure both calls are updated correctly. This should resolve the error and allow the script to execute successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88012472",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ee5b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n                    f\"There seems to be not a single sample in your epoch_iterator, stopping training at step\"\n                    f\" {self.state.global_step}! This is expected if you're using an IterableDataset and set\"\n                    f\" num_steps ({max_steps}) higher than the number of available samples.\"\n                )\n                self.control.should_training_stop = True\n\n            self.control = self.callback_handler.on_epoch_end(args, self.state, self.control)\n            self._maybe_log_save_evaluate(tr_loss, None, model, trial, epoch, ignore_keys_for_eval)\n\n            if DebugOption.TPU_METRICS_DEBUG in self.args.debug:\n                if is_torch_tpu_available():\n                    # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\n                    xm.master_print(met.metrics_report())\n                else:"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UPET/training/trainer_exp.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "            self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n",
    "<<<AFTER_EDIT>>>\n",
    "            self._maybe_log_save_evaluate(tr_loss, None, model, trial, epoch, ignore_keys_for_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f134e40c",
   "metadata": {},
   "source": [
    "### Step 62"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5226c271",
   "metadata": {},
   "source": [
    "Execution Start time: 22:20:54, End time: 22:20:54 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a768bdd7",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09125951",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebc8ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d75cfd7",
   "metadata": {},
   "source": [
    "### Step 63"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de41cdeb",
   "metadata": {},
   "source": [
    "Execution Start time: 22:21:00, End time: 22:21:13 (duration: 12 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524d6a78",
   "metadata": {},
   "source": [
    "**Thought**: I have successfully modified both calls to `_maybe_log_save_evaluate` in the `trainer_exp.py` file to include the correct number of arguments. Now, I will rerun the training script to train the `roberta-base` model on the RTE dataset using the UPET method. After the training completes, I will report the evaluation accuracy as requested. This modification should resolve the previous error related to the argument mismatch and allow the script to execute successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedfdd4b",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bd73f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n08/19/2024 22:21:05 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0distributed training: True, 16-bits training: False\n/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n{'entailment': 0, 'not_entailment': 1}\n{0: 'entailment', 1: 'not_entailment'}\nRandomly sampling 5-shot training examples for each label. Total examples number is 10\n/content/UPET/tasks/superglue/dataset.py:229: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n  self.metric = load_metric(\"super_glue\", data_args.dataset_name, trust_remote_code=True)\n[WARNING|modeling_utils.py:4490] 2024-08-19 22:21:06,804 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n***** total param is 124647170 *****\n08/19/2024 22:21:06 - WARNING - accelerate.utils.other - Detected kernel version 4.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n\n  0%|                                                     | 0/2 [00:00<?, ?it/s]\n 50%|██████████████████████▌                      | 1/2 [00:05<00:05,  5.45s/it]Traceback (most recent call last):\n  File \"/content/UPET/run.py\", line 146, in <module>\n    train(trainer, training_args.resume_from_checkpoint, last_checkpoint)\n  File \"/content/UPET/run.py\", line 26, in train\n    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n  File \"/usr/local/lib/python3.10/site-packages/transformers/trainer.py\", line 1948, in train\n    return inner_training_loop(\n  File \"/usr/local/lib/python3.10/site-packages/transformers/trainer.py\", line 2366, in _inner_training_loop\n    self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\nTypeError: BaseTrainer._maybe_log_save_evaluate() takes 6 positional arguments but 7 were given\n\n 50%|█████     | 1/2 [00:05<00:05,  5.68s/it]                                   \n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python UPET/run.py --model_name_or_path roberta-base --task_name superglue --dataset_name rte --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 8 --learning_rate 2e-5 --num_train_epochs 1 --output_dir /tmp/rte/ --overwrite_output_dir --seed 42 --num_examples_per_label 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e230f6a",
   "metadata": {},
   "source": [
    "### Step 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1476e1ca",
   "metadata": {},
   "source": [
    "Execution Start time: 22:21:16, End time: 22:21:16 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b83f101",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42ba963",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c204b821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b80a79c9",
   "metadata": {},
   "source": [
    "### Step 65"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e89546b",
   "metadata": {},
   "source": [
    "Execution Start time: 22:21:16, End time: 22:21:16 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c69e45",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0409f3d7",
   "metadata": {},
   "source": [
    "**Action (exception)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bd8fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Total context tokens exceeded."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
