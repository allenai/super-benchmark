{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58c671ea",
   "metadata": {},
   "source": [
    "### System Prompt\n",
    "<details>\n",
    "SETTING: You are an autonomous programmer, and you're working directly in the command line with a special Jupyter notebook interface.\n",
    "\n",
    "The special Jupyter notebook interface consists of a file editor that shows you 100 lines of a file at a time.\n",
    "You can execute commands in the notebook using:\n",
    "\n",
    "1. Bash commands: Commands starting with !.\n",
    "2. Python commands: Standard Python code.\n",
    "3. Magic commands: Commands starting with %, e.g., %cd <path>.\n",
    "\n",
    "Additionally, you can also use the following commands to help you navigate and edit files.\n",
    "\n",
    "COMMANDS:\n",
    "\n",
    "open:\n",
    "  docstring: opens the file at the given path in the editor. If line_number is provided, the window will be move to include that line\n",
    "  signature: open <path> [<line_number>]\n",
    "  arguments:\n",
    "    - path (string) [required]: the path to the file to open\n",
    "    - line_number (integer) [optional]: the line number to move the window to (if not provided, the window will start at the top of the file)\n",
    "\n",
    "create:\n",
    "  docstring: creates and opens a new file with the given name\n",
    "  signature: create <filename>\n",
    "  arguments:\n",
    "    - filename (string) [required]: the name of the file to create\n",
    "\n",
    "goto:\n",
    "  docstring: moves the window to show <line_number>\n",
    "  signature: goto <line_number>\n",
    "  arguments:\n",
    "    - line_number (integer) [required]: the line number to move the window to\n",
    "\n",
    "scroll_down:\n",
    "  docstring: moves the window down {WINDOW} lines\n",
    "  signature: scroll_down\n",
    "\n",
    "scroll_up:\n",
    "  docstring: moves the window down {WINDOW} lines\n",
    "  signature: scroll_up\n",
    "\n",
    "submit:\n",
    "  docstring: submits your final answer\n",
    "  signature: submit <answer>\n",
    "  arguments:\n",
    "    - answer (string) [required]: the answer to be submitted\n",
    "\n",
    "search_dir:\n",
    "  docstring: searches for search_term in all files in dir_path. If dir_path is not provided, searches in the current directory\n",
    "  signature: search_dir <search_term> [<dir_path>]\n",
    "  arguments:\n",
    "    - search_term (string) [required]: the term to search for\n",
    "    - dir_path (string) [optional]: the directory to search in (if not provided, searches in the current directory)\n",
    "\n",
    "search_file:\n",
    "  docstring: searches for search_term in file. If file is not provided, searches in the current open file\n",
    "  signature: search_file <search_term> [<file>]\n",
    "  arguments:\n",
    "    - search_term (string) [required]: the term to search for\n",
    "    - file (string) [optional]: the file to search in (if not provided, searches in the current open file)\n",
    "\n",
    "find_file:\n",
    "  docstring: finds all files with the given name in dir_path. If dir_path is not provided, searches in the current directory\n",
    "  signature: find_file <file_name> [<dir_path>]\n",
    "  arguments:\n",
    "    - file_name (string) [required]: the name of the file to search for\n",
    "    - dir_path (string) [optional]: the directory to search in (if not provided, searches in the current directory)\n",
    "\n",
    "edit:\n",
    "  docstring: replaces lines <start_line> through <end_line> (inclusive) with the given text in the open file. The replacement text is terminated by a line with only end_of_edit on it. All of the <replacement text> will be entered, so make sure your indentation is formatted properly. Python files will be checked for syntax errors after the edit. If the system detects a syntax error, the edit will not be executed. Simply try to edit the file again, but make sure to read the error message and modify the edit command you issue accordingly. Issuing the same command a second time will just lead to the same error message again.\n",
    "  signature: edit <start_line>:<end_line>\n",
    "<replacement_text>\n",
    "end_of_edit\n",
    "  arguments:\n",
    "    - start_line (integer) [required]: the line number to start the edit at\n",
    "    - end_line (integer) [required]: the line number to end the edit at (inclusive)\n",
    "    - replacement_text (string) [required]: the text to replace the current selection with\n",
    "\n",
    "\n",
    "\n",
    "Please note that THE EDIT COMMAND REQUIRES PROPER INDENTATION. \n",
    "If you'd like to add the line '        print(x)' you must fully write that out, with all those spaces before the code! Indentation is important and code that is not indented correctly will fail and require fixing before it can be run.\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "Your shell prompt is formatted as follows:\n",
    "(Open file: <path>)\n",
    "(Current directory: <cwd>)\n",
    "In [ ]\n",
    "\n",
    "You need to format your output using two fields: discussion and command.\n",
    "Your output should always include _one_ discussion and _one_ command field EXACTLY as in the following example:\n",
    "DISCUSSION\n",
    "First I'll start by using ls to see what files are in the current directory. Then maybe we can look at some relevant files to see what they look like.\n",
    "```\n",
    "!ls -a\n",
    "```\n",
    "\n",
    "You should only include a *SINGLE* command in the command section and then wait for a response from the shell before continuing with more discussion and commands. Everything you include in the DISCUSSION section will be saved for future reference.\n",
    "If you'd like to issue two commands at once, PLEASE DO NOT DO THAT! Please instead first submit just the first command, and then after receiving a response you'll be able to issue the second command. \n",
    "You're free to use any other bash commands you want (e.g. find, grep, cat, ls, cd) in addition to the special commands listed above.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394a35f6",
   "metadata": {},
   "source": [
    "### Query\n",
    "Apply the decomposed prompt tuning method to fine-tune the t5-small model on our sentence pair classification dataset, available at https://drive.google.com/file/d/1raGLX-2vdX1uizYx2DdqZ_JzAkVeyeDF/view?usp=drive_link. Report the resulting metrics using the `main_ddp.py` script. Report accuracy on the development set as a json structured as follows: {\"accuracy\": 0.0} (replace 0.0 with the actual value).\n",
    "\n",
    "Additional instructions:\n",
    "1. Run for only 1 epoch.\n",
    "2. Use the following hyperparameters: --enc_prompt_tokens 100 -ts 16 -e 1 --bottle_neck 10\n",
    "\n",
    "Git repository: https://github.com/xyaoooo/dpt\n",
    "\n",
    "I have already executed some of the required steps. Now, you should make the necessary changes to make sure the code runs on a CPU. Your goal is to successfully run\n",
    "```\n",
    "!python main_ddp.py --datasets=mrpc  --model_name=t5-small --enc_prompt_tokens 100 -ts 16 -e 1 --bottle_neck 10\n",
    "```\n",
    "and submit the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805ca41d",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111a5d47",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca23b6c2",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8043218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cloning into 'dpt'...\r\nremote: Enumerating objects: 45, done.\u001b[K\r\nremote: Counting objects:   2% (1/45)\u001b[K\rremote: Counting objects:   4% (2/45)\u001b[K\rremote: Counting objects:   6% (3/45)\u001b[K\rremote: Counting objects:   8% (4/45)\u001b[K\rremote: Counting objects:  11% (5/45)\u001b[K\rremote: Counting objects:  13% (6/45)\u001b[K\rremote: Counting objects:  15% (7/45)\u001b[K\rremote: Counting objects:  17% (8/45)\u001b[K\rremote: Counting objects:  20% (9/45)\u001b[K\rremote: Counting objects:  22% (10/45)\u001b[K\rremote: Counting objects:  24% (11/45)\u001b[K\rremote: Counting objects:  26% (12/45)\u001b[K\rremote: Counting objects:  28% (13/45)\u001b[K\rremote: Counting objects:  31% (14/45)\u001b[K\rremote: Counting objects:  33% (15/45)\u001b[K\rremote: Counting objects:  35% (16/45)\u001b[K\rremote: Counting objects:  37% (17/45)\u001b[K\rremote: Counting objects:  40% (18/45)\u001b[K\rremote: Counting objects:  42% (19/45)\u001b[K\rremote: Counting objects:  44% (20/45)\u001b[K\rremote: Counting objects:  46% (21/45)\u001b[K\rremote: Counting objects:  48% (22/45)\u001b[K\rremote: Counting objects:  51% (23/45)\u001b[K\rremote: Counting objects:  53% (24/45)\u001b[K\rremote: Counting objects:  55% (25/45)\u001b[K\rremote: Counting objects:  57% (26/45)\u001b[K\rremote: Counting objects:  60% (27/45)\u001b[K\rremote: Counting objects:  62% (28/45)\u001b[K\rremote: Counting objects:  64% (29/45)\u001b[K\rremote: Counting objects:  66% (30/45)\u001b[K\rremote: Counting objects:  68% (31/45)\u001b[K\rremote: Counting objects:  71% (32/45)\u001b[K\rremote: Counting objects:  73% (33/45)\u001b[K\rremote: Counting objects:  75% (34/45)\u001b[K\rremote: Counting objects:  77% (35/45)\u001b[K\rremote: Counting objects:  80% (36/45)\u001b[K\rremote: Counting objects:  82% (37/45)\u001b[K\rremote: Counting objects:  84% (38/45)\u001b[K\rremote: Counting objects:  86% (39/45)\u001b[K\rremote: Counting objects:  88% (40/45)\u001b[K\rremote: Counting objects:  91% (41/45)\u001b[K\rremote: Counting objects:  93% (42/45)\u001b[K\rremote: Counting objects:  95% (43/45)\u001b[K\rremote: Counting objects:  97% (44/45)\u001b[K\rremote: Counting objects: 100% (45/45)\u001b[K\rremote: Counting objects: 100% (45/45), done.\u001b[K\r\nremote: Compressing objects:   2% (1/41)\u001b[K\rremote: Compressing objects:   4% (2/41)\u001b[K\rremote: Compressing objects:   7% (3/41)\u001b[K\rremote: Compressing objects:   9% (4/41)\u001b[K\rremote: Compressing objects:  12% (5/41)\u001b[K\rremote: Compressing objects:  14% (6/41)\u001b[K\rremote: Compressing objects:  17% (7/41)\u001b[K\rremote: Compressing objects:  19% (8/41)\u001b[K\rremote: Compressing objects:  21% (9/41)\u001b[K\rremote: Compressing objects:  24% (10/41)\u001b[K\rremote: Compressing objects:  26% (11/41)\u001b[K\rremote: Compressing objects:  29% (12/41)\u001b[K\rremote: Compressing objects:  31% (13/41)\u001b[K\rremote: Compressing objects:  34% (14/41)\u001b[K\rremote: Compressing objects:  36% (15/41)\u001b[K\rremote: Compressing objects:  39% (16/41)\u001b[K\rremote: Compressing objects:  41% (17/41)\u001b[K\rremote: Compressing objects:  43% (18/41)\u001b[K\rremote: Compressing objects:  46% (19/41)\u001b[K\rremote: Compressing objects:  48% (20/41)\u001b[K\rremote: Compressing objects:  51% (21/41)\u001b[K\rremote: Compressing objects:  53% (22/41)\u001b[K\rremote: Compressing objects:  56% (23/41)\u001b[K\rremote: Compressing objects:  58% (24/41)\u001b[K\rremote: Compressing objects:  60% (25/41)\u001b[K\rremote: Compressing objects:  63% (26/41)\u001b[K\rremote: Compressing objects:  65% (27/41)\u001b[K\rremote: Compressing objects:  68% (28/41)\u001b[K\rremote: Compressing objects:  70% (29/41)\u001b[K\rremote: Compressing objects:  73% (30/41)\u001b[K\rremote: Compressing objects:  75% (31/41)\u001b[K\rremote: Compressing objects:  78% (32/41)\u001b[K\rremote: Compressing objects:  80% (33/41)\u001b[K\rremote: Compressing objects:  82% (34/41)\u001b[K\rremote: Compressing objects:  85% (35/41)\u001b[K\rremote: Compressing objects:  87% (36/41)\u001b[K\rremote: Compressing objects:  90% (37/41)\u001b[K\rremote: Compressing objects:  92% (38/41)\u001b[K\rremote: Compressing objects:  95% (39/41)\u001b[K\rremote: Compressing objects:  97% (40/41)\u001b[K\rremote: Compressing objects: 100% (41/41)\u001b[K\rremote: Compressing objects: 100% (41/41), done.\u001b[K\r\nremote: Total 45 (delta 18), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\r\nReceiving objects:   2% (1/45)\rReceiving objects:   4% (2/45)\rReceiving objects:   6% (3/45)\rReceiving objects:   8% (4/45)\rReceiving objects:  11% (5/45)\rReceiving objects:  13% (6/45)\rReceiving objects:  15% (7/45)\rReceiving objects:  17% (8/45)\rReceiving objects:  20% (9/45)\rReceiving objects:  22% (10/45)\rReceiving objects:  24% (11/45)\rReceiving objects:  26% (12/45)\rReceiving objects:  28% (13/45)\rReceiving objects:  31% (14/45)\rReceiving objects:  33% (15/45)\rReceiving objects:  35% (16/45)\rReceiving objects:  37% (17/45)\rReceiving objects:  40% (18/45)\rReceiving objects:  42% (19/45)\rReceiving objects:  44% (20/45)\rReceiving objects:  46% (21/45)\rReceiving objects:  48% (22/45)\rReceiving objects:  51% (23/45)\rReceiving objects:  53% (24/45)\rReceiving objects:  55% (25/45)\rReceiving objects:  57% (26/45)\rReceiving objects:  60% (27/45)\rReceiving objects:  62% (28/45)\rReceiving objects:  64% (29/45)\rReceiving objects:  66% (30/45)\rReceiving objects:  68% (31/45)\rReceiving objects:  71% (32/45)\rReceiving objects:  73% (33/45)\rReceiving objects:  75% (34/45)\rReceiving objects:  77% (35/45)\rReceiving objects:  80% (36/45)\rReceiving objects:  82% (37/45)\rReceiving objects:  84% (38/45)\rReceiving objects:  86% (39/45)\rReceiving objects:  88% (40/45)\rReceiving objects:  91% (41/45)\rReceiving objects:  93% (42/45)\rReceiving objects:  95% (43/45)\rReceiving objects:  97% (44/45)\rReceiving objects: 100% (45/45)\rReceiving objects: 100% (45/45), 39.40 KiB | 5.63 MiB/s, done.\r\nResolving deltas:   0% (0/18)\rResolving deltas:   5% (1/18)\rResolving deltas:  11% (2/18)\rResolving deltas:  16% (3/18)\rResolving deltas:  22% (4/18)\rResolving deltas:  33% (6/18)\rResolving deltas:  38% (7/18)\rResolving deltas:  44% (8/18)\rResolving deltas:  50% (9/18)\rResolving deltas:  55% (10/18)\rResolving deltas:  61% (11/18)\rResolving deltas:  72% (13/18)\rResolving deltas:  83% (15/18)\rResolving deltas:  88% (16/18)\rResolving deltas:  94% (17/18)\rResolving deltas: 100% (18/18)\rResolving deltas: 100% (18/18), done.\r\n/content/dpt\n/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!git clone https://github.com/xyaoooo/dpt\n",
    "%cd dpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf466e85",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfeac26a",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e3e944",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e2d5f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n    name = \"mrpc\"\n    labels_list = [\"0\", \"1\"]\n    metric = [metrics.accuracy, metrics.f1_score_with_invalid]\n    metric_names = [\"accuracy\", \"f1\"]\n    split_to_data_split = {\"train\": \"train\",\n                           \"validation\": \"validation\",\n                           \"test\": \"validation\"}\n\n    def load_dataset(self, split):\n        # return datasets.load_dataset('glue', 'mrpc', split=split, script_version=\"master\")\n        #return datasets.load_dataset('glue', 'mrpc', split=split)\n        files = {\n        'train': 'mrpc_csv/train.csv',\n        'validation': 'mrpc_csv/val.csv',\n        'test': 'mrpc_csv/test.csv'\n        }\n\n        # Load the dataset\n        return datasets.load_dataset('csv', data_files=files, split=split)\n\n    def preprocessor(self, example, add_prefix=True, add_vb=False):\n        src_texts = [\"sentence1:\", example['sentence1'],\n                     \"sentence2:\", example[\"sentence2\"]]\n        tgt_texts = [str(example['label'])]\n        if add_vb:\n            verbalizer = \"{ 0 : not equivalent, 1 : equivalent }\"\n        else:"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/dpt/data_utils.py. CHANGING:\n",
    "\n",
    "\n",
    "\n",
    "    def load_dataset(self, split):\n",
    "        # return datasets.load_dataset('glue', 'mrpc', split=split, script_version=\"master\")\n",
    "        return datasets.load_dataset('glue', 'mrpc', split=split)\n",
    "\n",
    "    def preprocessor(self, example, add_prefix=True, add_vb=False):\n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "\n",
    "    def load_dataset(self, split):\n",
    "        # return datasets.load_dataset('glue', 'mrpc', split=split, script_version=\"master\")\n",
    "        #return datasets.load_dataset('glue', 'mrpc', split=split)\n",
    "        files = {\n",
    "        'train': 'mrpc_csv/train.csv',\n",
    "        'validation': 'mrpc_csv/val.csv',\n",
    "        'test': 'mrpc_csv/test.csv'\n",
    "        }\n",
    "\n",
    "        # Load the dataset\n",
    "        return datasets.load_dataset('csv', data_files=files, split=split)\n",
    "\n",
    "    def preprocessor(self, example, add_prefix=True, add_vb=False):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a58c52",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd08c19",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96434e9",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e28fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n\nYour command has already run for 300 seconds. It is still running. You can continue to wait or interrupt it with \"action\": {\"type\": \"interrupt\"} or \"action\": {\"type\": \"continue_wait\"}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip3 install -q huggingface-hub numpy scikit-learn scipy sentencepiece spacy tabulate tokenizers torch torchvision tqdm transformers datasets===2.19.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc8d8ab",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be75b4ac",
   "metadata": {},
   "source": [
    "**Thought**: [continued after timeout]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b927a95",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc10972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Error: NameError: name 'continue_wait' is not defined\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcontinue_wait\u001b[49m\n\u001b[0;31mNameError\u001b[0m: name 'continue_wait' is not defined"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "continue_wait"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9f86e7",
   "metadata": {},
   "source": [
    "### Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b7e2b1",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebb1489",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d091e6b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Downloading...\nFrom: https://drive.google.com/uc?id=1raGLX-2vdX1uizYx2DdqZ_JzAkVeyeDF\nTo: /content/dpt/mrpc_csv.zip\n\r  0%|          | 0.00/3.96k [00:00<?, ?B/s]\r100%|██████████| 3.96k/3.96k [00:00<00:00, 9.11MB/s]\nArchive:  mrpc_csv.zip\r\n   creating: mrpc_csv/\r\n  inflating: mrpc_csv/val.csv        \r\n  inflating: mrpc_csv/test.csv       \r\n  inflating: mrpc_csv/train.csv      \r\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# URL of the Google Drive file\n",
    "file_url = \"https://drive.google.com/uc?id=1raGLX-2vdX1uizYx2DdqZ_JzAkVeyeDF\"\n",
    "output_file = \"mrpc_csv.zip\"\n",
    "\n",
    "# Download the file from Google Driveraise ValueError(f'mutable default {type(f.default)} for field ' ValueError: mutable default <class 'datasets.utils.version.Version'> for field version is not allowed: use default_factory\n",
    "gdown.download(file_url, output_file, quiet=False)\n",
    "!unzip mrpc_csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8814aec7",
   "metadata": {},
   "source": [
    "### Step 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6201ff1",
   "metadata": {},
   "source": [
    "Execution Start time: 18:38:13, End time: 18:38:13 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9276a9d0",
   "metadata": {},
   "source": [
    "**Thought**: Now that the dataset has been downloaded and extracted, I need to make sure the code runs on a CPU. I'll open the `main_ddp.py` script to check for any GPU-specific code and modify it to ensure it runs on a CPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94baca6d",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee4d110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/dpt/main_ddp.py (569 lines total)]\n1:import os\n2:import sys\n3:import random\n4:import argparse\n5:import numpy as np\n6:from tqdm import tqdm\n7:from datetime import datetime\n8:\n9:import torch\n10:import torch.nn as nn\n11:import torch.nn.functional as F\n12:from torch.utils.data import Dataset, DataLoader\n13:from torch import optim\n14:\n15:import torch.distributed as dist\n16:import torch.multiprocessing as mp\n17:from torch.nn.parallel import DistributedDataParallel as DDP\n18:from torch.utils.data.distributed import DistributedSampler\n19:\n20:import transformers\n21:from transformers import (\n22:    AutoTokenizer,\n23:    T5TokenizerFast,\n24:    T5ForConditionalGeneration,\n25:    AutoConfig,\n26:    AdamW,\n27:    get_scheduler,\n28:    set_seed,\n29:)\n30:transformers.logging.set_verbosity_error()\n31:\n32:from modeling_t5 import T5PromptForConditionalGeneration_param\n33:from data_utils import AutoTask\n34:from eval_utils import AutoPostProcessor\n35:from metrics import *\n36:from options import *\n37:from utils import *\n38:from loader import *\n39:\n40:import datasets\n41:from datasets import concatenate_datasets\n42:from datasets.utils.logging import set_verbosity_error\n43:from eval_utils import *\n44:set_verbosity_error()\n45:\n46:import logging\n47:logging.disable(logging.WARNING)\n48:\n49:import warnings\n50:warnings.filterwarnings(\"ignore\")\n51:\n52:\n53:def run(local_rank, args):\n54:\n55:    is_master = local_rank == 0\n56:    world_size = args.world_size\n57:    is_mp = world_size > 1\n58:    # set the device\n59:    device = local_rank\n60:\n61:    random.seed(args.seed)\n62:    np.random.seed(args.seed)\n63:    torch.manual_seed(args.seed)\n64:    torch.cuda.manual_seed_all(args.seed)\n65:    g = torch.Generator()\n66:    g.manual_seed(args.seed)\n67:\n68:    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n69:\n70:    ###################################################################################################\n71:    #   Load data                                                                                     #\n72:    ###################################################################################################\n73:    # Can processing multiple datasets\n74:    train_datasets, val_datasets = [], {}\n75:    for dataset_name in args.datasets_names:\n76:        train_dataset = AutoTask.get(dataset_name).get(split=\"train\",\n77:                                    split_validation_test=True,\n78:                                    add_prefix=args.add_task_prefix,\n79:                                    add_vb=args.add_verbalizer,\n80:                                    file_prefix=args.local_file_prefix,\n81:                                    n_obs=100000 if dataset_name == 'yelp_polarity' else None)\n82:        train_datasets.append(train_dataset)\n83:\n84:        val_dataset = AutoTask.get(dataset_name).get(split=\"validation\",\n85:                                    split_validation_test=True,\n86:                                    add_prefix=args.add_task_prefix,\n87:                                    add_vb=args.add_verbalizer,\n88:                                    file_prefix=args.local_file_prefix,\n89:                                    n_obs=None)\n90:        val_datasets.update({dataset_name: val_dataset})\n91:\n92:        \n93:        if is_master:\n94:            print(local_rank, dataset_name, 'Train\\t', train_dataset[0])\n95:            print(local_rank, dataset_name, 'Val\\t', val_dataset[0])\n96:            \n97:    # merge all datasets if there are multiple ones\n98:    train_datasets = concatenate_datasets(train_datasets)\n99:\n100:    if is_master:\n(469 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open main_ddp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1666f7ea",
   "metadata": {},
   "source": [
    "### Step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93103f57",
   "metadata": {},
   "source": [
    "Execution Start time: 18:38:16, End time: 18:38:17 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a3edb6",
   "metadata": {},
   "source": [
    "**Thought**: To ensure the code runs on a CPU, I need to modify the device setting in the `main_ddp.py` script. Currently, the device is set using `local_rank`, which is typically used for distributed GPU training. I'll change the device to `torch.device('cpu')` to ensure it runs on a CPU.\n",
    "\n",
    "Let's scroll down to see more of the file and locate where the device is set or used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950dcc23",
   "metadata": {},
   "source": [
    "**Action (scroll_down)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49557b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/dpt/main_ddp.py (569 lines total)]\n(98 more lines above)\n99:\n100:    if is_master:\n101:        print(local_rank, len(train_dataset), len(val_dataset))\n102:        print('# all training samples:', len(train_datasets))\n103:        print(train_datasets[0])\n104:\n105:    # Data loader \n106:    # Creating the Training and Validation dataset for further creation of Dataloader\n107:    training_set = CustomT5Dataset(train_datasets, tokenizer, \n108:                                    args.max_source_length, \n109:                                    args.max_target_length, \n110:                                    args.datasets_names\n111:                                    )\n112:    \n113:\n114:    val_sets = {data_name: CustomT5Dataset(data_set, tokenizer, \n115:                                           args.max_source_length, \n116:                                           args.max_target_length, \n117:                                           args.datasets_names\n118:                                           )\n119:               for data_name, data_set in val_datasets.items()}\n120:\n121:    # Defining the parameters for creation of dataloaders\n122:    train_params = {\n123:        'batch_size': args.train_batch_size,\n124:        'shuffle': not is_mp,  # not shuffle in DDP\n125:        'num_workers': 4,\n126:        'worker_init_fn': seed_worker,\n127:        'generator': g,\n128:        }\n129:\n130:    eval_params = {\n131:        'batch_size': args.eval_batch_size,\n132:        'shuffle': False,\n133:        'num_workers': 0,\n134:        }\n135:\n136:    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n137:    if is_mp:\n138:        sampler = DistributedSampler(training_set, num_replicas=world_size, rank=local_rank, shuffle=True)\n139:        TrainDataloader = DataLoader(training_set, sampler=sampler, **train_params)\n140:    else:\n141:        TrainDataloader = DataLoader(training_set, **train_params)\n142:\n143:    ValDataloaders = {data_name: DataLoader(data_set, **eval_params) for data_name, data_set in val_sets.items()}\n144:    \n145:    ###################################################################################################\n146:    #   Build the model                                                                                   #\n147:    ###################################################################################################\n148:    config = AutoConfig.from_pretrained(args.model_name)\n149:    config.len_enc_prompt = args.enc_prompt_tokens\n150:    config.len_dec_prompt = args.dec_prompt_tokens\n151:    config.add_enc_prompt = args.enc_prompt_tokens > 0\n152:    config.add_dec_prompt = args.dec_prompt_tokens > 0\n153:    config.num_tasks = len(args.datasets_names)\n154:    \n155:    config.bottle_neck = args.bottle_neck\n156:\n157:    model = T5PromptForConditionalGeneration_param.from_pretrained(args.model_name, config=config)\n158:\n159:    # Freeze the backbone model\n160:    for name, param in model.named_parameters():\n161:        param.requires_grad = False if 'prefix' not in name else True\n162:\n163:    if is_mp:\n164:        # initialize distributed data parallel (DDP)\n165:        dist.init_process_group(backend='nccl', rank=local_rank, world_size=world_size)\n166:\n167:        model = DDP(\n168:            model.to(local_rank),\n169:            device_ids=[local_rank],\n170:            find_unused_parameters=False\n171:        )\n172:    else:\n173:        model = model.to(device)\n174:\n175:    \n176:    if is_master:\n177:        print('Parameters to optimize: ', [n for n, p in model.named_parameters() if 'prefix' in n])\n178:    optimizer_grouped_parameters = [\n179:        {\n180:            \"params\": [p for n, p in model.named_parameters() if 'prefix' in n],\n181:            \"weight_decay\": args.weight_decay,\n182:        }\n183:    ]\n184: \n185:    optimizer = optim.AdamW(optimizer_grouped_parameters, lr=args.lr * np.sqrt(world_size))  # sacle the learning rate based on world_size\n186:\n187:    max_train_steps = args.max_train_steps if args.max_train_steps > 0 else args.n_epochs * len(TrainDataloader)\n188:    scheduler = get_scheduler(\n189:        name=args.lr_scheduler_type,\n190:        optimizer=optimizer,\n191:        num_warmup_steps=int(args.num_warmup_steps) if args.num_warmup_steps > 1 else int(args.num_warmup_steps * max_train_steps),\n192:        num_training_steps=max_train_steps\n193:    )\n194:\n195:    # Load the model or resume the training\n196:    resume_steps = 0\n197:    if args.from_checkpoint:\n198:        if is_mp:\n(371 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scroll_down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5417522c",
   "metadata": {},
   "source": [
    "### Step 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6f2757",
   "metadata": {},
   "source": [
    "Execution Start time: 18:38:19, End time: 18:38:19 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c95d891",
   "metadata": {},
   "source": [
    "**Thought**: I can see that the model is being moved to a device using `model.to(device)`. I need to ensure that the `device` variable is set to use the CPU. Let's scroll further down to find where the `device` variable is defined or used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4652fcda",
   "metadata": {},
   "source": [
    "**Action (scroll_down)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c740aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/dpt/main_ddp.py (569 lines total)]\n(196 more lines above)\n197:    if args.from_checkpoint:\n198:        if is_mp:\n199:            checkpoint = torch.load(args.from_checkpoint, map_location=torch.device(f'cuda:{local_rank}'))\n200:        else:\n201:            checkpoint = torch.load(args.from_checkpoint)\n202:\n203:        resume_steps = checkpoint['global_step']\n204:        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n205:        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n206:        param_dict = checkpoint['params']\n207:        for n, p in model.named_parameters():\n208:            if n in param_dict:\n209:                p.data = param_dict[n].clone().detach().to(device)\n210:\n211:        if is_master:\n212:            print('Resume the training from the checkpoint: ', args.from_checkpoint)\n213:\n214:    if args.auto_resume and args.save_ckpt_dir:\n215:        checkpoint_path = os.path.join(args.save_ckpt_dir, args.latest_ckpt_name)\n216:\n217:        if os.path.exists(checkpoint_path) or len(os.listdir(args.save_ckpt_dir)) > 0:\n218:            if not os.path.exists(checkpoint_path):\n219:                list_files = os.listdir(args.save_ckpt_dir)\n220:                # little parsing to get the step number: format -> sst2.qqp.mnli.qnli.squad.record.soft_prompts.source.step.900.pt\n221:                list_steps = [x.strip().split('.')[-2] for x in list_files]\n222:                max_idx = list_steps.index(max(list_steps))\n223:                checkpoint_path = os.path.join(args.save_ckpt_dir, list_files[max_idx])    \n224:\n225:            assert os.path.exists(checkpoint_path)\n226:\n227:            if is_mp:\n228:                checkpoint = torch.load(checkpoint_path, map_location=torch.device(f'cuda:{local_rank}'))\n229:            else:\n230:                checkpoint = torch.load(checkpoint_path)\n231:\n232:            resume_steps = checkpoint['global_step']\n233:            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n234:            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n235:            param_dict = checkpoint['params']\n236:            for n, p in model.named_parameters():\n237:                if n in param_dict:\n238:                    p.data = param_dict[n].clone().detach().to(device)\n239:\n240:            if is_master:\n241:                print(f'Auto-resume the training from the checkpoint: {checkpoint_path} from step {resume_steps}')\n242:        else:\n243:            if is_master:\n244:                print('No existing checkpoint; Start the training from scratch!')\n245:\n246:    \n247:    if is_master:\n248:        print('Prefix Parameters: ', [n for n, p in model.named_parameters() if 'prefix' in n])\n249:        print('Trainable Parameters: ', [n for n, p in model.named_parameters() if p.requires_grad])\n250:        print('#Trainable Parameters: ', sum(p.numel() for p in model.parameters() if p.requires_grad))\n251:\n252:        if args.prompt_type != 'dynamic':\n253:            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n254:            print(f'All trainable parameters: {trainable_params}; per task: {trainable_params / len(args.datasets_names)}')\n255:        else:\n256:            prompt_size = model.get_prompt_real_size()\n257:            trainable_params = prompt_size * 768\n258:            print(f'All trainable parameters: {trainable_params}; per task: {trainable_params / len(args.datasets_names)}')\n259:\n260:\n261:    ###################################################################################################\n262:    #   Training                                                                                      #\n263:    ###################################################################################################\n264:\n265:    if is_master:\n266:        print()\n267:        print('***** running training *****')\n268:        print(f'| batch_size: {args.train_batch_size} | num_epochs: {args.n_epochs} | num_train: {len(TrainDataloader)} |')\n269:\n270:    global_step = 0\n271:    best_dev_epoch = 0\n272:    best_dev_step = 0\n273:    best_epoch_dev = float('-inf')\n274:    best_step_dev = float('-inf')\n275:    val_res, test_res = 0, 0\n276:    final_test = 0\n277:    finetuned_checkpoint = None\n278:    # try:\n279:    while True:\n280:        for epoch in range(int(args.n_epochs)):\n281:            model.train()\n282:            step_count = 0\n283:\n284:            if epoch != 0 and len(args.datasets_names) > 1:\n285:                training_set.reset(epoch)\n286:\n287:            if is_mp:\n288:                sampler.set_epoch(epoch)\n289:\n290:            with tqdm(total=len(TrainDataloader), desc=f'Epoch {epoch}/{args.n_epochs}', unit='b', disable=args.close_tqdm) as pbar:\n291:                update_stride = len(TrainDataloader) // 100 if len(TrainDataloader) > 200 else 1\n292:\n293:                for step, batch in enumerate(TrainDataloader):\n294:                    global_step += 1\n295:\n296:                    if global_step <= resume_steps:\n(273 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scroll_down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49988a7",
   "metadata": {},
   "source": [
    "### Step 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1d6d76",
   "metadata": {},
   "source": [
    "Execution Start time: 18:38:21, End time: 18:38:22 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1362c4c",
   "metadata": {},
   "source": [
    "**Thought**: I still haven't found where the `device` is set. Let's scroll further down to locate the device setting or any GPU-specific code that needs to be modified for CPU usage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c894dd7d",
   "metadata": {},
   "source": [
    "**Action (scroll_down)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2907fb40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/dpt/main_ddp.py (569 lines total)]\n(294 more lines above)\n295:\n296:                    if global_step <= resume_steps:\n297:\n298:                        if step % update_stride == 0:\n299:                            pbar.update(update_stride)                        \n300:                        continue\n301:                    if len(batch['source_ids'].shape) == 3:\n302:                        source_ids = batch['source_ids'].squeeze(0).to(local_rank)\n303:                        source_mask = batch['source_mask'].squeeze(0).to(local_rank)\n304:                        labels = batch['target_ids'].squeeze(0).to(local_rank)\n305:                        task_ids = torch.tensor([x[0] for x in batch['task_ids']]).to(local_rank)\n306:                    else:\n307:                        source_ids = batch['source_ids'].to(local_rank)\n308:                        source_mask = batch['source_mask'].to(local_rank)\n309:                        labels = batch['target_ids'].to(local_rank)\n310:                        task_ids = batch['task_ids'].to(local_rank)\n311:\n312:                    outputs = model(input_ids=source_ids, attention_mask=source_mask, labels=labels, task_ids=task_ids)\n313:                    loss = outputs['loss']\n314:\n315:                    loss = loss / args.accumulate_steps\n316:                    loss.backward()\n317:                    step_count += 1\n318:\n319:                    if step_count == args.accumulate_steps:\n320:\n321:                        if args.max_grad_norm > 0:\n322:                            nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n323:\n324:                        step_count = 0\n325:                        optimizer.step()\n326:                        scheduler.step()\n327:                        optimizer.zero_grad()\n328:\n329:                    if step % update_stride == 0:\n330:                        pbar.set_postfix(**{'loss': loss.item(), 'lr': scheduler.get_last_lr()[0]})\n331:                        pbar.update(update_stride)\n332:\n333:                    if is_master and args.save_ckpt_dir:  # save the prompts\n334:                        if global_step % args.saving_steps == 0:\n335:                            checkpoint = {\n336:                                'global_step': global_step,\n337:                                'params': {n: p for n, p in model.named_parameters() if 'prefix' in n},\n338:                                'optimizer_state_dict': optimizer.state_dict(),\n339:                                'scheduler_state_dict': scheduler.state_dict(),\n340:                            }\n341:                            file_name = '.'.join(args.datasets) + '.soft_prompts.step.{}.pt'.format(global_step)\n342:                            save_checkpoint(args.save_ckpt_dir, checkpoint, file_name)\n343:                            print(f\"Saved soft prompts at: {os.path.join(args.save_ckpt_dir, file_name)}\")\n344:\n345:                            # Track the latest checkpoint for resuming\n346:                            save_checkpoint(args.save_ckpt_dir, checkpoint, args.latest_ckpt_name)\n347:\n348:                            if args.eval_in_train:\n349:                                if \"wsc\" in args.datasets_names[0] or \"WSC\" in args.datasets_names[0]:\n350:                                    res = task_evaluation_wsc(args, ValDataloaders, model, tokenizer, device)\n351:                                else:\n352:                                    res = task_evaluation(args, ValDataloaders, model, tokenizer, device)\n353:                                if len(res) > 1:\n354:                                    val_res = average_multi_task(res)\n355:                                else:\n356:                                    val_res = res[args.datasets_names[0]][TASK_TO_METRICS[args.datasets_names[0]][0]]\n357:\n358:                                if val_res > best_step_dev:\n359:                                    best_step_dev = val_res\n360:                                    best_dev_step = global_step\n361:                                    print(f'Step Best Val: {best_step_dev} at Step {global_step}: {res}')\n362:\n363:            # Epoch saving dring the training\n364:            if is_master and args.saving_each_epoch and global_step > resume_steps: # save prompts at the end of every epoch\n365:                checkpoint = {\n366:                    'global_step': global_step,\n367:                    'params': {n: p for n, p in model.named_parameters() if 'prefix' in n},\n368:                    'optimizer_state_dict': optimizer.state_dict(),\n369:                    'scheduler_state_dict': scheduler.state_dict(),\n370:                }\n371:                file_name = '.'.join(args.datasets) + '.soft_prompts.epoch.{}.pt'.format(epoch)\n372:                save_checkpoint(args.save_ckpt_dir, checkpoint, file_name)\n373:                print(f\"Saved soft prompts at: {os.path.join(args.save_ckpt_dir, file_name)}\")\n374:\n375:            # Epoch evaluation dring the training\n376:            if is_master and args.eval_in_train and global_step > resume_steps:\n377:                output_path = os.path.join(args.model_output_path, f\"dev_ep_{epoch}\")\n378:                if \"wsc\" in args.datasets_names[0] or \"WSC\" in args.datasets_names[0]:\n379:                    res = task_evaluation_wsc(args, ValDataloaders, model, tokenizer, device, output_path)\n380:                else:\n381:                    res = task_evaluation(args, ValDataloaders, model, tokenizer, device, output_path)\n382:                print(f'Epoch {epoch} - Validation: ', res)\n383:                if len(res) > 1:\n384:                    val_res = average_multi_task(res)\n385:                else:\n386:                    val_res = res[args.datasets_names[0]][TASK_TO_METRICS[args.datasets_names[0]][0]]\n387:\n388:\n389:                if val_res > best_epoch_dev:\n390:                    best_epoch_dev = val_res\n391:                    \n392:                    best_dev_epoch = epoch\n393:                    print(f'Epoch Best Val: {best_epoch_dev} at Epoch {best_dev_epoch}.')\n394:\n(175 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scroll_down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3343ca",
   "metadata": {},
   "source": [
    "### Step 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58330644",
   "metadata": {},
   "source": [
    "Execution Start time: 18:38:26, End time: 18:38:30 (duration: 3 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae6c865",
   "metadata": {},
   "source": [
    "**Thought**: I can see that the tensors are being moved to `local_rank`, which is typically used for GPU devices. To ensure the code runs on a CPU, I need to replace `local_rank` with `torch.device('cpu')`. Let's edit the file to make these changes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca9c03e",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb4ce1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/dpt/main_ddp.py (569 lines total)]\n(251 more lines above)\n252:        if args.prompt_type != 'dynamic':\n253:            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n254:            print(f'All trainable parameters: {trainable_params}; per task: {trainable_params / len(args.datasets_names)}')\n255:        else:\n256:            prompt_size = model.get_prompt_real_size()\n257:            trainable_params = prompt_size * 768\n258:            print(f'All trainable parameters: {trainable_params}; per task: {trainable_params / len(args.datasets_names)}')\n259:\n260:\n261:    ###################################################################################################\n262:    #   Training                                                                                      #\n263:    ###################################################################################################\n264:\n265:    if is_master:\n266:        print()\n267:        print('***** running training *****')\n268:        print(f'| batch_size: {args.train_batch_size} | num_epochs: {args.n_epochs} | num_train: {len(TrainDataloader)} |')\n269:\n270:    global_step = 0\n271:    best_dev_epoch = 0\n272:    best_dev_step = 0\n273:    best_epoch_dev = float('-inf')\n274:    best_step_dev = float('-inf')\n275:    val_res, test_res = 0, 0\n276:    final_test = 0\n277:    finetuned_checkpoint = None\n278:    # try:\n279:    while True:\n280:        for epoch in range(int(args.n_epochs)):\n281:            model.train()\n282:            step_count = 0\n283:\n284:            if epoch != 0 and len(args.datasets_names) > 1:\n285:                training_set.reset(epoch)\n286:\n287:            if is_mp:\n288:                sampler.set_epoch(epoch)\n289:\n290:            with tqdm(total=len(TrainDataloader), desc=f'Epoch {epoch}/{args.n_epochs}', unit='b', disable=args.close_tqdm) as pbar:\n291:                update_stride = len(TrainDataloader) // 100 if len(TrainDataloader) > 200 else 1\n292:\n293:                for step, batch in enumerate(TrainDataloader):\n294:                    global_step += 1\n295:\n296:                    if global_step <= resume_steps:\n297:\n298:                        if step % update_stride == 0:\n299:                            pbar.update(update_stride)                        \n300:                        continue\n301:                    if len(batch['source_ids'].shape) == 3:\n302:                        source_ids = batch['source_ids'].squeeze(0).to(torch.device('cpu'))\n303:                        source_mask = batch['source_mask'].squeeze(0).to(torch.device('cpu'))\n304:                        labels = batch['target_ids'].squeeze(0).to(torch.device('cpu'))\n305:                        task_ids = torch.tensor([x[0] for x in batch['task_ids']]).to(torch.device('cpu'))\n306:                    else:\n307:                        source_ids = batch['source_ids'].to(torch.device('cpu'))\n308:                        source_mask = batch['source_mask'].to(torch.device('cpu'))\n309:                        labels = batch['target_ids'].to(torch.device('cpu'))\n310:                        task_ids = batch['task_ids'].to(torch.device('cpu'))\n311:\n312:                    outputs = model(input_ids=source_ids, attention_mask=source_mask, labels=labels, task_ids=task_ids)\n313:                    loss = outputs['loss']\n314:\n315:                    loss = loss / args.accumulate_steps\n316:                    loss.backward()\n317:                    step_count += 1\n318:\n319:                    if step_count == args.accumulate_steps:\n320:\n321:                        if args.max_grad_norm > 0:\n322:                            nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n323:\n324:                        step_count = 0\n325:                        optimizer.step()\n326:                        scheduler.step()\n327:                        optimizer.zero_grad()\n328:\n329:                    if step % update_stride == 0:\n330:                        pbar.set_postfix(**{'loss': loss.item(), 'lr': scheduler.get_last_lr()[0]})\n331:                        pbar.update(update_stride)\n332:\n333:                    if is_master and args.save_ckpt_dir:  # save the prompts\n334:                        if global_step % args.saving_steps == 0:\n335:                            checkpoint = {\n336:                                'global_step': global_step,\n337:                                'params': {n: p for n, p in model.named_parameters() if 'prefix' in n},\n338:                                'optimizer_state_dict': optimizer.state_dict(),\n339:                                'scheduler_state_dict': scheduler.state_dict(),\n340:                            }\n341:                            file_name = '.'.join(args.datasets) + '.soft_prompts.step.{}.pt'.format(global_step)\n342:                            save_checkpoint(args.save_ckpt_dir, checkpoint, file_name)\n343:                            print(f\"Saved soft prompts at: {os.path.join(args.save_ckpt_dir, file_name)}\")\n344:\n345:                            # Track the latest checkpoint for resuming\n346:                            save_checkpoint(args.save_ckpt_dir, checkpoint, args.latest_ckpt_name)\n347:\n348:                            if args.eval_in_train:\n349:                                if \"wsc\" in args.datasets_names[0] or \"WSC\" in args.datasets_names[0]:\n350:                                    res = task_evaluation_wsc(args, ValDataloaders, model, tokenizer, device)\n351:                                else:\n(218 more lines below)\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 302:310\n",
    "                        source_ids = batch['source_ids'].squeeze(0).to(torch.device('cpu'))\n",
    "                        source_mask = batch['source_mask'].squeeze(0).to(torch.device('cpu'))\n",
    "                        labels = batch['target_ids'].squeeze(0).to(torch.device('cpu'))\n",
    "                        task_ids = torch.tensor([x[0] for x in batch['task_ids']]).to(torch.device('cpu'))\n",
    "                    else:\n",
    "                        source_ids = batch['source_ids'].to(torch.device('cpu'))\n",
    "                        source_mask = batch['source_mask'].to(torch.device('cpu'))\n",
    "                        labels = batch['target_ids'].to(torch.device('cpu'))\n",
    "                        task_ids = batch['task_ids'].to(torch.device('cpu'))\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4134576",
   "metadata": {},
   "source": [
    "### Step 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f34d25",
   "metadata": {},
   "source": [
    "Execution Start time: 18:38:32, End time: 18:38:46 (duration: 13 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5748227b",
   "metadata": {},
   "source": [
    "**Thought**: The necessary changes have been made to ensure the code runs on a CPU. Now, I will attempt to run the `main_ddp.py` script with the specified parameters to fine-tune the `t5-small` model on the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a6ed5c",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb53377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Traceback (most recent call last):\n  File \"/content/dpt/main_ddp.py\", line 33, in <module>\n    from data_utils import AutoTask\n  File \"/content/dpt/data_utils.py\", line 16, in <module>\n    import datasets\nModuleNotFoundError: No module named 'datasets'\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python main_ddp.py --datasets=mrpc --model_name=t5-small --enc_prompt_tokens 100 -ts 16 -e 1 --bottle_neck 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c46146e",
   "metadata": {},
   "source": [
    "### Step 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d35869e",
   "metadata": {},
   "source": [
    "Execution Start time: 18:38:48, End time: 18:38:59 (duration: 11 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70d175d",
   "metadata": {},
   "source": [
    "**Thought**: It seems like the `datasets` module is not found, which indicates it might not have been installed correctly. Let's install the `datasets` library again to ensure it's available for the script.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa1df70",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f8905e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/site-packages)\nCollecting datasets\n  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from datasets) (1.25.2)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/site-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from datasets) (2.0.3)\nCollecting requests>=2.32.2 (from datasets)\n  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/site-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2023.6.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/site-packages (from datasets) (3.8.3)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/site-packages (from datasets) (0.23.3)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from datasets) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (2.1.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.0.7)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.6.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDownloading datasets-2.21.0-py3-none-any.whl (527 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/527.3 kB ? eta -:--:--\n\n\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/64.9 kB ? eta -:--:--\n\n\u001b[?25hWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/site-packages)\nInstalling collected packages: requests, datasets\n  Attempting uninstall: requests\n    Found existing installation: requests 2.31.0\n    Uninstalling requests-2.31.0:\n      Successfully uninstalled requests-2.31.0\nSuccessfully installed datasets-2.21.0 requests-2.32.3\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d341ac1",
   "metadata": {},
   "source": [
    "### Step 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0161fa40",
   "metadata": {},
   "source": [
    "Execution Start time: 18:39:02, End time: 18:40:13 (duration: 70 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35737601",
   "metadata": {},
   "source": [
    "**Thought**: The `datasets` library has been successfully installed. Now, let's try running the `main_ddp.py` script again with the specified parameters to fine-tune the `t5-small` model on the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397d139a",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b310dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stating time:  08/22/2024 18:39:21\nRaw Arguments:  Namespace(seed=42, from_checkpoint='', auto_resume=False, save_ckpt_dir='./saved_models', load_source_path='', saving_steps=1000, saving_each_epoch=True, latest_ckpt_name='latest_checkpoint.pt', n_epochs=1, train_batch_size=16, eval_batch_size=128, lr=0.3, max_grad_norm=1.0, weight_decay=1e-05, accumulate_steps=1, lr_scheduler_type='linear', num_warmup_steps=0.1, max_train_steps=0, min_training_epoch=20, early_stopping_patience=10, mode='train', close_tqdm=True, max_epochs_before_stop=2, model_name='t5-small', datasets=['mrpc'], max_source_length=256, max_target_length=128, prompt_type='param', is_dynamic_share=False, eval_in_train=True, enc_prompt_tokens=100, dec_prompt_tokens=0, sharing_ratio=1.0, share_seed=42, add_task_prefix=True, add_verbalizer=False, port=12355, gupids=None, sampling_method='uniform', size_limit=262144, stoch_task=False, bottle_neck=10, model_output_path='./saved_outputs', local_file_prefix='/gpfs/u/home/DPTV/DPTVhnwz/scratch/mrqa_datasets/datasets', datasets_names=['mrpc'])\nProcess ID:  160\nUse single gpu!\n\ntokenizer_config.json:   0%|                        | 0.00/2.32k [00:00<?, ?B/s]\n\n\nspiece.model:   0%|                                  | 0.00/792k [00:00<?, ?B/s]\n\n\n\ntokenizer.json:   0%|                               | 0.00/1.39M [00:00<?, ?B/s]\n\n0 mrpc Train\t {'source': 'sentence1: The companies uniformly declined to give specific numbers on customer turnover , saying they will release those figures only when they report overall company performance at year-end . sentence2: The companies , however , declined to give specifics on customer turnover , saying they would release figures only when they report their overall company performance .', 'target': '1', 'task': 'mrpc'}\n0 mrpc Val\t {'source': 'sentence1: The additional contribution brings total U.S. food aid to North Korea this year to 100,000 tonnes . sentence2: The donation of 60,000 tons brings the total of U.S. contributions for the year to 100,000 .', 'target': '1', 'task': 'mrpc'}\n0 10 10\n# all training samples: 10\n{'source': 'sentence1: The companies uniformly declined to give specific numbers on customer turnover , saying they will release those figures only when they report overall company performance at year-end . sentence2: The companies , however , declined to give specifics on customer turnover , saying they would release figures only when they report their overall company performance .', 'target': '1', 'task': 'mrpc'}\n\nconfig.json:   0%|                                  | 0.00/1.21k [00:00<?, ?B/s]\n\n\nmodel.safetensors:   0%|                             | 0.00/242M [00:00<?, ?B/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngeneration_config.json:   0%|                         | 0.00/147 [00:00<?, ?B/s]\ngeneration_config.json: 100%|███████████████████| 147/147 [00:00<00:00, 634kB/s]\nTraceback (most recent call last):\n  File \"/content/dpt/main_ddp.py\", line 569, in <module>\n    main()\n  File \"/content/dpt/main_ddp.py\", line 562, in main\n    run(0, args)\n  File \"/content/dpt/main_ddp.py\", line 173, in run\n    model = model.to(device)\n  File \"/usr/local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 2692, in to\n    return super().to(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1174, in to\n    return self._apply(convert)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 805, in _apply\n    param_applied = fn(param)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1160, in convert\n    return t.to(\n  File \"/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 314, in _lazy_init\n    torch._C._cuda_init()\nRuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python main_ddp.py --datasets=mrpc --model_name=t5-small --enc_prompt_tokens 100 -ts 16 -e 1 --bottle_neck 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e97f17f",
   "metadata": {},
   "source": [
    "### Step 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15303bfb",
   "metadata": {},
   "source": [
    "Execution Start time: 18:40:15, End time: 18:40:16 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4860cdb2",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the code is still attempting to use a GPU, which requires an NVIDIA driver. I need to ensure that the `device` is set to CPU throughout the script. Let's search for the term \"device\" in the `main_ddp.py` file to locate all instances where the device is set or used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dc6ad3",
   "metadata": {},
   "source": [
    "**Action (search_file)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23e1903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Found 29 matches for \"device\" in /content/dpt/main_ddp.py:\nLine 58:    # set the device\nLine 59:    device = local_rank\nLine 169:            device_ids=[local_rank],\nLine 173:        model = model.to(device)\nLine 199:            checkpoint = torch.load(args.from_checkpoint, map_location=torch.device(f'cuda:{local_rank}'))\nLine 209:                p.data = param_dict[n].clone().detach().to(device)\nLine 228:                checkpoint = torch.load(checkpoint_path, map_location=torch.device(f'cuda:{local_rank}'))\nLine 238:                    p.data = param_dict[n].clone().detach().to(device)\nLine 302:                        source_ids = batch['source_ids'].squeeze(0).to(torch.device('cpu'))\nLine 303:                        source_mask = batch['source_mask'].squeeze(0).to(torch.device('cpu'))\nLine 304:                        labels = batch['target_ids'].squeeze(0).to(torch.device('cpu'))\nLine 305:                        task_ids = torch.tensor([x[0] for x in batch['task_ids']]).to(torch.device('cpu'))\nLine 307:                        source_ids = batch['source_ids'].to(torch.device('cpu'))\nLine 308:                        source_mask = batch['source_mask'].to(torch.device('cpu'))\nLine 309:                        labels = batch['target_ids'].to(torch.device('cpu'))\nLine 310:                        task_ids = batch['task_ids'].to(torch.device('cpu'))\nLine 350:                                    res = task_evaluation_wsc(args, ValDataloaders, model, tokenizer, device)\nLine 352:                                    res = task_evaluation(args, ValDataloaders, model, tokenizer, device)\nLine 379:                    res = task_evaluation_wsc(args, ValDataloaders, model, tokenizer, device, output_path)\nLine 381:                    res = task_evaluation(args, ValDataloaders, model, tokenizer, device, output_path)\nLine 407:def task_evaluation_wsc(args, dataloader_dict, model, tokenizer, device, output_path=None):\nLine 422:                source_ids = batch['source_ids'].to(device)\nLine 423:                source_mask = batch['source_mask'].to(device)\nLine 424:                task_ids = batch['task_ids'].to(device)\nLine 476:def task_evaluation(args, dataloader_dict, model, tokenizer, device, output_path=None):\nLine 486:                source_ids = batch['source_ids'].to(device)\nLine 487:                source_mask = batch['source_mask'].to(device)\nLine 488:                task_ids = batch['task_ids'].to(device)\nLine 543:    args.gupids = args.gupids if args.gupids else list(range(torch.cuda.device_count()))\nEnd of matches for \"device\" in /content/dpt/main_ddp.py\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_file device main_ddp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fcb7fb",
   "metadata": {},
   "source": [
    "### Step 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6280897a",
   "metadata": {},
   "source": [
    "Execution Start time: 18:40:18, End time: 18:40:20 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069a1d85",
   "metadata": {},
   "source": [
    "**Thought**: The `device` is initially set using `local_rank` at line 59. I need to change this to use the CPU. Let's edit the file to set `device = torch.device('cpu')` at line 59.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6416b5",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641b424e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/dpt/main_ddp.py (569 lines total)]\n(8 more lines above)\n9:import torch\n10:import torch.nn as nn\n11:import torch.nn.functional as F\n12:from torch.utils.data import Dataset, DataLoader\n13:from torch import optim\n14:\n15:import torch.distributed as dist\n16:import torch.multiprocessing as mp\n17:from torch.nn.parallel import DistributedDataParallel as DDP\n18:from torch.utils.data.distributed import DistributedSampler\n19:\n20:import transformers\n21:from transformers import (\n22:    AutoTokenizer,\n23:    T5TokenizerFast,\n24:    T5ForConditionalGeneration,\n25:    AutoConfig,\n26:    AdamW,\n27:    get_scheduler,\n28:    set_seed,\n29:)\n30:transformers.logging.set_verbosity_error()\n31:\n32:from modeling_t5 import T5PromptForConditionalGeneration_param\n33:from data_utils import AutoTask\n34:from eval_utils import AutoPostProcessor\n35:from metrics import *\n36:from options import *\n37:from utils import *\n38:from loader import *\n39:\n40:import datasets\n41:from datasets import concatenate_datasets\n42:from datasets.utils.logging import set_verbosity_error\n43:from eval_utils import *\n44:set_verbosity_error()\n45:\n46:import logging\n47:logging.disable(logging.WARNING)\n48:\n49:import warnings\n50:warnings.filterwarnings(\"ignore\")\n51:\n52:\n53:def run(local_rank, args):\n54:\n55:    is_master = local_rank == 0\n56:    world_size = args.world_size\n57:    is_mp = world_size > 1\n58:    # set the device\n59:    device = torch.device('cpu')\n60:\n61:    random.seed(args.seed)\n62:    np.random.seed(args.seed)\n63:    torch.manual_seed(args.seed)\n64:    torch.cuda.manual_seed_all(args.seed)\n65:    g = torch.Generator()\n66:    g.manual_seed(args.seed)\n67:\n68:    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n69:\n70:    ###################################################################################################\n71:    #   Load data                                                                                     #\n72:    ###################################################################################################\n73:    # Can processing multiple datasets\n74:    train_datasets, val_datasets = [], {}\n75:    for dataset_name in args.datasets_names:\n76:        train_dataset = AutoTask.get(dataset_name).get(split=\"train\",\n77:                                    split_validation_test=True,\n78:                                    add_prefix=args.add_task_prefix,\n79:                                    add_vb=args.add_verbalizer,\n80:                                    file_prefix=args.local_file_prefix,\n81:                                    n_obs=100000 if dataset_name == 'yelp_polarity' else None)\n82:        train_datasets.append(train_dataset)\n83:\n84:        val_dataset = AutoTask.get(dataset_name).get(split=\"validation\",\n85:                                    split_validation_test=True,\n86:                                    add_prefix=args.add_task_prefix,\n87:                                    add_vb=args.add_verbalizer,\n88:                                    file_prefix=args.local_file_prefix,\n89:                                    n_obs=None)\n90:        val_datasets.update({dataset_name: val_dataset})\n91:\n92:        \n93:        if is_master:\n94:            print(local_rank, dataset_name, 'Train\\t', train_dataset[0])\n95:            print(local_rank, dataset_name, 'Val\\t', val_dataset[0])\n96:            \n97:    # merge all datasets if there are multiple ones\n98:    train_datasets = concatenate_datasets(train_datasets)\n99:\n100:    if is_master:\n101:        print(local_rank, len(train_dataset), len(val_dataset))\n102:        print('# all training samples:', len(train_datasets))\n103:        print(train_datasets[0])\n104:\n105:    # Data loader \n106:    # Creating the Training and Validation dataset for further creation of Dataloader\n107:    training_set = CustomT5Dataset(train_datasets, tokenizer, \n108:                                    args.max_source_length, \n(461 more lines below)\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 59:59\n",
    "    device = torch.device('cpu')\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c565989d",
   "metadata": {},
   "source": [
    "### Step 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad3fb51",
   "metadata": {},
   "source": [
    "Execution Start time: 18:40:21, End time: 18:42:22 (duration: 120 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6a6442",
   "metadata": {},
   "source": [
    "**Thought**: The `device` has been successfully set to use the CPU. Now, let's try running the `main_ddp.py` script again with the specified parameters to fine-tune the `t5-small` model on the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e7b22f",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d1af21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stating time:  08/22/2024 18:40:32\nRaw Arguments:  Namespace(seed=42, from_checkpoint='', auto_resume=False, save_ckpt_dir='./saved_models', load_source_path='', saving_steps=1000, saving_each_epoch=True, latest_ckpt_name='latest_checkpoint.pt', n_epochs=1, train_batch_size=16, eval_batch_size=128, lr=0.3, max_grad_norm=1.0, weight_decay=1e-05, accumulate_steps=1, lr_scheduler_type='linear', num_warmup_steps=0.1, max_train_steps=0, min_training_epoch=20, early_stopping_patience=10, mode='train', close_tqdm=True, max_epochs_before_stop=2, model_name='t5-small', datasets=['mrpc'], max_source_length=256, max_target_length=128, prompt_type='param', is_dynamic_share=False, eval_in_train=True, enc_prompt_tokens=100, dec_prompt_tokens=0, sharing_ratio=1.0, share_seed=42, add_task_prefix=True, add_verbalizer=False, port=12355, gupids=None, sampling_method='uniform', size_limit=262144, stoch_task=False, bottle_neck=10, model_output_path='./saved_outputs', local_file_prefix='/gpfs/u/home/DPTV/DPTVhnwz/scratch/mrqa_datasets/datasets', datasets_names=['mrpc'])\nProcess ID:  251\nUse single gpu!\n0 mrpc Train\t {'source': 'sentence1: The companies uniformly declined to give specific numbers on customer turnover , saying they will release those figures only when they report overall company performance at year-end . sentence2: The companies , however , declined to give specifics on customer turnover , saying they would release figures only when they report their overall company performance .', 'target': '1', 'task': 'mrpc'}\n0 mrpc Val\t {'source': 'sentence1: The additional contribution brings total U.S. food aid to North Korea this year to 100,000 tonnes . sentence2: The donation of 60,000 tons brings the total of U.S. contributions for the year to 100,000 .', 'target': '1', 'task': 'mrpc'}\n0 10 10\n# all training samples: 10\n{'source': 'sentence1: The companies uniformly declined to give specific numbers on customer turnover , saying they will release those figures only when they report overall company performance at year-end . sentence2: The companies , however , declined to give specifics on customer turnover , saying they would release figures only when they report their overall company performance .', 'target': '1', 'task': 'mrpc'}\nParameters to optimize:  ['encoder.prefix_first', 'encoder.prefix_second']\nPrefix Parameters:  ['encoder.prefix_first', 'encoder.prefix_second']\nTrainable Parameters:  ['encoder.prefix_first', 'encoder.prefix_second']\n#Trainable Parameters:  6120\nAll trainable parameters: 6120; per task: 6120.0\n\n***** running training *****\n| batch_size: 16 | num_epochs: 1 | num_train: 1 |\nSaved soft prompts at: ./saved_models/mrpc.soft_prompts.epoch.0.pt\nEpoch 0 - Validation:  {'mrpc': {'accuracy': 0.0, 'f1': 0.0}}\nEpoch Best Val: 0.0 at Epoch 0.\n***** training ends *****\n\nbest dev acc: 0.00000 (at epoch 0)\nbest dev acc: -inf (at step 0)\n\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python main_ddp.py --datasets=mrpc --model_name=t5-small --enc_prompt_tokens 100 -ts 16 -e 1 --bottle_neck 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5c94ee",
   "metadata": {},
   "source": [
    "### Step 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6a46d8",
   "metadata": {},
   "source": [
    "Execution Start time: 18:42:23, End time: 18:42:23 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4f9503",
   "metadata": {},
   "source": [
    "**Thought**: The script has successfully run, and the accuracy on the development set is reported as 0.0. Now, I will submit the result in the required JSON format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5dcb78",
   "metadata": {},
   "source": [
    "**Action (submit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d61cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{accuracy: 0.0}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit {\"accuracy\": 0.0}"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
