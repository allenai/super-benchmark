{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5a5ef28",
   "metadata": {},
   "source": [
    "### Query\n",
    "Compare different ranking approaches (TF-IDF, BM25, DPH, PL2, DirichletLM) for the dataset of clarifying questions available at https://drive.google.com/drive/folders/14Zqkw4px7V6l732ZFu_evjEifRh-M7bA. Report the `ndcg_cut_20` metric for each approach, as a json structured as follows: {\"TF-IDF\": 0.0, \"BM25\": 0.0, \"DPH\": 0.0, \"PL2\": 0.0, \"DirichletLM\": 0.0} (replace 0.0 with the actual values).\n",
    "\n",
    "Additional instructions:\n",
    "\n",
    "\n",
    "Git repository: https://github.com/rahmanidashti/acqsurvey\n",
    "\n",
    "I have already executed some of the required steps. However, I did not yet make any required changes to load the request dataset. Your goal is to successfully load the dataset and complete the remaining steps to achieve the user request. You can re-run any of the pre-executed steps, and finally should successfully run\n",
    "```\n",
    "!python ranking_approaches.py\n",
    "```\n",
    "to submit the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672bc753",
   "metadata": {},
   "source": [
    "### Prompt\n",
    "<details>\n",
    "[{'role': 'system', 'content': 'Interact in a jupyter notebook to solve the request.\\nOutput a thought and an action.\\nThought can reason about the current situation, and action allows you to interact with the Jupyter notebook. There are three types of actions:\\n(1) execute, which executes a notebook cell with the command under the \"content\" field. This could be a bash command (beginning with !), a python command, or magic command (beginning with %).\\n(2) edit, which allows you to replace existing lines in a file with new lines. To replace lines, you will need to provide the lines as they appear before the replacement, and the lines to replace them with.\\nIn the \"content\" field, format should be ```{filename}\\n<<<BEFORE_EDIT>>>\\nlines before edit\\nlines before edit\\n<<<AFTER_EDIT>>>\\nlines after edit\\nlines after edit```\\n  - Include the entire lines WITH indentation. After editing, make sure indentation is correct.\\n(3) submit, which submits the final answer (or null if there\\'s no answer to submit), formatted as JSON (make sure to use double quotes for keys and values).\\nAlways output exactly one \"thought\" and one \"action\", and encapsulate the content with <content>...</content> tags.\\nI will provide the cell output (observation) for you.\\n\\nTips:\\n- When a github repository is given, you should start by cloning it. You can read the readme file and browse the repo for installation and usage instructions.\\n- Avoid using conda, instead use pip for package management.\\n- Each execution has a 5-minute timeout. After this period, you can decide whether to interrupt the kernel or continue waiting.\\n  You should interrupt if: (1) you are prompted with an interactive message (e.g., \"Do you want to continue? [Y/n]\"). In that case, you can bypass it by executing echo \"y\" | command, or (2) the execution is taking longer than expected because the wrong task is being executed, such as a different model or incorrect number of epochs.\\n  You should continue to wait if the current execution is correct and proceeding as expected.\\n\\nHere are some examples.\\nGit repository: EleutherAI/lm-evaluation-harness\\nQuery: Evaluate EleutherAI/gpt-j-6B on hellaswag. What\\'s the score?\\nThought: First, I need to clone the GitHub repository \\'EleutherAI/lm-evaluation-harness\\'.\\nAction: execute\\n<content>\\n!git clone https://github.com/EleutherAI/lm-evaluation-harness.git\\n</content>\\nObservation: ```\\nCloning into \\'lm-evaluation-harness\\'...\\n```\\nThought: The repository has been cloned. Next, I should check the README file for installation and usage instructions to proceed with evaluating EleutherAI/gpt-j-6B on hellaswag.\\nAction: execute\\n<content>\\n!cat lm-evaluation-harness/README.md\\n</content>\\nObservation: ```# Language Model Evaluation Harness ... truncated ...\\n```\\nThought: The README provides detailed instructions for installing the `lm-eval` package and evaluating models like GPT-J-6B on tasks like hellaswag. I need to install the package and its dependencies first.\\nAction: execute\\n<content>\\n!pip install -e lm-evaluation-harness/.\\n</content>\\nObservation: ```\\nObtaining file:///home/nora/lm-evaluation-harness\\n  Installing build dependencies ... truncated ...\\n```\\nThought: The `lm-eval` package and its dependencies have been installed successfully. Now, I can proceed with evaluating the EleutherAI/gpt-j-6B model on the hellaswag task as per the instructions in the README.\\nAction: execute\\n<content>\\n!lm_eval --model hf --model_args pretrained=EleutherAI/gpt-j-6B --tasks hellaswag --device cuda:0 --batch_size 8\\n</content>\\nObservation: ```\\n{\\n  \"hellaswag\": {\\n    \"acc\": 0.24603658536585363,\\n    \"acc_norm\": 0.34436059706446426,\\n  },  ... truncated ...\\n```\\nThought: The `lm-eval` was executed successfully, providing the metrics I needed. The `acc` metric is 0.2460. The `acc_norm` metric is 0.3444\\nAction: submit\\n<content>\\n{\"acc\": 0.2460, \"acc_norm\": 0.3444}\\n</content>\\n\\nQuery: use pandas to read the file \\'data.csv\\' and display the first 2 rows.\\nThought: First, I need to read the file \\'data.csv\\' using the pandas library.\\nAction: execute\\n<content>\\nimport pandas as pd\\npd.read_csv(\\'data.csv\\').head()\\n</content>\\nObservation: ```\\n    name\\n  0  Alice\\n   1    Bob\\n  2  Charlie\\n```\\nThought: The file \\'data.csv\\' has been read successfully and the first 2 rows have been displayed.\\nAction: submit\\n<content>\\n[\"Alice\", \"Bob\"]\\n</content>\\n\\nQuery: Change the model name in the \\'config.json\\' file from \\'roberta-base\\' to \\'roberta-large\\'.\\nThought: First, I need to read the \\'config.json\\' file to find the location where I can replace the model name.\\nAction: execute\\n<content>\\n!cat config.json\\n</content>\\nObservation: ```\\n{\\n    \"model\": \"roberta-base\",\\n    \"batch_size\": 8,\\n    \"epochs\": 10\\n}\\n```\\nThought: The file has been read successfully. Now, I can replace the model name in the \\'config.json\\' file.\\nAction: edit\\n<content>\\nconfig.json\\n<<<BEFORE_EDIT>>>\\n{\\n    \"model\": \"roberta-base\",\\n    \"batch_size\": 8,\\n<<<AFTER_EDIT>>>\\n{\\n    \"model\": \"roberta-large\",\\n    \"batch_size\": 8,\\n</content>\\nObservation: \"editing file config.json\"\\nThought: The model name has been replaced in the \\'config.json\\' file.\\nAction: submit\\n<content>\\nnull\\n</content>\\n'}, {'role': 'user', 'content': 'Query: Compare different ranking approaches (TF-IDF, BM25, DPH, PL2, DirichletLM) for the dataset of clarifying questions available at https://drive.google.com/drive/folders/14Zqkw4px7V6l732ZFu_evjEifRh-M7bA. Report the `ndcg_cut_20` metric for each approach, as a json structured as follows: {\"TF-IDF\": 0.0, \"BM25\": 0.0, \"DPH\": 0.0, \"PL2\": 0.0, \"DirichletLM\": 0.0} (replace 0.0 with the actual values).\\n\\nAdditional instructions:\\n\\n\\nGit repository: https://github.com/rahmanidashti/acqsurvey\\n\\nI have already executed some of the required steps. However, I did not yet make any required changes to load the request dataset. Your goal is to successfully load the dataset and complete the remaining steps to achieve the user request. You can re-run any of the pre-executed steps, and finally should successfully run\\n```\\n!python ranking_approaches.py\\n```\\nto submit the answer.'}]\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654d002a",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f589688",
   "metadata": {},
   "source": [
    "Execution Start time: 22:56:16, End time: 22:56:16 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1f56c0",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2bf4bf",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cf287a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare the different ranking approaches of this repository (TF-IDF, BM25, DPH, PL2, DirichletLM) for the provided dataset of clarifying questions. Report all metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5f5524",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb85e5d",
   "metadata": {},
   "source": [
    "Execution Start time: 22:56:16, End time: 22:56:21 (duration: 4 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d02379",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36f37b3",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62229193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cloning into 'acqsurvey'...\nremote: Enumerating objects: 368, done.\u001b[K\nremote: Counting objects: 100% (140/140)\u001b[K\nremote: Counting objects: 100% (140/140), done.\u001b[K\nremote: Compressing objects: 100% (124/124)\u001b[K\nremote: Compressing objects: 100% (124/124), done.\u001b[K\nremote: Total 368 (delta 76), reused 31 (delta 11), pack-reused 228 (from 1)\u001b[K\nReceiving objects: 100% (368/368), 78.38 MiB | 31.35 MiB/s\nReceiving objects: 100% (368/368), 79.16 MiB | 31.42 MiB/s, done.\nResolving deltas: 100% (187/187)\nResolving deltas: 100% (187/187), done.\nUpdating files: 100% (23/23)\nUpdating files: 100% (23/23), done.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!git clone https://github.com/rahmanidashti/acqsurvey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee94206b",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d5a890",
   "metadata": {},
   "source": [
    "Execution Start time: 22:56:21, End time: 22:56:21 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1b9313",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decc1f37",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9526925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/acqsurvey\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd acqsurvey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1569147d",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8464084",
   "metadata": {},
   "source": [
    "Execution Start time: 22:56:21, End time: 22:57:43 (duration: 82 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e36ee2",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4b74bd",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e19011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Could not find a version that satisfies the requirement torch==1.4.0 (from pyterrierbert) (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0)\nERROR: No matching distribution found for torch==1.4.0\nCollecting git+https://github.com/terrierteam/pyterrier_doc2query.git\n  Cloning https://github.com/terrierteam/pyterrier_doc2query.git to /tmp/pip-req-build-16p7jsz9\n  Running command git clone --filter=blob:none --quiet https://github.com/terrierteam/pyterrier_doc2query.git /tmp/pip-req-build-16p7jsz9\n  Resolved https://github.com/terrierteam/pyterrier_doc2query.git to commit 5b380d1c4baee7081fee3f25f040821320e83904\n  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\n\u001b[?25hRequirement already satisfied: python-terrier>=0.5.0 in /usr/local/lib/python3.10/site-packages (from pyterrier-doc2query==0.0.1) (0.10.1)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from pyterrier-doc2query==0.0.1) (2.0.3)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/site-packages (from pyterrier-doc2query==0.0.1) (4.40.2)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/site-packages (from pyterrier-doc2query==0.0.1) (2.2.1+cpu)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/site-packages (from pyterrier-doc2query==0.0.1) (0.1.99)\nCollecting GitPython (from pyterrier-doc2query==0.0.1)\n  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\nCollecting npids (from pyterrier-doc2query==0.0.1)\n  Downloading npids-0.0.3-py3-none-any.whl.metadata (5.5 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (1.25.2)\nRequirement already satisfied: wget in /usr/local/lib/python3.10/site-packages (from python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (3.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (4.66.4)\nRequirement already satisfied: pyjnius>=1.4.2 in /usr/local/lib/python3.10/site-packages (from python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (1.6.1)\nRequirement already satisfied: matchpy in /usr/local/lib/python3.10/site-packages (from python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (0.5.5)\nRequirement already satisfied: deprecated in /usr/local/lib/python3.10/site-packages (from python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (1.2.14)\nRequirement already satisfied: chest in /usr/local/lib/python3.10/site-packages (from python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (0.2.3)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/site-packages (from python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (1.11.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (2.31.0)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/site-packages (from python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (1.4.2)\nRequirement already satisfied: nptyping==1.4.4 in /usr/local/lib/python3.10/site-packages (from python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (1.4.4)\nRequirement already satisfied: more-itertools in /usr/local/lib/python3.10/site-packages (from python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (10.4.0)\nRequirement already satisfied: ir-datasets>=0.3.2 in /usr/local/lib/python3.10/site-packages (from python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (0.5.8)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (3.1.4)\nRequirement already satisfied: statsmodels in /usr/local/lib/python3.10/site-packages (from python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (0.14.2)\nRequirement already satisfied: ir-measures>=0.3.1 in /usr/local/lib/python3.10/site-packages (from python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (0.3.3)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/site-packages (from python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (0.3.8)\nRequirement already satisfied: pytrec-eval-terrier>=0.5.3 in /usr/local/lib/python3.10/site-packages (from python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (0.5.6)\nRequirement already satisfied: typish>=1.7.0 in /usr/local/lib/python3.10/site-packages (from nptyping==1.4.4->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (1.9.3)\nCollecting gitdb<5,>=4.0.1 (from GitPython->pyterrier-doc2query==0.0.1)\n  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas->pyterrier-doc2query==0.0.1) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas->pyterrier-doc2query==0.0.1) (2024.1)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/site-packages (from pandas->pyterrier-doc2query==0.0.1) (2024.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from torch->pyterrier-doc2query==0.0.1) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/site-packages (from torch->pyterrier-doc2query==0.0.1) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/site-packages (from torch->pyterrier-doc2query==0.0.1) (1.12)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch->pyterrier-doc2query==0.0.1) (3.3)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/site-packages (from torch->pyterrier-doc2query==0.0.1) (2023.6.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/site-packages (from transformers->pyterrier-doc2query==0.0.1) (0.23.3)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from transformers->pyterrier-doc2query==0.0.1) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers->pyterrier-doc2query==0.0.1) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers->pyterrier-doc2query==0.0.1) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/site-packages (from transformers->pyterrier-doc2query==0.0.1) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/site-packages (from transformers->pyterrier-doc2query==0.0.1) (0.4.3)\nCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython->pyterrier-doc2query==0.0.1)\n  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\nRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (4.12.3)\nRequirement already satisfied: inscriptis>=2.2.0 in /usr/local/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (2.5.0)\nRequirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (5.3.0)\nRequirement already satisfied: trec-car-tools>=2.5.4 in /usr/local/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (2.6)\nRequirement already satisfied: lz4>=3.1.10 in /usr/local/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (4.3.3)\nRequirement already satisfied: warc3-wet>=0.2.3 in /usr/local/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (0.2.5)\nRequirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /usr/local/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (0.2.5)\nRequirement already satisfied: zlib-state>=0.1.3 in /usr/local/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (0.1.6)\nRequirement already satisfied: ijson>=3.1.3 in /usr/local/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (3.3.0)\nRequirement already satisfied: unlzw3>=0.2.1 in /usr/local/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (0.2.2)\nRequirement already satisfied: cwl-eval>=1.0.10 in /usr/local/lib/python3.10/site-packages (from ir-measures>=0.3.1->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (1.0.12)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->pyterrier-doc2query==0.0.1) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (2.0.7)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (2024.6.2)\nRequirement already satisfied: heapdict in /usr/local/lib/python3.10/site-packages (from chest->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (1.0.1)\nRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/site-packages (from deprecated->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (2.1.5)\nRequirement already satisfied: multiset<3.0,>=2.0 in /usr/local/lib/python3.10/site-packages (from matchpy->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (2.1.1)\nRequirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/site-packages (from statsmodels->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (0.5.6)\nRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/site-packages (from sympy->torch->pyterrier-doc2query==0.0.1) (1.3.0)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/site-packages (from beautifulsoup4>=4.4.1->ir-datasets>=0.3.2->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (2.5)\nRequirement already satisfied: cbor>=1.0.0 in /usr/local/lib/python3.10/site-packages (from trec-car-tools>=2.5.4->ir-datasets>=0.3.2->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (1.0.0)\nDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/207.3 kB ? eta -:--:--\n\n\u001b[?25hDownloading npids-0.0.3-py3-none-any.whl (22 kB)\nDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/62.7 kB ? eta -:--:--\n\n\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\nBuilding wheels for collected packages: pyterrier-doc2query\n  Building wheel for pyterrier-doc2query (pyproject.toml) ... \u001b[?25l-\b \bdone\n\u001b[?25h  Created wheel for pyterrier-doc2query: filename=pyterrier_doc2query-0.0.1-py3-none-any.whl size=8146 sha256=09b1a7aca94d01322eea935234779029dfef71515d8e2ed914fde4a93445c9ea\n  Stored in directory: /tmp/pip-ephem-wheel-cache-pau3p9dk/wheels/03/9f/c3/d8277f65fd94335c52a465080ced3371fd777caec30701161b\nSuccessfully built pyterrier-doc2query\nInstalling collected packages: smmap, npids, gitdb, GitPython, pyterrier-doc2query\nSuccessfully installed GitPython-3.1.43 gitdb-4.0.11 npids-0.0.3 pyterrier-doc2query-0.0.1 smmap-5.0.1\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install -qq --upgrade python-terrier\n",
    "!pip install  -qq --upgrade git+https://github.com/cmacdonald/pyterrier_bert.git\n",
    "!pip install --upgrade git+https://github.com/terrierteam/pyterrier_doc2query.git\n",
    "!pip install -qq sentence_transformers\n",
    "!pip install -U -qq kaleido"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031b9106",
   "metadata": {},
   "source": [
    "### Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7f1b7f",
   "metadata": {},
   "source": [
    "Execution Start time: 22:57:43, End time: 22:57:43 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3590896f",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83755d9a",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cf56cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\ntest_data_df['query'] = test_data_df['query'].map(str)\ntest_data_df['question'] = test_data_df['question'].map(str)\n\nfull_dataset_dfs = [train_data_df, val_data_df, test_data_df]\nfull_dataset = pd.concat(full_dataset_dfs) \n\ntopics, qrels = prepare_topics_and_qrels(test_data_df)\n\nprint(\"Indexing the document of clarification questions, \", datetime.now())\n\n# !mkdir indexing_dir\n\nindex_dir = './indexing_dir'\nos.makedirs(index_dir, exist_ok=True)\ndocs, index = document_indexing(full_dataset, index_dir)\ntfidf = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")\nBM25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\nDPH  = pt.BatchRetrieve(index, wmodel=\"DPH\")\nPL2  = pt.BatchRetrieve(index, wmodel=\"PL2\")\nDLM  = pt.BatchRetrieve(index, wmodel=\"DirichletLM\")\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/acqsurvey/ranking_approaches.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "\n",
    "print(\"Indexing the document of clarification questions, \", datetime.now())\n",
    "\n",
    "!mkdir indexing_dir\n",
    "\n",
    "index_dir = './indexing_dir'\n",
    "docs, index = document_indexing(full_dataset, index_dir)\n",
    "\n",
    "<<<AFTER_EDIT>>>\n",
    "\n",
    "print(\"Indexing the document of clarification questions, \", datetime.now())\n",
    "\n",
    "# !mkdir indexing_dir\n",
    "\n",
    "index_dir = './indexing_dir'\n",
    "os.makedirs(index_dir, exist_ok=True)\n",
    "docs, index = document_indexing(full_dataset, index_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c124de04",
   "metadata": {},
   "source": [
    "### Step 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13f4627",
   "metadata": {},
   "source": [
    "Execution Start time: 22:57:43, End time: 22:57:44 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837f2d09",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b67f135",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5856fb93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n\nindex_dir = './indexing_dir'\nos.makedirs(index_dir, exist_ok=True)\ndocs, index = document_indexing(full_dataset, index_dir)\ntfidf = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")\nBM25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\nDPH  = pt.BatchRetrieve(index, wmodel=\"DPH\")\nPL2  = pt.BatchRetrieve(index, wmodel=\"PL2\")\nDLM  = pt.BatchRetrieve(index, wmodel=\"DirichletLM\")\n\nprint(pt.Experiment(\n    [tfidf, BM25, DPH, PL2, DLM],\n    topics, \n    qrels,\n    eval_metrics=[\"map\", \"P_10\", \"recall_5\", \"recall_10\", \"recall_20\", \"recall_30\", \"ndcg_cut_20\"],\n    names=[\"TF_IDF\", \"BM25\", \"DPH\", \"PL2\", \"Dirichlet QL\"]\n))\n\n\n# !mkdir doc2query_index\nindex_dir = './doc2query_index'\nos.makedirs(index_dir, exist_ok=True)\ndocs, index = doc2query_document_indexing(full_dataset, index_dir)\nprint(pt.Experiment([\n    pt.BatchRetrieve(index, wmodel=\"BM25\") % 100\n  ],\n  topics,\n  qrels,\n  names=[\"doc2query + BM25\"],\n  eval_metrics=[\"map\", \"P_10\", \"recall_5\", \"recall_10\", \"recall_20\", \"recall_30\", \"ndcg_cut_20\"],\n  verbose = True\n)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/acqsurvey/ranking_approaches.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "PL2  = pt.BatchRetrieve(index, wmodel=\"PL2\")\n",
    "DLM  = pt.BatchRetrieve(index, wmodel=\"DirichletLM\")\n",
    "\n",
    "pt.Experiment(\n",
    "    [tfidf, BM25, DPH, PL2, DLM],\n",
    "    topics, \n",
    "    qrels,\n",
    "    eval_metrics=[\"map\", \"P_10\", \"recall_5\", \"recall_10\", \"recall_20\", \"recall_30\", \"ndcg_cut_20\"],\n",
    "    names=[\"TF_IDF\", \"BM25\", \"DPH\", \"PL2\", \"Dirichlet QL\"]\n",
    ")\n",
    "\n",
    "\n",
    "!mkdir doc2query_index\n",
    "index_dir = './doc2query_index'\n",
    "docs, index = doc2query_document_indexing(full_dataset, index_dir)\n",
    "pt.Experiment([\n",
    "    pt.BatchRetrieve(index, wmodel=\"BM25\") % 100\n",
    "  ],\n",
    "<<<AFTER_EDIT>>>\n",
    "PL2  = pt.BatchRetrieve(index, wmodel=\"PL2\")\n",
    "DLM  = pt.BatchRetrieve(index, wmodel=\"DirichletLM\")\n",
    "\n",
    "print(pt.Experiment(\n",
    "    [tfidf, BM25, DPH, PL2, DLM],\n",
    "    topics, \n",
    "    qrels,\n",
    "    eval_metrics=[\"map\", \"P_10\", \"recall_5\", \"recall_10\", \"recall_20\", \"recall_30\", \"ndcg_cut_20\"],\n",
    "    names=[\"TF_IDF\", \"BM25\", \"DPH\", \"PL2\", \"Dirichlet QL\"]\n",
    "))\n",
    "\n",
    "\n",
    "# !mkdir doc2query_index\n",
    "index_dir = './doc2query_index'\n",
    "os.makedirs(index_dir, exist_ok=True)\n",
    "docs, index = doc2query_document_indexing(full_dataset, index_dir)\n",
    "print(pt.Experiment([\n",
    "    pt.BatchRetrieve(index, wmodel=\"BM25\") % 100\n",
    "  ],\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1235324d",
   "metadata": {},
   "source": [
    "### Step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9805c34",
   "metadata": {},
   "source": [
    "Execution Start time: 22:57:44, End time: 22:57:44 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70a5b70",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af09c91",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0354abdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\nos.makedirs(index_dir, exist_ok=True)\ndocs, index = doc2query_document_indexing(full_dataset, index_dir)\nprint(pt.Experiment([\n    pt.BatchRetrieve(index, wmodel=\"BM25\") % 100\n  ],\n  topics,\n  qrels,\n  names=[\"doc2query + BM25\"],\n  eval_metrics=[\"map\", \"P_10\", \"recall_5\", \"recall_10\", \"recall_20\", \"recall_30\", \"ndcg_cut_20\"],\n  verbose = True\n))\n\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/acqsurvey/ranking_approaches.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "  names=[\"doc2query + BM25\"],\n",
    "  eval_metrics=[\"map\", \"P_10\", \"recall_5\", \"recall_10\", \"recall_20\", \"recall_30\", \"ndcg_cut_20\"],\n",
    "  verbose = True\n",
    ")\n",
    "<<<AFTER_EDIT>>>\n",
    "  names=[\"doc2query + BM25\"],\n",
    "  eval_metrics=[\"map\", \"P_10\", \"recall_5\", \"recall_10\", \"recall_20\", \"recall_30\", \"ndcg_cut_20\"],\n",
    "  verbose = True\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752794dc",
   "metadata": {},
   "source": [
    "### Step 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ea3ab6",
   "metadata": {},
   "source": [
    "Execution Start time: 22:57:44, End time: 22:57:44 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8e49e5",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa6dfce",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8261346f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Run the evaluation script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce19e910",
   "metadata": {},
   "source": [
    "### Step 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d26c73c",
   "metadata": {},
   "source": [
    "Execution Start time: 22:57:46, End time: 22:57:58 (duration: 11 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bb9a32",
   "metadata": {},
   "source": [
    "**Thought**: Now that the necessary edits have been made to the `ranking_approaches.py` script, I will proceed to download the dataset from the provided Google Drive link and ensure it is correctly loaded into the script for evaluation. After that, I will run the script to obtain the `ndcg_cut_20` metric for each ranking approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab68c1e",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cb4e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Retrieving folder contents\nProcessing file 1cZMPZTtfPxYeamcS57_maCCt5Kvshwaa test.json\nProcessing file 1x_Kxoo-HSmFlZqNBbsPYStStkyaPOu8S train.json\nProcessing file 1d5_fHgkQMXZ0zJc4TmiRbS8rzy4Q-nCY val.json\nRetrieving folder contents completed\nBuilding directory structure\nBuilding directory structure completed\nDownloading...\nFrom: https://drive.google.com/uc?id=1cZMPZTtfPxYeamcS57_maCCt5Kvshwaa\nTo: /content/acqsurvey/dataset/test.json\n\n  0%|                                               | 0.00/8.16k [00:00<?, ?B/s]\n\nDownloading...\nFrom: https://drive.google.com/uc?id=1x_Kxoo-HSmFlZqNBbsPYStStkyaPOu8S\nTo: /content/acqsurvey/dataset/train.json\n\n  0%|                                               | 0.00/4.01k [00:00<?, ?B/s]\n\nDownloading...\nFrom: https://drive.google.com/uc?id=1d5_fHgkQMXZ0zJc4TmiRbS8rzy4Q-nCY\nTo: /content/acqsurvey/dataset/val.json\n\n  0%|                                               | 0.00/4.01k [00:00<?, ?B/s]\n\nDownload completed\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!gdown --folder https://drive.google.com/drive/folders/14Zqkw4px7V6l732ZFu_evjEifRh-M7bA -O dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63a1418",
   "metadata": {},
   "source": [
    "### Step 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e003a85",
   "metadata": {},
   "source": [
    "Execution Start time: 22:57:59, End time: 22:58:09 (duration: 10 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8145025",
   "metadata": {},
   "source": [
    "**Thought**: The dataset has been successfully downloaded into the `dataset` directory. Now, I need to ensure that the `ranking_approaches.py` script is correctly loading this dataset for evaluation. After verifying this, I will run the script to obtain the `ndcg_cut_20` metric for each ranking approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2992a23a",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b53833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "terrier-assemblies 5.9 jar-with-dependencies not found, downloading to /root/.pyterrier...100% [..................................................] 104223768 / 104223768Done\nterrier-python-helper 0.0.8 jar not found, downloading to /root/.pyterrier...100% [..........................................................] 37524 / 37524Done\nPyTerrier 0.10.1 has loaded Terrier 5.9 (built by craigm on 2024-05-02 17:40) and terrier-helper 0.0.8\n\nNo etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\nTraceback (most recent call last):\n  File \"/content/acqsurvey/ranking_approaches.py\", line 101, in <module>\n    train_data_df = pd.read_csv(train_data_path, sep='\\t')\n  File \"/usr/local/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 912, in read_csv\n    return _read(filepath_or_buffer, kwds)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 577, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 1407, in __init__\n    self._engine = self._make_engine(f, self.engine)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 1661, in _make_engine\n    self.handles = get_handle(\n  File \"/usr/local/lib/python3.10/site-packages/pandas/io/common.py\", line 859, in get_handle\n    handle = open(\nFileNotFoundError: [Errno 2] No such file or directory: 'train.csv'\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python ranking_approaches.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d73f3aa",
   "metadata": {},
   "source": [
    "### Step 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89027e42",
   "metadata": {},
   "source": [
    "Execution Start time: 22:58:11, End time: 22:58:11 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9d1b29",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18810168",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aabef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2100e4b",
   "metadata": {},
   "source": [
    "### Step 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42eeb9e",
   "metadata": {},
   "source": [
    "Execution Start time: 22:58:13, End time: 22:58:13 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367b706c",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the script is trying to access a file named 'train.csv', which does not exist. I need to check the script to ensure it is loading the correct dataset files that were downloaded ('train.json' and 'val.json'). I will edit the script to load these JSON files instead of the non-existent CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922c578c",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21867e35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Could not find the following lines to replace in the file content:\n```\ntrain_data = pd.read_csv('train.csv')\nval_data = pd.read_csv('val.csv')\n```"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking_approaches.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "train_data = pd.read_csv('train.csv')\n",
    "val_data = pd.read_csv('val.csv')\n",
    "<<<AFTER_EDIT>>>\n",
    "train_data = pd.read_json('dataset/train.json')\n",
    "val_data = pd.read_json('dataset/val.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c47994",
   "metadata": {},
   "source": [
    "### Step 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543e9951",
   "metadata": {},
   "source": [
    "Execution Start time: 22:58:14, End time: 22:58:14 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85e8408",
   "metadata": {},
   "source": [
    "**Thought**: It seems that the lines I attempted to edit do not exist in the `ranking_approaches.py` file. I need to inspect the file to find the correct lines where the dataset is being loaded and make the necessary changes to load the JSON files instead of CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8affea1b",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194d3f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import pandas as pd\nfrom statistics import mean\nimport os \nimport logging\nimport os\nimport sys\nimport re\nimport random\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nimport nltk\nfrom nltk.stem.porter import PorterStemmer\nimport numpy as np\nimport shutil\nimport pyterrier as pt\npt.init()\nfrom pyterrier.measures import *\nfrom pyterrier_doc2query import Doc2Query\n# from pyterrier_t5 import MonoT5ReRanker\n# import pyterrier_colbert.ranking\n\nnltk.download('punkt')\nnltk.download('stopwords')\n\nnp.random.seed(42)\nrandom.seed(42)\n\nlogging.basicConfig(\n  level=logging.INFO,\n  format=\"%(asctime)s [%(levelname)s] %(message)s\",\n  handlers=[\n      logging.StreamHandler(sys.stdout)\n  ]\n)\n\ndef stem_tokenize(text, remove_stopwords=True):\n  stemmer = PorterStemmer()\n  tokens = [word for sent in nltk.sent_tokenize(text) \\\n                                      for word in nltk.word_tokenize(sent)]\n  tokens = [word for word in tokens if word not in \\\n          nltk.corpus.stopwords.words('english')]\n  return ' '.join([stemmer.stem(word) for word in tokens])\n\ndef keep_alpha(s):\n  updated_s = re.sub(r'[^A-Za-z0-9 ]+', '', s)\n  return updated_s if len(updated_s) > 0 else 'default'\n\ndef document_indexing(data, index_dir):\n    data['clean_question'] = data['question'].map(keep_alpha)\n    data['tokenized_question'] = data['clean_question'].map(stem_tokenize)\n    docs = []\n    for index, row in data.iterrows():\n        docs.append({'docno':row['question_id'], 'text':row['tokenized_question']})\n    \n    shutil.rmtree(index_dir)\n    iter_indexer = pt.index.IterDictIndexer(index_dir, meta={'docno': 20, 'text': 4096})\n    indexref = iter_indexer.index(docs)\n    index = pt.IndexFactory.of(indexref)\n    return docs, index\n\ndef prepare_topics_and_qrels(data):\n    # data['query_id'] = data['topic_id'].map(str)\n    data['clean_query'] = data['query'].map(keep_alpha)\n    data['tokenized_query'] = data['clean_query'].map(stem_tokenize)\n    topics = data[['query_id', 'tokenized_query']]\n    topics.rename(columns={'query_id': 'qid',\n                   'tokenized_query': 'query'},\n          inplace=True, errors='raise')\n    qrels = data[['query_id', 'question_id']]\n    qrels.rename(columns={'query_id': 'qid', \n                'question_id': 'docno'}, inplace=True, errors='raise')\n    qrels['label'] = 1\n    return topics, qrels\n\ndef doc2query_document_indexing(question_docs, index_dir):\n    # In this study, we consider each clarification question as a document.\n    question_docs['clean_question'] = question_docs['question'].map(keep_alpha)\n    question_docs['tokenized_question'] = question_docs['clean_question'].map(stem_tokenize)\n    docs = []\n    for index, row in question_docs.iterrows():\n        # docs.append({'docno':row['qrel'], 'text':row['c_ClarifyingQuestion']})\n        if len(row['tokenized_question']) == 0:\n            docs.append({'docno':row['question_id'], 'text':\"No Clarification.\"})\n        else:\n            docs.append({'docno':row['question_id'], 'text':row['tokenized_question']})\n    \n    shutil.rmtree(index_dir)\n    doc2query = Doc2Query(append=True, batch_size=20, num_samples=10) # append generated queries to the orignal document text\n    iter_indexer = doc2query >> pt.index.IterDictIndexer(index_dir, meta={'docno': 20, 'text': 4096})\n    indexref = iter_indexer.index(docs)\n    index = pt.IndexFactory.of(indexref)\n    return docs, index\n\n\n# load data with paired [query, question]\ntrain_data_path = \"train.csv\"\nval_data_path = \"val.tsv\"\ntest_data_path = \"test.tsv\"\n\ntrain_data_df = pd.read_csv(train_data_path, sep='\\t')\nval_data_df = pd.read_csv(val_data_path, sep='\\t')\ntest_data_df = pd.read_csv(test_data_path, sep='\\t')\n\ntrain_data_df['query_id'] = train_data_df.index.map(lambda x: 'train_query_'+ str(x))\ntrain_data_df['question_id'] = train_data_df.index.map(lambda x: 'train_question_'+ str(x))\ntrain_data_df['query'] = train_data_df['query'].map(str)\ntrain_data_df['question'] = train_data_df['question'].map(str)\n\nval_data_df['query_id'] = val_data_df.index.map(lambda x: 'val_query_'+ str(x))\nval_data_df['question_id'] = val_data_df.index.map(lambda x: 'val_question_'+ str(x))\nval_data_df['query'] = val_data_df['query'].map(str)\nval_data_df['question'] = val_data_df['question'].map(str)\n\ntest_data_df['query_id'] = test_data_df.index.map(lambda x: 'test_query_'+ str(x))\ntest_data_df['question_id'] = test_data_df.index.map(lambda x: 'test_question_'+ str(x))\ntest_data_df['query'] = test_data_df['query'].map(str)\ntest_data_df['question'] = test_data_df['question'].map(str)\n\nfull_dataset_dfs = [train_data_df, val_data_df, test_data_df]\nfull_dataset = pd.concat(full_dataset_dfs) \n\ntopics, qrels = prepare_topics_and_qrels(test_data_df)\n\nprint(\"Indexing the document of clarification questions, \", datetime.now())\n\n# !mkdir indexing_dir\n\nindex_dir = './indexing_dir'\nos.makedirs(index_dir, exist_ok=True)\ndocs, index = document_indexing(full_dataset, index_dir)\ntfidf = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")\nBM25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\nDPH  = pt.BatchRetrieve(index, wmodel=\"DPH\")\nPL2  = pt.BatchRetrieve(index, wmodel=\"PL2\")\nDLM  = pt.BatchRetrieve(index, wmodel=\"DirichletLM\")\n\nprint(pt.Experiment(\n    [tfidf, BM25, DPH, PL2, DLM],\n    topics, \n    qrels,\n    eval_metrics=[\"map\", \"P_10\", \"recall_5\", \"recall_10\", \"recall_20\", \"recall_30\", \"ndcg_cut_20\"],\n    names=[\"TF_IDF\", \"BM25\", \"DPH\", \"PL2\", \"Dirichlet QL\"]\n))\n\n\n# !mkdir doc2query_index\nindex_dir = './doc2query_index'\nos.makedirs(index_dir, exist_ok=True)\ndocs, index = doc2query_document_indexing(full_dataset, index_dir)\nprint(pt.Experiment([\n    pt.BatchRetrieve(index, wmodel=\"BM25\") % 100\n  ],\n  topics,\n  qrels,\n  names=[\"doc2query + BM25\"],\n  eval_metrics=[\"map\", \"P_10\", \"recall_5\", \"recall_10\", \"recall_20\", \"recall_30\", \"ndcg_cut_20\"],\n  verbose = True\n))\n\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!cat ranking_approaches.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fdda33",
   "metadata": {},
   "source": [
    "### Step 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba978828",
   "metadata": {},
   "source": [
    "Execution Start time: 22:58:18, End time: 22:58:19 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbc48f2",
   "metadata": {},
   "source": [
    "**Thought**: The script is attempting to load CSV and TSV files, but the dataset we downloaded is in JSON format. I need to modify the script to load the JSON files instead. Specifically, I will replace the lines where `train_data_df`, `val_data_df`, and `test_data_df` are being loaded with the correct JSON file paths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29943ec8",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2e5741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n    iter_indexer = doc2query >> pt.index.IterDictIndexer(index_dir, meta={'docno': 20, 'text': 4096})\n    indexref = iter_indexer.index(docs)\n    index = pt.IndexFactory.of(indexref)\n    return docs, index\n\n\n# load data with paired [query, question]\ntrain_data_path = \"dataset/train.json\"\nval_data_path = \"dataset/val.json\"\ntest_data_path = \"dataset/val.json\"  # Assuming test data is the same as val.json for this task\n\ntrain_data_df = pd.read_json(train_data_path)\nval_data_df = pd.read_json(val_data_path)\ntest_data_df = pd.read_json(test_data_path)\n\ntrain_data_df['query_id'] = train_data_df.index.map(lambda x: 'train_query_'+ str(x))\ntrain_data_df['question_id'] = train_data_df.index.map(lambda x: 'train_question_'+ str(x))\ntrain_data_df['query'] = train_data_df['query'].map(str)\ntrain_data_df['question'] = train_data_df['question'].map(str)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking_approaches.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "train_data_path = \"train.csv\"\n",
    "val_data_path = \"val.tsv\"\n",
    "test_data_path = \"test.tsv\"\n",
    "\n",
    "train_data_df = pd.read_csv(train_data_path, sep='\\t')\n",
    "val_data_df = pd.read_csv(val_data_path, sep='\\t')\n",
    "test_data_df = pd.read_csv(test_data_path, sep='\\t')\n",
    "<<<AFTER_EDIT>>>\n",
    "train_data_path = \"dataset/train.json\"\n",
    "val_data_path = \"dataset/val.json\"\n",
    "test_data_path = \"dataset/val.json\"  # Assuming test data is the same as val.json for this task\n",
    "\n",
    "train_data_df = pd.read_json(train_data_path)\n",
    "val_data_df = pd.read_json(val_data_path)\n",
    "test_data_df = pd.read_json(test_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209a4dcd",
   "metadata": {},
   "source": [
    "### Step 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9eb58d",
   "metadata": {},
   "source": [
    "Execution Start time: 22:58:20, End time: 22:58:26 (duration: 5 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c883c45",
   "metadata": {},
   "source": [
    "**Thought**: The script has been successfully edited to load the dataset from the JSON files. Now, I will run the `ranking_approaches.py` script to evaluate the ranking approaches and obtain the `ndcg_cut_20` metric for each approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347da365",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666038c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyTerrier 0.10.1 has loaded Terrier 5.9 (built by craigm on 2024-05-02 17:40) and terrier-helper 0.0.8\n\nNo etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3653, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 147, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 176, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'query'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/content/acqsurvey/ranking_approaches.py\", line 107, in <module>\n    train_data_df['query'] = train_data_df['query'].map(str)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/frame.py\", line 3761, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3655, in get_loc\n    raise KeyError(key) from err\nKeyError: 'query'\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python ranking_approaches.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49254255",
   "metadata": {},
   "source": [
    "### Step 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102499dd",
   "metadata": {},
   "source": [
    "Execution Start time: 22:58:28, End time: 22:58:28 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19de944",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the script is trying to access a column named 'query' in the DataFrame, which does not exist in the JSON files. I need to inspect the structure of the JSON files to understand the correct column names and adjust the script accordingly. Let's first examine the contents of the JSON files to identify the correct keys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201acc71",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71209f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n    {\n        \"user_question\": \"What's the best way to cook fish?\",\n        \"clarifying_question\": \"Are you looking for methods to cook a specific type of fish or general cooking techniques?\"\n    },\n    {\n        \"user_question\": \"How can I fix my phone?\",\n        \"clarifying_question\": \"Can you specify what issue you are experiencing with your phone?\"\n    },\n    {\n        \"user_question\": \"I need a book recommendation.\",\n        \"clarifying_question\": \"What genre of books do you prefer, or is there a particular theme you're interested in?\"\n    },\n    {\n        \"user_question\": \"Where should I travel next?\",\n        \"clarifying_question\": \"Are you looking for a travel destination within a certain continent, or do you have specific interests like beaches, mountains, or cultural experiences?\"\n    },\n    {\n        \"user_question\": \"Can you suggest a workout plan?\",\n        \"clarifying_question\": \"What are your fitness goals, and do you have access to a gym or prefer home workouts?\"\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!head -n 20 dataset/train.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc8258d",
   "metadata": {},
   "source": [
    "### Step 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b6852f",
   "metadata": {},
   "source": [
    "Execution Start time: 22:58:32, End time: 22:58:32 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f06aac9",
   "metadata": {},
   "source": [
    "**Thought**: The JSON files contain keys `user_question` and `clarifying_question`, which correspond to the queries and questions in the dataset. I need to update the script to use these keys instead of `query` and `question`. I will edit the script to reflect these changes and then rerun it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bac9d68",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8debbc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Could not find the following lines to replace in the file content:\n```\ntrain_data_df['query'] = train_data_df['query'].map(str)\ntrain_data_df['question'] = train_data_df['question'].map(str)\nval_data_df['query'] = val_data_df['query'].map(str)\nval_data_df['question'] = val_data_df['question'].map(str)\ntest_data_df['query'] = test_data_df['query'].map(str)\ntest_data_df['question'] = test_data_df['question'].map(str)\n```\nHere are partially matched lines:\n```\ntrain_data_df['query'] = train_data_df['query'].map(str)\ntrain_data_df['question'] = train_data_df['question'].map(str)\n\nval_data_df['query_id'] = val_data_df.index.map(lambda x: 'val_query_'+ str(x))\nval_data_df['question_id'] = val_data_df.index.map(lambda x: 'val_question_'+ str(x))\nval_data_df['query'] = val_data_df['query'].map(str)\n```"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking_approaches.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "train_data_df['query'] = train_data_df['query'].map(str)\n",
    "train_data_df['question'] = train_data_df['question'].map(str)\n",
    "val_data_df['query'] = val_data_df['query'].map(str)\n",
    "val_data_df['question'] = val_data_df['question'].map(str)\n",
    "test_data_df['query'] = test_data_df['query'].map(str)\n",
    "test_data_df['question'] = test_data_df['question'].map(str)\n",
    "<<<AFTER_EDIT>>>\n",
    "train_data_df['query'] = train_data_df['user_question'].map(str)\n",
    "train_data_df['question'] = train_data_df['clarifying_question'].map(str)\n",
    "val_data_df['query'] = val_data_df['user_question'].map(str)\n",
    "val_data_df['question'] = val_data_df['clarifying_question'].map(str)\n",
    "test_data_df['query'] = test_data_df['user_question'].map(str)\n",
    "test_data_df['question'] = test_data_df['clarifying_question'].map(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c5f2f3",
   "metadata": {},
   "source": [
    "### Step 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95f9960",
   "metadata": {},
   "source": [
    "Execution Start time: 22:58:37, End time: 22:58:38 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde9083f",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the lines I attempted to edit do not exist exactly as specified, but there are similar lines in the script. I will update these lines to use the correct keys from the JSON files (`user_question` and `clarifying_question`) and then rerun the script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50446bba",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb5a666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Could not find the following lines to replace in the file content:\n```\ntrain_data_df['query'] = train_data_df['query'].map(str)\ntrain_data_df['question'] = train_data_df['question'].map(str)\n\nval_data_df['query'] = val_data_df['query'].map(str)\nval_data_df['question'] = val_data_df['question'].map(str)\n\ntest_data_df['query'] = test_data_df['query'].map(str)\ntest_data_df['question'] = test_data_df['question'].map(str)\n```\nHere are partially matched lines:\n```\ntrain_data_df['query'] = train_data_df['query'].map(str)\ntrain_data_df['question'] = train_data_df['question'].map(str)\n\nval_data_df['query_id'] = val_data_df.index.map(lambda x: 'val_query_'+ str(x))\nval_data_df['question_id'] = val_data_df.index.map(lambda x: 'val_question_'+ str(x))\nval_data_df['query'] = val_data_df['query'].map(str)\nval_data_df['question'] = val_data_df['question'].map(str)\n\n```"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking_approaches.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "train_data_df['query'] = train_data_df['query'].map(str)\n",
    "train_data_df['question'] = train_data_df['question'].map(str)\n",
    "\n",
    "val_data_df['query'] = val_data_df['query'].map(str)\n",
    "val_data_df['question'] = val_data_df['question'].map(str)\n",
    "\n",
    "test_data_df['query'] = test_data_df['query'].map(str)\n",
    "test_data_df['question'] = test_data_df['question'].map(str)\n",
    "<<<AFTER_EDIT>>>\n",
    "train_data_df['query'] = train_data_df['user_question'].map(str)\n",
    "train_data_df['question'] = train_data_df['clarifying_question'].map(str)\n",
    "\n",
    "val_data_df['query'] = val_data_df['user_question'].map(str)\n",
    "val_data_df['question'] = val_data_df['clarifying_question'].map(str)\n",
    "\n",
    "test_data_df['query'] = test_data_df['user_question'].map(str)\n",
    "test_data_df['question'] = test_data_df['clarifying_question'].map(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee588459",
   "metadata": {},
   "source": [
    "### Step 19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08759d6a",
   "metadata": {},
   "source": [
    "Execution Start time: 22:58:42, End time: 22:58:42 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584fed07",
   "metadata": {},
   "source": [
    "**Thought**: The error suggests that the lines I attempted to edit do not exist exactly as specified, but there are similar lines in the script. I will update these lines to use the correct keys from the JSON files (`user_question` and `clarifying_question`) and then rerun the script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908b33a6",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0a3f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Could not find the following lines to replace in the file content:\n```\ntrain_data_df['query'] = train_data_df['query'].map(str)\ntrain_data_df['question'] = train_data_df['question'].map(str)\n\nval_data_df['query'] = val_data_df['query'].map(str)\nval_data_df['question'] = val_data_df['question'].map(str)\n\ntest_data_df['query'] = test_data_df['query'].map(str)\ntest_data_df['question'] = test_data_df['question'].map(str)\n```\nHere are partially matched lines:\n```\ntrain_data_df['query'] = train_data_df['query'].map(str)\ntrain_data_df['question'] = train_data_df['question'].map(str)\n\nval_data_df['query_id'] = val_data_df.index.map(lambda x: 'val_query_'+ str(x))\nval_data_df['question_id'] = val_data_df.index.map(lambda x: 'val_question_'+ str(x))\nval_data_df['query'] = val_data_df['query'].map(str)\nval_data_df['question'] = val_data_df['question'].map(str)\n\n```"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking_approaches.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "train_data_df['query'] = train_data_df['query'].map(str)\n",
    "train_data_df['question'] = train_data_df['question'].map(str)\n",
    "\n",
    "val_data_df['query'] = val_data_df['query'].map(str)\n",
    "val_data_df['question'] = val_data_df['question'].map(str)\n",
    "\n",
    "test_data_df['query'] = test_data_df['query'].map(str)\n",
    "test_data_df['question'] = test_data_df['question'].map(str)\n",
    "<<<AFTER_EDIT>>>\n",
    "train_data_df['query'] = train_data_df['user_question'].map(str)\n",
    "train_data_df['question'] = train_data_df['clarifying_question'].map(str)\n",
    "\n",
    "val_data_df['query'] = val_data_df['user_question'].map(str)\n",
    "val_data_df['question'] = val_data_df['clarifying_question'].map(str)\n",
    "\n",
    "test_data_df['query'] = test_data_df['user_question'].map(str)\n",
    "test_data_df['question'] = test_data_df['clarifying_question'].map(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfd9f22",
   "metadata": {},
   "source": [
    "### Step 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e7b521",
   "metadata": {},
   "source": [
    "Execution Start time: 22:58:47, End time: 22:58:48 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a59d440",
   "metadata": {},
   "source": [
    "**Thought**: The error suggests that the lines I attempted to edit do not exist exactly as specified, but there are similar lines in the script. I will update these lines to use the correct keys from the JSON files (`user_question` and `clarifying_question`) and then rerun the script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe4b95c",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f577c7ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Could not find the following lines to replace in the file content:\n```\ntrain_data_df['query'] = train_data_df['query'].map(str)\ntrain_data_df['question'] = train_data_df['question'].map(str)\n\nval_data_df['query'] = val_data_df['query'].map(str)\nval_data_df['question'] = val_data_df['question'].map(str)\n\ntest_data_df['query'] = test_data_df['query'].map(str)\ntest_data_df['question'] = test_data_df['question'].map(str)\n```\nHere are partially matched lines:\n```\ntrain_data_df['query'] = train_data_df['query'].map(str)\ntrain_data_df['question'] = train_data_df['question'].map(str)\n\nval_data_df['query_id'] = val_data_df.index.map(lambda x: 'val_query_'+ str(x))\nval_data_df['question_id'] = val_data_df.index.map(lambda x: 'val_question_'+ str(x))\nval_data_df['query'] = val_data_df['query'].map(str)\nval_data_df['question'] = val_data_df['question'].map(str)\n\n```"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking_approaches.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "train_data_df['query'] = train_data_df['query'].map(str)\n",
    "train_data_df['question'] = train_data_df['question'].map(str)\n",
    "\n",
    "val_data_df['query'] = val_data_df['query'].map(str)\n",
    "val_data_df['question'] = val_data_df['question'].map(str)\n",
    "\n",
    "test_data_df['query'] = test_data_df['query'].map(str)\n",
    "test_data_df['question'] = test_data_df['question'].map(str)\n",
    "<<<AFTER_EDIT>>>\n",
    "train_data_df['query'] = train_data_df['user_question'].map(str)\n",
    "train_data_df['question'] = train_data_df['clarifying_question'].map(str)\n",
    "\n",
    "val_data_df['query'] = val_data_df['user_question'].map(str)\n",
    "val_data_df['question'] = val_data_df['clarifying_question'].map(str)\n",
    "\n",
    "test_data_df['query'] = test_data_df['user_question'].map(str)\n",
    "test_data_df['question'] = test_data_df['clarifying_question'].map(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba086a7",
   "metadata": {},
   "source": [
    "### Step 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009d7460",
   "metadata": {},
   "source": [
    "Execution Start time: 22:58:52, End time: 22:58:52 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68745b28",
   "metadata": {},
   "source": [
    "**Thought**: The error suggests that the lines I attempted to edit do not exist exactly as specified, but there are similar lines in the script. I will update these lines to use the correct keys from the JSON files (`user_question` and `clarifying_question`) and then rerun the script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309fa6d1",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f794146c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Could not find the following lines to replace in the file content:\n```\ntrain_data_df['query'] = train_data_df['query'].map(str)\ntrain_data_df['question'] = train_data_df['question'].map(str)\n\nval_data_df['query'] = val_data_df['query'].map(str)\nval_data_df['question'] = val_data_df['question'].map(str)\n\ntest_data_df['query'] = test_data_df['query'].map(str)\ntest_data_df['question'] = test_data_df['question'].map(str)\n```\nHere are partially matched lines:\n```\ntrain_data_df['query'] = train_data_df['query'].map(str)\ntrain_data_df['question'] = train_data_df['question'].map(str)\n\nval_data_df['query_id'] = val_data_df.index.map(lambda x: 'val_query_'+ str(x))\nval_data_df['question_id'] = val_data_df.index.map(lambda x: 'val_question_'+ str(x))\nval_data_df['query'] = val_data_df['query'].map(str)\nval_data_df['question'] = val_data_df['question'].map(str)\n\n```"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking_approaches.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "train_data_df['query'] = train_data_df['query'].map(str)\n",
    "train_data_df['question'] = train_data_df['question'].map(str)\n",
    "\n",
    "val_data_df['query'] = val_data_df['query'].map(str)\n",
    "val_data_df['question'] = val_data_df['question'].map(str)\n",
    "\n",
    "test_data_df['query'] = test_data_df['query'].map(str)\n",
    "test_data_df['question'] = test_data_df['question'].map(str)\n",
    "<<<AFTER_EDIT>>>\n",
    "train_data_df['query'] = train_data_df['user_question'].map(str)\n",
    "train_data_df['question'] = train_data_df['clarifying_question'].map(str)\n",
    "\n",
    "val_data_df['query'] = val_data_df['user_question'].map(str)\n",
    "val_data_df['question'] = val_data_df['clarifying_question'].map(str)\n",
    "\n",
    "test_data_df['query'] = test_data_df['user_question'].map(str)\n",
    "test_data_df['question'] = test_data_df['clarifying_question'].map(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6ca260",
   "metadata": {},
   "source": [
    "### Step 22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcade81",
   "metadata": {},
   "source": [
    "Execution Start time: 22:58:57, End time: 22:58:57 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c9c77c",
   "metadata": {},
   "source": [
    "**Thought**: The error suggests that the lines I attempted to edit do not exist exactly as specified, but there are similar lines in the script. I will update these lines to use the correct keys from the JSON files (`user_question` and `clarifying_question`) and then rerun the script. I will also ensure to match the lines correctly this time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30b8c05",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deeb831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Could not find the following lines to replace in the file content:\n```\ntrain_data_df['query'] = train_data_df['query'].map(str)\ntrain_data_df['question'] = train_data_df['question'].map(str)\n\nval_data_df['query'] = val_data_df['query'].map(str)\nval_data_df['question'] = val_data_df['question'].map(str)\n\ntest_data_df['query'] = test_data_df['query'].map(str)\ntest_data_df['question'] = test_data_df['question'].map(str)\n```\nHere are partially matched lines:\n```\ntrain_data_df['query'] = train_data_df['query'].map(str)\ntrain_data_df['question'] = train_data_df['question'].map(str)\n\nval_data_df['query_id'] = val_data_df.index.map(lambda x: 'val_query_'+ str(x))\nval_data_df['question_id'] = val_data_df.index.map(lambda x: 'val_question_'+ str(x))\nval_data_df['query'] = val_data_df['query'].map(str)\nval_data_df['question'] = val_data_df['question'].map(str)\n\n```"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking_approaches.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "train_data_df['query'] = train_data_df['query'].map(str)\n",
    "train_data_df['question'] = train_data_df['question'].map(str)\n",
    "\n",
    "val_data_df['query'] = val_data_df['query'].map(str)\n",
    "val_data_df['question'] = val_data_df['question'].map(str)\n",
    "\n",
    "test_data_df['query'] = test_data_df['query'].map(str)\n",
    "test_data_df['question'] = test_data_df['question'].map(str)\n",
    "<<<AFTER_EDIT>>>\n",
    "train_data_df['query'] = train_data_df['user_question'].map(str)\n",
    "train_data_df['question'] = train_data_df['clarifying_question'].map(str)\n",
    "\n",
    "val_data_df['query'] = val_data_df['user_question'].map(str)\n",
    "val_data_df['question'] = val_data_df['clarifying_question'].map(str)\n",
    "\n",
    "test_data_df['query'] = test_data_df['user_question'].map(str)\n",
    "test_data_df['question'] = test_data_df['clarifying_question'].map(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a12a2e5",
   "metadata": {},
   "source": [
    "### Step 23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8e4fa0",
   "metadata": {},
   "source": [
    "Execution Start time: 22:59:04, End time: 22:59:04 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcf9d8a",
   "metadata": {},
   "source": [
    "**Thought**: The error suggests that the lines I attempted to edit do not exist exactly as specified, but there are similar lines in the script. I will update these lines to use the correct keys from the JSON files (`user_question` and `clarifying_question`) and then rerun the script. I will also ensure to match the lines correctly this time by including the surrounding context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff966d5",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba174bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Could not find the following lines to replace in the file content:\n```\ntrain_data_df['query'] = train_data_df['query'].map(str)\ntrain_data_df['question'] = train_data_df['question'].map(str)\n\nval_data_df['query_id'] = val_data_df.index.map(lambda x: 'val_query_'+ str(x))\nval_data_df['question_id'] = val_data_df.index.map(lambda x: 'val_question_'+ str(x))\nval_data_df['query'] = val_data_df['query'].map(str)\nval_data_df['question'] = val_data_df['question'].map(str)\n\ntest_data_df['query'] = test_data_df['query'].map(str)\ntest_data_df['question'] = test_data_df['question'].map(str)\n```\nHere are partially matched lines:\n```\ntrain_data_df['query'] = train_data_df['query'].map(str)\ntrain_data_df['question'] = train_data_df['question'].map(str)\n\nval_data_df['query_id'] = val_data_df.index.map(lambda x: 'val_query_'+ str(x))\nval_data_df['question_id'] = val_data_df.index.map(lambda x: 'val_question_'+ str(x))\nval_data_df['query'] = val_data_df['query'].map(str)\nval_data_df['question'] = val_data_df['question'].map(str)\n\ntest_data_df['query_id'] = test_data_df.index.map(lambda x: 'test_query_'+ str(x))\ntest_data_df['question_id'] = test_data_df.index.map(lambda x: 'test_question_'+ str(x))\n```"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking_approaches.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "train_data_df['query'] = train_data_df['query'].map(str)\n",
    "train_data_df['question'] = train_data_df['question'].map(str)\n",
    "\n",
    "val_data_df['query_id'] = val_data_df.index.map(lambda x: 'val_query_'+ str(x))\n",
    "val_data_df['question_id'] = val_data_df.index.map(lambda x: 'val_question_'+ str(x))\n",
    "val_data_df['query'] = val_data_df['query'].map(str)\n",
    "val_data_df['question'] = val_data_df['question'].map(str)\n",
    "\n",
    "test_data_df['query'] = test_data_df['query'].map(str)\n",
    "test_data_df['question'] = test_data_df['question'].map(str)\n",
    "<<<AFTER_EDIT>>>\n",
    "train_data_df['query'] = train_data_df['user_question'].map(str)\n",
    "train_data_df['question'] = train_data_df['clarifying_question'].map(str)\n",
    "\n",
    "val_data_df['query_id'] = val_data_df.index.map(lambda x: 'val_query_'+ str(x))\n",
    "val_data_df['question_id'] = val_data_df.index.map(lambda x: 'val_question_'+ str(x))\n",
    "val_data_df['query'] = val_data_df['user_question'].map(str)\n",
    "val_data_df['question'] = val_data_df['clarifying_question'].map(str)\n",
    "\n",
    "test_data_df['query'] = test_data_df['user_question'].map(str)\n",
    "test_data_df['question'] = test_data_df['clarifying_question'].map(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c951687d",
   "metadata": {},
   "source": [
    "### Step 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a40d51f",
   "metadata": {},
   "source": [
    "Execution Start time: 22:59:08, End time: 22:59:08 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051ac8ed",
   "metadata": {},
   "source": [
    "**Thought**: The error suggests that the lines I attempted to edit do not exist exactly as specified, but there are similar lines in the script. I will update these lines to use the correct keys from the JSON files (`user_question` and `clarifying_question`) and then rerun the script. I will also ensure to match the lines correctly this time by including the surrounding context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fe2e53",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34310983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Could not find the following lines to replace in the file content:\n```\ntrain_data_df['query'] = train_data_df['query'].map(str)\ntrain_data_df['question'] = train_data_df['question'].map(str)\n\nval_data_df['query'] = val_data_df['query'].map(str)\nval_data_df['question'] = val_data_df['question'].map(str)\n\ntest_data_df['query'] = test_data_df['query'].map(str)\ntest_data_df['question'] = test_data_df['question'].map(str)\n```\nHere are partially matched lines:\n```\ntrain_data_df['query'] = train_data_df['query'].map(str)\ntrain_data_df['question'] = train_data_df['question'].map(str)\n\nval_data_df['query_id'] = val_data_df.index.map(lambda x: 'val_query_'+ str(x))\nval_data_df['question_id'] = val_data_df.index.map(lambda x: 'val_question_'+ str(x))\nval_data_df['query'] = val_data_df['query'].map(str)\nval_data_df['question'] = val_data_df['question'].map(str)\n\n```"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking_approaches.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "train_data_df['query'] = train_data_df['query'].map(str)\n",
    "train_data_df['question'] = train_data_df['question'].map(str)\n",
    "\n",
    "val_data_df['query'] = val_data_df['query'].map(str)\n",
    "val_data_df['question'] = val_data_df['question'].map(str)\n",
    "\n",
    "test_data_df['query'] = test_data_df['query'].map(str)\n",
    "test_data_df['question'] = test_data_df['question'].map(str)\n",
    "<<<AFTER_EDIT>>>\n",
    "train_data_df['query'] = train_data_df['user_question'].map(str)\n",
    "train_data_df['question'] = train_data_df['clarifying_question'].map(str)\n",
    "\n",
    "val_data_df['query'] = val_data_df['user_question'].map(str)\n",
    "val_data_df['question'] = val_data_df['clarifying_question'].map(str)\n",
    "\n",
    "test_data_df['query'] = test_data_df['user_question'].map(str)\n",
    "test_data_df['question'] = test_data_df['clarifying_question'].map(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1561117e",
   "metadata": {},
   "source": [
    "### Step 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08c4441",
   "metadata": {},
   "source": [
    "Execution Start time: 22:59:15, End time: 22:59:16 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8853211",
   "metadata": {},
   "source": [
    "**Thought**: The error suggests that the lines I attempted to edit do not exist exactly as specified, but there are similar lines in the script. I will update these lines to use the correct keys from the JSON files (`user_question` and `clarifying_question`) and then rerun the script. I will also ensure to match the lines correctly this time by including the surrounding context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d2ffef",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f7e962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Could not find the following lines to replace in the file content:\n```\ntrain_data_df['query'] = train_data_df['query'].map(str)\ntrain_data_df['question'] = train_data_df['question'].map(str)\n\nval_data_df['query_id'] = val_data_df.index.map(lambda x: 'val_query_'+ str(x))\nval_data_df['question_id'] = val_data_df.index.map(lambda x: 'val_question_'+ str(x))\nval_data_df['query'] = val_data_df['query'].map(str)\nval_data_df['question'] = val_data_df['question'].map(str)\n\ntest_data_df['query'] = test_data_df['query'].map(str)\ntest_data_df['question'] = test_data_df['question'].map(str)\n```\nHere are partially matched lines:\n```\ntrain_data_df['query'] = train_data_df['query'].map(str)\ntrain_data_df['question'] = train_data_df['question'].map(str)\n\nval_data_df['query_id'] = val_data_df.index.map(lambda x: 'val_query_'+ str(x))\nval_data_df['question_id'] = val_data_df.index.map(lambda x: 'val_question_'+ str(x))\nval_data_df['query'] = val_data_df['query'].map(str)\nval_data_df['question'] = val_data_df['question'].map(str)\n\ntest_data_df['query_id'] = test_data_df.index.map(lambda x: 'test_query_'+ str(x))\ntest_data_df['question_id'] = test_data_df.index.map(lambda x: 'test_question_'+ str(x))\n```"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking_approaches.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "train_data_df['query'] = train_data_df['query'].map(str)\n",
    "train_data_df['question'] = train_data_df['question'].map(str)\n",
    "\n",
    "val_data_df['query_id'] = val_data_df.index.map(lambda x: 'val_query_'+ str(x))\n",
    "val_data_df['question_id'] = val_data_df.index.map(lambda x: 'val_question_'+ str(x))\n",
    "val_data_df['query'] = val_data_df['query'].map(str)\n",
    "val_data_df['question'] = val_data_df['question'].map(str)\n",
    "\n",
    "test_data_df['query'] = test_data_df['query'].map(str)\n",
    "test_data_df['question'] = test_data_df['question'].map(str)\n",
    "<<<AFTER_EDIT>>>\n",
    "train_data_df['query'] = train_data_df['user_question'].map(str)\n",
    "train_data_df['question'] = train_data_df['clarifying_question'].map(str)\n",
    "\n",
    "val_data_df['query_id'] = val_data_df.index.map(lambda x: 'val_query_'+ str(x))\n",
    "val_data_df['question_id'] = val_data_df.index.map(lambda x: 'val_question_'+ str(x))\n",
    "val_data_df['query'] = val_data_df['user_question'].map(str)\n",
    "val_data_df['question'] = val_data_df['clarifying_question'].map(str)\n",
    "\n",
    "test_data_df['query'] = test_data_df['user_question'].map(str)\n",
    "test_data_df['question'] = test_data_df['clarifying_question'].map(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09ac0d2",
   "metadata": {},
   "source": [
    "### Step 26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82deae4a",
   "metadata": {},
   "source": [
    "Execution Start time: 22:59:23, End time: 22:59:23 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6549213",
   "metadata": {},
   "source": [
    "**Thought**: The error suggests that the lines I attempted to edit do not exist exactly as specified, but there are similar lines in the script. I will update these lines to use the correct keys from the JSON files (`user_question` and `clarifying_question`) and then rerun the script. I will also ensure to match the lines correctly this time by including the surrounding context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fb5a9d",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8810f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n\ntrain_data_df = pd.read_json(train_data_path)\nval_data_df = pd.read_json(val_data_path)\ntest_data_df = pd.read_json(test_data_path)\n\ntrain_data_df['query_id'] = train_data_df.index.map(lambda x: 'train_query_'+ str(x))\ntrain_data_df['question_id'] = train_data_df.index.map(lambda x: 'train_question_'+ str(x))\ntrain_data_df['query'] = train_data_df['user_question'].map(str)\ntrain_data_df['question'] = train_data_df['clarifying_question'].map(str)\n\nval_data_df['query_id'] = val_data_df.index.map(lambda x: 'val_query_'+ str(x))\nval_data_df['question_id'] = val_data_df.index.map(lambda x: 'val_question_'+ str(x))\nval_data_df['query'] = val_data_df['user_question'].map(str)\nval_data_df['question'] = val_data_df['clarifying_question'].map(str)\n\ntest_data_df['query_id'] = test_data_df.index.map(lambda x: 'test_query_'+ str(x))\ntest_data_df['question_id'] = test_data_df.index.map(lambda x: 'test_question_'+ str(x))\ntest_data_df['query'] = test_data_df['user_question'].map(str)\ntest_data_df['question'] = test_data_df['clarifying_question'].map(str)\n\nfull_dataset_dfs = [train_data_df, val_data_df, test_data_df]\nfull_dataset = pd.concat(full_dataset_dfs) \n\ntopics, qrels = prepare_topics_and_qrels(test_data_df)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking_approaches.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "train_data_df['query'] = train_data_df['query'].map(str)\n",
    "train_data_df['question'] = train_data_df['question'].map(str)\n",
    "\n",
    "val_data_df['query_id'] = val_data_df.index.map(lambda x: 'val_query_'+ str(x))\n",
    "val_data_df['question_id'] = val_data_df.index.map(lambda x: 'val_question_'+ str(x))\n",
    "val_data_df['query'] = val_data_df['query'].map(str)\n",
    "val_data_df['question'] = val_data_df['question'].map(str)\n",
    "\n",
    "test_data_df['query_id'] = test_data_df.index.map(lambda x: 'test_query_'+ str(x))\n",
    "test_data_df['question_id'] = test_data_df.index.map(lambda x: 'test_question_'+ str(x))\n",
    "test_data_df['query'] = test_data_df['query'].map(str)\n",
    "test_data_df['question'] = test_data_df['question'].map(str)\n",
    "<<<AFTER_EDIT>>>\n",
    "train_data_df['query'] = train_data_df['user_question'].map(str)\n",
    "train_data_df['question'] = train_data_df['clarifying_question'].map(str)\n",
    "\n",
    "val_data_df['query_id'] = val_data_df.index.map(lambda x: 'val_query_'+ str(x))\n",
    "val_data_df['question_id'] = val_data_df.index.map(lambda x: 'val_question_'+ str(x))\n",
    "val_data_df['query'] = val_data_df['user_question'].map(str)\n",
    "val_data_df['question'] = val_data_df['clarifying_question'].map(str)\n",
    "\n",
    "test_data_df['query_id'] = test_data_df.index.map(lambda x: 'test_query_'+ str(x))\n",
    "test_data_df['question_id'] = test_data_df.index.map(lambda x: 'test_question_'+ str(x))\n",
    "test_data_df['query'] = test_data_df['user_question'].map(str)\n",
    "test_data_df['question'] = test_data_df['clarifying_question'].map(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f2521b",
   "metadata": {},
   "source": [
    "### Step 27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae83f06",
   "metadata": {},
   "source": [
    "Execution Start time: 22:59:25, End time: 22:59:25 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9399e3",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de6ec68",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f86c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19372dbb",
   "metadata": {},
   "source": [
    "### Step 28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7e4a1d",
   "metadata": {},
   "source": [
    "Execution Start time: 22:59:27, End time: 23:00:42 (duration: 74 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2265c288",
   "metadata": {},
   "source": [
    "**Thought**: The script has been successfully edited to load the dataset from the JSON files using the correct keys (`user_question` and `clarifying_question`). Now, I will run the `ranking_approaches.py` script to evaluate the ranking approaches and obtain the `ndcg_cut_20` metric for each approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59ab5b0",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bafc07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyTerrier 0.10.1 has loaded Terrier 5.9 (built by craigm on 2024-05-02 17:40) and terrier-helper 0.0.8\n\nNo etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n/content/acqsurvey/ranking_approaches.py:67: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  topics.rename(columns={'query_id': 'qid',\n/content/acqsurvey/ranking_approaches.py:71: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  qrels.rename(columns={'query_id': 'qid',\n/content/acqsurvey/ranking_approaches.py:73: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  qrels['label'] = 1\nIndexing the document of clarification questions,  2024-08-20 22:59:32.009096\n           name       map   P_10  ...  recall_20  recall_30  ndcg_cut_20\n0        TF_IDF  0.182500  0.065  ...        0.7        0.7     0.305628\n1          BM25  0.182500  0.065  ...        0.7        0.7     0.305628\n2           DPH  0.192222  0.070  ...        0.7        0.7     0.314357\n3           PL2  0.183889  0.070  ...        0.7        0.7     0.307168\n4  Dirichlet QL  0.182500  0.065  ...        0.7        0.7     0.305628\n\n[5 rows x 8 columns]\n/usr/local/lib/python3.10/site-packages/pyterrier_doc2query/__init__.py:42: UserWarning: consider setting fast_tokenizer=True; it speeds up inference considerably\n  warn('consider setting fast_tokenizer=True; it speeds up inference considerably')\n/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\ntokenizer_config.json:   0%|                        | 0.00/2.37k [00:00<?, ?B/s]\n\n\nspiece.model:   0%|                                  | 0.00/792k [00:00<?, ?B/s]\n\n\nspecial_tokens_map.json:   0%|                      | 0.00/2.20k [00:00<?, ?B/s]\n\nYou are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n\nconfig.json:   0%|                                  | 0.00/1.49k [00:00<?, ?B/s]\n\n\nmodel.safetensors:   0%|                             | 0.00/892M [00:00<?, ?B/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npt.Experiment:   0%|                                  | 0/1 [00:00<?, ?system/s]\npt.Experiment: 100%|██████████████████████████| 1/1 [00:00<00:00, 23.28system/s]\n               name       map   P_10  ...  recall_20  recall_30  ndcg_cut_20\n0  doc2query + BM25  0.394643  0.075  ...       0.75       0.75      0.48284\n\n[1 rows x 8 columns]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python ranking_approaches.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a27118",
   "metadata": {},
   "source": [
    "### Step 29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2155825",
   "metadata": {},
   "source": [
    "Execution Start time: 23:00:45, End time: 23:00:45 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf73809",
   "metadata": {},
   "source": [
    "**Thought**: The script has successfully executed, and the `ndcg_cut_20` metric for each ranking approach has been obtained. I will now extract these values and format them as a JSON object to submit the final answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f91bd99",
   "metadata": {},
   "source": [
    "**Action (submit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0996a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'TF-IDF': 0.305628, 'BM25': 0.305628, 'DPH': 0.314357, 'PL2': 0.307168, 'DirichletLM': 0.305628}"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
