{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27abea4c",
   "metadata": {},
   "source": [
    "### System Prompt\n",
    "<details>\n",
    "SETTING: You are an autonomous programmer, and you're working directly in the command line with a special Jupyter notebook interface.\n",
    "\n",
    "The special Jupyter notebook interface consists of a file editor that shows you 100 lines of a file at a time.\n",
    "You can execute commands in the notebook using:\n",
    "\n",
    "1. Bash commands: Commands starting with !.\n",
    "2. Python commands: Standard Python code.\n",
    "3. Magic commands: Commands starting with %, e.g., %cd <path>.\n",
    "\n",
    "Additionally, you can also use the following commands to help you navigate and edit files.\n",
    "\n",
    "COMMANDS:\n",
    "\n",
    "open:\n",
    "  docstring: opens the file at the given path in the editor. If line_number is provided, the window will be move to include that line\n",
    "  signature: open <path> [<line_number>]\n",
    "  arguments:\n",
    "    - path (string) [required]: the path to the file to open\n",
    "    - line_number (integer) [optional]: the line number to move the window to (if not provided, the window will start at the top of the file)\n",
    "\n",
    "create:\n",
    "  docstring: creates and opens a new file with the given name\n",
    "  signature: create <filename>\n",
    "  arguments:\n",
    "    - filename (string) [required]: the name of the file to create\n",
    "\n",
    "goto:\n",
    "  docstring: moves the window to show <line_number>\n",
    "  signature: goto <line_number>\n",
    "  arguments:\n",
    "    - line_number (integer) [required]: the line number to move the window to\n",
    "\n",
    "scroll_down:\n",
    "  docstring: moves the window down {WINDOW} lines\n",
    "  signature: scroll_down\n",
    "\n",
    "scroll_up:\n",
    "  docstring: moves the window down {WINDOW} lines\n",
    "  signature: scroll_up\n",
    "\n",
    "submit:\n",
    "  docstring: submits your final answer\n",
    "  signature: submit <answer>\n",
    "  arguments:\n",
    "    - answer (string) [required]: the answer to be submitted\n",
    "\n",
    "search_dir:\n",
    "  docstring: searches for search_term in all files in dir_path. If dir_path is not provided, searches in the current directory\n",
    "  signature: search_dir <search_term> [<dir_path>]\n",
    "  arguments:\n",
    "    - search_term (string) [required]: the term to search for\n",
    "    - dir_path (string) [optional]: the directory to search in (if not provided, searches in the current directory)\n",
    "\n",
    "search_file:\n",
    "  docstring: searches for search_term in file. If file is not provided, searches in the current open file\n",
    "  signature: search_file <search_term> [<file>]\n",
    "  arguments:\n",
    "    - search_term (string) [required]: the term to search for\n",
    "    - file (string) [optional]: the file to search in (if not provided, searches in the current open file)\n",
    "\n",
    "find_file:\n",
    "  docstring: finds all files with the given name in dir_path. If dir_path is not provided, searches in the current directory\n",
    "  signature: find_file <file_name> [<dir_path>]\n",
    "  arguments:\n",
    "    - file_name (string) [required]: the name of the file to search for\n",
    "    - dir_path (string) [optional]: the directory to search in (if not provided, searches in the current directory)\n",
    "\n",
    "edit:\n",
    "  docstring: replaces lines <start_line> through <end_line> (inclusive) with the given text in the open file. The replacement text is terminated by a line with only end_of_edit on it. All of the <replacement text> will be entered, so make sure your indentation is formatted properly. Python files will be checked for syntax errors after the edit. If the system detects a syntax error, the edit will not be executed. Simply try to edit the file again, but make sure to read the error message and modify the edit command you issue accordingly. Issuing the same command a second time will just lead to the same error message again.\n",
    "  signature: edit <start_line>:<end_line>\n",
    "<replacement_text>\n",
    "end_of_edit\n",
    "  arguments:\n",
    "    - start_line (integer) [required]: the line number to start the edit at\n",
    "    - end_line (integer) [required]: the line number to end the edit at (inclusive)\n",
    "    - replacement_text (string) [required]: the text to replace the current selection with\n",
    "\n",
    "\n",
    "\n",
    "Please note that THE EDIT COMMAND REQUIRES PROPER INDENTATION. \n",
    "If you'd like to add the line '        print(x)' you must fully write that out, with all those spaces before the code! Indentation is important and code that is not indented correctly will fail and require fixing before it can be run.\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "Your shell prompt is formatted as follows:\n",
    "(Open file: <path>)\n",
    "(Current directory: <cwd>)\n",
    "In [ ]\n",
    "\n",
    "You need to format your output using two fields: discussion and command.\n",
    "Your output should always include _one_ discussion and _one_ command field EXACTLY as in the following example:\n",
    "DISCUSSION\n",
    "First I'll start by using ls to see what files are in the current directory. Then maybe we can look at some relevant files to see what they look like.\n",
    "```\n",
    "!ls -a\n",
    "```\n",
    "\n",
    "You should only include a *SINGLE* command in the command section and then wait for a response from the shell before continuing with more discussion and commands. Everything you include in the DISCUSSION section will be saved for future reference.\n",
    "If you'd like to issue two commands at once, PLEASE DO NOT DO THAT! Please instead first submit just the first command, and then after receiving a response you'll be able to issue the second command. \n",
    "You're free to use any other bash commands you want (e.g. find, grep, cat, ls, cd) in addition to the special commands listed above.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaa4f97",
   "metadata": {},
   "source": [
    "### Query\n",
    "Train and evaluate bart-base on the xsum dataset using the quick start pipeline. Report the rouge-1, rouge-2, rouge-L and rouge-w-1.2 metrics as a json structured as follows: {\"rouge-1\": 0.0, \"rouge-2\": 0.0, \"rouge-L\": 0.0, \"rouge-w-1.2\": 0.0} (replace 0.0 with the actual values).\n",
    "\n",
    "Additional instructions:\n",
    "1. Load only the first 10 rows of each set in the dataset.\n",
    "2. Train only one epoch.\n",
    "3. Codebase expects one line per sample. Replace \\n with ` [SEP] `.\n",
    "\n",
    "Git repository: https://github.com/RUCAIBox/TextBox\n",
    "\n",
    "I have already executed some of the required steps. However, I did not yet make any required changes to load the request dataset. Your goal is to successfully load the dataset and complete the remaining steps to achieve the user request. You can re-run any of the pre-executed steps, and finally should successfully run\n",
    "```\n",
    "!python run_textbox.py --model=BART --dataset=xsum --model_path=facebook/bart-base --epochs=1\n",
    "```\n",
    "to submit the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de81e4ea",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edac5db9",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f1f62a",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f9378f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Clone model repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96a4086",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cb955e",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30c3111",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc40f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cloning into 'TextBox'...\r\nremote: Enumerating objects: 11190, done.\u001b[K\r\nremote: Counting objects:   0% (1/2598)\u001b[K\rremote: Counting objects:   1% (26/2598)\u001b[K\rremote: Counting objects:   2% (52/2598)\u001b[K\rremote: Counting objects:   3% (78/2598)\u001b[K\rremote: Counting objects:   4% (104/2598)\u001b[K\rremote: Counting objects:   5% (130/2598)\u001b[K\rremote: Counting objects:   6% (156/2598)\u001b[K\rremote: Counting objects:   7% (182/2598)\u001b[K\rremote: Counting objects:   8% (208/2598)\u001b[K\rremote: Counting objects:   9% (234/2598)\u001b[K\rremote: Counting objects:  10% (260/2598)\u001b[K\rremote: Counting objects:  11% (286/2598)\u001b[K\rremote: Counting objects:  12% (312/2598)\u001b[K\rremote: Counting objects:  13% (338/2598)\u001b[K\rremote: Counting objects:  14% (364/2598)\u001b[K\rremote: Counting objects:  15% (390/2598)\u001b[K\rremote: Counting objects:  16% (416/2598)\u001b[K\rremote: Counting objects:  17% (442/2598)\u001b[K\rremote: Counting objects:  18% (468/2598)\u001b[K\rremote: Counting objects:  19% (494/2598)\u001b[K\rremote: Counting objects:  20% (520/2598)\u001b[K\rremote: Counting objects:  21% (546/2598)\u001b[K\rremote: Counting objects:  22% (572/2598)\u001b[K\rremote: Counting objects:  23% (598/2598)\u001b[K\rremote: Counting objects:  24% (624/2598)\u001b[K\rremote: Counting objects:  25% (650/2598)\u001b[K\rremote: Counting objects:  26% (676/2598)\u001b[K\rremote: Counting objects:  27% (702/2598)\u001b[K\rremote: Counting objects:  28% (728/2598)\u001b[K\rremote: Counting objects:  29% (754/2598)\u001b[K\rremote: Counting objects:  30% (780/2598)\u001b[K\rremote: Counting objects:  31% (806/2598)\u001b[K\rremote: Counting objects:  32% (832/2598)\u001b[K\rremote: Counting objects:  33% (858/2598)\u001b[K\rremote: Counting objects:  34% (884/2598)\u001b[K\rremote: Counting objects:  35% (910/2598)\u001b[K\rremote: Counting objects:  36% (936/2598)\u001b[K\rremote: Counting objects:  37% (962/2598)\u001b[K\rremote: Counting objects:  38% (988/2598)\u001b[K\rremote: Counting objects:  39% (1014/2598)\u001b[K\rremote: Counting objects:  40% (1040/2598)\u001b[K\rremote: Counting objects:  41% (1066/2598)\u001b[K\rremote: Counting objects:  42% (1092/2598)\u001b[K\rremote: Counting objects:  43% (1118/2598)\u001b[K\rremote: Counting objects:  44% (1144/2598)\u001b[K\rremote: Counting objects:  45% (1170/2598)\u001b[K\rremote: Counting objects:  46% (1196/2598)\u001b[K\rremote: Counting objects:  47% (1222/2598)\u001b[K\rremote: Counting objects:  48% (1248/2598)\u001b[K\rremote: Counting objects:  49% (1274/2598)\u001b[K\rremote: Counting objects:  50% (1299/2598)\u001b[K\rremote: Counting objects:  51% (1325/2598)\u001b[K\rremote: Counting objects:  52% (1351/2598)\u001b[K\rremote: Counting objects:  53% (1377/2598)\u001b[K\rremote: Counting objects:  54% (1403/2598)\u001b[K\rremote: Counting objects:  55% (1429/2598)\u001b[K\rremote: Counting objects:  56% (1455/2598)\u001b[K\rremote: Counting objects:  57% (1481/2598)\u001b[K\rremote: Counting objects:  58% (1507/2598)\u001b[K\rremote: Counting objects:  59% (1533/2598)\u001b[K\rremote: Counting objects:  60% (1559/2598)\u001b[K\rremote: Counting objects:  61% (1585/2598)\u001b[K\rremote: Counting objects:  62% (1611/2598)\u001b[K\rremote: Counting objects:  63% (1637/2598)\u001b[K\rremote: Counting objects:  64% (1663/2598)\u001b[K\rremote: Counting objects:  65% (1689/2598)\u001b[K\rremote: Counting objects:  66% (1715/2598)\u001b[K\rremote: Counting objects:  67% (1741/2598)\u001b[K\rremote: Counting objects:  68% (1767/2598)\u001b[K\rremote: Counting objects:  69% (1793/2598)\u001b[K\rremote: Counting objects:  70% (1819/2598)\u001b[K\rremote: Counting objects:  71% (1845/2598)\u001b[K\rremote: Counting objects:  72% (1871/2598)\u001b[K\rremote: Counting objects:  73% (1897/2598)\u001b[K\rremote: Counting objects:  74% (1923/2598)\u001b[K\rremote: Counting objects:  75% (1949/2598)\u001b[K\rremote: Counting objects:  76% (1975/2598)\u001b[K\rremote: Counting objects:  77% (2001/2598)\u001b[K\rremote: Counting objects:  78% (2027/2598)\u001b[K\rremote: Counting objects:  79% (2053/2598)\u001b[K\rremote: Counting objects:  80% (2079/2598)\u001b[K\rremote: Counting objects:  81% (2105/2598)\u001b[K\rremote: Counting objects:  82% (2131/2598)\u001b[K\rremote: Counting objects:  83% (2157/2598)\u001b[K\rremote: Counting objects:  84% (2183/2598)\u001b[K\rremote: Counting objects:  85% (2209/2598)\u001b[K\rremote: Counting objects:  86% (2235/2598)\u001b[K\rremote: Counting objects:  87% (2261/2598)\u001b[K\rremote: Counting objects:  88% (2287/2598)\u001b[K\rremote: Counting objects:  89% (2313/2598)\u001b[K\rremote: Counting objects:  90% (2339/2598)\u001b[K\rremote: Counting objects:  91% (2365/2598)\u001b[K\rremote: Counting objects:  92% (2391/2598)\u001b[K\rremote: Counting objects:  93% (2417/2598)\u001b[K\rremote: Counting objects:  94% (2443/2598)\u001b[K\rremote: Counting objects:  95% (2469/2598)\u001b[K\rremote: Counting objects:  96% (2495/2598)\u001b[K\rremote: Counting objects:  97% (2521/2598)\u001b[K\rremote: Counting objects:  98% (2547/2598)\u001b[K\rremote: Counting objects:  99% (2573/2598)\u001b[K\rremote: Counting objects: 100% (2598/2598)\u001b[K\rremote: Counting objects: 100% (2598/2598), done.\u001b[K\r\nremote: Compressing objects:   0% (1/1015)\u001b[K\rremote: Compressing objects:   1% (11/1015)\u001b[K\rremote: Compressing objects:   2% (21/1015)\u001b[K\rremote: Compressing objects:   3% (31/1015)\u001b[K\rremote: Compressing objects:   4% (41/1015)\u001b[K\rremote: Compressing objects:   5% (51/1015)\u001b[K\rremote: Compressing objects:   6% (61/1015)\u001b[K\rremote: Compressing objects:   7% (72/1015)\u001b[K\rremote: Compressing objects:   8% (82/1015)\u001b[K\rremote: Compressing objects:   9% (92/1015)\u001b[K\rremote: Compressing objects:  10% (102/1015)\u001b[K\rremote: Compressing objects:  11% (112/1015)\u001b[K\rremote: Compressing objects:  12% (122/1015)\u001b[K\rremote: Compressing objects:  13% (132/1015)\u001b[K\rremote: Compressing objects:  14% (143/1015)\u001b[K\rremote: Compressing objects:  15% (153/1015)\u001b[K\rremote: Compressing objects:  16% (163/1015)\u001b[K\rremote: Compressing objects:  17% (173/1015)\u001b[K\rremote: Compressing objects:  18% (183/1015)\u001b[K\rremote: Compressing objects:  19% (193/1015)\u001b[K\rremote: Compressing objects:  20% (203/1015)\u001b[K\rremote: Compressing objects:  21% (214/1015)\u001b[K\rremote: Compressing objects:  22% (224/1015)\u001b[K\rremote: Compressing objects:  23% (234/1015)\u001b[K\rremote: Compressing objects:  24% (244/1015)\u001b[K\rremote: Compressing objects:  25% (254/1015)\u001b[K\rremote: Compressing objects:  26% (264/1015)\u001b[K\rremote: Compressing objects:  27% (275/1015)\u001b[K\rremote: Compressing objects:  28% (285/1015)\u001b[K\rremote: Compressing objects:  29% (295/1015)\u001b[K\rremote: Compressing objects:  30% (305/1015)\u001b[K\rremote: Compressing objects:  31% (315/1015)\u001b[K\rremote: Compressing objects:  32% (325/1015)\u001b[K\rremote: Compressing objects:  33% (335/1015)\u001b[K\rremote: Compressing objects:  34% (346/1015)\u001b[K\rremote: Compressing objects:  35% (356/1015)\u001b[K\rremote: Compressing objects:  36% (366/1015)\u001b[K\rremote: Compressing objects:  37% (376/1015)\u001b[K\rremote: Compressing objects:  38% (386/1015)\u001b[K\rremote: Compressing objects:  39% (396/1015)\u001b[K\rremote: Compressing objects:  40% (406/1015)\u001b[K\rremote: Compressing objects:  41% (417/1015)\u001b[K\rremote: Compressing objects:  42% (427/1015)\u001b[K\rremote: Compressing objects:  43% (437/1015)\u001b[K\rremote: Compressing objects:  44% (447/1015)\u001b[K\rremote: Compressing objects:  45% (457/1015)\u001b[K\rremote: Compressing objects:  46% (467/1015)\u001b[K\rremote: Compressing objects:  47% (478/1015)\u001b[K\rremote: Compressing objects:  48% (488/1015)\u001b[K\rremote: Compressing objects:  49% (498/1015)\u001b[K\rremote: Compressing objects:  50% (508/1015)\u001b[K\rremote: Compressing objects:  51% (518/1015)\u001b[K\rremote: Compressing objects:  52% (528/1015)\u001b[K\rremote: Compressing objects:  53% (538/1015)\u001b[K\rremote: Compressing objects:  54% (549/1015)\u001b[K\rremote: Compressing objects:  55% (559/1015)\u001b[K\rremote: Compressing objects:  56% (569/1015)\u001b[K\rremote: Compressing objects:  57% (579/1015)\u001b[K\rremote: Compressing objects:  58% (589/1015)\u001b[K\rremote: Compressing objects:  59% (599/1015)\u001b[K\rremote: Compressing objects:  60% (609/1015)\u001b[K\rremote: Compressing objects:  61% (620/1015)\u001b[K\rremote: Compressing objects:  62% (630/1015)\u001b[K\rremote: Compressing objects:  63% (640/1015)\u001b[K\rremote: Compressing objects:  64% (650/1015)\u001b[K\rremote: Compressing objects:  65% (660/1015)\u001b[K\rremote: Compressing objects:  66% (670/1015)\u001b[K\rremote: Compressing objects:  67% (681/1015)\u001b[K\rremote: Compressing objects:  68% (691/1015)\u001b[K\rremote: Compressing objects:  69% (701/1015)\u001b[K\rremote: Compressing objects:  70% (711/1015)\u001b[K\rremote: Compressing objects:  71% (721/1015)\u001b[K\rremote: Compressing objects:  72% (731/1015)\u001b[K\rremote: Compressing objects:  73% (741/1015)\u001b[K\rremote: Compressing objects:  74% (752/1015)\u001b[K\rremote: Compressing objects:  75% (762/1015)\u001b[K\rremote: Compressing objects:  76% (772/1015)\u001b[K\rremote: Compressing objects:  77% (782/1015)\u001b[K\rremote: Compressing objects:  78% (792/1015)\u001b[K\rremote: Compressing objects:  79% (802/1015)\u001b[K\rremote: Compressing objects:  80% (812/1015)\u001b[K\rremote: Compressing objects:  81% (823/1015)\u001b[K\rremote: Compressing objects:  82% (833/1015)\u001b[K\rremote: Compressing objects:  83% (843/1015)\u001b[K\rremote: Compressing objects:  84% (853/1015)\u001b[K\rremote: Compressing objects:  85% (863/1015)\u001b[K\rremote: Compressing objects:  86% (873/1015)\u001b[K\rremote: Compressing objects:  87% (884/1015)\u001b[K\rremote: Compressing objects:  88% (894/1015)\u001b[K\rremote: Compressing objects:  89% (904/1015)\u001b[K\rremote: Compressing objects:  90% (914/1015)\u001b[K\rremote: Compressing objects:  91% (924/1015)\u001b[K\rremote: Compressing objects:  92% (934/1015)\u001b[K\rremote: Compressing objects:  93% (944/1015)\u001b[K\rremote: Compressing objects:  94% (955/1015)\u001b[K\rremote: Compressing objects:  95% (965/1015)\u001b[K\rremote: Compressing objects:  96% (975/1015)\u001b[K\rremote: Compressing objects:  97% (985/1015)\u001b[K\rremote: Compressing objects:  98% (995/1015)\u001b[K\rremote: Compressing objects:  99% (1005/1015)\u001b[K\rremote: Compressing objects: 100% (1015/1015)\u001b[K\rremote: Compressing objects: 100% (1015/1015), done.\u001b[K\r\nReceiving objects:   0% (1/11190)\rReceiving objects:   1% (112/11190)\rReceiving objects:   2% (224/11190)\rReceiving objects:   3% (336/11190)\rReceiving objects:   4% (448/11190)\rReceiving objects:   5% (560/11190)\rReceiving objects:   6% (672/11190)\rReceiving objects:   7% (784/11190)\rReceiving objects:   8% (896/11190)\rReceiving objects:   9% (1008/11190)\rReceiving objects:  10% (1119/11190)\rReceiving objects:  11% (1231/11190)\rReceiving objects:  12% (1343/11190)\rReceiving objects:  13% (1455/11190)\rReceiving objects:  14% (1567/11190)\rReceiving objects:  15% (1679/11190)\rReceiving objects:  16% (1791/11190)\rReceiving objects:  17% (1903/11190)\rReceiving objects:  18% (2015/11190)\rReceiving objects:  19% (2127/11190)\rReceiving objects:  20% (2238/11190)\rReceiving objects:  21% (2350/11190)\rReceiving objects:  22% (2462/11190)\rReceiving objects:  23% (2574/11190)\rReceiving objects:  24% (2686/11190)\rReceiving objects:  25% (2798/11190)\rReceiving objects:  26% (2910/11190)\rReceiving objects:  27% (3022/11190)\rReceiving objects:  28% (3134/11190)\rReceiving objects:  29% (3246/11190)\rReceiving objects:  30% (3357/11190)\rReceiving objects:  31% (3469/11190)\rReceiving objects:  32% (3581/11190)\rReceiving objects:  33% (3693/11190)\rReceiving objects:  34% (3805/11190)\rReceiving objects:  35% (3917/11190)\rReceiving objects:  36% (4029/11190)\rReceiving objects:  37% (4141/11190)\rReceiving objects:  38% (4253/11190)\rReceiving objects:  39% (4365/11190)\rReceiving objects:  40% (4476/11190)\rReceiving objects:  41% (4588/11190)\rReceiving objects:  42% (4700/11190)\rReceiving objects:  43% (4812/11190)\rReceiving objects:  44% (4924/11190)\rReceiving objects:  45% (5036/11190)\rReceiving objects:  46% (5148/11190)\rReceiving objects:  47% (5260/11190)\rReceiving objects:  48% (5372/11190)\rReceiving objects:  49% (5484/11190)\rReceiving objects:  50% (5595/11190)\rReceiving objects:  51% (5707/11190)\rReceiving objects:  52% (5819/11190)\rReceiving objects:  53% (5931/11190)\rReceiving objects:  54% (6043/11190)\rReceiving objects:  55% (6155/11190)\rReceiving objects:  56% (6267/11190)\rReceiving objects:  57% (6379/11190)\rReceiving objects:  58% (6491/11190)\rReceiving objects:  59% (6603/11190)\rReceiving objects:  60% (6714/11190)\rReceiving objects:  61% (6826/11190)\rReceiving objects:  62% (6938/11190)\rReceiving objects:  63% (7050/11190)\rReceiving objects:  64% (7162/11190)\rReceiving objects:  65% (7274/11190)\rReceiving objects:  66% (7386/11190)\rReceiving objects:  67% (7498/11190)\rReceiving objects:  68% (7610/11190)\rReceiving objects:  69% (7722/11190)\rReceiving objects:  70% (7833/11190)\rReceiving objects:  71% (7945/11190)\rReceiving objects:  72% (8057/11190)\rReceiving objects:  73% (8169/11190)\rReceiving objects:  74% (8281/11190)\rReceiving objects:  75% (8393/11190)\rReceiving objects:  76% (8505/11190)\rReceiving objects:  77% (8617/11190)\rReceiving objects:  78% (8729/11190)\rReceiving objects:  79% (8841/11190)\rReceiving objects:  80% (8952/11190)\rReceiving objects:  81% (9064/11190)\rReceiving objects:  82% (9176/11190)\rReceiving objects:  83% (9288/11190)\rReceiving objects:  84% (9400/11190)\rReceiving objects:  85% (9512/11190)\rReceiving objects:  86% (9624/11190)\rReceiving objects:  87% (9736/11190)\rReceiving objects:  88% (9848/11190)\rReceiving objects:  89% (9960/11190)\rReceiving objects:  89% (9982/11190), 45.90 MiB | 45.89 MiB/s\rReceiving objects:  89% (9983/11190), 104.00 MiB | 52.02 MiB/s\rReceiving objects:  90% (10071/11190), 104.00 MiB | 52.02 MiB/s\rReceiving objects:  91% (10183/11190), 104.00 MiB | 52.02 MiB/s\rReceiving objects:  92% (10295/11190), 104.00 MiB | 52.02 MiB/s\rReceiving objects:  93% (10407/11190), 104.00 MiB | 52.02 MiB/s\rReceiving objects:  94% (10519/11190), 104.00 MiB | 52.02 MiB/s\rReceiving objects:  95% (10631/11190), 104.00 MiB | 52.02 MiB/s\rReceiving objects:  96% (10743/11190), 104.00 MiB | 52.02 MiB/s\rReceiving objects:  97% (10855/11190), 104.00 MiB | 52.02 MiB/s\rReceiving objects:  98% (10967/11190), 104.00 MiB | 52.02 MiB/s\rReceiving objects:  99% (11079/11190), 104.00 MiB | 52.02 MiB/s\rremote: Total 11190 (delta 1526), reused 2492 (delta 1507), pack-reused 8592 (from 1)\u001b[K\r\nReceiving objects: 100% (11190/11190), 129.89 MiB | 51.97 MiB/s\rReceiving objects: 100% (11190/11190), 130.08 MiB | 51.85 MiB/s, done.\r\nResolving deltas:   0% (0/6913)\rResolving deltas:   1% (70/6913)\rResolving deltas:   2% (139/6913)\rResolving deltas:   3% (208/6913)\rResolving deltas:   4% (277/6913)\rResolving deltas:   5% (346/6913)\rResolving deltas:   6% (415/6913)\rResolving deltas:   7% (484/6913)\rResolving deltas:   8% (554/6913)\rResolving deltas:   9% (623/6913)\rResolving deltas:  10% (692/6913)\rResolving deltas:  11% (761/6913)\rResolving deltas:  12% (830/6913)\rResolving deltas:  13% (899/6913)\rResolving deltas:  14% (969/6913)\rResolving deltas:  15% (1037/6913)\rResolving deltas:  16% (1107/6913)\rResolving deltas:  17% (1176/6913)\rResolving deltas:  18% (1245/6913)\rResolving deltas:  19% (1314/6913)\rResolving deltas:  20% (1383/6913)\rResolving deltas:  21% (1452/6913)\rResolving deltas:  22% (1521/6913)\rResolving deltas:  23% (1590/6913)\rResolving deltas:  24% (1660/6913)\rResolving deltas:  25% (1729/6913)\rResolving deltas:  26% (1798/6913)\rResolving deltas:  27% (1867/6913)\rResolving deltas:  28% (1936/6913)\rResolving deltas:  29% (2005/6913)\rResolving deltas:  30% (2074/6913)\rResolving deltas:  31% (2144/6913)\rResolving deltas:  32% (2213/6913)\rResolving deltas:  33% (2282/6913)\rResolving deltas:  34% (2351/6913)\rResolving deltas:  35% (2420/6913)\rResolving deltas:  36% (2489/6913)\rResolving deltas:  37% (2558/6913)\rResolving deltas:  38% (2627/6913)\rResolving deltas:  39% (2697/6913)\rResolving deltas:  40% (2766/6913)\rResolving deltas:  41% (2835/6913)\rResolving deltas:  42% (2904/6913)\rResolving deltas:  43% (2973/6913)\rResolving deltas:  44% (3042/6913)\rResolving deltas:  45% (3111/6913)\rResolving deltas:  46% (3180/6913)\rResolving deltas:  47% (3250/6913)\rResolving deltas:  48% (3319/6913)\rResolving deltas:  49% (3388/6913)\rResolving deltas:  50% (3457/6913)\rResolving deltas:  51% (3526/6913)\rResolving deltas:  52% (3595/6913)\rResolving deltas:  53% (3664/6913)\rResolving deltas:  54% (3734/6913)\rResolving deltas:  55% (3803/6913)\rResolving deltas:  56% (3872/6913)\rResolving deltas:  57% (3941/6913)\rResolving deltas:  58% (4010/6913)\rResolving deltas:  59% (4079/6913)\rResolving deltas:  60% (4148/6913)\rResolving deltas:  61% (4217/6913)\rResolving deltas:  62% (4287/6913)\rResolving deltas:  63% (4356/6913)\rResolving deltas:  64% (4425/6913)\rResolving deltas:  65% (4494/6913)\rResolving deltas:  66% (4563/6913)\rResolving deltas:  67% (4632/6913)\rResolving deltas:  68% (4701/6913)\rResolving deltas:  69% (4770/6913)\rResolving deltas:  70% (4840/6913)\rResolving deltas:  71% (4909/6913)\rResolving deltas:  72% (4978/6913)\rResolving deltas:  73% (5047/6913)\rResolving deltas:  74% (5116/6913)\rResolving deltas:  75% (5185/6913)\rResolving deltas:  76% (5254/6913)\rResolving deltas:  77% (5324/6913)\rResolving deltas:  78% (5393/6913)\rResolving deltas:  79% (5462/6913)\rResolving deltas:  80% (5531/6913)\rResolving deltas:  81% (5600/6913)\rResolving deltas:  82% (5669/6913)\rResolving deltas:  83% (5738/6913)\rResolving deltas:  84% (5807/6913)\rResolving deltas:  85% (5877/6913)\rResolving deltas:  86% (5946/6913)\rResolving deltas:  87% (6015/6913)\rResolving deltas:  88% (6084/6913)\rResolving deltas:  89% (6153/6913)\rResolving deltas:  90% (6222/6913)\rResolving deltas:  91% (6291/6913)\rResolving deltas:  92% (6360/6913)\rResolving deltas:  93% (6430/6913)\rResolving deltas:  94% (6499/6913)\rResolving deltas:  95% (6568/6913)\rResolving deltas:  96% (6637/6913)\rResolving deltas:  97% (6706/6913)\rResolving deltas:  98% (6775/6913)\rResolving deltas:  99% (6844/6913)\rResolving deltas: 100% (6913/6913)\rResolving deltas: 100% (6913/6913), done.\r\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!git clone https://github.com/RUCAIBox/TextBox.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d99642b",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cfcdf1",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d805c3f1",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b349daef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Install model repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdd119e",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629e2d8e",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb93af7",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5788c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/TextBox\n/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd ./TextBox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26910a9",
   "metadata": {},
   "source": [
    "### Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ac605a",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dcdace",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab766baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LICENSE      TODOS.md\t instructions\t   run_hyper.py       textbox\r\nLeaderboard  asset\t packages\t   run_multi_seed.py\r\nMANIFEST.in  dataset\t requirements.txt  run_textbox.py\r\nREADME.md    install.sh  run_analysis.py   style.cfg\r\n\r\n████████╗███████╗██╗  ██╗████████╗██████╗  ██████╗ ██╗  ██╗\r\n╚══██╔══╝██╔════╝╚██╗██╔╝╚══██╔══╝██╔══██╗██╔═══██╗╚██╗██╔╝\r\n   ██║   █████╗   ╚███╔╝    ██║   ██████╔╝██║   ██║ ╚███╔╝\r\n   ██║   ██╔══╝   ██╔██╗    ██║   ██╔══██╗██║   ██║ ██╔██╗\r\n   ██║   ███████╗██╔╝ ██╗   ██║   ██████╔╝╚██████╔╝██╔╝ ██╗\r\n   ╚═╝   ╚══════╝╚═╝  ╚═╝   ╚═╝   ╚═════╝  ╚═════╝ ╚═╝  ╚═╝\r\n\r\nInstallation may take a few minutes.\r\n\u001b[0;32mInstalling requirements ...\u001b[0m\r\nCollecting git+https://github.com/microsoft/fastseq.git (from -r requirements.txt (line 19))\r\n  Cloning https://github.com/microsoft/fastseq.git to /tmp/pip-req-build-hmq24fx0\r\n  Running command git clone --filter=blob:none --quiet https://github.com/microsoft/fastseq.git /tmp/pip-req-build-hmq24fx0\r\n  Resolved https://github.com/microsoft/fastseq.git to commit 7b19f8fe5f2e66edee1aef286bb5e719eca15a7c\r\n  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \berror\r\n  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\r\n  \r\n  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\r\n  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\r\n  \u001b[31m╰─>\u001b[0m \u001b[31m[17 lines of output]\u001b[0m\r\n  \u001b[31m   \u001b[0m Traceback (most recent call last):\r\n  \u001b[31m   \u001b[0m   File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\r\n  \u001b[31m   \u001b[0m     main()\r\n  \u001b[31m   \u001b[0m   File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\r\n  \u001b[31m   \u001b[0m     json_out['return_val'] = hook(**hook_input['kwargs'])\r\n  \u001b[31m   \u001b[0m   File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 118, in get_requires_for_build_wheel\r\n  \u001b[31m   \u001b[0m     return hook(config_settings)\r\n  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-1m_bcvap/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 332, in get_requires_for_build_wheel\r\n  \u001b[31m   \u001b[0m     return self._get_build_requires(config_settings, requirements=[])\r\n  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-1m_bcvap/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 302, in _get_build_requires\r\n  \u001b[31m   \u001b[0m     self.run_setup()\r\n  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-1m_bcvap/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 502, in run_setup\r\n  \u001b[31m   \u001b[0m     super().run_setup(setup_script=setup_script)\r\n  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-1m_bcvap/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 318, in run_setup\r\n  \u001b[31m   \u001b[0m     exec(code, locals())\r\n  \u001b[31m   \u001b[0m   File \"<string>\", line 5, in <module>\r\n  \u001b[31m   \u001b[0m ModuleNotFoundError: No module named 'torch'\r\n  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\r\n  \r\n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\r\n\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\r\n\r\n\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\r\n\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\r\n\u001b[31m╰─>\u001b[0m See above for output.\r\n\r\n\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\r\n\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n\u001b[?25h\u001b[0;32mInstalling requirements (rouge) ...\u001b[0m\r\nCollecting git+https://github.com/pltrdy/pyrouge.git\r\n  Cloning https://github.com/pltrdy/pyrouge.git to /tmp/pip-req-build-ogdsudbl\r\n  Running command git clone --filter=blob:none --quiet https://github.com/pltrdy/pyrouge.git /tmp/pip-req-build-ogdsudbl\r\n  Resolved https://github.com/pltrdy/pyrouge.git to commit dfdff03cff1b69bfb18d42dadfb79c79f1caaa18\r\n  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\r\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n\u001b[?25hBuilding wheels for collected packages: pyrouge\r\n  Building wheel for pyrouge (pyproject.toml) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\r\n\u001b[?25h  Created wheel for pyrouge: filename=pyrouge-0.1.3-py3-none-any.whl size=191911 sha256=ac95a553e5e52fee843366f07167c5160302cb0faaa88c5a98a139242fe82df7\r\n  Stored in directory: /tmp/pip-ephem-wheel-cache-pp8_suxl/wheels/16/e8/c7/28be7897c7074d902b680d0a8ebf69e7746dbfca543df33021\r\nSuccessfully built pyrouge\r\nInstalling collected packages: pyrouge\r\nSuccessfully installed pyrouge-0.1.3\r\n\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\nCloning into 'files2rouge'...\r\nremote: Enumerating objects: 278, done.\u001b[K\r\nremote: Counting objects:   5% (1/20)\u001b[K\rremote: Counting objects:  10% (2/20)\u001b[K\rremote: Counting objects:  15% (3/20)\u001b[K\rremote: Counting objects:  20% (4/20)\u001b[K\rremote: Counting objects:  25% (5/20)\u001b[K\rremote: Counting objects:  30% (6/20)\u001b[K\rremote: Counting objects:  35% (7/20)\u001b[K\rremote: Counting objects:  40% (8/20)\u001b[K\rremote: Counting objects:  45% (9/20)\u001b[K\rremote: Counting objects:  50% (10/20)\u001b[K\rremote: Counting objects:  55% (11/20)\u001b[K\rremote: Counting objects:  60% (12/20)\u001b[K\rremote: Counting objects:  65% (13/20)\u001b[K\rremote: Counting objects:  70% (14/20)\u001b[K\rremote: Counting objects:  75% (15/20)\u001b[K\rremote: Counting objects:  80% (16/20)\u001b[K\rremote: Counting objects:  85% (17/20)\u001b[K\rremote: Counting objects:  90% (18/20)\u001b[K\rremote: Counting objects:  95% (19/20)\u001b[K\rremote: Counting objects: 100% (20/20)\u001b[K\rremote: Counting objects: 100% (20/20), done.\u001b[K\r\nremote: Compressing objects:   7% (1/14)\u001b[K\rremote: Compressing objects:  14% (2/14)\u001b[K\rremote: Compressing objects:  21% (3/14)\u001b[K\rremote: Compressing objects:  28% (4/14)\u001b[K\rremote: Compressing objects:  35% (5/14)\u001b[K\rremote: Compressing objects:  42% (6/14)\u001b[K\rremote: Compressing objects:  50% (7/14)\u001b[K\rremote: Compressing objects:  57% (8/14)\u001b[K\rremote: Compressing objects:  64% (9/14)\u001b[K\rremote: Compressing objects:  71% (10/14)\u001b[K\rremote: Compressing objects:  78% (11/14)\u001b[K\rremote: Compressing objects:  85% (12/14)\u001b[K\rremote: Compressing objects:  92% (13/14)\u001b[K\rremote: Compressing objects: 100% (14/14)\u001b[K\rremote: Compressing objects: 100% (14/14), done.\u001b[K\r\nReceiving objects:   0% (1/278)\rReceiving objects:   1% (3/278)\rReceiving objects:   2% (6/278)\rReceiving objects:   3% (9/278)\rReceiving objects:   4% (12/278)\rReceiving objects:   5% (14/278)\rReceiving objects:   6% (17/278)\rReceiving objects:   7% (20/278)\rReceiving objects:   8% (23/278)\rReceiving objects:   9% (26/278)\rReceiving objects:  10% (28/278)\rReceiving objects:  11% (31/278)\rReceiving objects:  12% (34/278)\rReceiving objects:  13% (37/278)\rReceiving objects:  14% (39/278)\rReceiving objects:  15% (42/278)\rReceiving objects:  16% (45/278)\rReceiving objects:  17% (48/278)\rReceiving objects:  18% (51/278)\rReceiving objects:  19% (53/278)\rReceiving objects:  20% (56/278)\rReceiving objects:  21% (59/278)\rReceiving objects:  22% (62/278)\rReceiving objects:  23% (64/278)\rReceiving objects:  24% (67/278)\rReceiving objects:  25% (70/278)\rReceiving objects:  26% (73/278)\rReceiving objects:  27% (76/278)\rReceiving objects:  28% (78/278)\rReceiving objects:  29% (81/278)\rReceiving objects:  30% (84/278)\rReceiving objects:  31% (87/278)\rReceiving objects:  32% (89/278)\rReceiving objects:  33% (92/278)\rReceiving objects:  34% (95/278)\rReceiving objects:  35% (98/278)\rReceiving objects:  36% (101/278)\rReceiving objects:  37% (103/278)\rReceiving objects:  38% (106/278)\rReceiving objects:  39% (109/278)\rremote: Total 278 (delta 9), reused 11 (delta 6), pack-reused 258 (from 1)\u001b[K\r\nReceiving objects:  40% (112/278)\rReceiving objects:  41% (114/278)\rReceiving objects:  42% (117/278)\rReceiving objects:  43% (120/278)\rReceiving objects:  44% (123/278)\rReceiving objects:  45% (126/278)\rReceiving objects:  46% (128/278)\rReceiving objects:  47% (131/278)\rReceiving objects:  48% (134/278)\rReceiving objects:  49% (137/278)\rReceiving objects:  50% (139/278)\rReceiving objects:  51% (142/278)\rReceiving objects:  52% (145/278)\rReceiving objects:  53% (148/278)\rReceiving objects:  54% (151/278)\rReceiving objects:  55% (153/278)\rReceiving objects:  56% (156/278)\rReceiving objects:  57% (159/278)\rReceiving objects:  58% (162/278)\rReceiving objects:  59% (165/278)\rReceiving objects:  60% (167/278)\rReceiving objects:  61% (170/278)\rReceiving objects:  62% (173/278)\rReceiving objects:  63% (176/278)\rReceiving objects:  64% (178/278)\rReceiving objects:  65% (181/278)\rReceiving objects:  66% (184/278)\rReceiving objects:  67% (187/278)\rReceiving objects:  68% (190/278)\rReceiving objects:  69% (192/278)\rReceiving objects:  70% (195/278)\rReceiving objects:  71% (198/278)\rReceiving objects:  72% (201/278)\rReceiving objects:  73% (203/278)\rReceiving objects:  74% (206/278)\rReceiving objects:  75% (209/278)\rReceiving objects:  76% (212/278)\rReceiving objects:  77% (215/278)\rReceiving objects:  78% (217/278)\rReceiving objects:  79% (220/278)\rReceiving objects:  80% (223/278)\rReceiving objects:  81% (226/278)\rReceiving objects:  82% (228/278)\rReceiving objects:  83% (231/278)\rReceiving objects:  84% (234/278)\rReceiving objects:  85% (237/278)\rReceiving objects:  86% (240/278)\rReceiving objects:  87% (242/278)\rReceiving objects:  88% (245/278)\rReceiving objects:  89% (248/278)\rReceiving objects:  90% (251/278)\rReceiving objects:  91% (253/278)\rReceiving objects:  92% (256/278)\rReceiving objects:  93% (259/278)\rReceiving objects:  94% (262/278)\rReceiving objects:  95% (265/278)\rReceiving objects:  96% (267/278)\rReceiving objects:  97% (270/278)\rReceiving objects:  98% (273/278)\rReceiving objects:  99% (276/278)\rReceiving objects: 100% (278/278)\rReceiving objects: 100% (278/278), 212.16 KiB | 19.29 MiB/s, done.\r\nResolving deltas:   0% (0/133)\rResolving deltas:   1% (2/133)\rResolving deltas:   2% (3/133)\rResolving deltas:   3% (4/133)\rResolving deltas:   4% (6/133)\rResolving deltas:   5% (7/133)\rResolving deltas:   6% (8/133)\rResolving deltas:   7% (10/133)\rResolving deltas:   8% (11/133)\rResolving deltas:   9% (12/133)\rResolving deltas:  10% (14/133)\rResolving deltas:  11% (15/133)\rResolving deltas:  12% (16/133)\rResolving deltas:  13% (18/133)\rResolving deltas:  14% (19/133)\rResolving deltas:  15% (20/133)\rResolving deltas:  16% (22/133)\rResolving deltas:  17% (23/133)\rResolving deltas:  18% (24/133)\rResolving deltas:  19% (26/133)\rResolving deltas:  20% (27/133)\rResolving deltas:  21% (28/133)\rResolving deltas:  22% (30/133)\rResolving deltas:  23% (31/133)\rResolving deltas:  24% (32/133)\rResolving deltas:  25% (34/133)\rResolving deltas:  26% (35/133)\rResolving deltas:  27% (36/133)\rResolving deltas:  28% (38/133)\rResolving deltas:  29% (39/133)\rResolving deltas:  30% (40/133)\rResolving deltas:  31% (42/133)\rResolving deltas:  32% (43/133)\rResolving deltas:  33% (44/133)\rResolving deltas:  34% (46/133)\rResolving deltas:  35% (47/133)\rResolving deltas:  36% (48/133)\rResolving deltas:  37% (50/133)\rResolving deltas:  38% (51/133)\rResolving deltas:  39% (52/133)\rResolving deltas:  40% (54/133)\rResolving deltas:  41% (55/133)\rResolving deltas:  42% (56/133)\rResolving deltas:  43% (58/133)\rResolving deltas:  44% (59/133)\rResolving deltas:  45% (60/133)\rResolving deltas:  46% (62/133)\rResolving deltas:  47% (63/133)\rResolving deltas:  48% (64/133)\rResolving deltas:  49% (66/133)\rResolving deltas:  50% (67/133)\rResolving deltas:  51% (68/133)\rResolving deltas:  52% (70/133)\rResolving deltas:  53% (71/133)\rResolving deltas:  54% (72/133)\rResolving deltas:  55% (74/133)\rResolving deltas:  56% (75/133)\rResolving deltas:  57% (76/133)\rResolving deltas:  58% (78/133)\rResolving deltas:  59% (79/133)\rResolving deltas:  60% (80/133)\rResolving deltas:  61% (82/133)\rResolving deltas:  62% (83/133)\rResolving deltas:  63% (84/133)\rResolving deltas:  64% (86/133)\rResolving deltas:  65% (87/133)\rResolving deltas:  66% (88/133)\rResolving deltas:  67% (90/133)\rResolving deltas:  68% (91/133)\rResolving deltas:  69% (92/133)\rResolving deltas:  70% (94/133)\rResolving deltas:  71% (95/133)\rResolving deltas:  72% (96/133)\rResolving deltas:  73% (98/133)\rResolving deltas:  74% (99/133)\rResolving deltas:  75% (100/133)\rResolving deltas:  76% (102/133)\rResolving deltas:  77% (103/133)\rResolving deltas:  78% (104/133)\rResolving deltas:  79% (106/133)\rResolving deltas:  80% (107/133)\rResolving deltas:  81% (108/133)\rResolving deltas:  82% (110/133)\rResolving deltas:  83% (111/133)\rResolving deltas:  84% (112/133)\rResolving deltas:  85% (114/133)\rResolving deltas:  86% (115/133)\rResolving deltas:  87% (116/133)\rResolving deltas:  88% (118/133)\rResolving deltas:  89% (119/133)\rResolving deltas:  90% (120/133)\rResolving deltas:  91% (122/133)\rResolving deltas:  92% (123/133)\rResolving deltas:  93% (124/133)\rResolving deltas:  94% (126/133)\rResolving deltas:  95% (127/133)\rResolving deltas:  96% (128/133)\rResolving deltas:  97% (130/133)\rResolving deltas:  98% (131/133)\rResolving deltas:  99% (132/133)\rResolving deltas: 100% (133/133)\rResolving deltas: 100% (133/133), done.\r\nfiles2rouge uses scripts and tools that will not be stored with the python package\r\nwhere do you want to save it? [default: /root/.files2rouge/]Copying './files2rouge/RELEASE-1.5.5/' to '/root/.files2rouge/'\r\nrunning install\r\n/usr/local/lib/python3.10/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\r\n!!\r\n\r\n        ********************************************************************************\r\n        Please avoid running ``setup.py`` directly.\r\n        Instead, use pypa/build, pypa/installer or other\r\n        standards-based tools.\r\n\r\n        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\r\n        ********************************************************************************\r\n\r\n!!\r\n  self.initialize_options()\r\n/usr/local/lib/python3.10/site-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\r\n!!\r\n\r\n        ********************************************************************************\r\n        Please avoid running ``setup.py`` and ``easy_install``.\r\n        Instead, use pypa/build, pypa/installer or other\r\n        standards-based tools.\r\n\r\n        See https://github.com/pypa/setuptools/issues/917 for details.\r\n        ********************************************************************************\r\n\r\n!!\r\n  self.initialize_options()\r\nrunning bdist_egg\r\nrunning egg_info\r\ncreating files2rouge.egg-info\r\nwriting files2rouge.egg-info/PKG-INFO\r\nwriting dependency_links to files2rouge.egg-info/dependency_links.txt\r\nwriting entry points to files2rouge.egg-info/entry_points.txt\r\nwriting top-level names to files2rouge.egg-info/top_level.txt\r\nwriting manifest file 'files2rouge.egg-info/SOURCES.txt'\r\nreading manifest file 'files2rouge.egg-info/SOURCES.txt'\r\nreading manifest template 'MANIFEST.in'\r\nadding license file 'LICENSE'\r\nwriting manifest file 'files2rouge.egg-info/SOURCES.txt'\r\ninstalling library code to build/bdist.linux-x86_64/egg\r\nrunning install_lib\r\nrunning build_py\r\ncreating build\r\ncreating build/lib\r\ncreating build/lib/files2rouge\r\ncopying files2rouge/__init__.py -> build/lib/files2rouge\r\ncopying files2rouge/files2rouge.py -> build/lib/files2rouge\r\ncopying files2rouge/settings.py -> build/lib/files2rouge\r\ncopying files2rouge/utils.py -> build/lib/files2rouge\r\ncopying files2rouge/settings.json -> build/lib/files2rouge\r\ncreating build/bdist.linux-x86_64\r\ncreating build/bdist.linux-x86_64/egg\r\ncreating build/bdist.linux-x86_64/egg/files2rouge\r\ncopying build/lib/files2rouge/__init__.py -> build/bdist.linux-x86_64/egg/files2rouge\r\ncopying build/lib/files2rouge/files2rouge.py -> build/bdist.linux-x86_64/egg/files2rouge\r\ncopying build/lib/files2rouge/settings.py -> build/bdist.linux-x86_64/egg/files2rouge\r\ncopying build/lib/files2rouge/utils.py -> build/bdist.linux-x86_64/egg/files2rouge\r\ncopying build/lib/files2rouge/settings.json -> build/bdist.linux-x86_64/egg/files2rouge\r\nbyte-compiling build/bdist.linux-x86_64/egg/files2rouge/__init__.py to __init__.cpython-310.pyc\r\nbyte-compiling build/bdist.linux-x86_64/egg/files2rouge/files2rouge.py to files2rouge.cpython-310.pyc\r\nbyte-compiling build/bdist.linux-x86_64/egg/files2rouge/settings.py to settings.cpython-310.pyc\r\nbyte-compiling build/bdist.linux-x86_64/egg/files2rouge/utils.py to utils.cpython-310.pyc\r\ncreating build/bdist.linux-x86_64/egg/EGG-INFO\r\ncopying files2rouge.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\r\ncopying files2rouge.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\r\ncopying files2rouge.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\r\ncopying files2rouge.egg-info/entry_points.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\r\ncopying files2rouge.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\r\nzip_safe flag not set; analyzing archive contents...\r\nfiles2rouge.__pycache__.settings.cpython-310: module references __file__\r\ncreating dist\r\ncreating 'dist/files2rouge-2.1.0-py3.10.egg' and adding 'build/bdist.linux-x86_64/egg' to it\r\nremoving 'build/bdist.linux-x86_64/egg' (and everything under it)\r\nProcessing files2rouge-2.1.0-py3.10.egg\r\ncreating /usr/local/lib/python3.10/site-packages/files2rouge-2.1.0-py3.10.egg\r\nExtracting files2rouge-2.1.0-py3.10.egg to /usr/local/lib/python3.10/site-packages\r\nAdding files2rouge 2.1.0 to easy-install.pth file\r\nInstalling files2rouge script to /usr/local/bin\r\n\r\nInstalled /usr/local/lib/python3.10/site-packages/files2rouge-2.1.0-py3.10.egg\r\nProcessing dependencies for files2rouge==2.1.0\r\nFinished processing dependencies for files2rouge==2.1.0\r\n\u001b[33mWARNING: Skipping py-rouge as it is not installed.\u001b[0m\u001b[33m\r\n\u001b[0mCollecting rouge\r\n  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\r\nRequirement already satisfied: six in /usr/local/lib/python3.10/site-packages (from rouge) (1.16.0)\r\nDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\r\nInstalling collected packages: rouge\r\nSuccessfully installed rouge-1.0.1\r\n\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ls\n",
    "!echo \"\" | bash install.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dc6c2f",
   "metadata": {},
   "source": [
    "### Step 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e84ad4",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bc8657",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b45bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Install additional packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7007f22",
   "metadata": {},
   "source": [
    "### Step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7163d95",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0167d843",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687cdb87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting accelerate==0.19.0\r\n  Downloading accelerate-0.19.0-py3-none-any.whl.metadata (16 kB)\r\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from accelerate==0.19.0) (1.25.2)\r\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from accelerate==0.19.0) (24.1)\r\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/site-packages (from accelerate==0.19.0) (5.9.8)\r\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/site-packages (from accelerate==0.19.0) (6.0.1)\r\nRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/site-packages (from accelerate==0.19.0) (2.2.1+cpu)\r\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.19.0) (3.15.1)\r\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.19.0) (4.12.2)\r\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.19.0) (1.12)\r\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.19.0) (3.3)\r\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.19.0) (3.1.4)\r\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.19.0) (2023.6.0)\r\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate==0.19.0) (2.1.5)\r\nRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate==0.19.0) (1.3.0)\r\nDownloading accelerate-0.19.0-py3-none-any.whl (219 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/219.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m239.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hInstalling collected packages: accelerate\r\nSuccessfully installed accelerate-0.19.0\r\n\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\nCollecting colorama\r\n  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\r\nDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\r\nInstalling collected packages: colorama\r\nSuccessfully installed colorama-0.4.6\r\n\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\nCollecting wandb\r\n  Downloading wandb-0.17.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\r\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/site-packages (from wandb) (8.1.7)\r\nCollecting docker-pycreds>=0.4.0 (from wandb)\r\n  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\r\nCollecting gitpython!=3.1.29,>=1.0.0 (from wandb)\r\n  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\r\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/site-packages (from wandb) (4.2.2)\r\nRequirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/site-packages (from wandb) (5.27.1)\r\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/site-packages (from wandb) (5.9.8)\r\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/site-packages (from wandb) (6.0.1)\r\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/site-packages (from wandb) (2.31.0)\r\nCollecting sentry-sdk>=1.0.0 (from wandb)\r\n  Downloading sentry_sdk-2.13.0-py2.py3-none-any.whl.metadata (9.7 kB)\r\nCollecting setproctitle (from wandb)\r\n  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\r\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from wandb) (68.1.2)\r\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\r\nCollecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\r\n  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\r\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\r\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\r\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.6.2)\r\nCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\r\n  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\r\nDownloading wandb-0.17.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.3 MB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/9.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m291.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\r\nDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/207.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m303.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading sentry_sdk-2.13.0-py2.py3-none-any.whl (309 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/309.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.1/309.1 kB\u001b[0m \u001b[31m302.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\r\nDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m252.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\r\nInstalling collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\r\nSuccessfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.13.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.7\r\n\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\nCollecting datasets==2.19.0\r\n  Downloading datasets-2.19.0-py3-none-any.whl.metadata (19 kB)\r\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (3.15.1)\r\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (1.25.2)\r\nCollecting pyarrow>=12.0.0 (from datasets==2.19.0)\r\n  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\r\nCollecting pyarrow-hotfix (from datasets==2.19.0)\r\n  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\r\nCollecting dill<0.3.9,>=0.3.0 (from datasets==2.19.0)\r\n  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\r\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (2.0.3)\r\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (2.31.0)\r\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (4.66.4)\r\nCollecting xxhash (from datasets==2.19.0)\r\n  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\r\nCollecting multiprocess (from datasets==2.19.0)\r\n  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\r\nRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets==2.19.0) (2023.6.0)\r\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (3.8.3)\r\nRequirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (0.23.3)\r\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (24.1)\r\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0) (6.0.1)\r\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0) (23.2.0)\r\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0) (2.1.1)\r\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0) (6.0.5)\r\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0) (4.0.3)\r\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0) (1.9.4)\r\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0) (1.4.1)\r\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0) (1.3.1)\r\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets==2.19.0) (4.12.2)\r\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.19.0) (3.7)\r\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.19.0) (2.0.7)\r\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.19.0) (2024.6.2)\r\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas->datasets==2.19.0) (2.9.0.post0)\r\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets==2.19.0) (2024.1)\r\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets==2.19.0) (2024.1)\r\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.19.0) (1.16.0)\r\nDownloading datasets-2.19.0-py3-none-any.whl (542 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/542.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m206.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m228.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/39.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/39.9 MB\u001b[0m \u001b[31m294.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/39.9 MB\u001b[0m \u001b[31m273.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m28.5/39.9 MB\u001b[0m \u001b[31m262.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m37.3/39.9 MB\u001b[0m \u001b[31m255.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m243.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/134.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m175.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\r\nDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\r\n\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/194.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m194.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hInstalling collected packages: xxhash, pyarrow-hotfix, pyarrow, dill, multiprocess, datasets\r\nSuccessfully installed datasets-2.19.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 pyarrow-hotfix-0.6 xxhash-3.5.0\r\n\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\nCollecting rouge-score\r\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\r\n  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\r\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/site-packages (from rouge-score) (2.1.0)\r\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/site-packages (from rouge-score) (3.8.1)\r\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from rouge-score) (1.25.2)\r\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/site-packages (from rouge-score) (1.16.0)\r\nRequirement already satisfied: click in /usr/local/lib/python3.10/site-packages (from nltk->rouge-score) (8.1.7)\r\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/site-packages (from nltk->rouge-score) (1.4.2)\r\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/site-packages (from nltk->rouge-score) (2023.12.25)\r\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from nltk->rouge-score) (4.66.4)\r\nBuilding wheels for collected packages: rouge-score\r\n  Building wheel for rouge-score (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=c15ce7de762dbb8d15e553182c19cf2551cdba98d87be4d06649852dcca98abd\r\n  Stored in directory: /tmp/pip-ephem-wheel-cache-v6numcax/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\r\nSuccessfully built rouge-score\r\nInstalling collected packages: rouge-score\r\nSuccessfully installed rouge-score-0.1.2\r\n\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install accelerate==0.19.0\n",
    "!pip install colorama\n",
    "!pip install wandb\n",
    "!pip install datasets==2.19.0\n",
    "!pip install rouge-score\n",
    "#!pip install rouge-score==0.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9782577",
   "metadata": {},
   "source": [
    "### Step 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfc9d1d",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9db751",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf61d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Download and format dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bd2472",
   "metadata": {},
   "source": [
    "### Step 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8314193",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68436f8e",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a2d6c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Installing additional dependencies and configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eb5815",
   "metadata": {},
   "source": [
    "### Step 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6800023d",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf568cb",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63b1067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 5%\r\rReading package lists... 5%\r\rReading package lists... 6%\r\rReading package lists... 6%\r\rReading package lists... 56%\r\rReading package lists... 56%\r\rReading package lists... 57%\r\rReading package lists... 57%\r\rReading package lists... 64%\r\rReading package lists... 64%\r\rReading package lists... 74%\r\rReading package lists... 74%\r\rReading package lists... 79%\r\rReading package lists... 79%\r\rReading package lists... 79%\r\rReading package lists... 79%\r\rReading package lists... 79%\r\rReading package lists... 79%\r\rReading package lists... 79%\r\rReading package lists... 79%\r\rReading package lists... 86%\r\rReading package lists... 86%\r\rReading package lists... 95%\r\rReading package lists... 95%\r\rReading package lists... 95%\r\rReading package lists... 99%\r\rReading package lists... 99%\r\rReading package lists... 99%\r\rReading package lists... 99%\r\rReading package lists... Done\r\r\n\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree... Done\r\r\n\rReading state information... 0% \r\rReading state information... 0%\r\rReading state information... Done\r\r\nThe following additional packages will be installed:\r\n  libauthen-sasl-perl libclone-perl libdata-dump-perl libencode-locale-perl\r\n  libfile-listing-perl libfont-afm-perl libhtml-form-perl libhtml-format-perl\r\n  libhtml-parser-perl libhtml-tagset-perl libhtml-tree-perl\r\n  libhttp-cookies-perl libhttp-daemon-perl libhttp-date-perl\r\n  libhttp-message-perl libhttp-negotiate-perl libio-html-perl\r\n  libio-socket-ssl-perl liblwp-mediatypes-perl liblwp-protocol-https-perl\r\n  libmailtools-perl libnet-http-perl libnet-smtp-ssl-perl libnet-ssleay-perl\r\n  libtimedate-perl libtry-tiny-perl liburi-perl libwww-perl\r\n  libwww-robotrules-perl libxml-namespacesupport-perl libxml-parser-perl\r\n  libxml-sax-base-perl libxml-sax-expat-perl libxml-sax-perl\r\n  perl-openssl-defaults\r\nSuggested packages:\r\n  libdigest-hmac-perl libgssapi-perl libcrypt-ssleay-perl libsub-name-perl\r\n  libbusiness-isbn-perl libauthen-ntlm-perl libxml-sax-expatxs-perl\r\nThe following NEW packages will be installed:\r\n  libauthen-sasl-perl libclone-perl libdata-dump-perl libencode-locale-perl\r\n  libfile-listing-perl libfont-afm-perl libhtml-form-perl libhtml-format-perl\r\n  libhtml-parser-perl libhtml-tagset-perl libhtml-tree-perl\r\n  libhttp-cookies-perl libhttp-daemon-perl libhttp-date-perl\r\n  libhttp-message-perl libhttp-negotiate-perl libio-html-perl\r\n  libio-socket-ssl-perl liblwp-mediatypes-perl liblwp-protocol-https-perl\r\n  libmailtools-perl libnet-http-perl libnet-smtp-ssl-perl libnet-ssleay-perl\r\n  libtimedate-perl libtry-tiny-perl liburi-perl libwww-perl\r\n  libwww-robotrules-perl libxml-libxml-perl libxml-namespacesupport-perl\r\n  libxml-parser-perl libxml-sax-base-perl libxml-sax-expat-perl\r\n  libxml-sax-perl libxml-xpath-perl perl-openssl-defaults\r\n0 upgraded, 37 newly installed, 0 to remove and 0 not upgraded.\r\nNeed to get 2295 kB of archives.\r\nAfter this operation, 7333 kB of additional disk space will be used.\r\n\r0% [Working]\r            \rGet:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libclone-perl amd64 0.45-1build3 [11.0 kB]\r\n\r0% [1 libclone-perl 11.0 kB/11.0 kB 100%]\r                                         \r1% [Working]\r            \rGet:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libdata-dump-perl all 1.25-1 [25.9 kB]\r\n\r1% [2 libdata-dump-perl 2606 B/25.9 kB 10%]\r                                           \r2% [Waiting for headers]\r                        \rGet:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libencode-locale-perl all 1.05-1.1 [11.8 kB]\r\n\r2% [3 libencode-locale-perl 2456 B/11.8 kB 21%]\r                                               \r3% [Waiting for headers]\r                        \rGet:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtimedate-perl all 2.3300-2 [34.0 kB]\r\n\r3% [4 libtimedate-perl 1970 B/34.0 kB 6%]\r                                         \r5% [Waiting for headers]\r                        \rGet:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libhttp-date-perl all 6.05-1 [9920 B]\r\n\r5% [5 libhttp-date-perl 699 B/9920 B 7%]\r                                        \r6% [Waiting for headers]\r                        \rGet:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfile-listing-perl all 6.14-1 [11.2 kB]\r\n\r6% [6 libfile-listing-perl 2073 B/11.2 kB 18%]\r                                              \r7% [Waiting for headers]\r                        \rGet:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfont-afm-perl all 1.20-3 [13.6 kB]\r\n\r7% [7 libfont-afm-perl 2159 B/13.6 kB 16%]\r                                          \r8% [Waiting for headers]\r                        \rGet:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libhtml-tagset-perl all 3.20-4 [12.5 kB]\r\n\r8% [8 libhtml-tagset-perl 2606 B/12.5 kB 21%]\r                                             \r9% [Waiting for headers]\r                        \rGet:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 liburi-perl all 5.10-1 [78.8 kB]\r\n\r9% [9 liburi-perl 1443 B/78.8 kB 2%]\r                                    \r12% [Waiting for headers]\r                         \rGet:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libhtml-parser-perl amd64 3.76-1build2 [88.4 kB]\r\n\r12% [10 libhtml-parser-perl 687 B/88.4 kB 1%]\r                                             \r16% [Waiting for headers]\r                         \rGet:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libio-html-perl all 1.004-2 [15.4 kB]\r\n\r16% [11 libio-html-perl 776 B/15.4 kB 5%]\r                                         \r17% [Waiting for headers]\r                         \rGet:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblwp-mediatypes-perl all 6.04-1 [19.5 kB]\r\n\r17% [12 liblwp-mediatypes-perl 2870 B/19.5 kB 15%]\r                                                  \r18% [Waiting for headers]\r                         \rGet:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 libhttp-message-perl all 6.36-1 [76.8 kB]\r\n\r18% [13 libhttp-message-perl 483 B/76.8 kB 1%]\r                                              \r21% [Waiting for headers]\r                         \rGet:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 libhtml-form-perl all 6.07-1 [22.2 kB]\r\n\r21% [14 libhtml-form-perl 3758 B/22.2 kB 17%]\r                                             \r23% [Waiting for headers]\r                         \rGet:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libhtml-tree-perl all 5.07-2 [200 kB]\r\n\r23% [15 libhtml-tree-perl 5436 B/200 kB 3%]\r                                           \r30% [Waiting for headers]\r                         \rGet:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 libhtml-format-perl all 2.12-1.1 [41.3 kB]\r\n\r30% [16 libhtml-format-perl 746 B/41.3 kB 2%]\r                                             \r32% [Waiting for headers]\r                         \rGet:17 http://archive.ubuntu.com/ubuntu jammy/main amd64 libhttp-cookies-perl all 6.10-1 [18.4 kB]\r\n\r32% [17 libhttp-cookies-perl 5792 B/18.4 kB 31%]\r                                                \r33% [Waiting for headers]\r                         \rGet:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libhttp-daemon-perl all 6.13-1ubuntu0.1 [22.9 kB]\r\n\r33% [18 libhttp-daemon-perl 4442 B/22.9 kB 19%]\r                                               \r35% [Waiting for headers]\r                         \rGet:19 http://archive.ubuntu.com/ubuntu jammy/main amd64 libhttp-negotiate-perl all 6.01-1 [12.5 kB]\r\n\r35% [19 libhttp-negotiate-perl 4436 B/12.5 kB 35%]\r                                                  \r36% [Waiting for headers]\r                         \rGet:20 http://archive.ubuntu.com/ubuntu jammy/main amd64 perl-openssl-defaults amd64 5build2 [7542 B]\r\n\r36% [20 perl-openssl-defaults 5503 B/7542 B 73%]\r                                                \r36% [Waiting for headers]\r                         \rGet:21 http://archive.ubuntu.com/ubuntu jammy/main amd64 libnet-ssleay-perl amd64 1.92-1build2 [327 kB]\r\n\r36% [21 libnet-ssleay-perl 3461 B/327 kB 1%]\r                                            \r48% [Waiting for headers]\r                         \rGet:22 http://archive.ubuntu.com/ubuntu jammy/main amd64 libio-socket-ssl-perl all 2.074-2 [192 kB]\r\n\r49% [22 libio-socket-ssl-perl 17.1 kB/192 kB 9%]\r                                                \r56% [Waiting for headers]\r                         \rGet:23 http://archive.ubuntu.com/ubuntu jammy/main amd64 libnet-http-perl all 6.22-1 [23.2 kB]\r\n\r56% [23 libnet-http-perl 8192 B/23.2 kB 35%]\r                                            \r57% [Waiting for headers]\r                         \rGet:24 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtry-tiny-perl all 0.31-1 [21.8 kB]\r\n\r57% [24 libtry-tiny-perl 7878 B/21.8 kB 36%]\r                                            \r58% [Waiting for headers]\r                         \rGet:25 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwww-robotrules-perl all 6.02-1 [12.6 kB]\r\n\r58% [25 libwww-robotrules-perl 6587 B/12.6 kB 52%]\r                                                  \r59% [Waiting for headers]\r                         \rGet:26 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwww-perl all 6.61-1 [141 kB]\r\n\r60% [26 libwww-perl 28.8 kB/141 kB 20%]\r                                       \r65% [Waiting for headers]\r                         \rGet:27 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblwp-protocol-https-perl all 6.10-1 [10.9 kB]\r\n\r                         \rGet:28 http://archive.ubuntu.com/ubuntu jammy/main amd64 libnet-smtp-ssl-perl all 1.04-1 [5948 B]\r\n\r66% [28 libnet-smtp-ssl-perl 5948 B/5948 B 100%]\r                                                \r66% [Working]\r             \rGet:29 http://archive.ubuntu.com/ubuntu jammy/main amd64 libmailtools-perl all 2.21-1 [80.7 kB]\r\n\r67% [29 libmailtools-perl 8192 B/80.7 kB 10%]\r                                             \r70% [Working]\r             \rGet:30 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxml-namespacesupport-perl all 1.12-1.1 [13.2 kB]\r\n\r70% [30 libxml-namespacesupport-perl 11.2 kB/13.2 kB 85%]\r                                                         \r71% [Waiting for headers]\r                         \rGet:31 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxml-sax-base-perl all 1.09-1.1 [19.0 kB]\r\n\r71% [31 libxml-sax-base-perl 16.5 kB/19.0 kB 87%]\r                                                 \r72% [Waiting for headers]\r                         \rGet:32 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxml-sax-perl all 1.02+dfsg-3 [57.0 kB]\r\n\r73% [32 libxml-sax-perl 25.8 kB/57.0 kB 45%]\r                                            \r74% [Waiting for headers]\r                         \rGet:33 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxml-libxml-perl amd64 2.0207+dfsg+really+2.0134-1 [325 kB]\r\n\r75% [33 libxml-libxml-perl 6158 B/325 kB 2%]\r                                            \r86% [Waiting for headers]\r                         \rGet:34 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxml-parser-perl amd64 2.46-3build1 [212 kB]\r\n\r86% [34 libxml-parser-perl 4008 B/212 kB 2%]\r                                            \r94% [Waiting for headers]\r                         \rGet:35 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxml-sax-expat-perl all 0.51-1 [10.5 kB]\r\n\r95% [35 libxml-sax-expat-perl 10.5 kB/10.5 kB 100%]\r                                                   \r95% [Working]\r             \rGet:36 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libxml-xpath-perl all 1.44-1 [66.2 kB]\r\n\r96% [36 libxml-xpath-perl 14.9 kB/66.2 kB 23%]\r                                              \r98% [Waiting for headers]\r                         \rGet:37 http://archive.ubuntu.com/ubuntu jammy/main amd64 libauthen-sasl-perl all 2.1600-1.1 [43.1 kB]\r\n\r98% [37 libauthen-sasl-perl 9634 B/43.1 kB 22%]\r                                               \r100% [Working]\r              \rFetched 2295 kB in 1s (2330 kB/s)\r\ndebconf: delaying package configuration, since apt-utils is not installed\r\nSelecting previously unselected package libclone-perl.\r\n(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 65058 files and directories currently installed.)\r\nPreparing to unpack .../00-libclone-perl_0.45-1build3_amd64.deb ...\r\nUnpacking libclone-perl (0.45-1build3) ...\r\nSelecting previously unselected package libdata-dump-perl.\r\nPreparing to unpack .../01-libdata-dump-perl_1.25-1_all.deb ...\r\nUnpacking libdata-dump-perl (1.25-1) ...\r\nSelecting previously unselected package libencode-locale-perl.\r\nPreparing to unpack .../02-libencode-locale-perl_1.05-1.1_all.deb ...\r\nUnpacking libencode-locale-perl (1.05-1.1) ...\r\nSelecting previously unselected package libtimedate-perl.\r\nPreparing to unpack .../03-libtimedate-perl_2.3300-2_all.deb ...\r\nUnpacking libtimedate-perl (2.3300-2) ...\r\nSelecting previously unselected package libhttp-date-perl.\r\nPreparing to unpack .../04-libhttp-date-perl_6.05-1_all.deb ...\r\nUnpacking libhttp-date-perl (6.05-1) ...\r\nSelecting previously unselected package libfile-listing-perl.\r\nPreparing to unpack .../05-libfile-listing-perl_6.14-1_all.deb ...\r\nUnpacking libfile-listing-perl (6.14-1) ...\r\nSelecting previously unselected package libfont-afm-perl.\r\nPreparing to unpack .../06-libfont-afm-perl_1.20-3_all.deb ...\r\nUnpacking libfont-afm-perl (1.20-3) ...\r\nSelecting previously unselected package libhtml-tagset-perl.\r\nPreparing to unpack .../07-libhtml-tagset-perl_3.20-4_all.deb ...\r\nUnpacking libhtml-tagset-perl (3.20-4) ...\r\nSelecting previously unselected package liburi-perl.\r\nPreparing to unpack .../08-liburi-perl_5.10-1_all.deb ...\r\nUnpacking liburi-perl (5.10-1) ...\r\nSelecting previously unselected package libhtml-parser-perl:amd64.\r\nPreparing to unpack .../09-libhtml-parser-perl_3.76-1build2_amd64.deb ...\r\nUnpacking libhtml-parser-perl:amd64 (3.76-1build2) ...\r\nSelecting previously unselected package libio-html-perl.\r\nPreparing to unpack .../10-libio-html-perl_1.004-2_all.deb ...\r\nUnpacking libio-html-perl (1.004-2) ...\r\nSelecting previously unselected package liblwp-mediatypes-perl.\r\nPreparing to unpack .../11-liblwp-mediatypes-perl_6.04-1_all.deb ...\r\nUnpacking liblwp-mediatypes-perl (6.04-1) ...\r\nSelecting previously unselected package libhttp-message-perl.\r\nPreparing to unpack .../12-libhttp-message-perl_6.36-1_all.deb ...\r\nUnpacking libhttp-message-perl (6.36-1) ...\r\nSelecting previously unselected package libhtml-form-perl.\r\nPreparing to unpack .../13-libhtml-form-perl_6.07-1_all.deb ...\r\nUnpacking libhtml-form-perl (6.07-1) ...\r\nSelecting previously unselected package libhtml-tree-perl.\r\nPreparing to unpack .../14-libhtml-tree-perl_5.07-2_all.deb ...\r\nUnpacking libhtml-tree-perl (5.07-2) ...\r\nSelecting previously unselected package libhtml-format-perl.\r\nPreparing to unpack .../15-libhtml-format-perl_2.12-1.1_all.deb ...\r\nUnpacking libhtml-format-perl (2.12-1.1) ...\r\nSelecting previously unselected package libhttp-cookies-perl.\r\nPreparing to unpack .../16-libhttp-cookies-perl_6.10-1_all.deb ...\r\nUnpacking libhttp-cookies-perl (6.10-1) ...\r\nSelecting previously unselected package libhttp-daemon-perl.\r\nPreparing to unpack .../17-libhttp-daemon-perl_6.13-1ubuntu0.1_all.deb ...\r\nUnpacking libhttp-daemon-perl (6.13-1ubuntu0.1) ...\r\nSelecting previously unselected package libhttp-negotiate-perl.\r\nPreparing to unpack .../18-libhttp-negotiate-perl_6.01-1_all.deb ...\r\nUnpacking libhttp-negotiate-perl (6.01-1) ...\r\nSelecting previously unselected package perl-openssl-defaults:amd64.\r\nPreparing to unpack .../19-perl-openssl-defaults_5build2_amd64.deb ...\r\nUnpacking perl-openssl-defaults:amd64 (5build2) ...\r\nSelecting previously unselected package libnet-ssleay-perl:amd64.\r\nPreparing to unpack .../20-libnet-ssleay-perl_1.92-1build2_amd64.deb ...\r\nUnpacking libnet-ssleay-perl:amd64 (1.92-1build2) ...\r\nSelecting previously unselected package libio-socket-ssl-perl.\r\nPreparing to unpack .../21-libio-socket-ssl-perl_2.074-2_all.deb ...\r\nUnpacking libio-socket-ssl-perl (2.074-2) ...\r\nSelecting previously unselected package libnet-http-perl.\r\nPreparing to unpack .../22-libnet-http-perl_6.22-1_all.deb ...\r\nUnpacking libnet-http-perl (6.22-1) ...\r\nSelecting previously unselected package libtry-tiny-perl.\r\nPreparing to unpack .../23-libtry-tiny-perl_0.31-1_all.deb ...\r\nUnpacking libtry-tiny-perl (0.31-1) ...\r\nSelecting previously unselected package libwww-robotrules-perl.\r\nPreparing to unpack .../24-libwww-robotrules-perl_6.02-1_all.deb ...\r\nUnpacking libwww-robotrules-perl (6.02-1) ...\r\nSelecting previously unselected package libwww-perl.\r\nPreparing to unpack .../25-libwww-perl_6.61-1_all.deb ...\r\nUnpacking libwww-perl (6.61-1) ...\r\nSelecting previously unselected package liblwp-protocol-https-perl.\r\nPreparing to unpack .../26-liblwp-protocol-https-perl_6.10-1_all.deb ...\r\nUnpacking liblwp-protocol-https-perl (6.10-1) ...\r\nSelecting previously unselected package libnet-smtp-ssl-perl.\r\nPreparing to unpack .../27-libnet-smtp-ssl-perl_1.04-1_all.deb ...\r\nUnpacking libnet-smtp-ssl-perl (1.04-1) ...\r\nSelecting previously unselected package libmailtools-perl.\r\nPreparing to unpack .../28-libmailtools-perl_2.21-1_all.deb ...\r\nUnpacking libmailtools-perl (2.21-1) ...\r\nSelecting previously unselected package libxml-namespacesupport-perl.\r\nPreparing to unpack .../29-libxml-namespacesupport-perl_1.12-1.1_all.deb ...\r\nUnpacking libxml-namespacesupport-perl (1.12-1.1) ...\r\nSelecting previously unselected package libxml-sax-base-perl.\r\nPreparing to unpack .../30-libxml-sax-base-perl_1.09-1.1_all.deb ...\r\nUnpacking libxml-sax-base-perl (1.09-1.1) ...\r\nSelecting previously unselected package libxml-sax-perl.\r\nPreparing to unpack .../31-libxml-sax-perl_1.02+dfsg-3_all.deb ...\r\nUnpacking libxml-sax-perl (1.02+dfsg-3) ...\r\nSelecting previously unselected package libxml-libxml-perl.\r\nPreparing to unpack .../32-libxml-libxml-perl_2.0207+dfsg+really+2.0134-1_amd64.deb ...\r\nUnpacking libxml-libxml-perl (2.0207+dfsg+really+2.0134-1) ...\r\nSelecting previously unselected package libxml-parser-perl:amd64.\r\nPreparing to unpack .../33-libxml-parser-perl_2.46-3build1_amd64.deb ...\r\nUnpacking libxml-parser-perl:amd64 (2.46-3build1) ...\r\nSelecting previously unselected package libxml-sax-expat-perl.\r\nPreparing to unpack .../34-libxml-sax-expat-perl_0.51-1_all.deb ...\r\nUnpacking libxml-sax-expat-perl (0.51-1) ...\r\nSelecting previously unselected package libxml-xpath-perl.\r\nPreparing to unpack .../35-libxml-xpath-perl_1.44-1_all.deb ...\r\nUnpacking libxml-xpath-perl (1.44-1) ...\r\nSelecting previously unselected package libauthen-sasl-perl.\r\nPreparing to unpack .../36-libauthen-sasl-perl_2.1600-1.1_all.deb ...\r\nUnpacking libauthen-sasl-perl (2.1600-1.1) ...\r\nSetting up libfont-afm-perl (1.20-3) ...\r\nSetting up libclone-perl (0.45-1build3) ...\r\nSetting up libhtml-tagset-perl (3.20-4) ...\r\nSetting up libauthen-sasl-perl (2.1600-1.1) ...\r\nSetting up liblwp-mediatypes-perl (6.04-1) ...\r\nSetting up libtry-tiny-perl (0.31-1) ...\r\nSetting up perl-openssl-defaults:amd64 (5build2) ...\r\nSetting up libxml-namespacesupport-perl (1.12-1.1) ...\r\nSetting up libencode-locale-perl (1.05-1.1) ...\r\nSetting up libxml-sax-base-perl (1.09-1.1) ...\r\nSetting up libdata-dump-perl (1.25-1) ...\r\nSetting up libio-html-perl (1.004-2) ...\r\nSetting up libtimedate-perl (2.3300-2) ...\r\nSetting up liburi-perl (5.10-1) ...\r\nSetting up libnet-ssleay-perl:amd64 (1.92-1build2) ...\r\nSetting up libhttp-date-perl (6.05-1) ...\r\nSetting up libfile-listing-perl (6.14-1) ...\r\nSetting up libnet-http-perl (6.22-1) ...\r\nSetting up libxml-sax-perl (1.02+dfsg-3) ...\r\nupdate-perl-sax-parsers: Registering Perl SAX parser XML::SAX::PurePerl with priority 10...\r\nupdate-perl-sax-parsers: Updating overall Perl SAX parser modules info file...\r\ndebconf: unable to initialize frontend: Dialog\r\ndebconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\r\ndebconf: falling back to frontend: Readline\r\ndebconf: unable to initialize frontend: Readline\r\ndebconf: (This frontend requires a controlling tty.)\r\ndebconf: falling back to frontend: Teletype\r\n\r\nCreating config file /etc/perl/XML/SAX/ParserDetails.ini with new version\r\nSetting up libxml-libxml-perl (2.0207+dfsg+really+2.0134-1) ...\r\nupdate-perl-sax-parsers: Registering Perl SAX parser XML::LibXML::SAX::Parser with priority 50...\r\nupdate-perl-sax-parsers: Registering Perl SAX parser XML::LibXML::SAX with priority 50...\r\nupdate-perl-sax-parsers: Updating overall Perl SAX parser modules info file...\r\ndebconf: unable to initialize frontend: Dialog\r\ndebconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\r\ndebconf: falling back to frontend: Readline\r\ndebconf: unable to initialize frontend: Readline\r\ndebconf: (This frontend requires a controlling tty.)\r\ndebconf: falling back to frontend: Teletype\r\nReplacing config file /etc/perl/XML/SAX/ParserDetails.ini with new version\r\nSetting up libwww-robotrules-perl (6.02-1) ...\r\nSetting up libhtml-parser-perl:amd64 (3.76-1build2) ...\r\nSetting up libio-socket-ssl-perl (2.074-2) ...\r\nSetting up libhttp-message-perl (6.36-1) ...\r\nSetting up libhtml-form-perl (6.07-1) ...\r\nSetting up libhttp-negotiate-perl (6.01-1) ...\r\nSetting up libhttp-cookies-perl (6.10-1) ...\r\nSetting up libhtml-tree-perl (5.07-2) ...\r\nSetting up libhtml-format-perl (2.12-1.1) ...\r\nSetting up libnet-smtp-ssl-perl (1.04-1) ...\r\nSetting up libmailtools-perl (2.21-1) ...\r\nSetting up libhttp-daemon-perl (6.13-1ubuntu0.1) ...\r\nSetting up liblwp-protocol-https-perl (6.10-1) ...\r\nSetting up libwww-perl (6.61-1) ...\r\nSetting up libxml-parser-perl:amd64 (2.46-3build1) ...\r\nSetting up libxml-xpath-perl (1.44-1) ...\r\nSetting up libxml-sax-expat-perl (0.51-1) ...\r\nupdate-perl-sax-parsers: Registering Perl SAX parser XML::SAX::Expat with priority 50...\r\nupdate-perl-sax-parsers: Updating overall Perl SAX parser modules info file...\r\ndebconf: unable to initialize frontend: Dialog\r\ndebconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\r\ndebconf: falling back to frontend: Readline\r\ndebconf: unable to initialize frontend: Readline\r\ndebconf: (This frontend requires a controlling tty.)\r\ndebconf: falling back to frontend: Teletype\r\nReplacing config file /etc/perl/XML/SAX/ParserDetails.ini with new version\r\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!sudo apt-get install -y libxml-xpath-perl libxml-libxml-perl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c77ea77",
   "metadata": {},
   "source": [
    "### Step 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3488b5",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b2bde4",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd6c8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\nTrue"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9afb6a",
   "metadata": {},
   "source": [
    "### Step 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92e4eef",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9d7482",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e53883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Use the same config as in samsum dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c815f23",
   "metadata": {},
   "source": [
    "### Step 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901f0a18",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc51286",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe8d876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!cp textbox/properties/dataset/samsum.yaml textbox/properties/dataset/xsum.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47db2cfd",
   "metadata": {},
   "source": [
    "### Step 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f066e436",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558fd7ee",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bf8557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Run model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0cfe99",
   "metadata": {},
   "source": [
    "### Step 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17602ac",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcbff03",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5898415e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1072f83f",
   "metadata": {},
   "source": [
    "### Step 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375fd693",
   "metadata": {},
   "source": [
    "Execution Start time: 04:38:53, End time: 04:38:53 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb2c04c",
   "metadata": {},
   "source": [
    "**Thought**: Now that the environment is set up, I need to modify the dataset loading process to ensure that only the first 10 rows of each set are loaded, and that the newline characters are replaced with ` [SEP] `. Let's start by locating the dataset loading code.\n",
    "\n",
    "I'll search for the term \"dataset\" in the repository to find where the dataset is being loaded or configured. This will help us identify where to make the necessary changes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144e1093",
   "metadata": {},
   "source": [
    "**Action (search_dir)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eed9ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "More than 152 files matched for \"dataset\" in /content/TextBox. Please narrow your search.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_dir \"dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebaf481",
   "metadata": {},
   "source": [
    "### Step 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e210684",
   "metadata": {},
   "source": [
    "Execution Start time: 04:38:54, End time: 04:38:54 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6334338",
   "metadata": {},
   "source": [
    "**Thought**: To narrow down the search, I'll look for the term \"load_dataset\" which is likely to be used when loading datasets using libraries like Hugging Face's `datasets` library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb5ec3f",
   "metadata": {},
   "source": [
    "**Action (search_dir)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36542d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No matches found for \"load_dataset\" in /content/TextBox\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_dir \"load_dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fba6dc",
   "metadata": {},
   "source": [
    "### Step 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9367e0",
   "metadata": {},
   "source": [
    "Execution Start time: 04:38:56, End time: 04:38:56 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca1c831",
   "metadata": {},
   "source": [
    "**Thought**: Since \"load_dataset\" didn't yield any results, I'll try searching for \"datasets\" to see if there's a configuration or import related to the datasets library, which might help us locate the dataset loading code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0c7113",
   "metadata": {},
   "source": [
    "**Action (search_dir)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a71640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Found 31 matches for \"datasets\" in /content/TextBox:\n/content/TextBox/Leaderboard/Chinese (1 matches)\n/content/TextBox/Leaderboard/Data-to-text (1 matches)\n/content/TextBox/Leaderboard/Data-to-text (1 matches)\n/content/TextBox/Leaderboard/Data-to-text (2 matches)\n/content/TextBox/Leaderboard/Open-ended (1 matches)\n/content/TextBox/Leaderboard/Open-ended (1 matches)\n/content/TextBox/Leaderboard/Open-ended (1 matches)\n/content/TextBox/Leaderboard/Question (1 matches)\n/content/TextBox/Leaderboard/Question (1 matches)\n/content/TextBox/Leaderboard/Question (1 matches)\n/content/TextBox/Leaderboard/Question (1 matches)\n/content/TextBox/Leaderboard/Question (1 matches)\n/content/TextBox/Leaderboard/Question (1 matches)\n/content/TextBox/Leaderboard/Question (1 matches)\n/content/TextBox/Leaderboard/Story (1 matches)\n/content/TextBox/Leaderboard/Story (1 matches)\n/content/TextBox/Leaderboard/Task-oriented (1 matches)\n/content/TextBox/Leaderboard/Task-oriented (1 matches)\n/content/TextBox/Leaderboard/Task-oriented (2 matches)\n/content/TextBox/Leaderboard/Text (1 matches)\n/content/TextBox/README.md (2 matches)\n/content/TextBox/TODOS.md (2 matches)\n/content/TextBox/asset/dataset.md (2 matches)\n/content/TextBox/instructions/Prompting (1 matches)\n/content/TextBox/run_analysis.py (1 matches)\n/content/TextBox/run_textbox.py (1 matches)\nEnd of matches for \"datasets\" in /content/TextBox\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_dir \"datasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4daeb85",
   "metadata": {},
   "source": [
    "### Step 19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f75999",
   "metadata": {},
   "source": [
    "Execution Start time: 04:38:57, End time: 04:38:58 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72263a2f",
   "metadata": {},
   "source": [
    "**Thought**: The search results show a match in `run_textbox.py`, which is likely the main script for running models. This file might contain the dataset loading logic or reference it. Let's open `run_textbox.py` to examine how datasets are being handled.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a74229f",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703fb86f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/TextBox/run_textbox.py (12 lines total)]\n1:import argparse\n2:from textbox import run_textbox\n3:\n4:if __name__ == '__main__':\n5:    parser = argparse.ArgumentParser()\n6:    parser.add_argument('--model', '-m', type=str, default='BART', help='name of models')\n7:    parser.add_argument('--dataset', '-d', type=str, default='samsum', help='name of datasets')\n8:    parser.add_argument('--config_files', type=str, nargs='*', default=list(), help='config files')\n9:\n10:    args, _ = parser.parse_known_args()\n11:\n12:    run_textbox(model=args.model, dataset=args.dataset, config_file_list=args.config_files, config_dict={})\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open run_textbox.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359e1200",
   "metadata": {},
   "source": [
    "### Step 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd75a94",
   "metadata": {},
   "source": [
    "Execution Start time: 04:38:59, End time: 04:39:00 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee31189",
   "metadata": {},
   "source": [
    "**Thought**: The `run_textbox.py` script is primarily responsible for parsing command-line arguments and calling the `run_textbox` function from the `textbox` module. The actual dataset loading is likely handled within the `textbox` module.\n",
    "\n",
    "To find where the dataset is loaded, I'll search for the `run_textbox` function definition within the `textbox` module. This will help us trace where the dataset loading logic is implemented.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d50f286",
   "metadata": {},
   "source": [
    "**Action (search_dir)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a076238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Found 1 matches for \"def run_textbox\" in /content/TextBox/textbox:\n/content/TextBox/textbox/quick_start/quick_start.py (1 matches)\nEnd of matches for \"def run_textbox\" in /content/TextBox/textbox\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_dir \"def run_textbox\" textbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936dec58",
   "metadata": {},
   "source": [
    "### Step 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e83c9b",
   "metadata": {},
   "source": [
    "Execution Start time: 04:39:01, End time: 04:39:01 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4628e759",
   "metadata": {},
   "source": [
    "**Thought**: The `run_textbox` function is defined in `quick_start.py` within the `textbox/quick_start` directory. Let's open this file to examine how the dataset is being loaded and where we can make the necessary modifications to load only the first 10 rows and replace newline characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e018301b",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc45a119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/TextBox/textbox/quick_start/quick_start.py (21 lines total)]\n1:from typing import Optional\n2:\n3:\n4:def run_textbox(\n5:    model: Optional[str] = None,\n6:    dataset: Optional[str] = None,\n7:    config_file_list: Optional[list] = None,\n8:    config_dict: Optional[dict] = None\n9:):\n10:    r\"\"\" A fast running api, which includes the complete process of\n11:    training and testing a model on a specified dataset\n12:\n13:    Args:\n14:        model (str): model name\n15:        dataset (str): dataset name\n16:        config_file_list (list): config files used to modify experiment parameters\n17:        config_dict (dict): parameters dictionary used to modify experiment parameters\n18:    \"\"\"\n19:    from textbox.quick_start.experiment import Experiment\n20:    experiment = Experiment(model, dataset, config_file_list, config_dict)\n21:    experiment.run()\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open textbox/quick_start/quick_start.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41acb64",
   "metadata": {},
   "source": [
    "### Step 22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3d774c",
   "metadata": {},
   "source": [
    "Execution Start time: 04:39:04, End time: 04:39:04 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a6e700",
   "metadata": {},
   "source": [
    "**Thought**: The `run_textbox` function initializes an `Experiment` object and calls its `run` method. The dataset loading logic is likely within the `Experiment` class. Let's search for the `Experiment` class definition in the `textbox/quick_start/experiment.py` file to locate the dataset loading code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2decd96",
   "metadata": {},
   "source": [
    "**Action (search_dir)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdc5bf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Directory textbox/quick_start/experiment.py not found\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_dir \"class Experiment\" textbox/quick_start/experiment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52acccfa",
   "metadata": {},
   "source": [
    "### Step 23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb04efd",
   "metadata": {},
   "source": [
    "Execution Start time: 04:39:05, End time: 04:39:05 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804d90eb",
   "metadata": {},
   "source": [
    "**Thought**: It seems I made an incorrect assumption about the file path. Let's search for the `Experiment` class within the `textbox/quick_start` directory to find the correct file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dd5460",
   "metadata": {},
   "source": [
    "**Action (search_dir)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d0887a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Found 1 matches for \"class Experiment\" in /content/TextBox/textbox/quick_start:\n/content/TextBox/textbox/quick_start/experiment.py (1 matches)\nEnd of matches for \"class Experiment\" in /content/TextBox/textbox/quick_start\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_dir \"class Experiment\" textbox/quick_start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e902c88",
   "metadata": {},
   "source": [
    "### Step 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaa3c19",
   "metadata": {},
   "source": [
    "Execution Start time: 04:39:06, End time: 04:39:07 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8f6089",
   "metadata": {},
   "source": [
    "**Thought**: The `Experiment` class is defined in `textbox/quick_start/experiment.py`. Let's open this file to examine how the dataset is being loaded and where we can make the necessary modifications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283f7d5a",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93baaae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/TextBox/textbox/quick_start/experiment.py (142 lines total)]\n1:import os\n2:import logging\n3:from copy import copy\n4:from accelerate.logging import get_logger\n5:\n6:from typing import Optional, Tuple, Any, List, Dict\n7:\n8:from accelerate import Accelerator\n9:from torch.utils.data import DataLoader\n10:\n11:from ..config.configurator import Config\n12:from ..data.utils import data_preparation\n13:from ..trainer.trainer import Trainer\n14:from ..utils.dashboard import SummaryTracker\n15:from ..utils.logger import init_logger\n16:from ..utils.utils import get_model, get_tokenizer, get_trainer, init_seed\n17:\n18:ResultType = Dict[str, Any]\n19:\n20:\n21:class Experiment:\n22:    r\"\"\" A fast running api, which includes the complete process of\n23:    training and testing a model on a specified dataset\n24:\n25:    Args:\n26:        model (str): model name\n27:        dataset (str): dataset name\n28:        config_file_list (list): config files used to modify experiment parameters\n29:        config_dict (dict): parameters dictionary used to modify experiment parameters\n30:    \"\"\"\n31:\n32:    def __init__(\n33:        self,\n34:        model: Optional[str] = None,\n35:        dataset: Optional[str] = None,\n36:        config_file_list: Optional[List[str]] = None,\n37:        config_dict: Optional[Dict[str, Any]] = None,\n38:    ):\n39:        self.config = Config(model, dataset, config_file_list, config_dict)\n40:        wandb_setting = 'wandb ' + self.config['wandb']\n41:        os.system(wandb_setting)\n42:        self.__extended_config = None\n43:\n44:        from accelerate import DistributedDataParallelKwargs\n45:        ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=self.config['find_unused_parameters'])\n46:        self.accelerator = Accelerator(\n47:            gradient_accumulation_steps=self.config['accumulation_steps'], kwargs_handlers=[ddp_kwargs]\n48:        )\n49:        self.config.update({\n50:            '_is_local_main_process': self.accelerator.is_local_main_process,\n51:            'device': self.accelerator.device\n52:        })\n53:        self.logger = self.init_logger(self.config)\n54:        self.summary_tracker = SummaryTracker.basicConfig(self.get_config())\n55:        self.train_data, self.valid_data, self.test_data, self.tokenizer = \\\n56:            self._init_data(self.get_config(), self.accelerator)\n57:\n58:    def get_config(self) -> Config:\n59:        config = copy(self.config)\n60:        if self.__extended_config is not None:\n61:            config.update(self.__extended_config)\n62:        return config\n63:\n64:    @staticmethod\n65:    def init_logger(config: Config) -> logging.Logger:\n66:\n67:        # logger initialization\n68:        init_logger(\n69:            filename=config['filename'],\n70:            log_level=config['state'],\n71:            enabled=config['_is_local_main_process'],\n72:            saved_dir=config['saved_dir']\n73:        )\n74:        logger = get_logger(__name__)\n75:        logger.info(config)\n76:\n77:        return logger\n78:\n79:    @staticmethod\n80:    def _init_data(config: Config, accelerator: Accelerator) -> Tuple[DataLoader, DataLoader, DataLoader, Any]:\n81:        tokenizer = get_tokenizer(config)\n82:        train_data, valid_data, test_data = data_preparation(config, tokenizer)\n83:        train_data, valid_data, test_data = accelerator.prepare(train_data, valid_data, test_data)\n84:        return train_data, valid_data, test_data, tokenizer\n85:\n86:    def _on_experiment_start(self, extended_config: Optional[dict]):\n87:        \"\"\"(Re-)initialize configuration. Since for now config and trainer is modifiable, this\n88:        function is needed to ensure they were aligned to initial configuration.\n89:        \"\"\"\n90:        self.__extended_config = extended_config\n91:        config = self.get_config()\n92:        init_seed(config['seed'], config['reproducibility'])\n93:\n94:        self.model = get_model(config['model'])(config, self.tokenizer).to(config['device'])\n95:        self.logger.info(self.model)\n96:        self.trainer: Trainer = get_trainer(config['model'])(config, self.model, self.accelerator)\n97:        self.do_train = config['do_train']\n98:        self.do_valid = config['do_valid']\n99:        self.do_test = config['do_test']\n100:        self.valid_result: Optional[ResultType] = None\n(42 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open textbox/quick_start/experiment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab47404c",
   "metadata": {},
   "source": [
    "### Step 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aeb801",
   "metadata": {},
   "source": [
    "Execution Start time: 04:39:08, End time: 04:39:08 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333de4fe",
   "metadata": {},
   "source": [
    "**Thought**: The `Experiment` class's `_init_data` method is responsible for initializing the dataset. It calls the `data_preparation` function, which likely handles the loading and preparation of the dataset. Let's search for the `data_preparation` function definition to see where and how the dataset is being loaded.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61c7c66",
   "metadata": {},
   "source": [
    "**Action (search_dir)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8917a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Directory textbox/data/utils.py not found\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_dir \"def data_preparation\" textbox/data/utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d471d60d",
   "metadata": {},
   "source": [
    "### Step 26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fb790b",
   "metadata": {},
   "source": [
    "Execution Start time: 04:39:10, End time: 04:39:10 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0057826",
   "metadata": {},
   "source": [
    "**Thought**: It seems I made an incorrect assumption about the file path again. Let's search for the `data_preparation` function within the `textbox` directory to find the correct file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aea75b",
   "metadata": {},
   "source": [
    "**Action (search_dir)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05e9308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Found 1 matches for \"def data_preparation\" in /content/TextBox/textbox:\n/content/TextBox/textbox/data/utils.py (1 matches)\nEnd of matches for \"def data_preparation\" in /content/TextBox/textbox\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_dir \"def data_preparation\" textbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a30ab6",
   "metadata": {},
   "source": [
    "### Step 27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b837e7",
   "metadata": {},
   "source": [
    "Execution Start time: 04:39:11, End time: 04:39:12 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12188c59",
   "metadata": {},
   "source": [
    "**Thought**: The `data_preparation` function is defined in `textbox/data/utils.py`. Let's open this file to examine how the dataset is being loaded and where we can make the necessary modifications to load only the first 10 rows and replace newline characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47541767",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8736747b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/TextBox/textbox/data/utils.py (64 lines total)]\n1:from torch.utils.data import DataLoader\n2:from textbox.data.denoising_dataset import DenoisingCollate\n3:from ..data.unilm_dataset import UnilmCollate\n4:from textbox.data.abstract_dataset import AbstractDataset, AbstractCollate\n5:from accelerate.logging import get_logger\n6:\n7:collate_options = {'disabled': AbstractCollate, 'denoising': DenoisingCollate, 'unilm': UnilmCollate}\n8:\n9:\n10:def data_preparation(config, tokenizer):\n11:    collate_name = config['pretrain_task']\n12:    if config['model_name'] == 'unilm':\n13:        collate_name = 'unilm'\n14:    collate_fn = collate_options.get(collate_name, AbstractCollate)\n15:    logger = get_logger(__name__)\n16:    logger.info(f'Pretrain type: {collate_fn.get_type()}')\n17:\n18:    if config['dataset'] == 'multiwoz':\n19:        assert config['eval_batch_size'] % 3 == 0\n20:\n21:    dataloaders = []\n22:    if config['do_train']:\n23:        train_dataset = AbstractDataset(config, 'train')\n24:        train_dataset.tokenize(tokenizer)\n25:        train_dataloader = DataLoader(\n26:            train_dataset,\n27:            batch_size=config['train_batch_size'],\n28:            shuffle=True,\n29:            pin_memory=True,\n30:            collate_fn=collate_fn(config, tokenizer, 'train')\n31:        )\n32:        dataloaders.append(train_dataloader)\n33:    else:\n34:        dataloaders.append(None)\n35:\n36:    if config['do_valid']:\n37:        valid_dataset = AbstractDataset(config, 'valid')\n38:        valid_dataset.tokenize(tokenizer)\n39:        valid_dataloader = DataLoader(\n40:            valid_dataset,\n41:            batch_size=config['eval_batch_size'],\n42:            shuffle=False,\n43:            pin_memory=True,\n44:            collate_fn=collate_fn(config, tokenizer, 'valid')\n45:        )\n46:        dataloaders.append(valid_dataloader)\n47:    else:\n48:        dataloaders.append(None)\n49:\n50:    if config['do_test']:\n51:        test_dataset = AbstractDataset(config, 'test')\n52:        test_dataset.tokenize(tokenizer)\n53:        test_dataloader = DataLoader(\n54:            test_dataset,\n55:            batch_size=config['eval_batch_size'],\n56:            shuffle=False,\n57:            pin_memory=True,\n58:            collate_fn=collate_fn(config, tokenizer, 'test')\n59:        )\n60:        dataloaders.append(test_dataloader)\n61:    else:\n62:        dataloaders.append(None)\n63:\n64:    return dataloaders\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open textbox/data/utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f69c35d",
   "metadata": {},
   "source": [
    "### Step 28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be89205",
   "metadata": {},
   "source": [
    "Execution Start time: 04:39:14, End time: 04:39:14 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9e46c0",
   "metadata": {},
   "source": [
    "**Thought**: The `data_preparation` function initializes datasets for training, validation, and testing using the `AbstractDataset` class. To modify the dataset loading to only include the first 10 rows, we need to adjust the `AbstractDataset` class. Let's search for the `AbstractDataset` class definition to locate where the dataset is being loaded.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a971a880",
   "metadata": {},
   "source": [
    "**Action (search_dir)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c70790c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Found 1 matches for \"class AbstractDataset\" in /content/TextBox/textbox/data:\n/content/TextBox/textbox/data/abstract_dataset.py (1 matches)\nEnd of matches for \"class AbstractDataset\" in /content/TextBox/textbox/data\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_dir \"class AbstractDataset\" textbox/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099777d8",
   "metadata": {},
   "source": [
    "### Step 29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9b8e82",
   "metadata": {},
   "source": [
    "Execution Start time: 04:39:15, End time: 04:39:15 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b862242",
   "metadata": {},
   "source": [
    "**Thought**: The `AbstractDataset` class is defined in `textbox/data/abstract_dataset.py`. Let's open this file to examine how the dataset is being loaded and where we can make the necessary modifications to load only the first 10 rows and replace newline characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a072617f",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aedb6e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/TextBox/textbox/data/abstract_dataset.py (205 lines total)]\n1:import os\n2:import torch, warnings\n3:import math\n4:from torch.utils.data import DataLoader, Dataset\n5:from textbox import CLM_MODELS\n6:from typing import List\n7:from torch.nn.utils.rnn import pad_sequence\n8:from textbox.data.misc import load_data, _pad_sequence\n9:\n10:MAX_TOKENIZE_NUM = 1000000\n11:\n12:\n13:class AbstractDataset(Dataset):\n14:\n15:    def __init__(self, config, set):\n16:        super().__init__()\n17:        self.config = config\n18:        self.quick_test = config[\"quick_test\"]\n19:        if isinstance(self.quick_test, bool):\n20:            self.quick_test = 32 if self.quick_test else 0\n21:        self.set = set\n22:        source_filename = os.path.join(config[\"data_path\"], f\"{set}.src\")\n23:        target_filename = os.path.join(config[\"data_path\"], f\"{set}.tgt\")\n24:\n25:        self.source_text = load_data(source_filename, max_length=self.quick_test)\n26:        self.pretraining = config['pretrain_task']\n27:        self.is_casual_model = bool(config[\"model_name\"] in CLM_MODELS)\n28:        if self.pretraining is None and self.pretraining != 'disabled':\n29:            self.target_text = load_data(target_filename, max_length=self.quick_test)\n30:        self.source_length = self.config[\"src_len\"]\n31:        self.target_length = self.config[\"tgt_len\"]\n32:        self.paired_text = bool(\n33:            set == \"train\" or (self.set == 'valid' and self.config['metrics_for_best_model'] == ['loss'])\n34:        )\n35:        if self.paired_text and self.pretraining is None:\n36:            assert len(self.source_text) == len(self.target_text)\n37:\n38:    def __len__(self):\n39:        return len(self.source_text)\n40:\n41:    def __getitem__(self, idx):\n42:        sample = {\n43:            \"source_text\": self.source_text[idx],\n44:        }\n45:        if self.pretraining is None:\n46:            sample.update({\"source_ids\": self.source_ids[idx], \"target_text\": self.target_text[idx]})\n47:            if self.paired_text:\n48:                sample.update({\"target_ids\": self.target_ids[idx]})\n49:        return sample\n50:\n51:    def _init_process(self):\n52:        if self.source_length > self.tokenizer.model_max_length:\n53:            warnings.warn(\n54:                f\"The max length of source text {self.source_length} exceeds the max length {self.tokenizer.model_max_length} of {self.config['model']} model, and will be set to {self.tokenizer.model_max_length}.\"\n55:            )\n56:            self.source_length = self.tokenizer.model_max_length\n57:\n58:        if self.target_length > self.tokenizer.model_max_length:\n59:            warnings.warn(\n60:                f\"The max length of target text {self.target_length} exceeds the max length {self.tokenizer.model_max_length} of {self.config['model']} model, and will be set to {self.tokenizer.model_max_length}.\"\n61:            )\n62:            self.target_length = self.tokenizer.model_max_length\n63:        if self.config[\"model_name\"] in [\"unilm\"] + CLM_MODELS:\n64:            if self.source_length + self.target_length > self.tokenizer.model_max_length:\n65:                tgt_len = math.floor(\n66:                    self.target_length / (self.source_length + self.target_length) * self.tokenizer.model_max_length\n67:                )\n68:                src_len = self.tokenizer.model_max_length - tgt_len\n69:                warnings.warn(\n70:                    f\"The max length of the sum of source text {self.source_length} and target text {self.target_length}\"\n71:                    f\" exceeds the max length {self.tokenizer.model_max_length} of {self.config['model']} model, \"\n72:                    f\"and will be set to {src_len} and {tgt_len}, respectively.\"\n73:                )\n74:                self.target_length = tgt_len\n75:                self.source_length = src_len\n76:\n77:        if (self.config[\"efficient_methods\"] and \"prompt-tuning\" in self.config[\"efficient_methods\"]):\n78:            prompt_length = self.config[\"efficient_kwargs\"][\"prompt_length\"]\n79:            if self.config[\"model_name\"] in CLM_MODELS:\n80:                if (self.source_length + self.target_length + prompt_length > self.tokenizer.model_max_length):\n81:                    warnings.warn(\n82:                        f\"The length of source text {self.source_length}, target text {self.target_length} and prompt {prompt_length} exceeds the max length {self.tokenizer.model_max_length} of {self.config['model']} model, and the max length of source sentence will be set to {self.tokenizer.model_max_length - prompt_length - self.target_length}.\"\n83:                    )\n84:                    self.source_length = (self.tokenizer.model_max_length - prompt_length - self.target_length)\n85:            elif self.source_length + prompt_length > self.tokenizer.model_max_length:\n86:                warnings.warn(\n87:                    f\"The length of source text {self.source_length} and prompt {prompt_length} exceeds the max length {self.tokenizer.model_max_length} of {self.config['model']} model, and the max length of source sentence will be set to {self.tokenizer.model_max_length - prompt_length}.\"\n88:                )\n89:                self.source_length = self.tokenizer.model_max_length - prompt_length\n90:\n91:        self.config[\"src_len\"] = self.source_length\n92:        self.config[\"tgt_len\"] = self.target_length\n93:\n94:    def _process_prompt(self):\n95:        prefix = self.config[\"prefix_prompt\"] or \"\"\n96:        suffix = self.config[\"suffix_prompt\"] or \"\"\n97:\n98:        self.prefix_ids = self.tokenizer.encode(prefix, add_special_tokens=False)\n99:        self.suffix_ids = self.tokenizer.encode(suffix, add_special_tokens=False)\n100:\n(105 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open textbox/data/abstract_dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e266d59c",
   "metadata": {},
   "source": [
    "### Step 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3624740f",
   "metadata": {},
   "source": [
    "Execution Start time: 04:39:19, End time: 04:39:19 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13b44f7",
   "metadata": {},
   "source": [
    "**Thought**: The `AbstractDataset` class uses the `load_data` function to load the dataset. The `max_length` parameter in `load_data` is set to `self.quick_test`, which is determined by the `quick_test` configuration. We can modify this configuration to ensure only the first 10 rows are loaded.\n",
    "\n",
    "Additionally, we need to replace newline characters with ` [SEP] `. This can be done by modifying the `load_data` function or processing the data after loading. Let's first locate the `load_data` function to see where we can make these changes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3a4fcb",
   "metadata": {},
   "source": [
    "**Action (search_dir)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453a74d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Directory textbox/data/misc.py not found\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_dir \"def load_data\" textbox/data/misc.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576832f7",
   "metadata": {},
   "source": [
    "### Step 31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b18be01",
   "metadata": {},
   "source": [
    "Execution Start time: 04:39:20, End time: 04:39:20 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0adcf3",
   "metadata": {},
   "source": [
    "**Thought**: It seems I made an incorrect assumption about the file path again. Let's search for the `load_data` function within the `textbox/data` directory to find the correct file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5491a36b",
   "metadata": {},
   "source": [
    "**Action (search_dir)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e891e61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Found 1 matches for \"def load_data\" in /content/TextBox/textbox/data:\n/content/TextBox/textbox/data/misc.py (1 matches)\nEnd of matches for \"def load_data\" in /content/TextBox/textbox/data\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_dir \"def load_data\" textbox/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746517f1",
   "metadata": {},
   "source": [
    "### Step 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d12c12",
   "metadata": {},
   "source": [
    "Execution Start time: 04:39:22, End time: 04:39:22 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c12ee2",
   "metadata": {},
   "source": [
    "**Thought**: The `load_data` function is defined in `textbox/data/misc.py`. Let's open this file to examine how the dataset is being loaded and where we can make the necessary modifications to load only the first 10 rows and replace newline characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9902a0",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c25d2b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/TextBox/textbox/data/misc.py (85 lines total)]\n1:from typing import List, Optional\n2:import itertools, os\n3:import torch\n4:from torch.nn.utils.rnn import pad_sequence\n5:\n6:\n7:def load_data(dataset_path: str, max_length: int = 0):\n8:    \"\"\"Load dataset from split (train, valid, test).\n9:    This is designed for single sentence format.\n10:\n11:    Args:\n12:        dataset_path (str): path of dataset dir.\n13:        max_length: (default = 0) The amount of line about to load. If 0, quick_test will be disabled.\n14:\n15:    Returns:\n16:        List[List[str]]: the text list loaded from dataset path.\n17:    \"\"\"\n18:    if not os.path.isfile(dataset_path):\n19:        raise ValueError('File {} not exist'.format(os.path.abspath(dataset_path)))\n20:\n21:    text = []\n22:    with open(dataset_path, \"r\") as fin:\n23:        if max_length:\n24:            fin = itertools.islice(fin, max_length)\n25:        for line in fin:\n26:            l = line.strip()\n27:            if len(l) >= 2 and ((l[0] == '\"' and l[-1] == '\"') or (l[0] == \"'\" and l[-1] == \"'\") or\n28:                                (l[0] == '[' and l[-1] == ']')):\n29:                try:\n30:                    l = eval(l)\n31:                    if not isinstance(l, list):\n32:                        l = str(l)\n33:                except:\n34:                    pass\n35:            text.append(l)\n36:    return text\n37:\n38:\n39:def _pad_sequence(tensors: List[torch.Tensor], padding_value: int, padding_side: str = 'right'):\n40:    \"\"\"\n41:    Pad encoded inputs (on left/right and up to max length in the batch)\n42:    \"\"\"\n43:    max_len = max(tensor.size(0) for tensor in tensors)\n44:    padded_tensors = []\n45:    if padding_side == 'right':\n46:        return pad_sequence(tensors, batch_first=True, padding_value=padding_value)\n47:    elif padding_side == 'left':\n48:        for tensor in tensors:\n49:            padding_length = max_len - len(tensor)\n50:            padded_tensor = torch.cat([torch.full([padding_length], padding_value, dtype=tensor.dtype), tensor], dim=-1)\n51:            padded_tensors.append(padded_tensor)\n52:    padded_tensors = torch.stack(padded_tensors, dim=0)\n53:    return padded_tensors\n54:\n55:\n56:def _collate_batch(samples, tokenizer, pad_to_multiple_of: Optional[int] = None):\n57:    \"\"\"Collate `samples` into a batch, using the information in `tokenizer` for padding if necessary.\"\"\"\n58:    # Tensorize if necessary.\n59:    if isinstance(samples[0], (list, tuple)):\n60:        samples = [torch.tensor(e, dtype=torch.long) for e in samples]\n61:\n62:    # Check if padding is necessary.\n63:    length_of_first = samples[0].size(0)\n64:    are_tensors_same_length = all(x.size(0) == length_of_first for x in samples)\n65:    if are_tensors_same_length and (pad_to_multiple_of is None or length_of_first % pad_to_multiple_of == 0):\n66:        return torch.stack(samples, dim=0)\n67:\n68:    # If yes, check if we have a `pad_token`.\n69:    if tokenizer._pad_token is None:\n70:        raise ValueError(\n71:            \"You are attempting to pad samples but the tokenizer you are using\"\n72:            f\" ({tokenizer.__class__.__name__}) does not have a pad token.\"\n73:        )\n74:\n75:    # Creating the full tensor and filling it with our data.\n76:    max_length = max(x.size(0) for x in samples)\n77:    if pad_to_multiple_of is not None and (max_length % pad_to_multiple_of != 0):\n78:        max_length = ((max_length // pad_to_multiple_of) + 1) * pad_to_multiple_of\n79:    result = samples[0].new_full([len(samples), max_length], tokenizer.pad_token_id)\n80:    for i, example in enumerate(samples):\n81:        if tokenizer.padding_side == \"right\":\n82:            result[i, :example.shape[0]] = example\n83:        else:\n84:            result[i, -example.shape[0]:] = example\n85:    return result\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open textbox/data/misc.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e79e346",
   "metadata": {},
   "source": [
    "### Step 33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c8848b",
   "metadata": {},
   "source": [
    "Execution Start time: 04:39:26, End time: 04:39:27 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a21b88",
   "metadata": {},
   "source": [
    "**Thought**: The `load_data` function in `textbox/data/misc.py` already has a mechanism to limit the number of lines loaded using the `max_length` parameter. We can utilize this to load only the first 10 rows by setting `max_length` to 10.\n",
    "\n",
    "Additionally, we need to replace newline characters with ` [SEP] `. We can modify the loop that reads lines to perform this replacement.\n",
    "\n",
    "Let's edit the `load_data` function to implement these changes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ee5c95",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02e2484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/TextBox/textbox/data/misc.py (85 lines total)]\n1:from typing import List, Optional\n2:import itertools, os\n3:import torch\n4:from torch.nn.utils.rnn import pad_sequence\n5:\n6:\n7:def load_data(dataset_path: str, max_length: int = 0):\n8:    \"\"\"Load dataset from split (train, valid, test).\n9:    This is designed for single sentence format.\n10:\n11:    Args:\n12:        dataset_path (str): path of dataset dir.\n13:        max_length: (default = 0) The amount of line about to load. If 0, quick_test will be disabled.\n14:\n15:    Returns:\n16:        List[List[str]]: the text list loaded from dataset path.\n17:    \"\"\"\n18:    if not os.path.isfile(dataset_path):\n19:        raise ValueError('File {} not exist'.format(os.path.abspath(dataset_path)))\n20:\n21:    text = []\n22:    with open(dataset_path, \"r\") as fin:\n23:        if max_length:\n24:            fin = itertools.islice(fin, max_length)\n25:        for line in fin:\n26:            l = line.strip().replace(\"\\n\", \" [SEP] \")\n27:            if len(l) >= 2 and ((l[0] == '\"' and l[-1] == '\"') or (l[0] == \"'\" and l[-1] == \"'\") or\n28:                                (l[0] == '[' and l[-1] == ']')):\n29:                try:\n30:                    l = eval(l)\n31:                    if not isinstance(l, list):\n32:                        l = str(l)\n33:                except:\n34:                    pass\n35:            text.append(l)\n36:    return text\n37:\n38:\n39:def _pad_sequence(tensors: List[torch.Tensor], padding_value: int, padding_side: str = 'right'):\n40:    \"\"\"\n41:    Pad encoded inputs (on left/right and up to max length in the batch)\n42:    \"\"\"\n43:    max_len = max(tensor.size(0) for tensor in tensors)\n44:    padded_tensors = []\n45:    if padding_side == 'right':\n46:        return pad_sequence(tensors, batch_first=True, padding_value=padding_value)\n47:    elif padding_side == 'left':\n48:        for tensor in tensors:\n49:            padding_length = max_len - len(tensor)\n50:            padded_tensor = torch.cat([torch.full([padding_length], padding_value, dtype=tensor.dtype), tensor], dim=-1)\n51:            padded_tensors.append(padded_tensor)\n52:    padded_tensors = torch.stack(padded_tensors, dim=0)\n53:    return padded_tensors\n54:\n55:\n56:def _collate_batch(samples, tokenizer, pad_to_multiple_of: Optional[int] = None):\n57:    \"\"\"Collate `samples` into a batch, using the information in `tokenizer` for padding if necessary.\"\"\"\n58:    # Tensorize if necessary.\n59:    if isinstance(samples[0], (list, tuple)):\n60:        samples = [torch.tensor(e, dtype=torch.long) for e in samples]\n61:\n62:    # Check if padding is necessary.\n63:    length_of_first = samples[0].size(0)\n64:    are_tensors_same_length = all(x.size(0) == length_of_first for x in samples)\n65:    if are_tensors_same_length and (pad_to_multiple_of is None or length_of_first % pad_to_multiple_of == 0):\n66:        return torch.stack(samples, dim=0)\n67:\n68:    # If yes, check if we have a `pad_token`.\n69:    if tokenizer._pad_token is None:\n70:        raise ValueError(\n71:            \"You are attempting to pad samples but the tokenizer you are using\"\n72:            f\" ({tokenizer.__class__.__name__}) does not have a pad token.\"\n73:        )\n74:\n75:    # Creating the full tensor and filling it with our data.\n76:    max_length = max(x.size(0) for x in samples)\n77:    if pad_to_multiple_of is not None and (max_length % pad_to_multiple_of != 0):\n78:        max_length = ((max_length // pad_to_multiple_of) + 1) * pad_to_multiple_of\n79:    result = samples[0].new_full([len(samples), max_length], tokenizer.pad_token_id)\n80:    for i, example in enumerate(samples):\n81:        if tokenizer.padding_side == \"right\":\n82:            result[i, :example.shape[0]] = example\n83:        else:\n84:            result[i, -example.shape[0]:] = example\n85:    return result\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 22:35\n",
    "    with open(dataset_path, \"r\") as fin:\n",
    "        if max_length:\n",
    "            fin = itertools.islice(fin, max_length)\n",
    "        for line in fin:\n",
    "            l = line.strip().replace(\"\\n\", \" [SEP] \")\n",
    "            if len(l) >= 2 and ((l[0] == '\"' and l[-1] == '\"') or (l[0] == \"'\" and l[-1] == \"'\") or\n",
    "                                (l[0] == '[' and l[-1] == ']')):\n",
    "                try:\n",
    "                    l = eval(l)\n",
    "                    if not isinstance(l, list):\n",
    "                        l = str(l)\n",
    "                except:\n",
    "                    pass\n",
    "            text.append(l)\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef7f298",
   "metadata": {},
   "source": [
    "### Step 34"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95bd4f0",
   "metadata": {},
   "source": [
    "Execution Start time: 04:39:28, End time: 04:39:37 (duration: 8 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8968d384",
   "metadata": {},
   "source": [
    "**Thought**: The changes to the `load_data` function have been made successfully. Now, let's proceed to run the model with the updated dataset loading logic. We'll execute the command provided in the task to train and evaluate `bart-base` on the `xsum` dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9850b6c5",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1af560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "W&B online. Running your script from this directory will now sync to the cloud.\n22 Aug 04:39    INFO 66 parameters found.\n================================================================================\n\n# General Hyper Parameters: \n\ngpu_id: 0\nuse_gpu: True\ndevice: cpu\nseed: 2020\nreproducibility: True\ncmd: run_textbox.py --model=BART --dataset=xsum --model_path=facebook/bart-base --epochs=1\nfilename: BART-xsum-2024-Aug-22_04-39-34\nsaved_dir: saved/\nstate: INFO\nwandb: online\n\n\n# Training Hyper Parameters: \n\ndo_train: True\ndo_valid: True\noptimizer: adamw\nadafactor_kwargs: {'lr': 0.001, 'scale_parameter': False, 'relative_step': False, 'warmup_init': False}\noptimizer_kwargs: {}\nvalid_steps: 1\nvalid_strategy: epoch\nstopping_steps: 2\nepochs: 1\nlearning_rate: 3e-05\ntrain_batch_size: 4\ngrad_clip: 0.1\naccumulation_steps: 48\ndisable_tqdm: False\nresume_training: True\n\n\n# Evaluation Hyper Parameters: \n\ndo_test: True\nlower_evaluation: True\nmultiref_strategy: max\nbleu_max_ngrams: 4\nbleu_type: nltk\nsmoothing_function: 0\ncorpus_bleu: False\nrouge_max_ngrams: 2\nrouge_type: files2rouge\nmeteor_type: pycocoevalcap\nchrf_type: m-popovic\ndistinct_max_ngrams: 4\ninter_distinct: True\nunique_max_ngrams: 4\nself_bleu_max_ngrams: 4\ntgt_lang: en\nmetrics: ['rouge']\neval_batch_size: 16\ncorpus_meteor: True\n\n\n# Model Hyper Parameters: \n\nmodel: BART\nmodel_name: bart\nmodel_path: facebook/bart-base\nconfig_kwargs: {}\ntokenizer_kwargs: {'use_fast': True}\ngeneration_kwargs: {'num_beams': 5, 'no_repeat_ngram_size': 3, 'early_stopping': True}\nefficient_kwargs: {}\nefficient_methods: []\nefficient_unfreeze_model: False\nlabel_smoothing: 0.1\n\n\n# Dataset Hyper Parameters: \n\ndataset: xsum\ndata_path: dataset/xsum\ntgt_lang: en\nsrc_len: 1024\ntgt_len: 128\ntruncate: tail\nprefix_prompt: Summarize: \nmetrics_for_best_model: ['rouge-1', 'rouge-2', 'rouge-l']\n\n\n# Unrecognized Hyper Parameters: \n\nfind_unused_parameters: False\ntokenizer_add_tokens: []\nload_type: from_pretrained\n\n================================================================================\n/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\nconfig.json:   0%|                                  | 0.00/1.72k [00:00<?, ?B/s]\n\n\nvocab.json:   0%|                                    | 0.00/899k [00:00<?, ?B/s]\n\n\nmerges.txt:   0%|                                    | 0.00/456k [00:00<?, ?B/s]\n\n\ntokenizer.json:   0%|                               | 0.00/1.36M [00:00<?, ?B/s]\n\n22 Aug 04:39    INFO Pretrain type: pretrain disabled\n╭───────────────────── Traceback (most recent call last) ──────────────────────╮\n│ /content/TextBox/run_textbox.py:12 in <module>                               │\n│                                                                              │\n│    9 │                                                                       │\n│   10 │   args, _ = parser.parse_known_args()                                 │\n│   11 │                                                                       │\n│ ❱ 12 │   run_textbox(model=args.model, dataset=args.dataset, config_file_lis │\n│   13                                                                         │\n│                                                                              │\n│ /content/TextBox/textbox/quick_start/quick_start.py:20 in run_textbox        │\n│                                                                              │\n│   17 │   │   config_dict (dict): parameters dictionary used to modify experi │\n│   18 │   \"\"\"                                                                 │\n│   19 │   from textbox.quick_start.experiment import Experiment               │\n│ ❱ 20 │   experiment = Experiment(model, dataset, config_file_list, config_di │\n│   21 │   experiment.run()                                                    │\n│   22                                                                         │\n│                                                                              │\n│ /content/TextBox/textbox/quick_start/experiment.py:56 in __init__            │\n│                                                                              │\n│    53 │   │   self.logger = self.init_logger(self.config)                    │\n│    54 │   │   self.summary_tracker = SummaryTracker.basicConfig(self.get_con │\n│    55 │   │   self.train_data, self.valid_data, self.test_data, self.tokeniz │\n│ ❱  56 │   │   │   self._init_data(self.get_config(), self.accelerator)       │\n│    57 │                                                                      │\n│    58 │   def get_config(self) -> Config:                                    │\n│    59 │   │   config = copy(self.config)                                     │\n│                                                                              │\n│ /content/TextBox/textbox/quick_start/experiment.py:82 in _init_data          │\n│                                                                              │\n│    79 │   @staticmethod                                                      │\n│    80 │   def _init_data(config: Config, accelerator: Accelerator) -> Tuple[ │\n│    81 │   │   tokenizer = get_tokenizer(config)                              │\n│ ❱  82 │   │   train_data, valid_data, test_data = data_preparation(config, t │\n│    83 │   │   train_data, valid_data, test_data = accelerator.prepare(train_ │\n│    84 │   │   return train_data, valid_data, test_data, tokenizer            │\n│    85                                                                        │\n│                                                                              │\n│ /content/TextBox/textbox/data/utils.py:23 in data_preparation                │\n│                                                                              │\n│   20 │                                                                       │\n│   21 │   dataloaders = []                                                    │\n│   22 │   if config['do_train']:                                              │\n│ ❱ 23 │   │   train_dataset = AbstractDataset(config, 'train')                │\n│   24 │   │   train_dataset.tokenize(tokenizer)                               │\n│   25 │   │   train_dataloader = DataLoader(                                  │\n│   26 │   │   │   train_dataset,                                              │\n│                                                                              │\n│ /content/TextBox/textbox/data/abstract_dataset.py:25 in __init__             │\n│                                                                              │\n│    22 │   │   source_filename = os.path.join(config[\"data_path\"], f\"{set}.sr │\n│    23 │   │   target_filename = os.path.join(config[\"data_path\"], f\"{set}.tg │\n│    24 │   │                                                                  │\n│ ❱  25 │   │   self.source_text = load_data(source_filename, max_length=self. │\n│    26 │   │   self.pretraining = config['pretrain_task']                     │\n│    27 │   │   self.is_casual_model = bool(config[\"model_name\"] in CLM_MODELS │\n│    28 │   │   if self.pretraining is None and self.pretraining != 'disabled' │\n│                                                                              │\n│ /content/TextBox/textbox/data/misc.py:19 in load_data                        │\n│                                                                              │\n│   16 │   │   List[List[str]]: the text list loaded from dataset path.        │\n│   17 │   \"\"\"                                                                 │\n│   18 │   if not os.path.isfile(dataset_path):                                │\n│ ❱ 19 │   │   raise ValueError('File {} not exist'.format(os.path.abspath(dat │\n│   20 │                                                                       │\n│   21 │   text = []                                                           │\n│   22 │   with open(dataset_path, \"r\") as fin:                                │\n╰──────────────────────────────────────────────────────────────────────────────╯\nValueError: File /content/TextBox/dataset/xsum/train.src not exist\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python run_textbox.py --model=BART --dataset=xsum --model_path=facebook/bart-base --epochs=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e476e548",
   "metadata": {},
   "source": [
    "### Step 35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6a9cba",
   "metadata": {},
   "source": [
    "Execution Start time: 04:39:40, End time: 04:39:40 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96980cd9",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the file `/content/TextBox/dataset/xsum/train.src` does not exist. This suggests that the dataset files are missing or not correctly placed. We need to ensure that the dataset is downloaded and placed in the correct directory.\n",
    "\n",
    "Let's first check the `dataset/xsum` directory to see what files are present and verify if the dataset is correctly downloaded.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16f3b1b",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5743474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ls: cannot access 'dataset/xsum': No such file or directory\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ls dataset/xsum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9d3911",
   "metadata": {},
   "source": [
    "### Step 36"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca66736c",
   "metadata": {},
   "source": [
    "Execution Start time: 04:39:43, End time: 04:39:43 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd411e0",
   "metadata": {},
   "source": [
    "**Thought**: The `dataset/xsum` directory does not exist, which means the dataset hasn't been downloaded or placed correctly. We need to download the XSum dataset and place it in the appropriate directory.\n",
    "\n",
    "Let's download the XSum dataset using the `datasets` library and save it in the `dataset/xsum` directory. We'll create the directory and download the dataset using a Python script.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e639e0c",
   "metadata": {},
   "source": [
    "**Action (create)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747e570c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "File 'download_xsum.py' created.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create download_xsum.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982d2a14",
   "metadata": {},
   "source": [
    "### Step 37"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3828b98e",
   "metadata": {},
   "source": [
    "Execution Start time: 04:39:46, End time: 04:39:46 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce30a556",
   "metadata": {},
   "source": [
    "**Thought**: Let's write a script in `download_xsum.py` to download the XSum dataset using the `datasets` library and save the first 10 rows of each set in the `dataset/xsum` directory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bf63d4",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61af309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Usage: edit <start_line>:<end_line>\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit download_xsum.py\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Create the dataset/xsum directory if it doesn't exist\n",
    "os.makedirs('dataset/xsum', exist_ok=True)\n",
    "\n",
    "# Load the XSum dataset\n",
    "dataset = load_dataset('xsum')\n",
    "\n",
    "# Save the first 10 rows of each set\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    with open(f'dataset/xsum/{split}.src', 'w') as src_file, open(f'dataset/xsum/{split}.tgt', 'w') as tgt_file:\n",
    "        for example in dataset[split].select(range(10)):\n",
    "            src_file.write(example['document'].replace('\\n', ' [SEP] ') + '\\n')\n",
    "            tgt_file.write(example['summary'].replace('\\n', ' [SEP] ') + '\\n')\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3d61d6",
   "metadata": {},
   "source": [
    "### Step 38"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a1803b",
   "metadata": {},
   "source": [
    "Execution Start time: 04:39:49, End time: 04:39:50 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8cd4a9",
   "metadata": {},
   "source": [
    "**Thought**: It seems I forgot to specify the line numbers for the edit command. Let's try again by specifying the correct line numbers to write the script in `download_xsum.py`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752748b7",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9454b7d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\n\nERRORS:\n- E999 SyntaxError: unterminated triple-quoted string literal (detected at line 85)\n\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /content/TextBox/textbox/data/misc.py (85 lines total)]\n1:import os\n2:from datasets import load_dataset\n3:\n4:# Create the dataset/xsum directory if it doesn't exist\n5:os.makedirs('dataset/xsum', exist_ok=True)\n6:\n7:# Load the XSum dataset\n8:dataset = load_dataset('xsum')\n9:\n10:# Save the first 10 rows of each set\n11:for split in ['train', 'validation', 'test']:\n12:    with open(f'dataset/xsum/{split}.src', 'w') as src_file, open(f'dataset/xsum/{split}.tgt', 'w') as tgt_file:\n13:        for example in dataset[split].select(range(10)):\n14:            src_file.write(example['document'].replace('\\n', ' [SEP] ') + '\\n')\n15:            tgt_file.write(example['summary'].replace('\\n', ' [SEP] ') + '\\n')\n16:        List[List[str]]: the text list loaded from dataset path.\n17:    \"\"\"\n18:    if not os.path.isfile(dataset_path):\n19:        raise ValueError('File {} not exist'.format(os.path.abspath(dataset_path)))\n20:\n21:    text = []\n22:    with open(dataset_path, \"r\") as fin:\n23:        if max_length:\n24:            fin = itertools.islice(fin, max_length)\n(61 more lines below)\n-------------------------------------------------\n\nThis is the original code before your edit\n-------------------------------------------------\n[File: /content/TextBox/textbox/data/misc.py (85 lines total)]\n1:from typing import List, Optional\n2:import itertools, os\n3:import torch\n4:from torch.nn.utils.rnn import pad_sequence\n5:\n6:\n7:def load_data(dataset_path: str, max_length: int = 0):\n8:    \"\"\"Load dataset from split (train, valid, test).\n9:    This is designed for single sentence format.\n10:\n11:    Args:\n12:        dataset_path (str): path of dataset dir.\n13:        max_length: (default = 0) The amount of line about to load. If 0, quick_test will be disabled.\n14:\n15:    Returns:\n16:        List[List[str]]: the text list loaded from dataset path.\n17:    \"\"\"\n18:    if not os.path.isfile(dataset_path):\n19:        raise ValueError('File {} not exist'.format(os.path.abspath(dataset_path)))\n20:\n21:    text = []\n22:    with open(dataset_path, \"r\") as fin:\n23:        if max_length:\n24:            fin = itertools.islice(fin, max_length)\n(61 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 1:15\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Create the dataset/xsum directory if it doesn't exist\n",
    "os.makedirs('dataset/xsum', exist_ok=True)\n",
    "\n",
    "# Load the XSum dataset\n",
    "dataset = load_dataset('xsum')\n",
    "\n",
    "# Save the first 10 rows of each set\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    with open(f'dataset/xsum/{split}.src', 'w') as src_file, open(f'dataset/xsum/{split}.tgt', 'w') as tgt_file:\n",
    "        for example in dataset[split].select(range(10)):\n",
    "            src_file.write(example['document'].replace('\\n', ' [SEP] ') + '\\n')\n",
    "            tgt_file.write(example['summary'].replace('\\n', ' [SEP] ') + '\\n')\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88b9324",
   "metadata": {},
   "source": [
    "### Step 39"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d773bc9",
   "metadata": {},
   "source": [
    "Execution Start time: 04:39:52, End time: 04:39:52 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa12531e",
   "metadata": {},
   "source": [
    "**Thought**: It seems I mistakenly attempted to edit the wrong file. Let's create and edit the `download_xsum.py` file correctly this time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397bfd48",
   "metadata": {},
   "source": [
    "**Action (create)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd856fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Error: File 'download_xsum.py' already exists.\n[File: /content/TextBox/download_xsum.py (1 lines total)]\n1:\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create download_xsum.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60299f6e",
   "metadata": {},
   "source": [
    "### Step 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90541263",
   "metadata": {},
   "source": [
    "Execution Start time: 04:39:56, End time: 04:39:57 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dd5e93",
   "metadata": {},
   "source": [
    "**Thought**: The `download_xsum.py` file already exists and is currently open. Let's proceed to edit this file to include the script for downloading and processing the XSum dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aed472",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b67393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/TextBox/download_xsum.py (15 lines total)]\n1:import os\n2:from datasets import load_dataset\n3:\n4:# Create the dataset/xsum directory if it doesn't exist\n5:os.makedirs('dataset/xsum', exist_ok=True)\n6:\n7:# Load the XSum dataset\n8:dataset = load_dataset('xsum')\n9:\n10:# Save the first 10 rows of each set\n11:for split in ['train', 'validation', 'test']:\n12:    with open(f'dataset/xsum/{split}.src', 'w') as src_file, open(f'dataset/xsum/{split}.tgt', 'w') as tgt_file:\n13:        for example in dataset[split].select(range(10)):\n14:            src_file.write(example['document'].replace('\\n', ' [SEP] ') + '\\n')\n15:            tgt_file.write(example['summary'].replace('\\n', ' [SEP] ') + '\\n')\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 1:15\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Create the dataset/xsum directory if it doesn't exist\n",
    "os.makedirs('dataset/xsum', exist_ok=True)\n",
    "\n",
    "# Load the XSum dataset\n",
    "dataset = load_dataset('xsum')\n",
    "\n",
    "# Save the first 10 rows of each set\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    with open(f'dataset/xsum/{split}.src', 'w') as src_file, open(f'dataset/xsum/{split}.tgt', 'w') as tgt_file:\n",
    "        for example in dataset[split].select(range(10)):\n",
    "            src_file.write(example['document'].replace('\\n', ' [SEP] ') + '\\n')\n",
    "            tgt_file.write(example['summary'].replace('\\n', ' [SEP] ') + '\\n')\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5a181c",
   "metadata": {},
   "source": [
    "### Step 41"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25622979",
   "metadata": {},
   "source": [
    "Execution Start time: 04:39:58, End time: 04:41:18 (duration: 79 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e549ab50",
   "metadata": {},
   "source": [
    "**Thought**: The script for downloading and processing the XSum dataset has been successfully added to `download_xsum.py`. Now, let's execute this script to download the dataset and prepare it for training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde49663",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af256bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/usr/local/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for xsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/xsum\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\n\nDownloading builder script:   0%|                   | 0.00/5.76k [00:00<?, ?B/s]\n\n\nDownloading readme:   0%|                           | 0.00/6.24k [00:00<?, ?B/s]\n\n\nDownloading data:   0%|                              | 0.00/255M [00:00<?, ?B/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDownloading data:   0%|                             | 0.00/1.00M [00:00<?, ?B/s]\n\n\nGenerating train split:   0%|                 | 0/204045 [00:00<?, ? examples/s]\nGenerating train split:   0%|     | 240/204045 [00:00<01:35, 2135.66 examples/s]\nGenerating train split:   0%|     | 887/204045 [00:00<00:44, 4551.29 examples/s]\nGenerating train split:   1%|    | 1425/204045 [00:00<00:41, 4912.27 examples/s]\nGenerating train split:   1%|    | 2020/204045 [00:00<00:38, 5311.74 examples/s]\nGenerating train split:   1%|    | 2684/204045 [00:00<00:34, 5782.09 examples/s]\nGenerating train split:   2%|    | 3324/204045 [00:00<00:33, 5922.33 examples/s]\nGenerating train split:   2%|    | 3988/204045 [00:00<00:32, 6150.38 examples/s]\nGenerating train split:   2%|    | 4897/204045 [00:00<00:32, 6105.82 examples/s]\nGenerating train split:   3%|    | 5788/204045 [00:01<00:32, 6040.31 examples/s]\nGenerating train split:   3%|▏   | 6408/204045 [00:01<00:32, 6079.81 examples/s]\nGenerating train split:   3%|▏   | 7030/204045 [00:01<00:32, 6115.70 examples/s]\nGenerating train split:   4%|▏   | 7660/204045 [00:01<00:31, 6166.18 examples/s]\nGenerating train split:   4%|▏   | 8520/204045 [00:01<00:32, 6001.31 examples/s]\nGenerating train split:   4%|▏   | 9151/204045 [00:01<00:32, 6080.96 examples/s]\nGenerating train split:   5%|▏   | 9812/204045 [00:01<00:31, 6224.01 examples/s]\nGenerating train split:   5%|▏  | 10440/204045 [00:01<00:31, 6232.40 examples/s]\nGenerating train split:   6%|▏  | 11376/204045 [00:01<00:30, 6226.59 examples/s]\nGenerating train split:   6%|▏  | 12323/204045 [00:02<00:30, 6231.51 examples/s]\nGenerating train split:   6%|▏  | 12980/204045 [00:02<00:30, 6313.21 examples/s]\nGenerating train split:   7%|▏  | 13897/204045 [00:02<00:30, 6239.93 examples/s]\nGenerating train split:   7%|▏  | 14837/204045 [00:02<00:30, 6247.28 examples/s]\nGenerating train split:   8%|▏  | 15785/204045 [00:02<00:30, 6268.19 examples/s]\nGenerating train split:   8%|▏  | 16680/204045 [00:02<00:30, 6171.10 examples/s]\nGenerating train split:   8%|▎  | 17323/204045 [00:02<00:30, 6169.51 examples/s]\nGenerating train split:   9%|▎  | 17983/204045 [00:02<00:29, 6271.67 examples/s]\nGenerating train split:   9%|▎  | 18614/204045 [00:03<00:29, 6279.58 examples/s]\nGenerating train split:  10%|▎  | 19560/204045 [00:03<00:29, 6285.97 examples/s]\nGenerating train split:  10%|▎  | 20519/204045 [00:03<00:29, 6316.10 examples/s]\nGenerating train split:  11%|▎  | 21450/204045 [00:03<00:29, 6276.74 examples/s]\nGenerating train split:  11%|▎  | 22373/204045 [00:03<00:29, 6236.02 examples/s]\nGenerating train split:  11%|▎  | 23000/204045 [00:03<00:29, 6146.84 examples/s]\nGenerating train split:  12%|▎  | 23659/204045 [00:03<00:28, 6253.01 examples/s]\nGenerating train split:  12%|▎  | 24604/204045 [00:04<00:28, 6262.68 examples/s]\nGenerating train split:  13%|▍  | 25554/204045 [00:04<00:28, 6280.92 examples/s]\nGenerating train split:  13%|▍  | 26479/204045 [00:04<00:28, 6239.02 examples/s]\nGenerating train split:  13%|▍  | 27427/204045 [00:04<00:28, 6257.75 examples/s]\nGenerating train split:  14%|▍  | 28356/204045 [00:04<00:28, 6231.60 examples/s]\nGenerating train split:  14%|▍  | 29000/204045 [00:04<00:27, 6259.00 examples/s]\nGenerating train split:  15%|▍  | 29663/204045 [00:04<00:27, 6344.35 examples/s]\nGenerating train split:  15%|▍  | 30315/204045 [00:04<00:27, 6205.93 examples/s]\nGenerating train split:  15%|▍  | 30963/204045 [00:05<00:27, 6266.71 examples/s]\nGenerating train split:  16%|▍  | 31886/204045 [00:05<00:27, 6222.45 examples/s]\nGenerating train split:  16%|▍  | 32827/204045 [00:05<00:27, 6236.92 examples/s]\nGenerating train split:  17%|▍  | 33778/204045 [00:05<00:27, 6264.82 examples/s]\nGenerating train split:  17%|▌  | 34739/204045 [00:05<00:26, 6300.58 examples/s]\nGenerating train split:  17%|▌  | 35695/204045 [00:05<00:26, 6319.03 examples/s]\nGenerating train split:  18%|▌  | 36639/204045 [00:05<00:26, 6242.27 examples/s]\nGenerating train split:  18%|▌  | 37264/204045 [00:06<00:26, 6242.85 examples/s]\nGenerating train split:  19%|▌  | 37920/204045 [00:06<00:26, 6318.16 examples/s]\nGenerating train split:  19%|▌  | 38872/204045 [00:06<00:26, 6319.52 examples/s]\nGenerating train split:  20%|▌  | 39826/204045 [00:06<00:25, 6330.23 examples/s]\nGenerating train split:  20%|▌  | 40773/204045 [00:06<00:25, 6320.20 examples/s]\nGenerating train split:  20%|▌  | 41727/204045 [00:06<00:25, 6331.15 examples/s]\nGenerating train split:  21%|▌  | 42363/204045 [00:06<00:25, 6335.55 examples/s]\nGenerating train split:  21%|▋  | 43000/204045 [00:06<00:25, 6287.25 examples/s]\nGenerating train split:  21%|▋  | 43683/204045 [00:07<00:24, 6422.45 examples/s]\nGenerating train split:  22%|▋  | 44327/204045 [00:07<00:25, 6388.64 examples/s]\nGenerating train split:  22%|▋  | 45000/204045 [00:07<00:24, 6431.61 examples/s]\nGenerating train split:  22%|▋  | 45665/204045 [00:07<00:24, 6491.13 examples/s]\nGenerating train split:  23%|▋  | 46614/204045 [00:07<00:24, 6417.90 examples/s]\nGenerating train split:  23%|▋  | 47588/204045 [00:07<00:24, 6440.13 examples/s]\nGenerating train split:  24%|▋  | 48574/204045 [00:07<00:23, 6481.77 examples/s]\nGenerating train split:  24%|▋  | 49567/204045 [00:07<00:23, 6521.05 examples/s]\nGenerating train split:  25%|▋  | 50548/204045 [00:08<00:23, 6524.17 examples/s]\nGenerating train split:  25%|▊  | 51520/204045 [00:08<00:23, 6505.44 examples/s]\nGenerating train split:  26%|▊  | 52453/204045 [00:08<00:23, 6414.27 examples/s]\nGenerating train split:  26%|▊  | 53400/204045 [00:08<00:23, 6377.44 examples/s]\nGenerating train split:  27%|▊  | 54339/204045 [00:08<00:23, 6338.46 examples/s]\nGenerating train split:  27%|▊  | 55000/204045 [00:08<00:23, 6332.87 examples/s]\nGenerating train split:  27%|▊  | 55660/204045 [00:08<00:23, 6393.65 examples/s]\nGenerating train split:  28%|▊  | 56613/204045 [00:09<00:23, 6372.23 examples/s]\nGenerating train split:  28%|▊  | 57356/204045 [00:09<00:25, 5698.28 examples/s]\nGenerating train split:  28%|▊  | 58000/204045 [00:09<00:25, 5801.76 examples/s]\nGenerating train split:  29%|▊  | 58650/204045 [00:09<00:24, 5973.04 examples/s]\nGenerating train split:  29%|▊  | 59261/204045 [00:09<00:24, 6004.93 examples/s]\nGenerating train split:  29%|▉  | 59921/204045 [00:09<00:23, 6161.13 examples/s]\nGenerating train split:  30%|▉  | 60854/204045 [00:09<00:23, 6181.75 examples/s]\nGenerating train split:  30%|▉  | 61807/204045 [00:09<00:22, 6232.07 examples/s]\nGenerating train split:  31%|▉  | 62763/204045 [00:10<00:22, 6274.63 examples/s]\nGenerating train split:  31%|▉  | 63396/204045 [00:10<00:22, 6286.34 examples/s]\nGenerating train split:  32%|▉  | 64361/204045 [00:10<00:22, 6332.28 examples/s]\nGenerating train split:  32%|▉  | 65000/204045 [00:10<00:22, 6255.25 examples/s]\nGenerating train split:  32%|▉  | 65670/204045 [00:10<00:21, 6367.64 examples/s]\nGenerating train split:  33%|▉  | 66329/204045 [00:10<00:21, 6425.74 examples/s]\nGenerating train split:  33%|▉  | 67000/204045 [00:10<00:21, 6458.88 examples/s]\nGenerating train split:  33%|▉  | 67688/204045 [00:10<00:20, 6570.72 examples/s]\nGenerating train split:  34%|█  | 68687/204045 [00:11<00:20, 6567.28 examples/s]\nGenerating train split:  34%|█  | 69682/204045 [00:11<00:20, 6581.09 examples/s]\nGenerating train split:  35%|█  | 70668/204045 [00:11<00:20, 6561.71 examples/s]\nGenerating train split:  35%|█  | 71662/204045 [00:11<00:20, 6515.03 examples/s]\nGenerating train split:  35%|█  | 72339/204045 [00:11<00:20, 6468.92 examples/s]\nGenerating train split:  36%|█  | 73000/204045 [00:11<00:20, 6449.46 examples/s]\nGenerating train split:  36%|█  | 73685/204045 [00:11<00:19, 6550.16 examples/s]\nGenerating train split:  37%|█  | 74657/204045 [00:11<00:20, 6426.41 examples/s]\nGenerating train split:  37%|█  | 75326/204045 [00:12<00:20, 6401.39 examples/s]\nGenerating train split:  37%|█  | 75989/204045 [00:12<00:19, 6454.09 examples/s]\nGenerating train split:  38%|█▏ | 76906/204045 [00:12<00:20, 6328.55 examples/s]\nGenerating train split:  38%|█▏ | 77868/204045 [00:12<00:19, 6354.20 examples/s]\nGenerating train split:  39%|█▏ | 78802/204045 [00:12<00:19, 6307.30 examples/s]\nGenerating train split:  39%|█▏ | 79747/204045 [00:12<00:19, 6301.02 examples/s]\nGenerating train split:  40%|█▏ | 80698/204045 [00:12<00:19, 6310.62 examples/s]\nGenerating train split:  40%|█▏ | 81663/204045 [00:13<00:19, 6319.20 examples/s]\nGenerating train split:  40%|█▏ | 82586/204045 [00:13<00:19, 6262.46 examples/s]\nGenerating train split:  41%|█▏ | 83222/204045 [00:13<00:19, 6283.80 examples/s]\nGenerating train split:  41%|█▏ | 83881/204045 [00:13<00:18, 6355.66 examples/s]\nGenerating train split:  42%|█▏ | 84813/204045 [00:13<00:18, 6302.13 examples/s]\nGenerating train split:  42%|█▎ | 85769/204045 [00:13<00:18, 6324.06 examples/s]\nGenerating train split:  42%|█▎ | 86709/204045 [00:13<00:18, 6302.79 examples/s]\nGenerating train split:  43%|█▎ | 87667/204045 [00:14<00:18, 6291.42 examples/s]\nGenerating train split:  43%|█▎ | 88321/204045 [00:14<00:18, 6209.55 examples/s]\nGenerating train split:  44%|█▎ | 88988/204045 [00:14<00:18, 6318.54 examples/s]\nGenerating train split:  44%|█▎ | 89913/204045 [00:14<00:18, 6259.87 examples/s]\nGenerating train split:  45%|█▎ | 90861/204045 [00:14<00:18, 6272.42 examples/s]\nGenerating train split:  45%|█▎ | 91497/204045 [00:14<00:17, 6290.10 examples/s]\nGenerating train split:  45%|█▎ | 92129/204045 [00:14<00:17, 6296.64 examples/s]\nGenerating train split:  45%|█▎ | 92784/204045 [00:14<00:17, 6361.46 examples/s]\nGenerating train split:  46%|█▍ | 93722/204045 [00:14<00:17, 6315.27 examples/s]\nGenerating train split:  46%|█▍ | 94689/204045 [00:15<00:17, 6355.79 examples/s]\nGenerating train split:  47%|█▍ | 95660/204045 [00:15<00:17, 6342.26 examples/s]\nGenerating train split:  47%|█▍ | 96592/204045 [00:15<00:17, 6297.79 examples/s]\nGenerating train split:  48%|█▍ | 97527/204045 [00:15<00:16, 6275.36 examples/s]\nGenerating train split:  48%|█▍ | 98456/204045 [00:15<00:16, 6247.23 examples/s]\nGenerating train split:  49%|█▍ | 99374/204045 [00:15<00:16, 6207.67 examples/s]\nGenerating train split:  49%|▉ | 100001/204045 [00:15<00:16, 6216.52 examples/s]\nGenerating train split:  49%|▉ | 100671/204045 [00:16<00:16, 6332.53 examples/s]\nGenerating train split:  50%|▉ | 101333/204045 [00:16<00:16, 6351.94 examples/s]\nGenerating train split:  50%|▉ | 101980/204045 [00:16<00:16, 6376.03 examples/s]\nGenerating train split:  50%|█ | 102935/204045 [00:16<00:15, 6365.94 examples/s]\nGenerating train split:  51%|█ | 103903/204045 [00:16<00:15, 6393.03 examples/s]\nGenerating train split:  51%|█ | 104849/204045 [00:16<00:15, 6363.07 examples/s]\nGenerating train split:  52%|█ | 105766/204045 [00:16<00:15, 6279.88 examples/s]\nGenerating train split:  52%|█ | 106723/204045 [00:17<00:15, 6305.28 examples/s]\nGenerating train split:  53%|█ | 107677/204045 [00:17<00:15, 6319.81 examples/s]\nGenerating train split:  53%|█ | 108336/204045 [00:17<00:15, 6296.12 examples/s]\nGenerating train split:  53%|█ | 109000/204045 [00:17<00:14, 6354.77 examples/s]\nGenerating train split:  54%|█ | 109664/204045 [00:17<00:14, 6422.78 examples/s]\nGenerating train split:  54%|█ | 110324/204045 [00:17<00:14, 6402.36 examples/s]\nGenerating train split:  54%|█ | 111000/204045 [00:17<00:14, 6486.60 examples/s]\nGenerating train split:  55%|█ | 111669/204045 [00:17<00:14, 6538.66 examples/s]\nGenerating train split:  55%|█ | 112340/204045 [00:17<00:14, 6514.93 examples/s]\nGenerating train split:  55%|█ | 113000/204045 [00:17<00:14, 6460.53 examples/s]\nGenerating train split:  56%|█ | 113968/204045 [00:18<00:13, 6450.46 examples/s]\nGenerating train split:  56%|█▏| 114908/204045 [00:18<00:13, 6379.57 examples/s]\nGenerating train split:  57%|█▏| 115834/204045 [00:18<00:13, 6303.40 examples/s]\nGenerating train split:  57%|█▏| 116780/204045 [00:18<00:13, 6268.12 examples/s]\nGenerating train split:  58%|█▏| 117409/204045 [00:18<00:13, 6267.11 examples/s]\nGenerating train split:  58%|█▏| 118041/204045 [00:18<00:13, 6275.67 examples/s]\nGenerating train split:  58%|█▏| 118694/204045 [00:18<00:13, 6331.62 examples/s]\nGenerating train split:  59%|█▏| 119662/204045 [00:19<00:13, 6345.94 examples/s]\nGenerating train split:  59%|█▏| 120604/204045 [00:19<00:13, 6321.43 examples/s]\nGenerating train split:  60%|█▏| 121561/204045 [00:19<00:13, 6336.78 examples/s]\nGenerating train split:  60%|█▏| 122506/204045 [00:19<00:12, 6317.85 examples/s]\nGenerating train split:  61%|█▏| 123458/204045 [00:19<00:12, 6318.82 examples/s]\nGenerating train split:  61%|█▏| 124385/204045 [00:19<00:12, 6268.56 examples/s]\nGenerating train split:  61%|█▏| 125329/204045 [00:19<00:12, 6220.13 examples/s]\nGenerating train split:  62%|█▏| 126000/204045 [00:20<00:12, 6221.87 examples/s]\nGenerating train split:  62%|█▏| 126660/204045 [00:20<00:12, 6309.46 examples/s]\nGenerating train split:  63%|█▎| 127603/204045 [00:20<00:12, 6298.55 examples/s]\nGenerating train split:  63%|█▎| 128236/204045 [00:20<00:12, 6304.22 examples/s]\nGenerating train split:  63%|█▎| 128902/204045 [00:20<00:11, 6384.50 examples/s]\nGenerating train split:  63%|█▎| 129548/204045 [00:20<00:11, 6403.02 examples/s]\nGenerating train split:  64%|█▎| 130197/204045 [00:20<00:11, 6395.44 examples/s]\nGenerating train split:  64%|█▎| 130890/204045 [00:20<00:11, 6544.45 examples/s]\nGenerating train split:  65%|█▎| 131882/204045 [00:20<00:10, 6566.69 examples/s]\nGenerating train split:  65%|█▎| 132865/204045 [00:21<00:10, 6555.48 examples/s]\nGenerating train split:  66%|█▎| 133836/204045 [00:21<00:10, 6519.73 examples/s]\nGenerating train split:  66%|█▎| 134832/204045 [00:21<00:10, 6553.91 examples/s]\nGenerating train split:  67%|█▎| 135803/204045 [00:21<00:10, 6525.74 examples/s]\nGenerating train split:  67%|█▎| 136753/204045 [00:21<00:10, 6459.21 examples/s]\nGenerating train split:  67%|█▎| 137678/204045 [00:21<00:10, 6364.73 examples/s]\nGenerating train split:  68%|█▎| 138327/204045 [00:21<00:10, 6384.86 examples/s]\nGenerating train split:  68%|█▎| 139000/204045 [00:22<00:10, 6399.21 examples/s]\nGenerating train split:  68%|█▎| 139679/204045 [00:22<00:09, 6498.16 examples/s]\nGenerating train split:  69%|█▍| 140337/204045 [00:22<00:09, 6500.48 examples/s]\nGenerating train split:  69%|█▍| 141000/204045 [00:22<00:09, 6481.80 examples/s]\nGenerating train split:  69%|█▍| 141655/204045 [00:22<00:09, 6493.78 examples/s]\nGenerating train split:  70%|█▍| 142645/204045 [00:22<00:09, 6527.51 examples/s]\nGenerating train split:  70%|█▍| 143299/204045 [00:22<00:09, 6529.05 examples/s]\nGenerating train split:  71%|█▍| 143974/204045 [00:22<00:09, 6582.68 examples/s]\nGenerating train split:  71%|█▍| 144893/204045 [00:22<00:09, 6411.04 examples/s]\nGenerating train split:  71%|█▍| 145851/204045 [00:23<00:09, 6400.45 examples/s]\nGenerating train split:  72%|█▍| 146803/204045 [00:23<00:08, 6380.08 examples/s]\nGenerating train split:  72%|█▍| 147772/204045 [00:23<00:08, 6402.44 examples/s]\nGenerating train split:  73%|█▍| 148694/204045 [00:23<00:08, 6319.26 examples/s]\nGenerating train split:  73%|█▍| 149653/204045 [00:23<00:08, 6318.20 examples/s]\nGenerating train split:  74%|█▍| 150618/204045 [00:23<00:08, 6349.63 examples/s]\nGenerating train split:  74%|█▍| 151273/204045 [00:23<00:08, 6392.16 examples/s]\nGenerating train split:  74%|█▍| 151945/204045 [00:24<00:08, 6470.68 examples/s]\nGenerating train split:  75%|█▍| 152597/204045 [00:24<00:07, 6480.77 examples/s]\nGenerating train split:  75%|█▌| 153556/204045 [00:24<00:07, 6435.99 examples/s]\nGenerating train split:  76%|█▌| 154493/204045 [00:24<00:07, 6369.01 examples/s]\nGenerating train split:  76%|█▌| 155469/204045 [00:24<00:07, 6408.58 examples/s]\nGenerating train split:  77%|█▌| 156446/204045 [00:24<00:07, 6439.02 examples/s]\nGenerating train split:  77%|█▌| 157385/204045 [00:24<00:07, 6380.23 examples/s]\nGenerating train split:  77%|█▌| 158033/204045 [00:25<00:07, 6401.18 examples/s]\nGenerating train split:  78%|█▌| 158703/204045 [00:25<00:07, 6470.21 examples/s]\nGenerating train split:  78%|█▌| 159670/204045 [00:25<00:06, 6459.24 examples/s]\nGenerating train split:  79%|█▌| 160333/204045 [00:25<00:06, 6440.24 examples/s]\nGenerating train split:  79%|█▌| 161000/204045 [00:25<00:06, 6387.85 examples/s]\nGenerating train split:  79%|█▌| 161682/204045 [00:25<00:06, 6496.79 examples/s]\nGenerating train split:  80%|█▌| 162336/204045 [00:25<00:06, 6485.96 examples/s]\nGenerating train split:  80%|█▌| 163000/204045 [00:25<00:06, 6452.32 examples/s]\nGenerating train split:  80%|█▌| 163653/204045 [00:25<00:06, 6471.22 examples/s]\nGenerating train split:  81%|█▌| 164634/204045 [00:26<00:06, 6494.93 examples/s]\nGenerating train split:  81%|█▌| 165624/204045 [00:26<00:05, 6530.02 examples/s]\nGenerating train split:  82%|█▋| 166595/204045 [00:26<00:05, 6505.40 examples/s]\nGenerating train split:  82%|█▋| 167546/204045 [00:26<00:05, 6443.08 examples/s]\nGenerating train split:  83%|█▋| 168488/204045 [00:26<00:05, 6389.43 examples/s]\nGenerating train split:  83%|█▋| 169130/204045 [00:26<00:05, 6393.57 examples/s]\nGenerating train split:  83%|█▋| 169812/204045 [00:26<00:05, 6494.96 examples/s]\nGenerating train split:  84%|█▋| 170799/204045 [00:27<00:05, 6513.38 examples/s]\nGenerating train split:  84%|█▋| 171771/204045 [00:27<00:04, 6492.35 examples/s]\nGenerating train split:  85%|█▋| 172751/204045 [00:27<00:04, 6500.37 examples/s]\nGenerating train split:  85%|█▋| 173728/204045 [00:27<00:04, 6498.62 examples/s]\nGenerating train split:  86%|█▋| 174674/204045 [00:27<00:04, 6430.70 examples/s]\nGenerating train split:  86%|█▋| 175349/204045 [00:27<00:04, 6455.43 examples/s]\nGenerating train split:  86%|█▋| 176000/204045 [00:27<00:04, 6300.82 examples/s]\nGenerating train split:  87%|█▋| 176674/204045 [00:27<00:04, 6410.74 examples/s]\nGenerating train split:  87%|█▋| 177620/204045 [00:28<00:04, 6372.37 examples/s]\nGenerating train split:  87%|█▋| 178260/204045 [00:28<00:04, 6378.03 examples/s]\nGenerating train split:  88%|█▊| 178917/204045 [00:28<00:03, 6426.00 examples/s]\nGenerating train split:  88%|█▊| 179891/204045 [00:28<00:03, 6443.71 examples/s]\nGenerating train split:  89%|█▊| 180834/204045 [00:28<00:03, 6386.55 examples/s]\nGenerating train split:  89%|█▊| 181784/204045 [00:28<00:03, 6362.21 examples/s]\nGenerating train split:  90%|█▊| 182728/204045 [00:28<00:03, 6338.65 examples/s]\nGenerating train split:  90%|█▊| 183658/204045 [00:29<00:03, 6290.98 examples/s]\nGenerating train split:  90%|█▊| 184602/204045 [00:29<00:03, 6286.07 examples/s]\nGenerating train split:  91%|█▊| 185540/204045 [00:29<00:02, 6264.39 examples/s]\nGenerating train split:  91%|█▊| 186476/204045 [00:29<00:02, 6255.05 examples/s]\nGenerating train split:  92%|█▊| 187414/204045 [00:29<00:02, 6253.40 examples/s]\nGenerating train split:  92%|█▊| 188045/204045 [00:29<00:02, 6260.13 examples/s]\nGenerating train split:  92%|█▊| 188724/204045 [00:29<00:02, 6387.03 examples/s]\nGenerating train split:  93%|█▊| 189715/204045 [00:29<00:02, 6460.49 examples/s]\nGenerating train split:  93%|█▊| 190691/204045 [00:30<00:02, 6474.55 examples/s]\nGenerating train split:  94%|█▉| 191679/204045 [00:30<00:01, 6464.99 examples/s]\nGenerating train split:  94%|█▉| 192327/204045 [00:30<00:01, 6424.52 examples/s]\nGenerating train split:  95%|█▉| 193000/204045 [00:30<00:01, 6392.34 examples/s]\nGenerating train split:  95%|█▉| 193682/204045 [00:30<00:01, 6498.69 examples/s]\nGenerating train split:  95%|█▉| 194347/204045 [00:30<00:01, 6533.65 examples/s]\nGenerating train split:  96%|█▉| 195334/204045 [00:30<00:01, 6480.26 examples/s]\nGenerating train split:  96%|█▉| 196000/204045 [00:30<00:01, 6484.07 examples/s]\nGenerating train split:  96%|█▉| 196667/204045 [00:31<00:01, 6531.73 examples/s]\nGenerating train split:  97%|█▉| 197637/204045 [00:31<00:00, 6504.55 examples/s]\nGenerating train split:  97%|█▉| 198613/204045 [00:31<00:00, 6501.70 examples/s]\nGenerating train split:  98%|█▉| 199588/204045 [00:31<00:00, 6497.92 examples/s]\nGenerating train split:  98%|█▉| 200496/204045 [00:31<00:00, 6354.00 examples/s]\nGenerating train split:  99%|█▉| 201449/204045 [00:31<00:00, 6350.33 examples/s]\nGenerating train split:  99%|█▉| 202407/204045 [00:31<00:00, 6353.83 examples/s]\nGenerating train split: 100%|█▉| 203361/204045 [00:32<00:00, 6353.18 examples/s]\nGenerating train split: 100%|█▉| 204002/204045 [00:32<00:00, 6359.54 examples/s]\nGenerating train split: 100%|██| 204045/204045 [00:32<00:00, 6334.79 examples/s]\n\nGenerating validation split:   0%|             | 0/11332 [00:00<?, ? examples/s]\nGenerating validation split:   0%|   | 34/11332 [00:00<00:33, 332.62 examples/s]\nGenerating validation split:   1%|   | 88/11332 [00:00<00:25, 445.68 examples/s]\nGenerating validation split:   1%|  | 148/11332 [00:00<00:21, 513.89 examples/s]\nGenerating validation split:   2%|  | 229/11332 [00:00<00:21, 521.05 examples/s]\nGenerating validation split:   3%|  | 291/11332 [00:00<00:20, 547.19 examples/s]\nGenerating validation split:   3%|  | 368/11332 [00:00<00:20, 526.26 examples/s]\nGenerating validation split:   4%|  | 428/11332 [00:00<00:20, 542.05 examples/s]\nGenerating validation split:   4%|  | 487/11332 [00:00<00:19, 543.49 examples/s]\nGenerating validation split:   5%|  | 543/11332 [00:01<00:19, 545.06 examples/s]\nGenerating validation split:   5%|  | 622/11332 [00:01<00:20, 535.25 examples/s]\nGenerating validation split:   6%|  | 681/11332 [00:01<00:19, 545.25 examples/s]\nGenerating validation split:   7%|▏ | 769/11332 [00:01<00:18, 558.92 examples/s]\nGenerating validation split:   7%|▏ | 836/11332 [00:01<00:18, 567.15 examples/s]\nGenerating validation split:   8%|▏ | 909/11332 [00:01<00:17, 608.29 examples/s]\nGenerating validation split:   9%|▏ | 971/11332 [00:01<00:16, 609.98 examples/s]\nGenerating validation split:   9%| | 1050/11332 [00:01<00:18, 559.28 examples/s]\nGenerating validation split:  10%| | 1111/11332 [00:02<00:17, 571.34 examples/s]\nGenerating validation split:  11%| | 1198/11332 [00:02<00:17, 563.80 examples/s]\nGenerating validation split:  11%| | 1271/11332 [00:02<00:19, 528.48 examples/s]\nGenerating validation split:  12%| | 1350/11332 [00:02<00:19, 519.26 examples/s]\nGenerating validation split:  13%|▏| 1426/11332 [00:02<00:19, 509.46 examples/s]\nGenerating validation split:  13%|▏| 1486/11332 [00:02<00:18, 527.27 examples/s]\nGenerating validation split:  14%|▏| 1568/11332 [00:02<00:18, 528.15 examples/s]\nGenerating validation split:  14%|▏| 1625/11332 [00:03<00:18, 533.00 examples/s]\nGenerating validation split:  15%|▏| 1690/11332 [00:03<00:19, 495.51 examples/s]\nGenerating validation split:  16%|▏| 1762/11332 [00:03<00:17, 544.85 examples/s]\nGenerating validation split:  16%|▏| 1849/11332 [00:03<00:17, 552.24 examples/s]\nGenerating validation split:  17%|▏| 1938/11332 [00:03<00:16, 561.97 examples/s]\nGenerating validation split:  18%|▏| 2000/11332 [00:03<00:16, 567.05 examples/s]\nGenerating validation split:  18%|▏| 2074/11332 [00:03<00:15, 607.57 examples/s]\nGenerating validation split:  19%|▏| 2145/11332 [00:03<00:16, 552.63 examples/s]\nGenerating validation split:  19%|▏| 2206/11332 [00:04<00:16, 563.56 examples/s]\nGenerating validation split:  20%|▏| 2300/11332 [00:04<00:15, 566.32 examples/s]\nGenerating validation split:  21%|▏| 2364/11332 [00:04<00:15, 583.33 examples/s]\nGenerating validation split:  22%|▏| 2449/11332 [00:04<00:15, 563.82 examples/s]\nGenerating validation split:  22%|▏| 2516/11332 [00:04<00:15, 576.71 examples/s]\nGenerating validation split:  23%|▏| 2585/11332 [00:04<00:14, 600.45 examples/s]\nGenerating validation split:  24%|▏| 2678/11332 [00:04<00:14, 605.20 examples/s]\nGenerating validation split:  24%|▏| 2756/11332 [00:04<00:15, 569.61 examples/s]\nGenerating validation split:  25%|▏| 2823/11332 [00:05<00:16, 527.05 examples/s]\nGenerating validation split:  25%|▎| 2886/11332 [00:05<00:15, 543.59 examples/s]\nGenerating validation split:  26%|▎| 2966/11332 [00:05<00:15, 532.85 examples/s]\nGenerating validation split:  27%|▎| 3034/11332 [00:05<00:16, 503.74 examples/s]\nGenerating validation split:  27%|▎| 3103/11332 [00:05<00:15, 538.82 examples/s]\nGenerating validation split:  28%|▎| 3186/11332 [00:05<00:15, 533.11 examples/s]\nGenerating validation split:  29%|▎| 3270/11332 [00:05<00:14, 537.99 examples/s]\nGenerating validation split:  29%|▎| 3328/11332 [00:06<00:14, 542.18 examples/s]\nGenerating validation split:  30%|▎| 3391/11332 [00:06<00:14, 560.19 examples/s]\nGenerating validation split:  31%|▎| 3475/11332 [00:06<00:14, 556.45 examples/s]\nGenerating validation split:  31%|▎| 3543/11332 [00:06<00:13, 581.64 examples/s]\nGenerating validation split:  32%|▎| 3623/11332 [00:06<00:13, 561.81 examples/s]\nGenerating validation split:  33%|▎| 3699/11332 [00:06<00:14, 539.25 examples/s]\nGenerating validation split:  33%|▎| 3791/11332 [00:06<00:13, 543.59 examples/s]\nGenerating validation split:  34%|▎| 3849/11332 [00:07<00:13, 548.87 examples/s]\nGenerating validation split:  34%|▎| 3909/11332 [00:07<00:13, 548.91 examples/s]\nGenerating validation split:  35%|▎| 4000/11332 [00:07<00:13, 549.05 examples/s]\nGenerating validation split:  36%|▎| 4075/11332 [00:07<00:13, 523.73 examples/s]\nGenerating validation split:  37%|▎| 4142/11332 [00:07<00:14, 498.88 examples/s]\nGenerating validation split:  37%|▎| 4203/11332 [00:07<00:13, 519.74 examples/s]\nGenerating validation split:  38%|▍| 4261/11332 [00:07<00:13, 530.92 examples/s]\nGenerating validation split:  38%|▍| 4315/11332 [00:07<00:13, 528.86 examples/s]\nGenerating validation split:  39%|▍| 4376/11332 [00:08<00:12, 543.31 examples/s]\nGenerating validation split:  39%|▍| 4458/11332 [00:08<00:12, 540.24 examples/s]\nGenerating validation split:  40%|▍| 4516/11332 [00:08<00:12, 546.54 examples/s]\nGenerating validation split:  40%|▍| 4572/11332 [00:08<00:12, 548.00 examples/s]\nGenerating validation split:  41%|▍| 4633/11332 [00:08<00:11, 564.64 examples/s]\nGenerating validation split:  41%|▍| 4695/11332 [00:08<00:11, 571.88 examples/s]\nGenerating validation split:  42%|▍| 4756/11332 [00:08<00:11, 580.06 examples/s]\nGenerating validation split:  43%|▍| 4835/11332 [00:08<00:10, 634.44 examples/s]\nGenerating validation split:  43%|▍| 4925/11332 [00:08<00:10, 617.04 examples/s]\nGenerating validation split:  44%|▍| 5011/11332 [00:09<00:10, 599.31 examples/s]\nGenerating validation split:  45%|▍| 5086/11332 [00:09<00:11, 555.91 examples/s]\nGenerating validation split:  46%|▍| 5176/11332 [00:09<00:10, 564.22 examples/s]\nGenerating validation split:  46%|▍| 5249/11332 [00:09<00:11, 535.33 examples/s]\nGenerating validation split:  47%|▍| 5311/11332 [00:09<00:10, 553.80 examples/s]\nGenerating validation split:  48%|▍| 5387/11332 [00:09<00:09, 601.87 examples/s]\nGenerating validation split:  48%|▍| 5455/11332 [00:09<00:09, 614.05 examples/s]\nGenerating validation split:  49%|▍| 5535/11332 [00:10<00:10, 576.43 examples/s]\nGenerating validation split:  50%|▍| 5612/11332 [00:10<00:10, 546.91 examples/s]\nGenerating validation split:  50%|▌| 5696/11332 [00:10<00:10, 545.61 examples/s]\nGenerating validation split:  51%|▌| 5770/11332 [00:10<00:10, 527.59 examples/s]\nGenerating validation split:  51%|▌| 5835/11332 [00:10<00:09, 553.89 examples/s]\nGenerating validation split:  52%|▌| 5907/11332 [00:10<00:09, 585.93 examples/s]\nGenerating validation split:  53%|▌| 5986/11332 [00:10<00:09, 564.18 examples/s]\nGenerating validation split:  53%|▌| 6062/11332 [00:10<00:09, 542.26 examples/s]\nGenerating validation split:  54%|▌| 6118/11332 [00:11<00:09, 544.78 examples/s]\nGenerating validation split:  55%|▌| 6197/11332 [00:11<00:09, 531.85 examples/s]\nGenerating validation split:  55%|▌| 6253/11332 [00:11<00:09, 535.50 examples/s]\nGenerating validation split:  56%|▌| 6313/11332 [00:11<00:09, 549.76 examples/s]\nGenerating validation split:  56%|▌| 6398/11332 [00:11<00:08, 549.33 examples/s]\nGenerating validation split:  57%|▌| 6484/11332 [00:11<00:08, 544.87 examples/s]\nGenerating validation split:  58%|▌| 6540/11332 [00:11<00:08, 545.13 examples/s]\nGenerating validation split:  58%|▌| 6603/11332 [00:11<00:08, 564.25 examples/s]\nGenerating validation split:  59%|▌| 6665/11332 [00:12<00:08, 571.35 examples/s]\nGenerating validation split:  59%|▌| 6731/11332 [00:12<00:07, 587.25 examples/s]\nGenerating validation split:  60%|▌| 6821/11332 [00:12<00:07, 583.82 examples/s]\nGenerating validation split:  61%|▌| 6901/11332 [00:12<00:07, 559.82 examples/s]\nGenerating validation split:  61%|▌| 6959/11332 [00:12<00:07, 562.36 examples/s]\nGenerating validation split:  62%|▌| 7021/11332 [00:12<00:08, 508.72 examples/s]\nGenerating validation split:  62%|▌| 7077/11332 [00:12<00:08, 518.87 examples/s]\nGenerating validation split:  63%|▋| 7135/11332 [00:12<00:07, 528.47 examples/s]\nGenerating validation split:  64%|▋| 7199/11332 [00:13<00:07, 556.23 examples/s]\nGenerating validation split:  64%|▋| 7264/11332 [00:13<00:07, 564.06 examples/s]\nGenerating validation split:  65%|▋| 7337/11332 [00:13<00:07, 531.24 examples/s]\nGenerating validation split:  65%|▋| 7416/11332 [00:13<00:07, 523.01 examples/s]\nGenerating validation split:  66%|▋| 7490/11332 [00:13<00:07, 508.43 examples/s]\nGenerating validation split:  67%|▋| 7551/11332 [00:13<00:07, 528.62 examples/s]\nGenerating validation split:  67%|▋| 7605/11332 [00:13<00:07, 529.89 examples/s]\nGenerating validation split:  68%|▋| 7660/11332 [00:13<00:06, 532.32 examples/s]\nGenerating validation split:  68%|▋| 7714/11332 [00:14<00:06, 528.45 examples/s]\nGenerating validation split:  69%|▋| 7793/11332 [00:14<00:06, 516.53 examples/s]\nGenerating validation split:  69%|▋| 7863/11332 [00:14<00:06, 559.67 examples/s]\nGenerating validation split:  70%|▋| 7937/11332 [00:14<00:06, 525.57 examples/s]\nGenerating validation split:  71%|▋| 8009/11332 [00:14<00:06, 507.36 examples/s]\nGenerating validation split:  71%|▋| 8075/11332 [00:14<00:06, 538.80 examples/s]\nGenerating validation split:  72%|▋| 8143/11332 [00:14<00:05, 570.76 examples/s]\nGenerating validation split:  73%|▋| 8216/11332 [00:14<00:05, 601.37 examples/s]\nGenerating validation split:  73%|▋| 8296/11332 [00:15<00:05, 570.19 examples/s]\nGenerating validation split:  74%|▋| 8357/11332 [00:15<00:05, 576.00 examples/s]\nGenerating validation split:  74%|▋| 8440/11332 [00:15<00:05, 556.49 examples/s]\nGenerating validation split:  75%|▊| 8523/11332 [00:15<00:05, 550.43 examples/s]\nGenerating validation split:  76%|▊| 8606/11332 [00:15<00:05, 544.28 examples/s]\nGenerating validation split:  77%|▊| 8696/11332 [00:15<00:04, 553.15 examples/s]\nGenerating validation split:  77%|▊| 8762/11332 [00:15<00:04, 568.28 examples/s]\nGenerating validation split:  78%|▊| 8828/11332 [00:16<00:04, 587.95 examples/s]\nGenerating validation split:  79%|▊| 8910/11332 [00:16<00:03, 637.26 examples/s]\nGenerating validation split:  79%|▊| 9000/11332 [00:16<00:03, 612.76 examples/s]\nGenerating validation split:  80%|▊| 9095/11332 [00:16<00:03, 611.37 examples/s]\nGenerating validation split:  81%|▊| 9174/11332 [00:16<00:03, 575.76 examples/s]\nGenerating validation split:  82%|▊| 9245/11332 [00:16<00:03, 533.32 examples/s]\nGenerating validation split:  82%|▊| 9323/11332 [00:16<00:03, 527.39 examples/s]\nGenerating validation split:  83%|▊| 9382/11332 [00:17<00:03, 538.45 examples/s]\nGenerating validation split:  83%|▊| 9437/11332 [00:17<00:03, 538.40 examples/s]\nGenerating validation split:  84%|▊| 9517/11332 [00:17<00:03, 534.10 examples/s]\nGenerating validation split:  85%|▊| 9581/11332 [00:17<00:03, 558.64 examples/s]\nGenerating validation split:  85%|▊| 9659/11332 [00:17<00:03, 543.53 examples/s]\nGenerating validation split:  86%|▊| 9719/11332 [00:17<00:02, 555.25 examples/s]\nGenerating validation split:  86%|▊| 9780/11332 [00:17<00:02, 568.07 examples/s]\nGenerating validation split:  87%|▊| 9858/11332 [00:17<00:02, 538.02 examples/s]\nGenerating validation split:  88%|▉| 9925/11332 [00:17<00:02, 566.27 examples/s]\nGenerating validation split:  88%|▉| 10014/11332 [00:18<00:02, 565.02 examples/s\nGenerating validation split:  89%|▉| 10093/11332 [00:18<00:02, 546.55 examples/s\nGenerating validation split:  90%|▉| 10153/11332 [00:18<00:02, 557.63 examples/s\nGenerating validation split:  90%|▉| 10224/11332 [00:18<00:01, 594.51 examples/s\nGenerating validation split:  91%|▉| 10314/11332 [00:18<00:01, 590.20 examples/s\nGenerating validation split:  92%|▉| 10406/11332 [00:18<00:01, 594.52 examples/s\nGenerating validation split:  92%|▉| 10469/11332 [00:18<00:01, 602.33 examples/s\nGenerating validation split:  93%|▉| 10556/11332 [00:19<00:01, 587.22 examples/s\nGenerating validation split:  94%|▉| 10637/11332 [00:19<00:01, 564.62 examples/s\nGenerating validation split:  95%|▉| 10712/11332 [00:19<00:01, 541.46 examples/s\nGenerating validation split:  95%|▉| 10793/11332 [00:19<00:01, 538.52 examples/s\nGenerating validation split:  96%|▉| 10882/11332 [00:19<00:00, 549.14 examples/s\nGenerating validation split:  97%|▉| 10948/11332 [00:19<00:00, 572.90 examples/s\nGenerating validation split:  97%|▉| 11016/11332 [00:19<00:00, 529.53 examples/s\nGenerating validation split:  98%|▉| 11099/11332 [00:20<00:00, 532.57 examples/s\nGenerating validation split:  98%|▉| 11156/11332 [00:20<00:00, 522.44 examples/s\nGenerating validation split:  99%|▉| 11212/11332 [00:20<00:00, 529.92 examples/s\nGenerating validation split:  99%|▉| 11274/11332 [00:20<00:00, 548.13 examples/s\nGenerating validation split: 100%|█| 11332/11332 [00:20<00:00, 552.61 examples/s\n\nGenerating test split:   0%|                   | 0/11334 [00:00<?, ? examples/s]\nGenerating test split:   0%|         | 35/11334 [00:00<00:32, 342.99 examples/s]\nGenerating test split:   1%|         | 90/11334 [00:00<00:24, 459.86 examples/s]\nGenerating test split:   1%|        | 147/11334 [00:00<00:22, 501.60 examples/s]\nGenerating test split:   2%|▏       | 199/11334 [00:00<00:22, 495.52 examples/s]\nGenerating test split:   2%|▏       | 250/11334 [00:00<00:22, 494.45 examples/s]\nGenerating test split:   3%|▏       | 314/11334 [00:00<00:20, 537.96 examples/s]\nGenerating test split:   3%|▎       | 371/11334 [00:00<00:20, 539.81 examples/s]\nGenerating test split:   4%|▎       | 431/11334 [00:00<00:19, 551.36 examples/s]\nGenerating test split:   4%|▎       | 508/11334 [00:00<00:20, 534.50 examples/s]\nGenerating test split:   5%|▍       | 595/11334 [00:01<00:19, 548.41 examples/s]\nGenerating test split:   6%|▍       | 665/11334 [00:01<00:18, 583.98 examples/s]\nGenerating test split:   6%|▌       | 731/11334 [00:01<00:20, 529.91 examples/s]\nGenerating test split:   7%|▌       | 788/11334 [00:01<00:19, 534.51 examples/s]\nGenerating test split:   8%|▌       | 874/11334 [00:01<00:19, 542.70 examples/s]\nGenerating test split:   8%|▋       | 949/11334 [00:01<00:19, 521.69 examples/s]\nGenerating test split:   9%|▋      | 1012/11334 [00:01<00:19, 543.24 examples/s]\nGenerating test split:  10%|▋      | 1082/11334 [00:02<00:17, 580.03 examples/s]\nGenerating test split:  10%|▋      | 1167/11334 [00:02<00:17, 571.39 examples/s]\nGenerating test split:  11%|▊      | 1226/11334 [00:02<00:17, 569.77 examples/s]\nGenerating test split:  11%|▊      | 1293/11334 [00:02<00:16, 594.50 examples/s]\nGenerating test split:  12%|▊      | 1372/11334 [00:02<00:17, 553.98 examples/s]\nGenerating test split:  13%|▉      | 1438/11334 [00:02<00:17, 574.64 examples/s]\nGenerating test split:  13%|▉      | 1514/11334 [00:02<00:17, 546.24 examples/s]\nGenerating test split:  14%|▉      | 1585/11334 [00:02<00:18, 521.62 examples/s]\nGenerating test split:  15%|█      | 1650/11334 [00:03<00:17, 547.71 examples/s]\nGenerating test split:  15%|█      | 1711/11334 [00:03<00:17, 559.88 examples/s]\nGenerating test split:  16%|█      | 1779/11334 [00:03<00:16, 589.58 examples/s]\nGenerating test split:  16%|█▏     | 1842/11334 [00:03<00:18, 525.36 examples/s]\nGenerating test split:  17%|█▏     | 1902/11334 [00:03<00:17, 536.32 examples/s]\nGenerating test split:  17%|█▏     | 1968/11334 [00:03<00:16, 566.25 examples/s]\nGenerating test split:  18%|█▎     | 2035/11334 [00:03<00:15, 585.13 examples/s]\nGenerating test split:  19%|█▎     | 2133/11334 [00:03<00:15, 604.64 examples/s]\nGenerating test split:  19%|█▎     | 2195/11334 [00:03<00:15, 603.13 examples/s]\nGenerating test split:  20%|█▍     | 2269/11334 [00:04<00:16, 562.13 examples/s]\nGenerating test split:  21%|█▍     | 2353/11334 [00:04<00:16, 553.33 examples/s]\nGenerating test split:  21%|█▍     | 2419/11334 [00:04<00:17, 509.76 examples/s]\nGenerating test split:  22%|█▌     | 2481/11334 [00:04<00:16, 523.59 examples/s]\nGenerating test split:  23%|█▌     | 2555/11334 [00:04<00:15, 573.92 examples/s]\nGenerating test split:  23%|█▌     | 2630/11334 [00:04<00:15, 544.16 examples/s]\nGenerating test split:  24%|█▋     | 2689/11334 [00:04<00:15, 551.50 examples/s]\nGenerating test split:  24%|█▋     | 2758/11334 [00:05<00:16, 515.70 examples/s]\nGenerating test split:  25%|█▋     | 2830/11334 [00:05<00:15, 551.95 examples/s]\nGenerating test split:  26%|█▊     | 2915/11334 [00:05<00:15, 547.07 examples/s]\nGenerating test split:  26%|█▊     | 2978/11334 [00:05<00:14, 563.12 examples/s]\nGenerating test split:  27%|█▉     | 3061/11334 [00:05<00:14, 553.20 examples/s]\nGenerating test split:  28%|█▉     | 3127/11334 [00:05<00:14, 571.15 examples/s]\nGenerating test split:  28%|█▉     | 3187/11334 [00:05<00:14, 570.26 examples/s]\nGenerating test split:  29%|██     | 3270/11334 [00:05<00:14, 560.96 examples/s]\nGenerating test split:  29%|██     | 3330/11334 [00:06<00:14, 563.73 examples/s]\nGenerating test split:  30%|██     | 3389/11334 [00:06<00:13, 568.51 examples/s]\nGenerating test split:  31%|██▏    | 3472/11334 [00:06<00:14, 561.32 examples/s]\nGenerating test split:  31%|██▏    | 3553/11334 [00:06<00:14, 547.53 examples/s]\nGenerating test split:  32%|██▏    | 3609/11334 [00:06<00:14, 546.01 examples/s]\nGenerating test split:  32%|██▎    | 3670/11334 [00:06<00:13, 554.99 examples/s]\nGenerating test split:  33%|██▎    | 3727/11334 [00:06<00:13, 552.86 examples/s]\nGenerating test split:  34%|██▎    | 3798/11334 [00:06<00:12, 593.53 examples/s]\nGenerating test split:  34%|██▍    | 3883/11334 [00:07<00:12, 577.50 examples/s]\nGenerating test split:  35%|██▍    | 3943/11334 [00:07<00:12, 578.09 examples/s]\nGenerating test split:  35%|██▍    | 4023/11334 [00:07<00:13, 558.89 examples/s]\nGenerating test split:  36%|██▌    | 4087/11334 [00:07<00:12, 574.35 examples/s]\nGenerating test split:  37%|██▌    | 4156/11334 [00:07<00:13, 529.59 examples/s]\nGenerating test split:  37%|██▌    | 4219/11334 [00:07<00:12, 551.44 examples/s]\nGenerating test split:  38%|██▋    | 4291/11334 [00:07<00:13, 516.52 examples/s]\nGenerating test split:  38%|██▋    | 4347/11334 [00:07<00:13, 525.29 examples/s]\nGenerating test split:  39%|██▋    | 4409/11334 [00:08<00:12, 540.77 examples/s]\nGenerating test split:  39%|██▊    | 4465/11334 [00:08<00:12, 543.14 examples/s]\nGenerating test split:  40%|██▊    | 4552/11334 [00:08<00:12, 553.78 examples/s]\nGenerating test split:  41%|██▊    | 4618/11334 [00:08<00:11, 577.10 examples/s]\nGenerating test split:  41%|██▉    | 4677/11334 [00:08<00:11, 577.97 examples/s]\nGenerating test split:  42%|██▉    | 4765/11334 [00:08<00:11, 574.45 examples/s]\nGenerating test split:  43%|██▉    | 4825/11334 [00:08<00:11, 574.99 examples/s]\nGenerating test split:  43%|███    | 4892/11334 [00:08<00:10, 594.54 examples/s]\nGenerating test split:  44%|███    | 4985/11334 [00:08<00:10, 590.18 examples/s]\nGenerating test split:  45%|███▏   | 5079/11334 [00:09<00:10, 597.98 examples/s]\nGenerating test split:  46%|███▏   | 5161/11334 [00:09<00:10, 569.55 examples/s]\nGenerating test split:  46%|███▏   | 5253/11334 [00:09<00:10, 578.68 examples/s]\nGenerating test split:  47%|███▎   | 5317/11334 [00:09<00:10, 580.47 examples/s]\nGenerating test split:  48%|███▎   | 5404/11334 [00:09<00:10, 579.59 examples/s]\nGenerating test split:  48%|███▍   | 5489/11334 [00:09<00:10, 567.39 examples/s]\nGenerating test split:  49%|███▍   | 5552/11334 [00:09<00:09, 578.56 examples/s]\nGenerating test split:  50%|███▍   | 5630/11334 [00:10<00:10, 553.06 examples/s]\nGenerating test split:  50%|███▌   | 5708/11334 [00:10<00:10, 537.67 examples/s]\nGenerating test split:  51%|███▌   | 5771/11334 [00:10<00:10, 550.52 examples/s]\nGenerating test split:  52%|███▌   | 5855/11334 [00:10<00:10, 546.09 examples/s]\nGenerating test split:  52%|███▋   | 5937/11334 [00:10<00:09, 542.40 examples/s]\nGenerating test split:  53%|███▋   | 5996/11334 [00:10<00:09, 550.59 examples/s]\nGenerating test split:  53%|███▋   | 6057/11334 [00:10<00:09, 561.66 examples/s]\nGenerating test split:  54%|███▊   | 6139/11334 [00:11<00:08, 625.73 examples/s]\nGenerating test split:  55%|███▊   | 6229/11334 [00:11<00:08, 613.23 examples/s]\nGenerating test split:  56%|███▉   | 6292/11334 [00:11<00:08, 608.65 examples/s]\nGenerating test split:  56%|███▉   | 6355/11334 [00:11<00:08, 609.66 examples/s]\nGenerating test split:  57%|███▉   | 6443/11334 [00:11<00:08, 596.20 examples/s]\nGenerating test split:  58%|████   | 6523/11334 [00:11<00:08, 566.88 examples/s]\nGenerating test split:  58%|████   | 6616/11334 [00:11<00:08, 574.29 examples/s]\nGenerating test split:  59%|████   | 6674/11334 [00:11<00:08, 572.42 examples/s]\nGenerating test split:  59%|████▏  | 6739/11334 [00:12<00:07, 578.78 examples/s]\nGenerating test split:  60%|████▏  | 6819/11334 [00:12<00:08, 551.71 examples/s]\nGenerating test split:  61%|████▏  | 6881/11334 [00:12<00:07, 564.73 examples/s]\nGenerating test split:  61%|████▎  | 6945/11334 [00:12<00:07, 583.46 examples/s]\nGenerating test split:  62%|████▎  | 7011/11334 [00:12<00:07, 600.11 examples/s]\nGenerating test split:  63%|████▍  | 7100/11334 [00:12<00:07, 588.64 examples/s]\nGenerating test split:  63%|████▍  | 7166/11334 [00:12<00:06, 597.05 examples/s]\nGenerating test split:  64%|████▍  | 7250/11334 [00:12<00:07, 576.34 examples/s]\nGenerating test split:  65%|████▌  | 7336/11334 [00:13<00:07, 563.78 examples/s]\nGenerating test split:  65%|████▌  | 7407/11334 [00:13<00:07, 527.03 examples/s]\nGenerating test split:  66%|████▌  | 7468/11334 [00:13<00:07, 544.37 examples/s]\nGenerating test split:  66%|████▋  | 7531/11334 [00:13<00:06, 560.49 examples/s]\nGenerating test split:  67%|████▋  | 7606/11334 [00:13<00:06, 532.58 examples/s]\nGenerating test split:  68%|████▋  | 7671/11334 [00:13<00:06, 557.27 examples/s]\nGenerating test split:  68%|████▊  | 7758/11334 [00:13<00:06, 563.25 examples/s]\nGenerating test split:  69%|████▊  | 7818/11334 [00:13<00:06, 570.31 examples/s]\nGenerating test split:  69%|████▊  | 7877/11334 [00:14<00:06, 568.05 examples/s]\nGenerating test split:  70%|████▉  | 7954/11334 [00:14<00:06, 537.94 examples/s]\nGenerating test split:  71%|████▉  | 8027/11334 [00:14<00:06, 512.50 examples/s]\nGenerating test split:  71%|████▉  | 8084/11334 [00:14<00:06, 518.49 examples/s]\nGenerating test split:  72%|█████  | 8144/11334 [00:14<00:05, 535.75 examples/s]\nGenerating test split:  72%|█████  | 8203/11334 [00:14<00:05, 541.62 examples/s]\nGenerating test split:  73%|█████  | 8268/11334 [00:14<00:06, 493.31 examples/s]\nGenerating test split:  74%|█████▏ | 8333/11334 [00:14<00:05, 531.82 examples/s]\nGenerating test split:  74%|█████▏ | 8391/11334 [00:15<00:05, 541.68 examples/s]\nGenerating test split:  75%|█████▏ | 8472/11334 [00:15<00:05, 533.39 examples/s]\nGenerating test split:  75%|█████▎ | 8539/11334 [00:15<00:05, 556.59 examples/s]\nGenerating test split:  76%|█████▎ | 8611/11334 [00:15<00:04, 590.78 examples/s]\nGenerating test split:  77%|█████▎ | 8680/11334 [00:15<00:04, 615.68 examples/s]\nGenerating test split:  77%|█████▍ | 8758/11334 [00:15<00:04, 578.42 examples/s]\nGenerating test split:  78%|█████▍ | 8827/11334 [00:15<00:04, 534.62 examples/s]\nGenerating test split:  78%|█████▍ | 8896/11334 [00:15<00:04, 570.26 examples/s]\nGenerating test split:  79%|█████▌ | 8968/11334 [00:16<00:04, 530.07 examples/s]\nGenerating test split:  80%|█████▌ | 9039/11334 [00:16<00:04, 509.34 examples/s]\nGenerating test split:  80%|█████▌ | 9103/11334 [00:16<00:04, 539.22 examples/s]\nGenerating test split:  81%|█████▋ | 9185/11334 [00:16<00:04, 535.62 examples/s]\nGenerating test split:  82%|█████▋ | 9245/11334 [00:16<00:03, 550.25 examples/s]\nGenerating test split:  82%|█████▋ | 9309/11334 [00:16<00:03, 567.79 examples/s]\nGenerating test split:  83%|█████▊ | 9369/11334 [00:16<00:03, 573.67 examples/s]\nGenerating test split:  83%|█████▊ | 9450/11334 [00:16<00:03, 550.21 examples/s]\nGenerating test split:  84%|█████▉ | 9515/11334 [00:17<00:03, 572.91 examples/s]\nGenerating test split:  85%|█████▉ | 9591/11334 [00:17<00:03, 542.61 examples/s]\nGenerating test split:  85%|█████▉ | 9650/11334 [00:17<00:03, 550.35 examples/s]\nGenerating test split:  86%|██████ | 9728/11334 [00:17<00:03, 532.78 examples/s]\nGenerating test split:  86%|██████ | 9798/11334 [00:17<00:03, 508.99 examples/s]\nGenerating test split:  87%|██████ | 9853/11334 [00:17<00:02, 514.28 examples/s]\nGenerating test split:  88%|██████▏| 9929/11334 [00:17<00:02, 508.00 examples/s]\nGenerating test split:  88%|██████▏| 9997/11334 [00:18<00:02, 488.13 examples/s]\nGenerating test split:  89%|█████▎| 10059/11334 [00:18<00:02, 514.49 examples/s]\nGenerating test split:  89%|█████▎| 10128/11334 [00:18<00:02, 549.86 examples/s]\nGenerating test split:  90%|█████▍| 10213/11334 [00:18<00:02, 545.01 examples/s]\nGenerating test split:  91%|█████▍| 10276/11334 [00:18<00:01, 564.90 examples/s]\nGenerating test split:  91%|█████▍| 10341/11334 [00:18<00:01, 515.85 examples/s]\nGenerating test split:  92%|█████▌| 10396/11334 [00:18<00:01, 517.99 examples/s]\nGenerating test split:  92%|█████▌| 10466/11334 [00:18<00:01, 498.34 examples/s]\nGenerating test split:  93%|█████▌| 10521/11334 [00:19<00:01, 506.88 examples/s]\nGenerating test split:  93%|█████▌| 10591/11334 [00:19<00:01, 554.49 examples/s]\nGenerating test split:  94%|█████▋| 10655/11334 [00:19<00:01, 572.47 examples/s]\nGenerating test split:  95%|█████▋| 10718/11334 [00:19<00:01, 586.02 examples/s]\nGenerating test split:  95%|█████▋| 10779/11334 [00:19<00:00, 585.84 examples/s]\nGenerating test split:  96%|█████▋| 10842/11334 [00:19<00:00, 587.27 examples/s]\nGenerating test split:  96%|█████▊| 10926/11334 [00:19<00:00, 572.71 examples/s]\nGenerating test split:  97%|█████▊| 10994/11334 [00:19<00:00, 596.60 examples/s]\nGenerating test split:  98%|█████▊| 11079/11334 [00:19<00:00, 582.40 examples/s]\nGenerating test split:  98%|█████▉| 11140/11334 [00:20<00:00, 588.51 examples/s]\nGenerating test split:  99%|█████▉| 11224/11334 [00:20<00:00, 574.43 examples/s]\nGenerating test split: 100%|█████▉| 11287/11334 [00:20<00:00, 586.62 examples/s]\nGenerating test split: 100%|██████| 11334/11334 [00:20<00:00, 555.76 examples/s]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python download_xsum.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210042be",
   "metadata": {},
   "source": [
    "### Step 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf91f29",
   "metadata": {},
   "source": [
    "Execution Start time: 04:41:24, End time: 04:41:31 (duration: 7 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4bd3ab",
   "metadata": {},
   "source": [
    "**Thought**: The XSum dataset has been successfully downloaded and processed. Now that the dataset is in place, let's proceed to run the model with the updated dataset loading logic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f2172b",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d766408e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "W&B online. Running your script from this directory will now sync to the cloud.\n22 Aug 04:41    INFO 66 parameters found.\n================================================================================\n\n# General Hyper Parameters: \n\ngpu_id: 0\nuse_gpu: True\ndevice: cpu\nseed: 2020\nreproducibility: True\ncmd: run_textbox.py --model=BART --dataset=xsum --model_path=facebook/bart-base --epochs=1\nfilename: BART-xsum-2024-Aug-22_04-41-29\nsaved_dir: saved/\nstate: INFO\nwandb: online\n\n\n# Training Hyper Parameters: \n\ndo_train: True\ndo_valid: True\noptimizer: adamw\nadafactor_kwargs: {'lr': 0.001, 'scale_parameter': False, 'relative_step': False, 'warmup_init': False}\noptimizer_kwargs: {}\nvalid_steps: 1\nvalid_strategy: epoch\nstopping_steps: 2\nepochs: 1\nlearning_rate: 3e-05\ntrain_batch_size: 4\ngrad_clip: 0.1\naccumulation_steps: 48\ndisable_tqdm: False\nresume_training: True\n\n\n# Evaluation Hyper Parameters: \n\ndo_test: True\nlower_evaluation: True\nmultiref_strategy: max\nbleu_max_ngrams: 4\nbleu_type: nltk\nsmoothing_function: 0\ncorpus_bleu: False\nrouge_max_ngrams: 2\nrouge_type: files2rouge\nmeteor_type: pycocoevalcap\nchrf_type: m-popovic\ndistinct_max_ngrams: 4\ninter_distinct: True\nunique_max_ngrams: 4\nself_bleu_max_ngrams: 4\ntgt_lang: en\nmetrics: ['rouge']\neval_batch_size: 16\ncorpus_meteor: True\n\n\n# Model Hyper Parameters: \n\nmodel: BART\nmodel_name: bart\nmodel_path: facebook/bart-base\nconfig_kwargs: {}\ntokenizer_kwargs: {'use_fast': True}\ngeneration_kwargs: {'num_beams': 5, 'no_repeat_ngram_size': 3, 'early_stopping': True}\nefficient_kwargs: {}\nefficient_methods: []\nefficient_unfreeze_model: False\nlabel_smoothing: 0.1\n\n\n# Dataset Hyper Parameters: \n\ndataset: xsum\ndata_path: dataset/xsum\ntgt_lang: en\nsrc_len: 1024\ntgt_len: 128\ntruncate: tail\nprefix_prompt: Summarize: \nmetrics_for_best_model: ['rouge-1', 'rouge-2', 'rouge-l']\n\n\n# Unrecognized Hyper Parameters: \n\nfind_unused_parameters: False\ntokenizer_add_tokens: []\nload_type: from_pretrained\n\n================================================================================\n/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n22 Aug 04:41    INFO Pretrain type: pretrain disabled\n╭───────────────────── Traceback (most recent call last) ──────────────────────╮\n│ /content/TextBox/run_textbox.py:12 in <module>                               │\n│                                                                              │\n│    9 │                                                                       │\n│   10 │   args, _ = parser.parse_known_args()                                 │\n│   11 │                                                                       │\n│ ❱ 12 │   run_textbox(model=args.model, dataset=args.dataset, config_file_lis │\n│   13                                                                         │\n│                                                                              │\n│ /content/TextBox/textbox/quick_start/quick_start.py:20 in run_textbox        │\n│                                                                              │\n│   17 │   │   config_dict (dict): parameters dictionary used to modify experi │\n│   18 │   \"\"\"                                                                 │\n│   19 │   from textbox.quick_start.experiment import Experiment               │\n│ ❱ 20 │   experiment = Experiment(model, dataset, config_file_list, config_di │\n│   21 │   experiment.run()                                                    │\n│   22                                                                         │\n│                                                                              │\n│ /content/TextBox/textbox/quick_start/experiment.py:56 in __init__            │\n│                                                                              │\n│    53 │   │   self.logger = self.init_logger(self.config)                    │\n│    54 │   │   self.summary_tracker = SummaryTracker.basicConfig(self.get_con │\n│    55 │   │   self.train_data, self.valid_data, self.test_data, self.tokeniz │\n│ ❱  56 │   │   │   self._init_data(self.get_config(), self.accelerator)       │\n│    57 │                                                                      │\n│    58 │   def get_config(self) -> Config:                                    │\n│    59 │   │   config = copy(self.config)                                     │\n│                                                                              │\n│ /content/TextBox/textbox/quick_start/experiment.py:82 in _init_data          │\n│                                                                              │\n│    79 │   @staticmethod                                                      │\n│    80 │   def _init_data(config: Config, accelerator: Accelerator) -> Tuple[ │\n│    81 │   │   tokenizer = get_tokenizer(config)                              │\n│ ❱  82 │   │   train_data, valid_data, test_data = data_preparation(config, t │\n│    83 │   │   train_data, valid_data, test_data = accelerator.prepare(train_ │\n│    84 │   │   return train_data, valid_data, test_data, tokenizer            │\n│    85                                                                        │\n│                                                                              │\n│ /content/TextBox/textbox/data/utils.py:37 in data_preparation                │\n│                                                                              │\n│   34 │   │   dataloaders.append(None)                                        │\n│   35 │                                                                       │\n│   36 │   if config['do_valid']:                                              │\n│ ❱ 37 │   │   valid_dataset = AbstractDataset(config, 'valid')                │\n│   38 │   │   valid_dataset.tokenize(tokenizer)                               │\n│   39 │   │   valid_dataloader = DataLoader(                                  │\n│   40 │   │   │   valid_dataset,                                              │\n│                                                                              │\n│ /content/TextBox/textbox/data/abstract_dataset.py:25 in __init__             │\n│                                                                              │\n│    22 │   │   source_filename = os.path.join(config[\"data_path\"], f\"{set}.sr │\n│    23 │   │   target_filename = os.path.join(config[\"data_path\"], f\"{set}.tg │\n│    24 │   │                                                                  │\n│ ❱  25 │   │   self.source_text = load_data(source_filename, max_length=self. │\n│    26 │   │   self.pretraining = config['pretrain_task']                     │\n│    27 │   │   self.is_casual_model = bool(config[\"model_name\"] in CLM_MODELS │\n│    28 │   │   if self.pretraining is None and self.pretraining != 'disabled' │\n│                                                                              │\n│ /content/TextBox/textbox/data/misc.py:19 in load_data                        │\n│                                                                              │\n│   16 │   │   List[List[str]]: the text list loaded from dataset path.        │\n│   17 │   \"\"\"                                                                 │\n│   18 │   if not os.path.isfile(dataset_path):                                │\n│ ❱ 19 │   │   raise ValueError('File {} not exist'.format(os.path.abspath(dat │\n│   20 │                                                                       │\n│   21 │   text = []                                                           │\n│   22 │   with open(dataset_path, \"r\") as fin:                                │\n╰──────────────────────────────────────────────────────────────────────────────╯\nValueError: File /content/TextBox/dataset/xsum/valid.src not exist\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python run_textbox.py --model=BART --dataset=xsum --model_path=facebook/bart-base --epochs=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c6b9e0",
   "metadata": {},
   "source": [
    "### Step 43"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b5b901",
   "metadata": {},
   "source": [
    "Execution Start time: 04:41:34, End time: 04:41:34 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559faf69",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the file `/content/TextBox/dataset/xsum/valid.src` does not exist. This suggests that the dataset files for the validation set might not have been created correctly. Let's verify the contents of the `dataset/xsum` directory to ensure all necessary files are present.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c398e22c",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c970e3ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test.src  test.tgt  train.src  train.tgt  validation.src  validation.tgt\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ls dataset/xsum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eec235c",
   "metadata": {},
   "source": [
    "### Step 44"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6d7ecd",
   "metadata": {},
   "source": [
    "Execution Start time: 04:41:36, End time: 04:41:36 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf5c510",
   "metadata": {},
   "source": [
    "**Thought**: The `dataset/xsum` directory contains the necessary files, including `validation.src` and `validation.tgt`. It seems there might be a mismatch in the expected file names. The script might be looking for `valid.src` instead of `validation.src`.\n",
    "\n",
    "Let's search for the term \"valid\" in the repository to identify where the file names are being referenced and make the necessary adjustments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7214dfe9",
   "metadata": {},
   "source": [
    "**Action (search_dir)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badc3c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Found 216 matches for \"valid\" in /content/TextBox:\n/content/TextBox/Leaderboard/Open-ended (1 matches)\n/content/TextBox/README.md (2 matches)\n/content/TextBox/TODOS.md (5 matches)\n/content/TextBox/asset/basic_training.md (4 matches)\n/content/TextBox/asset/dataset.md (2 matches)\n/content/TextBox/asset/example.log (6 matches)\n/content/TextBox/asset/pretraining.md (2 matches)\n/content/TextBox/dataset/samsum/test.tgt (1 matches)\n/content/TextBox/dataset/samsum/train.src (15 matches)\n/content/TextBox/dataset/samsum/train.tgt (7 matches)\n/content/TextBox/dataset/samsum/valid.src (1 matches)\n/content/TextBox/download_xsum.py (1 matches)\n/content/TextBox/files2rouge/experiments/openNMT.0.md (1 matches)\n/content/TextBox/files2rouge/files2rouge/RELEASE-1.5.5/ROUGE-1.5.5.pl (1 matches)\n/content/TextBox/files2rouge/files2rouge/RELEASE-1.5.5/XML/DOM.pm (3 matches)\n/content/TextBox/files2rouge/files2rouge/RELEASE-1.5.5/XML/DOM/Document.pod (1 matches)\n/content/TextBox/files2rouge/files2rouge/RELEASE-1.5.5/XML/DOM/Element.pod (3 matches)\n/content/TextBox/files2rouge/files2rouge/RELEASE-1.5.5/XML/DOM/NamedNodeMap.pod (2 matches)\n/content/TextBox/files2rouge/files2rouge/RELEASE-1.5.5/XML/DOM/NodeList.pod (1 matches)\n/content/TextBox/files2rouge/files2rouge/RELEASE-1.5.5/XML/RegExp.pm (1 matches)\n/content/TextBox/saved/BART-xsum-2024-Aug-22_04-39-34/project.log (3 matches)\n/content/TextBox/saved/BART-xsum-2024-Aug-22_04-41-29/project.log (3 matches)\n/content/TextBox/textbox/config/configurator.py (3 matches)\n/content/TextBox/textbox/data/abstract_dataset.py (2 matches)\n/content/TextBox/textbox/data/misc.py (1 matches)\n/content/TextBox/textbox/data/utils.py (7 matches)\n/content/TextBox/textbox/evaluator/multiwoz_evaluator.py (1 matches)\n/content/TextBox/textbox/evaluator/utils/multiwoz/db_ops.py (3 matches)\n/content/TextBox/textbox/evaluator/utils/multiwoz/eval.py (1 matches)\n/content/TextBox/textbox/properties/overall.yaml (2 matches)\n/content/TextBox/textbox/quick_start/experiment.py (12 matches)\n/content/TextBox/textbox/quick_start/multi_seed.py (7 matches)\n/content/TextBox/textbox/trainer/trainer.py (67 matches)\n/content/TextBox/textbox/utils/argument_list.py (3 matches)\n/content/TextBox/textbox/utils/dashboard.py (40 matches)\n/content/TextBox/textbox/utils/utils.py (1 matches)\nEnd of matches for \"valid\" in /content/TextBox\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_dir \"valid\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9ff80d",
   "metadata": {},
   "source": [
    "### Step 45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8397c2f",
   "metadata": {},
   "source": [
    "Execution Start time: 04:41:39, End time: 04:41:39 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381f28bb",
   "metadata": {},
   "source": [
    "**Thought**: The search results show multiple matches for \"valid,\" but the relevant match is likely in the `textbox/quick_start/experiment.py` file, which had several matches. This file might be where the dataset file names are being referenced.\n",
    "\n",
    "Let's open `textbox/quick_start/experiment.py` to examine how the validation dataset file is being referenced and make any necessary adjustments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9468493",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e876d6f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/TextBox/textbox/quick_start/experiment.py (142 lines total)]\n1:import os\n2:import logging\n3:from copy import copy\n4:from accelerate.logging import get_logger\n5:\n6:from typing import Optional, Tuple, Any, List, Dict\n7:\n8:from accelerate import Accelerator\n9:from torch.utils.data import DataLoader\n10:\n11:from ..config.configurator import Config\n12:from ..data.utils import data_preparation\n13:from ..trainer.trainer import Trainer\n14:from ..utils.dashboard import SummaryTracker\n15:from ..utils.logger import init_logger\n16:from ..utils.utils import get_model, get_tokenizer, get_trainer, init_seed\n17:\n18:ResultType = Dict[str, Any]\n19:\n20:\n21:class Experiment:\n22:    r\"\"\" A fast running api, which includes the complete process of\n23:    training and testing a model on a specified dataset\n24:\n25:    Args:\n26:        model (str): model name\n27:        dataset (str): dataset name\n28:        config_file_list (list): config files used to modify experiment parameters\n29:        config_dict (dict): parameters dictionary used to modify experiment parameters\n30:    \"\"\"\n31:\n32:    def __init__(\n33:        self,\n34:        model: Optional[str] = None,\n35:        dataset: Optional[str] = None,\n36:        config_file_list: Optional[List[str]] = None,\n37:        config_dict: Optional[Dict[str, Any]] = None,\n38:    ):\n39:        self.config = Config(model, dataset, config_file_list, config_dict)\n40:        wandb_setting = 'wandb ' + self.config['wandb']\n41:        os.system(wandb_setting)\n42:        self.__extended_config = None\n43:\n44:        from accelerate import DistributedDataParallelKwargs\n45:        ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=self.config['find_unused_parameters'])\n46:        self.accelerator = Accelerator(\n47:            gradient_accumulation_steps=self.config['accumulation_steps'], kwargs_handlers=[ddp_kwargs]\n48:        )\n49:        self.config.update({\n50:            '_is_local_main_process': self.accelerator.is_local_main_process,\n51:            'device': self.accelerator.device\n52:        })\n53:        self.logger = self.init_logger(self.config)\n54:        self.summary_tracker = SummaryTracker.basicConfig(self.get_config())\n55:        self.train_data, self.valid_data, self.test_data, self.tokenizer = \\\n56:            self._init_data(self.get_config(), self.accelerator)\n57:\n58:    def get_config(self) -> Config:\n59:        config = copy(self.config)\n60:        if self.__extended_config is not None:\n61:            config.update(self.__extended_config)\n62:        return config\n63:\n64:    @staticmethod\n65:    def init_logger(config: Config) -> logging.Logger:\n66:\n67:        # logger initialization\n68:        init_logger(\n69:            filename=config['filename'],\n70:            log_level=config['state'],\n71:            enabled=config['_is_local_main_process'],\n72:            saved_dir=config['saved_dir']\n73:        )\n74:        logger = get_logger(__name__)\n75:        logger.info(config)\n76:\n77:        return logger\n78:\n79:    @staticmethod\n80:    def _init_data(config: Config, accelerator: Accelerator) -> Tuple[DataLoader, DataLoader, DataLoader, Any]:\n81:        tokenizer = get_tokenizer(config)\n82:        train_data, valid_data, test_data = data_preparation(config, tokenizer)\n83:        train_data, valid_data, test_data = accelerator.prepare(train_data, valid_data, test_data)\n84:        return train_data, valid_data, test_data, tokenizer\n85:\n86:    def _on_experiment_start(self, extended_config: Optional[dict]):\n87:        \"\"\"(Re-)initialize configuration. Since for now config and trainer is modifiable, this\n88:        function is needed to ensure they were aligned to initial configuration.\n89:        \"\"\"\n90:        self.__extended_config = extended_config\n91:        config = self.get_config()\n92:        init_seed(config['seed'], config['reproducibility'])\n93:\n94:        self.model = get_model(config['model'])(config, self.tokenizer).to(config['device'])\n95:        self.logger.info(self.model)\n96:        self.trainer: Trainer = get_trainer(config['model'])(config, self.model, self.accelerator)\n97:        self.do_train = config['do_train']\n98:        self.do_valid = config['do_valid']\n99:        self.do_test = config['do_test']\n100:        self.valid_result: Optional[ResultType] = None\n(42 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open textbox/quick_start/experiment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb3c900",
   "metadata": {},
   "source": [
    "### Step 46"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f4e1ce",
   "metadata": {},
   "source": [
    "Execution Start time: 04:41:41, End time: 04:41:41 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11d20de",
   "metadata": {},
   "source": [
    "**Thought**: The `Experiment` class references the `data_preparation` function to load datasets, but it doesn't explicitly mention file names. The issue might be in the configuration or another part of the code that specifies the file names.\n",
    "\n",
    "Let's search for \"valid.src\" in the repository to identify where the file names are being referenced and make the necessary adjustments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bd4b92",
   "metadata": {},
   "source": [
    "**Action (search_dir)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d0202b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Found 3 matches for \"valid.src\" in /content/TextBox:\n/content/TextBox/asset/dataset.md (1 matches)\n/content/TextBox/asset/pretraining.md (1 matches)\n/content/TextBox/files2rouge/experiments/openNMT.0.md (1 matches)\nEnd of matches for \"valid.src\" in /content/TextBox\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_dir \"valid.src\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb4142e",
   "metadata": {},
   "source": [
    "### Step 47"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683a7add",
   "metadata": {},
   "source": [
    "Execution Start time: 04:41:43, End time: 04:41:43 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83777cc",
   "metadata": {},
   "source": [
    "**Thought**: The search results indicate that \"valid.src\" is not directly referenced in the code files but appears in markdown documentation files. This suggests that the issue might be related to a configuration setting or a default assumption in the code.\n",
    "\n",
    "Let's check the configuration files or any default settings in the code that might be assuming the file name \"valid.src\" instead of \"validation.src\". I'll start by examining the configuration files in the `textbox/properties/dataset` directory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20520963",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c42e794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adgen.yaml   e2e.yaml\t    sgd.yaml\t  wmt13-en-es.yaml  wmt19-en-ru.yaml\ncg.yaml      gyafc_em.yaml  squad.yaml\t  wmt13-es-en.yaml  wmt19-en-zh.yaml\ncmv.yaml     gyafc_fr.yaml  squadqg.yaml  wmt14-en-fr.yaml  wmt19-ru-en.yaml\ncnndm.yaml   lcsts.yaml     totto.yaml\t  wmt14-fr-en.yaml  wmt19-zh-en.yaml\ncoqa.yaml    multiwoz.yaml  webnlg.yaml   wmt16-de-en.yaml  wp.yaml\ncoqaqg.yaml  paranmt.yaml   webnlg2.yaml  wmt16-en-de.yaml  xsum.yaml\ncsl.yaml     pc.yaml\t    wia.yaml\t  wmt16-en-ro.yaml\nda.yaml      quora.yaml     wikibio.yaml  wmt16-ro-en.yaml\ndart.yaml    roc.yaml\t    wikip.yaml\t  wmt19-cs-en.yaml\ndd.yaml      samsum.yaml    wle.yaml\t  wmt19-en-cs.yaml\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ls textbox/properties/dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc9f580",
   "metadata": {},
   "source": [
    "### Step 48"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e71341",
   "metadata": {},
   "source": [
    "Execution Start time: 04:41:45, End time: 04:41:45 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea9f829",
   "metadata": {},
   "source": [
    "**Thought**: The `textbox/properties/dataset` directory contains a configuration file named `xsum.yaml`, which is likely used for configuring the XSum dataset. Let's open this file to check if there are any settings related to file names that need to be adjusted.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bc7c45",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea2b31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/TextBox/textbox/properties/dataset/xsum.yaml (9 lines total)]\n1:src_len: 1024\n2:tgt_len: 128\n3:metrics: ['rouge']\n4:metrics_for_best_model: ['rouge-1', 'rouge-2', 'rouge-l']\n5:\n6:train_batch_size: 4\n7:accumulation_steps: 48\n8:eval_batch_size: 16\n9:prefix_prompt: 'Summarize: '\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open textbox/properties/dataset/xsum.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c501aad",
   "metadata": {},
   "source": [
    "### Step 49"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ef3916",
   "metadata": {},
   "source": [
    "Execution Start time: 04:41:47, End time: 04:41:47 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9344de4",
   "metadata": {},
   "source": [
    "**Thought**: The `xsum.yaml` configuration file does not explicitly mention file names for the dataset. The issue might be related to a default assumption in the code or another configuration setting.\n",
    "\n",
    "Let's search for the term \"validation\" in the repository to see if there are any references to \"validation.src\" that might need to be adjusted.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8be3e93",
   "metadata": {},
   "source": [
    "**Action (search_dir)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cc1cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Found 30 matches for \"validation\" in /content/TextBox:\n/content/TextBox/Leaderboard/Open-ended (1 matches)\n/content/TextBox/README.md (2 matches)\n/content/TextBox/TODOS.md (2 matches)\n/content/TextBox/asset/dataset.md (1 matches)\n/content/TextBox/asset/example.log (2 matches)\n/content/TextBox/asset/pretraining.md (2 matches)\n/content/TextBox/download_xsum.py (1 matches)\n/content/TextBox/textbox/evaluator/multiwoz_evaluator.py (1 matches)\n/content/TextBox/textbox/evaluator/utils/multiwoz/eval.py (1 matches)\n/content/TextBox/textbox/quick_start/experiment.py (1 matches)\n/content/TextBox/textbox/trainer/trainer.py (9 matches)\n/content/TextBox/textbox/utils/argument_list.py (1 matches)\n/content/TextBox/textbox/utils/dashboard.py (6 matches)\nEnd of matches for \"validation\" in /content/TextBox\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_dir \"validation\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a659f2e6",
   "metadata": {},
   "source": [
    "### Step 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7731b29",
   "metadata": {},
   "source": [
    "Execution Start time: 04:41:49, End time: 04:41:50 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052db634",
   "metadata": {},
   "source": [
    "**Thought**: The search results show a match in `download_xsum.py`, which is where we created the dataset files. The issue might be related to how the dataset is being referenced in the code.\n",
    "\n",
    "Let's check the `textbox/quick_start/experiment.py` file, which had a match for \"validation\", to see if there's any reference to the dataset file names that might need adjustment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46fc228",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3b7ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/TextBox/textbox/quick_start/experiment.py (142 lines total)]\n1:import os\n2:import logging\n3:from copy import copy\n4:from accelerate.logging import get_logger\n5:\n6:from typing import Optional, Tuple, Any, List, Dict\n7:\n8:from accelerate import Accelerator\n9:from torch.utils.data import DataLoader\n10:\n11:from ..config.configurator import Config\n12:from ..data.utils import data_preparation\n13:from ..trainer.trainer import Trainer\n14:from ..utils.dashboard import SummaryTracker\n15:from ..utils.logger import init_logger\n16:from ..utils.utils import get_model, get_tokenizer, get_trainer, init_seed\n17:\n18:ResultType = Dict[str, Any]\n19:\n20:\n21:class Experiment:\n22:    r\"\"\" A fast running api, which includes the complete process of\n23:    training and testing a model on a specified dataset\n24:\n25:    Args:\n26:        model (str): model name\n27:        dataset (str): dataset name\n28:        config_file_list (list): config files used to modify experiment parameters\n29:        config_dict (dict): parameters dictionary used to modify experiment parameters\n30:    \"\"\"\n31:\n32:    def __init__(\n33:        self,\n34:        model: Optional[str] = None,\n35:        dataset: Optional[str] = None,\n36:        config_file_list: Optional[List[str]] = None,\n37:        config_dict: Optional[Dict[str, Any]] = None,\n38:    ):\n39:        self.config = Config(model, dataset, config_file_list, config_dict)\n40:        wandb_setting = 'wandb ' + self.config['wandb']\n41:        os.system(wandb_setting)\n42:        self.__extended_config = None\n43:\n44:        from accelerate import DistributedDataParallelKwargs\n45:        ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=self.config['find_unused_parameters'])\n46:        self.accelerator = Accelerator(\n47:            gradient_accumulation_steps=self.config['accumulation_steps'], kwargs_handlers=[ddp_kwargs]\n48:        )\n49:        self.config.update({\n50:            '_is_local_main_process': self.accelerator.is_local_main_process,\n51:            'device': self.accelerator.device\n52:        })\n53:        self.logger = self.init_logger(self.config)\n54:        self.summary_tracker = SummaryTracker.basicConfig(self.get_config())\n55:        self.train_data, self.valid_data, self.test_data, self.tokenizer = \\\n56:            self._init_data(self.get_config(), self.accelerator)\n57:\n58:    def get_config(self) -> Config:\n59:        config = copy(self.config)\n60:        if self.__extended_config is not None:\n61:            config.update(self.__extended_config)\n62:        return config\n63:\n64:    @staticmethod\n65:    def init_logger(config: Config) -> logging.Logger:\n66:\n67:        # logger initialization\n68:        init_logger(\n69:            filename=config['filename'],\n70:            log_level=config['state'],\n71:            enabled=config['_is_local_main_process'],\n72:            saved_dir=config['saved_dir']\n73:        )\n74:        logger = get_logger(__name__)\n75:        logger.info(config)\n76:\n77:        return logger\n78:\n79:    @staticmethod\n80:    def _init_data(config: Config, accelerator: Accelerator) -> Tuple[DataLoader, DataLoader, DataLoader, Any]:\n81:        tokenizer = get_tokenizer(config)\n82:        train_data, valid_data, test_data = data_preparation(config, tokenizer)\n83:        train_data, valid_data, test_data = accelerator.prepare(train_data, valid_data, test_data)\n84:        return train_data, valid_data, test_data, tokenizer\n85:\n86:    def _on_experiment_start(self, extended_config: Optional[dict]):\n87:        \"\"\"(Re-)initialize configuration. Since for now config and trainer is modifiable, this\n88:        function is needed to ensure they were aligned to initial configuration.\n89:        \"\"\"\n90:        self.__extended_config = extended_config\n91:        config = self.get_config()\n92:        init_seed(config['seed'], config['reproducibility'])\n93:\n94:        self.model = get_model(config['model'])(config, self.tokenizer).to(config['device'])\n95:        self.logger.info(self.model)\n96:        self.trainer: Trainer = get_trainer(config['model'])(config, self.model, self.accelerator)\n97:        self.do_train = config['do_train']\n98:        self.do_valid = config['do_valid']\n99:        self.do_test = config['do_test']\n100:        self.valid_result: Optional[ResultType] = None\n(42 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open textbox/quick_start/experiment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50c1615",
   "metadata": {},
   "source": [
    "### Step 51"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c14bb3f",
   "metadata": {},
   "source": [
    "Execution Start time: 04:41:52, End time: 04:41:52 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309caaeb",
   "metadata": {},
   "source": [
    "**Thought**: The `Experiment` class in `experiment.py` uses the `data_preparation` function to load datasets, but it doesn't explicitly mention file names. The issue might be related to a default assumption in the code or another configuration setting.\n",
    "\n",
    "Let's check the `data_preparation` function in `textbox/data/utils.py` to see if there's any reference to the dataset file names that might need adjustment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e91de1",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c3fc48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/TextBox/textbox/data/utils.py (64 lines total)]\n1:from torch.utils.data import DataLoader\n2:from textbox.data.denoising_dataset import DenoisingCollate\n3:from ..data.unilm_dataset import UnilmCollate\n4:from textbox.data.abstract_dataset import AbstractDataset, AbstractCollate\n5:from accelerate.logging import get_logger\n6:\n7:collate_options = {'disabled': AbstractCollate, 'denoising': DenoisingCollate, 'unilm': UnilmCollate}\n8:\n9:\n10:def data_preparation(config, tokenizer):\n11:    collate_name = config['pretrain_task']\n12:    if config['model_name'] == 'unilm':\n13:        collate_name = 'unilm'\n14:    collate_fn = collate_options.get(collate_name, AbstractCollate)\n15:    logger = get_logger(__name__)\n16:    logger.info(f'Pretrain type: {collate_fn.get_type()}')\n17:\n18:    if config['dataset'] == 'multiwoz':\n19:        assert config['eval_batch_size'] % 3 == 0\n20:\n21:    dataloaders = []\n22:    if config['do_train']:\n23:        train_dataset = AbstractDataset(config, 'train')\n24:        train_dataset.tokenize(tokenizer)\n25:        train_dataloader = DataLoader(\n26:            train_dataset,\n27:            batch_size=config['train_batch_size'],\n28:            shuffle=True,\n29:            pin_memory=True,\n30:            collate_fn=collate_fn(config, tokenizer, 'train')\n31:        )\n32:        dataloaders.append(train_dataloader)\n33:    else:\n34:        dataloaders.append(None)\n35:\n36:    if config['do_valid']:\n37:        valid_dataset = AbstractDataset(config, 'valid')\n38:        valid_dataset.tokenize(tokenizer)\n39:        valid_dataloader = DataLoader(\n40:            valid_dataset,\n41:            batch_size=config['eval_batch_size'],\n42:            shuffle=False,\n43:            pin_memory=True,\n44:            collate_fn=collate_fn(config, tokenizer, 'valid')\n45:        )\n46:        dataloaders.append(valid_dataloader)\n47:    else:\n48:        dataloaders.append(None)\n49:\n50:    if config['do_test']:\n51:        test_dataset = AbstractDataset(config, 'test')\n52:        test_dataset.tokenize(tokenizer)\n53:        test_dataloader = DataLoader(\n54:            test_dataset,\n55:            batch_size=config['eval_batch_size'],\n56:            shuffle=False,\n57:            pin_memory=True,\n58:            collate_fn=collate_fn(config, tokenizer, 'test')\n59:        )\n60:        dataloaders.append(test_dataloader)\n61:    else:\n62:        dataloaders.append(None)\n63:\n64:    return dataloaders\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open textbox/data/utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72bbe4e",
   "metadata": {},
   "source": [
    "### Step 52"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362ba639",
   "metadata": {},
   "source": [
    "Execution Start time: 04:41:57, End time: 04:41:57 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec254d9a",
   "metadata": {},
   "source": [
    "**Thought**: The `data_preparation` function uses the `AbstractDataset` class to load datasets for training, validation, and testing. The `AbstractDataset` class is initialized with a split name ('train', 'valid', 'test'), which is used to determine the dataset file names.\n",
    "\n",
    "The issue might be that the code expects the validation file to be named `valid.src` instead of `validation.src`. Let's modify the `download_xsum.py` script to create files with the expected names.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24969cd",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c236589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Usage: edit <start_line>:<end_line>\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit download_xsum.py\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Create the dataset/xsum directory if it doesn't exist\n",
    "os.makedirs('dataset/xsum', exist_ok=True)\n",
    "\n",
    "# Load the XSum dataset\n",
    "dataset = load_dataset('xsum')\n",
    "\n",
    "# Save the first 10 rows of each set\n",
    "for split, filename in zip(['train', 'validation', 'test'], ['train', 'valid', 'test']):\n",
    "    with open(f'dataset/xsum/{filename}.src', 'w') as src_file, open(f'dataset/xsum/{filename}.tgt', 'w') as tgt_file:\n",
    "        for example in dataset[split].select(range(10)):\n",
    "            src_file.write(example['document'].replace('\\n', ' [SEP] ') + '\\n')\n",
    "            tgt_file.write(example['summary'].replace('\\n', ' [SEP] ') + '\\n')\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f85dfd",
   "metadata": {},
   "source": [
    "### Step 53"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e9d5e2",
   "metadata": {},
   "source": [
    "Execution Start time: 04:41:59, End time: 04:41:59 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94879ce8",
   "metadata": {},
   "source": [
    "**Thought**: I forgot to specify the line numbers for the edit command. Let's try again by specifying the correct line numbers to update the `download_xsum.py` script.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dbb2b5",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9a6e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/TextBox/download_xsum.py (15 lines total)]\n1:import os\n2:from datasets import load_dataset\n3:\n4:# Create the dataset/xsum directory if it doesn't exist\n5:os.makedirs('dataset/xsum', exist_ok=True)\n6:\n7:# Load the XSum dataset\n8:dataset = load_dataset('xsum')\n9:\n10:# Save the first 10 rows of each set\n11:for split in ['train', 'validation', 'test']:\n12:    with open(f'dataset/xsum/{split}.src', 'w') as src_file, open(f'dataset/xsum/{split}.tgt', 'w') as tgt_file:\n13:        for example in dataset[split].select(range(10)):\n14:            src_file.write(example['document'].replace('\\n', ' [SEP] ') + '\\n')\n15:            tgt_file.write(example['summary'].replace('\\n', ' [SEP] ') + '\\n')\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open download_xsum.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afefdd26",
   "metadata": {},
   "source": [
    "### Step 54"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19518e8",
   "metadata": {},
   "source": [
    "Execution Start time: 04:42:02, End time: 04:42:03 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d270f370",
   "metadata": {},
   "source": [
    "**Thought**: Let's edit the `download_xsum.py` script to create files with the expected names by changing 'validation' to 'valid'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d718f4",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423b5964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/TextBox/download_xsum.py (15 lines total)]\n1:import os\n2:from datasets import load_dataset\n3:\n4:# Create the dataset/xsum directory if it doesn't exist\n5:os.makedirs('dataset/xsum', exist_ok=True)\n6:\n7:# Load the XSum dataset\n8:dataset = load_dataset('xsum')\n9:\n10:# Save the first 10 rows of each set\n11:for split, filename in zip(['train', 'validation', 'test'], ['train', 'valid', 'test']):\n12:    with open(f'dataset/xsum/{filename}.src', 'w') as src_file, open(f'dataset/xsum/{filename}.tgt', 'w') as tgt_file:\n13:        for example in dataset[split].select(range(10)):\n14:            src_file.write(example['document'].replace('\\n', ' [SEP] ') + '\\n')\n15:            tgt_file.write(example['summary'].replace('\\n', ' [SEP] ') + '\\n')\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 11:15\n",
    "for split, filename in zip(['train', 'validation', 'test'], ['train', 'valid', 'test']):\n",
    "    with open(f'dataset/xsum/{filename}.src', 'w') as src_file, open(f'dataset/xsum/{filename}.tgt', 'w') as tgt_file:\n",
    "        for example in dataset[split].select(range(10)):\n",
    "            src_file.write(example['document'].replace('\\n', ' [SEP] ') + '\\n')\n",
    "            tgt_file.write(example['summary'].replace('\\n', ' [SEP] ') + '\\n')\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ba02bd",
   "metadata": {},
   "source": [
    "### Step 55"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0997886d",
   "metadata": {},
   "source": [
    "Execution Start time: 04:42:04, End time: 04:42:07 (duration: 2 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211b631e",
   "metadata": {},
   "source": [
    "**Thought**: The `download_xsum.py` script has been updated to create files with the expected names. Let's execute this script again to ensure the dataset files are correctly named.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018ae61e",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9121e6cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/usr/local/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for xsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/xsum\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python download_xsum.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272d9492",
   "metadata": {},
   "source": [
    "### Step 56"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ef211a",
   "metadata": {},
   "source": [
    "Execution Start time: 04:42:08, End time: 04:45:14 (duration: 185 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89fc74c",
   "metadata": {},
   "source": [
    "**Thought**: The dataset files have been successfully updated with the correct names. Now, let's proceed to run the model with the updated dataset loading logic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7855fdc1",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ad20e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "W&B online. Running your script from this directory will now sync to the cloud.\n22 Aug 04:42    INFO 66 parameters found.\n================================================================================\n\n# General Hyper Parameters: \n\ngpu_id: 0\nuse_gpu: True\ndevice: cpu\nseed: 2020\nreproducibility: True\ncmd: run_textbox.py --model=BART --dataset=xsum --model_path=facebook/bart-base --epochs=1\nfilename: BART-xsum-2024-Aug-22_04-42-13\nsaved_dir: saved/\nstate: INFO\nwandb: online\n\n\n# Training Hyper Parameters: \n\ndo_train: True\ndo_valid: True\noptimizer: adamw\nadafactor_kwargs: {'lr': 0.001, 'scale_parameter': False, 'relative_step': False, 'warmup_init': False}\noptimizer_kwargs: {}\nvalid_steps: 1\nvalid_strategy: epoch\nstopping_steps: 2\nepochs: 1\nlearning_rate: 3e-05\ntrain_batch_size: 4\ngrad_clip: 0.1\naccumulation_steps: 48\ndisable_tqdm: False\nresume_training: True\n\n\n# Evaluation Hyper Parameters: \n\ndo_test: True\nlower_evaluation: True\nmultiref_strategy: max\nbleu_max_ngrams: 4\nbleu_type: nltk\nsmoothing_function: 0\ncorpus_bleu: False\nrouge_max_ngrams: 2\nrouge_type: files2rouge\nmeteor_type: pycocoevalcap\nchrf_type: m-popovic\ndistinct_max_ngrams: 4\ninter_distinct: True\nunique_max_ngrams: 4\nself_bleu_max_ngrams: 4\ntgt_lang: en\nmetrics: ['rouge']\neval_batch_size: 16\ncorpus_meteor: True\n\n\n# Model Hyper Parameters: \n\nmodel: BART\nmodel_name: bart\nmodel_path: facebook/bart-base\nconfig_kwargs: {}\ntokenizer_kwargs: {'use_fast': True}\ngeneration_kwargs: {'num_beams': 5, 'no_repeat_ngram_size': 3, 'early_stopping': True}\nefficient_kwargs: {}\nefficient_methods: []\nefficient_unfreeze_model: False\nlabel_smoothing: 0.1\n\n\n# Dataset Hyper Parameters: \n\ndataset: xsum\ndata_path: dataset/xsum\ntgt_lang: en\nsrc_len: 1024\ntgt_len: 128\ntruncate: tail\nprefix_prompt: Summarize: \nmetrics_for_best_model: ['rouge-1', 'rouge-2', 'rouge-l']\n\n\n# Unrecognized Hyper Parameters: \n\nfind_unused_parameters: False\ntokenizer_add_tokens: []\nload_type: from_pretrained\n\n================================================================================\n/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n22 Aug 04:42    INFO Pretrain type: pretrain disabled\nwandb: Tracking run with wandb version 0.17.7\nwandb: W&B syncing is set to `offline` in this directory.  \nwandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.\n/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\nmodel.safetensors:   0%|                             | 0.00/558M [00:00<?, ?B/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n22 Aug 04:42    INFO Pretrained_Models(\n  (model): BartForConditionalGeneration(\n    (model): BartModel(\n      (shared): Embedding(50265, 768, padding_idx=1)\n      (encoder): BartEncoder(\n        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n        (layers): ModuleList(\n          (0-5): 6 x BartEncoderLayer(\n            (self_attn): BartSdpaAttention(\n            )\n            (activation_fn): GELUActivation()\n          )\n        )\n      )\n      (decoder): BartDecoder(\n        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n        (layers): ModuleList(\n          (0-5): 6 x BartDecoderLayer(\n            (self_attn): BartSdpaAttention(\n            )\n            (activation_fn): GELUActivation()\n            (encoder_attn): BartSdpaAttention(\n            )\n          )\n        )\n      )\n    )\n  )\n)\nTrainable parameters: 139420416\n22 Aug 04:42    INFO ====== Start training ======\n\ntrain    1:   0%|                            | 0/1 [00:00<?, ?step/s, loss=None]\ntrain    1: 100%|████████████████████| 1/1 [00:26<00:00, 26.08s/step, loss=None]\ntrain    1: 100%|████████████████████| 1/1 [00:26<00:00, 26.08s/step, loss=5.06]\ntrain    1: 100%|████████████████████| 1/1 [00:26<00:00, 26.08s/step, loss=5.06]\n22 Aug 04:42    INFO Train epoch  1 [time: 26.08s, loss: 5.06]\n\ngenerating:   0%|                                         | 0/1 [00:00<?, ?it/s]\ngenerating: 100%|█████████████████████████████████| 1/1 [01:05<00:00, 65.83s/it]\ngenerating: 100%|█████████████████████████████████| 1/1 [01:05<00:00, 65.83s/it]\n22 Aug 04:43    INFO  Validation  1 (best) [time: 66.08s, score: 33.16, <rouge-1: 17.04>, <rouge-2: 2.25>, <rouge-l: 13.87>, rouge-w-1.2: 9.45]\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n22 Aug 04:43    INFO Soft link created: saved/BART-xsum-2024-Aug-22_04-42-13/checkpoint_best -> /content/TextBox/saved/BART-xsum-2024-Aug-22_04-42-13/checkpoint_epoch-1\n22 Aug 04:43    INFO ====== Finished training, best validation result at train epoch 1 ======\n22 Aug 04:43    INFO Best valid result: score: 33.16, <rouge-1: 17.04>, <rouge-2: 2.25>, <rouge-l: 13.87>, rouge-w-1.2: 9.45\n22 Aug 04:43    INFO Loading model structure and parameters from saved/BART-xsum-2024-Aug-22_04-42-13/checkpoint_best ...\n\ngenerating:   0%|                                         | 0/1 [00:00<?, ?it/s]\ngenerating: 100%|█████████████████████████████████| 1/1 [01:15<00:00, 75.43s/it]\ngenerating: 100%|█████████████████████████████████| 1/1 [01:15<00:00, 75.43s/it]\n22 Aug 04:45    INFO Evaluation result:\n score: 33.49,\n <rouge-1: 16.42>,\n <rouge-2: 2.81>,\n <rouge-l: 14.26>,\n rouge-w-1.2: 9.62\n22 Aug 04:45    INFO Epoch   (best) [time: 75.97s, score: 33.49, <rouge-1: 16.42>, <rouge-2: 2.81>, <rouge-l: 14.26>, rouge-w-1.2: 9.62]\nwandb: - 0.000 MB of 0.000 MB uploaded\nwandb:                                                                                \nwandb: \nwandb: Run history:\nwandb:          loss/train ▁\nwandb:     metrics/rouge-1 ▁\nwandb:     metrics/rouge-2 ▁\nwandb:     metrics/rouge-l ▁\nwandb: metrics/rouge-w-1.2 ▁\nwandb:        test/rouge-1 ▁\nwandb:        test/rouge-2 ▁\nwandb:        test/rouge-l ▁\nwandb:    test/rouge-w-1.2 ▁\nwandb: \nwandb: Run summary:\nwandb:          loss/train 4.68944\nwandb:     metrics/rouge-1 17.04\nwandb:     metrics/rouge-2 2.25\nwandb:     metrics/rouge-l 13.87\nwandb: metrics/rouge-w-1.2 9.45\nwandb:        test/rouge-1 16.42\nwandb:        test/rouge-2 2.81\nwandb:        test/rouge-l 14.26\nwandb:    test/rouge-w-1.2 9.62\nwandb: \nwandb: You can sync this run to the cloud by running:\nwandb: wandb sync saved/BART-xsum-2024-Aug-22_04-42-13/wandb/offline-run-20240822_044215-wn4pc0f0\nwandb: Find logs at: saved/BART-xsum-2024-Aug-22_04-42-13/wandb/offline-run-20240822_044215-wn4pc0f0/logs\nwandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python run_textbox.py --model=BART --dataset=xsum --model_path=facebook/bart-base --epochs=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615a017d",
   "metadata": {},
   "source": [
    "### Step 57"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533738fc",
   "metadata": {},
   "source": [
    "Execution Start time: 04:45:16, End time: 04:45:16 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b7d1aa",
   "metadata": {},
   "source": [
    "**Thought**: The model has been successfully trained and evaluated on the XSum dataset. The evaluation results provide the following metrics:\n",
    "\n",
    "- Rouge-1: 16.42\n",
    "- Rouge-2: 2.81\n",
    "- Rouge-L: 14.26\n",
    "- Rouge-w-1.2: 9.62\n",
    "\n",
    "Let's format these results as a JSON object and submit the answer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779b0561",
   "metadata": {},
   "source": [
    "**Action (submit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459bfb67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{rouge-1: 16.42, rouge-2: 2.81, rouge-L: 14.26, rouge-w-1.2: 9.62}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit {\"rouge-1\": 16.42, \"rouge-2\": 2.81, \"rouge-L\": 14.26, \"rouge-w-1.2\": 9.62}"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
