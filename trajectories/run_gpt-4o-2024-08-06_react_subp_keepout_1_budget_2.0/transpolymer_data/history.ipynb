{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd40dc64",
   "metadata": {},
   "source": [
    "### Query\n",
    "Pre-train the TransPolymer model on \"data/pretrain.csv\" using the data in the repository, the config.yaml configuration and the Pretrain.py script provided. Report the last training loss as a json structured as follows: {\"train_loss\": 0.0} (replace 0.0 with the actual value).\n",
    "\n",
    "Additional instructions:\n",
    "1. Load only the first 10 rows of in the training dataset\n",
    "2. Train only one epoch.\n",
    "3. Do not use distributed training, train on a single CPU with xpu_backend set to `gloo`.\n",
    "4. Set the batch size to 1.\n",
    "5. Set pytorch random seed to 0.\n",
    "\n",
    "Git repository: https://github.com/ChangwenXu98/TransPolymer\n",
    "\n",
    "I have already executed some of the required steps. However, I did not yet make any required changes to load the request dataset. Your goal is to successfully load the dataset and complete the remaining steps to achieve the user request. You can re-run any of the pre-executed steps, and finally should successfully run\n",
    "```\n",
    "# Without the --nproc_per_node argument\n",
    "!CUDA_VISIBLE_DEVICES=\",\" python Pretrain.py\n",
    "```\n",
    "to submit the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475b57f1",
   "metadata": {},
   "source": [
    "### Prompt\n",
    "<details>\n",
    "[{'role': 'system', 'content': 'Interact in a jupyter notebook to solve the request.\\nOutput a thought and an action.\\nThought can reason about the current situation, and action allows you to interact with the Jupyter notebook. There are three types of actions:\\n(1) execute, which executes a notebook cell with the command under the \"content\" field. This could be a bash command (beginning with !), a python command, or magic command (beginning with %).\\n(2) edit, which allows you to replace existing lines in a file with new lines. To replace lines, you will need to provide the lines as they appear before the replacement, and the lines to replace them with.\\nIn the \"content\" field, format should be ```{filename}\\n<<<BEFORE_EDIT>>>\\nlines before edit\\nlines before edit\\n<<<AFTER_EDIT>>>\\nlines after edit\\nlines after edit```\\n  - Include the entire lines WITH indentation. After editing, make sure indentation is correct.\\n(3) submit, which submits the final answer (or null if there\\'s no answer to submit), formatted as JSON (make sure to use double quotes for keys and values).\\nAlways output exactly one \"thought\" and one \"action\", and encapsulate the content with <content>...</content> tags.\\nI will provide the cell output (observation) for you.\\n\\nTips:\\n- When a github repository is given, you should start by cloning it. You can read the readme file and browse the repo for installation and usage instructions.\\n- Avoid using conda, instead use pip for package management.\\n- Each execution has a 5-minute timeout. After this period, you can decide whether to interrupt the kernel or continue waiting.\\n  You should interrupt if: (1) you are prompted with an interactive message (e.g., \"Do you want to continue? [Y/n]\"). In that case, you can bypass it by executing echo \"y\" | command, or (2) the execution is taking longer than expected because the wrong task is being executed, such as a different model or incorrect number of epochs.\\n  You should continue to wait if the current execution is correct and proceeding as expected.\\n\\nHere are some examples.\\nGit repository: EleutherAI/lm-evaluation-harness\\nQuery: Evaluate EleutherAI/gpt-j-6B on hellaswag. What\\'s the score?\\nThought: First, I need to clone the GitHub repository \\'EleutherAI/lm-evaluation-harness\\'.\\nAction: execute\\n<content>\\n!git clone https://github.com/EleutherAI/lm-evaluation-harness.git\\n</content>\\nObservation: ```\\nCloning into \\'lm-evaluation-harness\\'...\\n```\\nThought: The repository has been cloned. Next, I should check the README file for installation and usage instructions to proceed with evaluating EleutherAI/gpt-j-6B on hellaswag.\\nAction: execute\\n<content>\\n!cat lm-evaluation-harness/README.md\\n</content>\\nObservation: ```# Language Model Evaluation Harness ... truncated ...\\n```\\nThought: The README provides detailed instructions for installing the `lm-eval` package and evaluating models like GPT-J-6B on tasks like hellaswag. I need to install the package and its dependencies first.\\nAction: execute\\n<content>\\n!pip install -e lm-evaluation-harness/.\\n</content>\\nObservation: ```\\nObtaining file:///home/nora/lm-evaluation-harness\\n  Installing build dependencies ... truncated ...\\n```\\nThought: The `lm-eval` package and its dependencies have been installed successfully. Now, I can proceed with evaluating the EleutherAI/gpt-j-6B model on the hellaswag task as per the instructions in the README.\\nAction: execute\\n<content>\\n!lm_eval --model hf --model_args pretrained=EleutherAI/gpt-j-6B --tasks hellaswag --device cuda:0 --batch_size 8\\n</content>\\nObservation: ```\\n{\\n  \"hellaswag\": {\\n    \"acc\": 0.24603658536585363,\\n    \"acc_norm\": 0.34436059706446426,\\n  },  ... truncated ...\\n```\\nThought: The `lm-eval` was executed successfully, providing the metrics I needed. The `acc` metric is 0.2460. The `acc_norm` metric is 0.3444\\nAction: submit\\n<content>\\n{\"acc\": 0.2460, \"acc_norm\": 0.3444}\\n</content>\\n\\nQuery: use pandas to read the file \\'data.csv\\' and display the first 2 rows.\\nThought: First, I need to read the file \\'data.csv\\' using the pandas library.\\nAction: execute\\n<content>\\nimport pandas as pd\\npd.read_csv(\\'data.csv\\').head()\\n</content>\\nObservation: ```\\n    name\\n  0  Alice\\n   1    Bob\\n  2  Charlie\\n```\\nThought: The file \\'data.csv\\' has been read successfully and the first 2 rows have been displayed.\\nAction: submit\\n<content>\\n[\"Alice\", \"Bob\"]\\n</content>\\n\\nQuery: Change the model name in the \\'config.json\\' file from \\'roberta-base\\' to \\'roberta-large\\'.\\nThought: First, I need to read the \\'config.json\\' file to find the location where I can replace the model name.\\nAction: execute\\n<content>\\n!cat config.json\\n</content>\\nObservation: ```\\n{\\n    \"model\": \"roberta-base\",\\n    \"batch_size\": 8,\\n    \"epochs\": 10\\n}\\n```\\nThought: The file has been read successfully. Now, I can replace the model name in the \\'config.json\\' file.\\nAction: edit\\n<content>\\nconfig.json\\n<<<BEFORE_EDIT>>>\\n{\\n    \"model\": \"roberta-base\",\\n    \"batch_size\": 8,\\n<<<AFTER_EDIT>>>\\n{\\n    \"model\": \"roberta-large\",\\n    \"batch_size\": 8,\\n</content>\\nObservation: \"editing file config.json\"\\nThought: The model name has been replaced in the \\'config.json\\' file.\\nAction: submit\\n<content>\\nnull\\n</content>\\n'}, {'role': 'user', 'content': 'Query: Pre-train the TransPolymer model on \"data/pretrain.csv\" using the data in the repository, the config.yaml configuration and the Pretrain.py script provided. Report the last training loss as a json structured as follows: {\"train_loss\": 0.0} (replace 0.0 with the actual value).\\n\\nAdditional instructions:\\n1. Load only the first 10 rows of in the training dataset\\n2. Train only one epoch.\\n3. Do not use distributed training, train on a single CPU with xpu_backend set to `gloo`.\\n4. Set the batch size to 1.\\n5. Set pytorch random seed to 0.\\n\\nGit repository: https://github.com/ChangwenXu98/TransPolymer\\n\\nI have already executed some of the required steps. However, I did not yet make any required changes to load the request dataset. Your goal is to successfully load the dataset and complete the remaining steps to achieve the user request. You can re-run any of the pre-executed steps, and finally should successfully run\\n```\\n# Without the --nproc_per_node argument\\n!CUDA_VISIBLE_DEVICES=\",\" python Pretrain.py\\n```\\nto submit the answer.'}]\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f965475",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc796de0",
   "metadata": {},
   "source": [
    "Execution Start time: 00:17:43, End time: 00:17:43 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37efd8f4",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de29f7a",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e63596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Repository Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5b1f15",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ebc720",
   "metadata": {},
   "source": [
    "Execution Start time: 00:17:43, End time: 00:17:43 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cca6873",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690a1163",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab6be0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cloning into 'TransPolymer'...\nremote: Enumerating objects: 177, done.\u001b[K\nremote: Counting objects: 100% (26/26)\u001b[K\nremote: Counting objects: 100% (26/26), done.\u001b[K\nremote: Compressing objects: 100% (14/14)\u001b[K\nremote: Compressing objects: 100% (14/14), done.\u001b[K\nremote: Total 177 (delta 16), reused 12 (delta 12), pack-reused 151 (from 1)\u001b[K\nReceiving objects: 100% (177/177)\nReceiving objects: 100% (177/177), 1.68 MiB | 45.14 MiB/s, done.\nResolving deltas: 100% (70/70)\nResolving deltas: 100% (70/70), done.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!git clone https://github.com/ChangwenXu98/TransPolymer.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a01d21",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144ba9db",
   "metadata": {},
   "source": [
    "Execution Start time: 00:17:43, End time: 00:17:43 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53408693",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5841e8b",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f64608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/TransPolymer\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd TransPolymer/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e55c2b5",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de95ff7f",
   "metadata": {},
   "source": [
    "Execution Start time: 00:17:43, End time: 00:17:44 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cfc032",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5297a50e",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4000ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attention_vis.py              Pretrain.py  config_attention.yaml  dataset.py\nDownstream.py                 README.md    config_finetune.yaml   figs/\nLICENSE.md                    ckpt/        config_tSNE.yaml       tSNE.py\nPolymerSmilesTokenization.py  config.yaml  data/\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b79dc5",
   "metadata": {},
   "source": [
    "### Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6670b6de",
   "metadata": {},
   "source": [
    "Execution Start time: 00:17:44, End time: 00:17:44 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe2ec4e",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6fd6fb",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0573bf2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5K\tckpt/pretrain.pt\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pretrained model\n",
    "!du -sh ckpt/pretrain.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0697cd0c",
   "metadata": {},
   "source": [
    "### Step 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6b8d51",
   "metadata": {},
   "source": [
    "Execution Start time: 00:17:44, End time: 00:17:44 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ecc6b3",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbd75bc",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347352ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Package Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d918e84c",
   "metadata": {},
   "source": [
    "### Step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7631c14e",
   "metadata": {},
   "source": [
    "Execution Start time: 00:17:44, End time: 00:17:49 (duration: 5 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21580640",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69720389",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea5b598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting rdkit\n  Downloading rdkit-2024.3.5-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.9 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from rdkit) (1.25.2)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/site-packages (from rdkit) (10.3.0)\nDownloading rdkit-2024.3.5-cp310-cp310-manylinux_2_28_x86_64.whl (33.1 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/33.1 MB ? eta -:--:--\n\n\n\n\n\n\u001b[?25hInstalling collected packages: rdkit\nSuccessfully installed rdkit-2024.3.5\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install rdkit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51061016",
   "metadata": {},
   "source": [
    "### Step 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9886c3",
   "metadata": {},
   "source": [
    "Execution Start time: 00:17:49, End time: 00:18:00 (duration: 11 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126e3f06",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f715ed",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2a4e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting transformers==4.20.1\n  Downloading transformers-4.20.1-py3-none-any.whl.metadata (77 kB)\n\u001b[?25l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/77.3 kB ? eta -:--:--\n\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from transformers==4.20.1) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/site-packages (from transformers==4.20.1) (0.23.3)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from transformers==4.20.1) (1.25.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from transformers==4.20.1) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers==4.20.1) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers==4.20.1) (2023.12.25)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers==4.20.1) (2.31.0)\nCollecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers==4.20.1)\n  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.5 kB)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/site-packages (from transformers==4.20.1) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.20.1) (2023.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.20.1) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.20.1) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.20.1) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.20.1) (2.0.7)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.20.1) (2024.6.2)\nDownloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/4.4 MB ? eta -:--:--\n\n\u001b[?25hDownloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/6.6 MB ? eta -:--:--\n\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.1\n    Uninstalling tokenizers-0.19.1:\n      Successfully uninstalled tokenizers-0.19.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.40.2\n    Uninstalling transformers-4.40.2:\n      Successfully uninstalled transformers-4.40.2\nSuccessfully installed tokenizers-0.12.1 transformers-4.20.1\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: newer version breaks the code\n",
    "!pip install transformers==4.20.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b457fefb",
   "metadata": {},
   "source": [
    "### Step 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae9f2a2",
   "metadata": {},
   "source": [
    "Execution Start time: 00:18:00, End time: 00:18:00 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67749bfd",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaba6c6c",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588c3e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For distriubted NN training\n",
    "# !pip install fairscale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425cf8a3",
   "metadata": {},
   "source": [
    "### Step 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e555aef",
   "metadata": {},
   "source": [
    "Execution Start time: 00:18:00, End time: 00:18:02 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc05f240",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a974eb",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70460d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch                     2.2.1+cpu\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip list | grep torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b639ad5",
   "metadata": {},
   "source": [
    "### Step 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8350b3e",
   "metadata": {},
   "source": [
    "Execution Start time: 00:18:02, End time: 00:18:07 (duration: 5 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bced0792",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcdc949",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78f1bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Looking in links: https://developer.intel.com/ipex-whl-stable-cpu\nCollecting oneccl_bind_pt==2.3.0\n\u001b[?25l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/41.3 MB ? eta -:--:--\n\n\n\n\n\n\n\n\n\n\n\n\u001b[?25hInstalling collected packages: oneccl_bind_pt\nSuccessfully installed oneccl_bind_pt-2.3.0+cpu\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CCL for multi GPU training\n",
    "!pip install oneccl_bind_pt==2.3.0 -f https://developer.intel.com/ipex-whl-stable-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff179c96",
   "metadata": {},
   "source": [
    "### Step 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d2650f",
   "metadata": {},
   "source": [
    "Execution Start time: 00:18:07, End time: 00:18:07 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820832d0",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff5a51c",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d592d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Configuration Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fff74e1",
   "metadata": {},
   "source": [
    "### Step 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2557f5b8",
   "metadata": {},
   "source": [
    "Execution Start time: 00:18:07, End time: 00:18:08 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa451dd",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af506641",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbf677c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file_path: 'data/pretrain.csv'          # pretrain data path\n\n\n\nepochs: 1                              # total number of epochs\n\nbatch_size: 1                         # batch size\n\nlr_rate: 0.00005                        # learning rate\n\nscheduler_type: 'linear'                # scheduler type\n\nweight_decay: 0.0                       # weight decay for AdamW\n\nwarmup_ratio: 0.05                      # warm-up ratio for scheduler\n\nsave_strategy: 'epoch'                  # save strategy of trainer\n\noverwrite_output_dir: True              # whether to overwrite output directory (i.e. True/False)\n\nsave_total_limit: 3                    # save total limit of trainer\n\nfp16: False                              # float precision 16 (i.e. True/False)\n\nlogging_strategy: 'epoch'               # logging frequency\n\nevaluation_strategy: 'epoch'            # validation frequency\n\nreport_to: 'tensorboard'                # integrations to report the results and logs to\n\ndataloader_num_workers: 2              # Number of subprocesses to use for data loading\n\nsharded_ddp: False                   # option of Sharded DDP training\n\nsave_path: 'ckpt/pretrain.pt'           # logging and save path of the pretrained model\n\nload_checkpoint: False\n\n\n\nmax_position_embeddings: 514            # max position embeddings of Transformer\n\nblocksize: 175                          # max length of sequences after tokenization\n\nnum_attention_heads: 12                 # number of attention heads in each hidden layer\n\nnum_hidden_layers: 6                    # number of hidden layers\n\nhidden_dropout_prob: 0.1                # hidden layer dropout\n\nattention_probs_dropout_prob: 0.1       # attention dropout\n\nmlm_probability: 0.15                   # masked probability in mlm\n\n\n\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set number of epochs to 1\n",
    "!sed -i 's/epochs: 30/epochs: 1/' config.yaml\n",
    "\n",
    "# set batch size to 1\n",
    "!sed -i 's/batch_size: 100/batch_size: 1/' config.yaml\n",
    "\n",
    "# set number of workers to 2\n",
    "!sed -i 's/dataloader_num_workers: 18/dataloader_num_workers: 2/' config.yaml\n",
    "\n",
    "# set fp16 to False\n",
    "!sed -i 's/fp16: True/fp16: False/' config.yaml\n",
    "\n",
    "# set sharding to True\n",
    "# !sed -i 's/sharded_ddp: False/sharded_ddp: True/' config.yaml\n",
    "\n",
    "!cat config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795b3da5",
   "metadata": {},
   "source": [
    "### Step 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed489d2",
   "metadata": {},
   "source": [
    "Execution Start time: 00:18:08, End time: 00:18:08 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563c7830",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fc84ce",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721012ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c8a023",
   "metadata": {},
   "source": [
    "### Step 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c52d14",
   "metadata": {},
   "source": [
    "Execution Start time: 00:18:08, End time: 00:18:09 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72861951",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8205be12",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99469d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        weight_decay=pretrain_config['weight_decay'],\n\n        warmup_ratio=pretrain_config['warmup_ratio'],\n\n        report_to=pretrain_config['report_to'],\n\n        dataloader_num_workers=pretrain_config['dataloader_num_workers'],\n\n        sharded_ddp=pretrain_config['sharded_ddp'],\n\n        xpu_backend=\"gloo\", no_cuda=True,\n    )\n\n\n\n    \"\"\"Set Trainer\"\"\"\n\n    trainer = Trainer(\n\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enable fsdp (fully sharded data parallel)\n",
    "# !sed -i '78i\\        fsdp=\"full_shard offload\"' Pretrain.py\n",
    "!sed -i '78i\\        xpu_backend=\"gloo\", no_cuda=True,' Pretrain.py\n",
    "# !sed -i '78i\\        no_cuda=True,' Pretrain.py\n",
    "# !sed -i '78d' Pretrain.py\n",
    "!head -82 Pretrain.py | tail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce40b48",
   "metadata": {},
   "source": [
    "### Step 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8906dda1",
   "metadata": {},
   "source": [
    "Execution Start time: 00:18:09, End time: 00:18:09 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ea7818",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4d01a2",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c598dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n\"\"\"Import PolymerSmilesTokenizer from PolymerSmilesTokenization.py\"\"\"\nfrom PolymerSmilesTokenization import PolymerSmilesTokenizer\n\n\"\"\"Import LoadPretrainData\"\"\"\nfrom dataset import LoadPretrainData\n\n\"\"\"Device\"\"\"\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntorch.cuda.is_available() #checking if CUDA + Colab GPU works\n\n# set seeds\ntorch.manual_seed(0)\nimport random\nrandom.seed(0)\nimport numpy as np\nnp.random.seed(0)\n\n\"\"\"train-validation split\"\"\"\ndef split(file_path):\n    dataset = pd.read_csv(file_path, header=None).values\n    train_data, valid_data = train_test_split(dataset, test_size=0.2, random_state=1)\n    return train_data, valid_data\n\ndef main(pretrain_config):\n    \"\"\"Use Roberta configuration\"\"\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/TransPolymer/Pretrain.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.is_available() #checking if CUDA + Colab GPU works\n",
    "\n",
    "\"\"\"train-validation split\"\"\"\n",
    "def split(file_path):\n",
    "<<<AFTER_EDIT>>>\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.is_available() #checking if CUDA + Colab GPU works\n",
    "\n",
    "# set seeds\n",
    "torch.manual_seed(0)\n",
    "import random\n",
    "random.seed(0)\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "\"\"\"train-validation split\"\"\"\n",
    "def split(file_path):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e412ae4",
   "metadata": {},
   "source": [
    "### Step 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0922ed97",
   "metadata": {},
   "source": [
    "Execution Start time: 00:18:09, End time: 00:18:09 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf15ac2",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2fcb40",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a044516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Training \n",
    "# Tried to change the code inside Pretrain.py to use 'gloo' backend. Couldn't make it work! Looks like transformers Trainer only supports CCL & MPI libraries for mult-cpu training. Reference: https://huggingface.co/docs/transformers/en/perf_train_cpu_many#usage-in-trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9508c491",
   "metadata": {},
   "source": [
    "### Step 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e452f8c6",
   "metadata": {},
   "source": [
    "Execution Start time: 00:18:09, End time: 00:18:09 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdf7151",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7227057",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764ffada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Training with 1 CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fa157e",
   "metadata": {},
   "source": [
    "### Step 19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12294af5",
   "metadata": {},
   "source": [
    "Execution Start time: 00:18:11, End time: 00:18:12 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f5c49d",
   "metadata": {},
   "source": [
    "**Thought**: I need to modify the dataset loading part in the `Pretrain.py` script to ensure it loads only the first 10 rows of the training dataset. This will help in fulfilling the user's request to train on a limited dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d319f544",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6475934c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\nrandom.seed(0)\nimport numpy as np\nnp.random.seed(0)\n\n\"\"\"train-validation split\"\"\"\ndef split(file_path):\n    dataset = pd.read_csv(file_path, header=None).values\n    train_data, valid_data = train_test_split(dataset[:10], test_size=0.2, random_state=1)\n    return train_data, valid_data\n\ndef main(pretrain_config):\n    \"\"\"Use Roberta configuration\"\"\"\n    config = RobertaConfig(\n        vocab_size=50265,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pretrain.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "    train_data, valid_data = train_test_split(dataset, test_size=0.2, random_state=1)\n",
    "<<<AFTER_EDIT>>>\n",
    "    train_data, valid_data = train_test_split(dataset[:10], test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8a3167",
   "metadata": {},
   "source": [
    "### Step 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea4c9e6",
   "metadata": {},
   "source": [
    "Execution Start time: 00:18:13, End time: 00:18:28 (duration: 14 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0428a0be",
   "metadata": {},
   "source": [
    "**Thought**: The dataset loading part in the `Pretrain.py` script has been modified to load only the first 10 rows. Now, I can proceed to run the training script with the specified configurations to obtain the last training loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97edeef8",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855494e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\nDownloading:   0%|                                   | 0.00/878k [00:00<?, ?B/s]\n\n\nDownloading:   0%|                                   | 0.00/446k [00:00<?, ?B/s]\n\n\nDownloading:   0%|                                   | 0.00/25.0 [00:00<?, ?B/s]\nDownloading: 100%|████████████████████████████| 25.0/25.0 [00:00<00:00, 222kB/s]\n\nDownloading:   0%|                                    | 0.00/481 [00:00<?, ?B/s]\n\nThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \nThe class this function is called from is 'PolymerSmilesTokenizer'.\n/usr/local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n***** Running training *****\n  Num examples = 2\n  Num Epochs = 1\n  Instantaneous batch size per device = 1\n  Total train batch size (w. parallel, distributed & accumulation) = 1\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2\n\n  0%|                                                     | 0/2 [00:00<?, ?it/s]\n 50%|██████████████████████▌                      | 1/2 [00:02<00:02,  2.89s/it]\n100%|█████████████████████████████████████████████| 2/2 [00:04<00:00,  1.90s/it]\n                                                                                \n{'loss': 5.2776, 'learning_rate': 0.0, 'epoch': 1.0}\n\n100%|█████████████████████████████████████████████| 2/2 [00:04<00:00,  1.90s/it]***** Running Evaluation *****\n  Num examples = 1\n  Batch size = 1\n\n\n  0%|                                                     | 0/1 [00:00<?, ?it/s]\u001b[A\n                                                                                \n\n\n                                                                                \n\u001b[A{'eval_loss': 11.016891479492188, 'eval_runtime': 0.4526, 'eval_samples_per_second': 2.209, 'eval_steps_per_second': 2.209, 'epoch': 1.0}\n\n100%|█████████████████████████████████████████████| 2/2 [00:04<00:00,  1.90s/it]\n\n100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 115.02it/s]\u001b[A\n\n                                                                                \u001b[ASaving model checkpoint to ckpt/pretrain.pt/checkpoint-2\nConfiguration saved in ckpt/pretrain.pt/checkpoint-2/config.json\nModel weights saved in ckpt/pretrain.pt/checkpoint-2/pytorch_model.bin\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n\n                                                                                \n{'train_runtime': 5.2939, 'train_samples_per_second': 0.378, 'train_steps_per_second': 0.378, 'train_loss': 5.277614116668701, 'epoch': 1.0}\n\n100%|█████████████████████████████████████████████| 2/2 [00:05<00:00,  1.90s/it]\n100%|█████████████████████████████████████████████| 2/2 [00:05<00:00,  2.64s/it]\nSaving model checkpoint to ckpt/pretrain.pt\nConfiguration saved in ckpt/pretrain.pt/config.json\nModel weights saved in ckpt/pretrain.pt/pytorch_model.bin\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=\",\" python Pretrain.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f56350c",
   "metadata": {},
   "source": [
    "### Step 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1710e9f",
   "metadata": {},
   "source": [
    "Execution Start time: 00:18:29, End time: 00:18:29 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf8996f",
   "metadata": {},
   "source": [
    "**Thought**: The training has been completed successfully with the specified configurations. The last training loss is 5.2776. I will now submit this result in the required JSON format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4091daf",
   "metadata": {},
   "source": [
    "**Action (submit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ab4cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'train_loss': 5.2776}"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
