{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61de8e22",
   "metadata": {},
   "source": [
    "### System Prompt\n",
    "<details>\n",
    "SETTING: You are an autonomous programmer, and you're working directly in the command line with a special Jupyter notebook interface.\n",
    "\n",
    "The special Jupyter notebook interface consists of a file editor that shows you 100 lines of a file at a time.\n",
    "You can execute commands in the notebook using:\n",
    "\n",
    "1. Bash commands: Commands starting with !.\n",
    "2. Python commands: Standard Python code.\n",
    "3. Magic commands: Commands starting with %, e.g., %cd <path>.\n",
    "\n",
    "Additionally, you can also use the following commands to help you navigate and edit files.\n",
    "\n",
    "COMMANDS:\n",
    "\n",
    "open:\n",
    "  docstring: opens the file at the given path in the editor. If line_number is provided, the window will be move to include that line\n",
    "  signature: open <path> [<line_number>]\n",
    "  arguments:\n",
    "    - path (string) [required]: the path to the file to open\n",
    "    - line_number (integer) [optional]: the line number to move the window to (if not provided, the window will start at the top of the file)\n",
    "\n",
    "create:\n",
    "  docstring: creates and opens a new file with the given name\n",
    "  signature: create <filename>\n",
    "  arguments:\n",
    "    - filename (string) [required]: the name of the file to create\n",
    "\n",
    "goto:\n",
    "  docstring: moves the window to show <line_number>\n",
    "  signature: goto <line_number>\n",
    "  arguments:\n",
    "    - line_number (integer) [required]: the line number to move the window to\n",
    "\n",
    "scroll_down:\n",
    "  docstring: moves the window down {WINDOW} lines\n",
    "  signature: scroll_down\n",
    "\n",
    "scroll_up:\n",
    "  docstring: moves the window down {WINDOW} lines\n",
    "  signature: scroll_up\n",
    "\n",
    "submit:\n",
    "  docstring: submits your final answer\n",
    "  signature: submit <answer>\n",
    "  arguments:\n",
    "    - answer (string) [required]: the answer to be submitted\n",
    "\n",
    "search_dir:\n",
    "  docstring: searches for search_term in all files in dir_path. If dir_path is not provided, searches in the current directory\n",
    "  signature: search_dir <search_term> [<dir_path>]\n",
    "  arguments:\n",
    "    - search_term (string) [required]: the term to search for\n",
    "    - dir_path (string) [optional]: the directory to search in (if not provided, searches in the current directory)\n",
    "\n",
    "search_file:\n",
    "  docstring: searches for search_term in file. If file is not provided, searches in the current open file\n",
    "  signature: search_file <search_term> [<file>]\n",
    "  arguments:\n",
    "    - search_term (string) [required]: the term to search for\n",
    "    - file (string) [optional]: the file to search in (if not provided, searches in the current open file)\n",
    "\n",
    "find_file:\n",
    "  docstring: finds all files with the given name in dir_path. If dir_path is not provided, searches in the current directory\n",
    "  signature: find_file <file_name> [<dir_path>]\n",
    "  arguments:\n",
    "    - file_name (string) [required]: the name of the file to search for\n",
    "    - dir_path (string) [optional]: the directory to search in (if not provided, searches in the current directory)\n",
    "\n",
    "edit:\n",
    "  docstring: replaces lines <start_line> through <end_line> (inclusive) with the given text in the open file. The replacement text is terminated by a line with only end_of_edit on it. All of the <replacement text> will be entered, so make sure your indentation is formatted properly. Python files will be checked for syntax errors after the edit. If the system detects a syntax error, the edit will not be executed. Simply try to edit the file again, but make sure to read the error message and modify the edit command you issue accordingly. Issuing the same command a second time will just lead to the same error message again.\n",
    "  signature: edit <start_line>:<end_line>\n",
    "<replacement_text>\n",
    "end_of_edit\n",
    "  arguments:\n",
    "    - start_line (integer) [required]: the line number to start the edit at\n",
    "    - end_line (integer) [required]: the line number to end the edit at (inclusive)\n",
    "    - replacement_text (string) [required]: the text to replace the current selection with\n",
    "\n",
    "\n",
    "\n",
    "Please note that THE EDIT COMMAND REQUIRES PROPER INDENTATION. \n",
    "If you'd like to add the line '        print(x)' you must fully write that out, with all those spaces before the code! Indentation is important and code that is not indented correctly will fail and require fixing before it can be run.\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "Your shell prompt is formatted as follows:\n",
    "(Open file: <path>)\n",
    "(Current directory: <cwd>)\n",
    "In [ ]\n",
    "\n",
    "You need to format your output using two fields: discussion and command.\n",
    "Your output should always include _one_ discussion and _one_ command field EXACTLY as in the following example:\n",
    "DISCUSSION\n",
    "First I'll start by using ls to see what files are in the current directory. Then maybe we can look at some relevant files to see what they look like.\n",
    "```\n",
    "!ls -a\n",
    "```\n",
    "\n",
    "You should only include a *SINGLE* command in the command section and then wait for a response from the shell before continuing with more discussion and commands. Everything you include in the DISCUSSION section will be saved for future reference.\n",
    "If you'd like to issue two commands at once, PLEASE DO NOT DO THAT! Please instead first submit just the first command, and then after receiving a response you'll be able to issue the second command. \n",
    "You're free to use any other bash commands you want (e.g. find, grep, cat, ls, cd) in addition to the special commands listed above.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd867c63",
   "metadata": {},
   "source": [
    "### Query\n",
    "Train using the \"MeZO\" method (no prefix-tuning / lora) on the RTE dataset. Report the accuracy and dev_accuracy as a json structured as follows: {\"accuracy\": 0.0, \"dev_accuracy\": 0.0} (replace 0.0 with the actual values).\n",
    "\n",
    "Additional instructions:\n",
    "1. Load only the first 10 rows of each set in the dataset\n",
    "2. Train only one epoch. 3. Do not follow the download link in the readme, the repository uses huggingface to download the dataset.\n",
    "4. Use `--trainer zo` and `--train_set_seed 1234`\n",
    "\n",
    "Git repository: https://github.com/princeton-nlp/MeZO\n",
    "\n",
    "I have already executed some of the required steps. Now, you should install all required dependencies. Once dependencies are installed, you can re-run any of the pre-executed steps, and finally should successfully run\n",
    "```\n",
    "!python run.py \\\n",
    "    --task RTE \\\n",
    "    --output_dir result/output \\\n",
    "    --num_train 10 \\\n",
    "    --num_dev 10 \\\n",
    "    --num_eval 10 \\\n",
    "    --trainer zo \\\n",
    "    --train_set_seed 1234 \\\n",
    "    --train_as_classification \\\n",
    "    --num_train_epochs 1\n",
    "```\n",
    "to submit the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698ae78f",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de545a9",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2ac683",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd15799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025b9202",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a03a362",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e41f70f",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08138ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: There is no need to download the dataset using the provided link. Instead, it can be directly downloaded from HuggingFace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25094c4",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d874cb",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7feff78",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca6f4f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Code setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc33eb52",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2ce336",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0ce518",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ae75ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cloning into 'MeZO'...\r\nremote: Enumerating objects: 173, done.\u001b[K\r\nremote: Counting objects:   1% (1/75)\u001b[K\rremote: Counting objects:   2% (2/75)\u001b[K\rremote: Counting objects:   4% (3/75)\u001b[K\rremote: Counting objects:   5% (4/75)\u001b[K\rremote: Counting objects:   6% (5/75)\u001b[K\rremote: Counting objects:   8% (6/75)\u001b[K\rremote: Counting objects:   9% (7/75)\u001b[K\rremote: Counting objects:  10% (8/75)\u001b[K\rremote: Counting objects:  12% (9/75)\u001b[K\rremote: Counting objects:  13% (10/75)\u001b[K\rremote: Counting objects:  14% (11/75)\u001b[K\rremote: Counting objects:  16% (12/75)\u001b[K\rremote: Counting objects:  17% (13/75)\u001b[K\rremote: Counting objects:  18% (14/75)\u001b[K\rremote: Counting objects:  20% (15/75)\u001b[K\rremote: Counting objects:  21% (16/75)\u001b[K\rremote: Counting objects:  22% (17/75)\u001b[K\rremote: Counting objects:  24% (18/75)\u001b[K\rremote: Counting objects:  25% (19/75)\u001b[K\rremote: Counting objects:  26% (20/75)\u001b[K\rremote: Counting objects:  28% (21/75)\u001b[K\rremote: Counting objects:  29% (22/75)\u001b[K\rremote: Counting objects:  30% (23/75)\u001b[K\rremote: Counting objects:  32% (24/75)\u001b[K\rremote: Counting objects:  33% (25/75)\u001b[K\rremote: Counting objects:  34% (26/75)\u001b[K\rremote: Counting objects:  36% (27/75)\u001b[K\rremote: Counting objects:  37% (28/75)\u001b[K\rremote: Counting objects:  38% (29/75)\u001b[K\rremote: Counting objects:  40% (30/75)\u001b[K\rremote: Counting objects:  41% (31/75)\u001b[K\rremote: Counting objects:  42% (32/75)\u001b[K\rremote: Counting objects:  44% (33/75)\u001b[K\rremote: Counting objects:  45% (34/75)\u001b[K\rremote: Counting objects:  46% (35/75)\u001b[K\rremote: Counting objects:  48% (36/75)\u001b[K\rremote: Counting objects:  49% (37/75)\u001b[K\rremote: Counting objects:  50% (38/75)\u001b[K\rremote: Counting objects:  52% (39/75)\u001b[K\rremote: Counting objects:  53% (40/75)\u001b[K\rremote: Counting objects:  54% (41/75)\u001b[K\rremote: Counting objects:  56% (42/75)\u001b[K\rremote: Counting objects:  57% (43/75)\u001b[K\rremote: Counting objects:  58% (44/75)\u001b[K\rremote: Counting objects:  60% (45/75)\u001b[K\rremote: Counting objects:  61% (46/75)\u001b[K\rremote: Counting objects:  62% (47/75)\u001b[K\rremote: Counting objects:  64% (48/75)\u001b[K\rremote: Counting objects:  65% (49/75)\u001b[K\rremote: Counting objects:  66% (50/75)\u001b[K\rremote: Counting objects:  68% (51/75)\u001b[K\rremote: Counting objects:  69% (52/75)\u001b[K\rremote: Counting objects:  70% (53/75)\u001b[K\rremote: Counting objects:  72% (54/75)\u001b[K\rremote: Counting objects:  73% (55/75)\u001b[K\rremote: Counting objects:  74% (56/75)\u001b[K\rremote: Counting objects:  76% (57/75)\u001b[K\rremote: Counting objects:  77% (58/75)\u001b[K\rremote: Counting objects:  78% (59/75)\u001b[K\rremote: Counting objects:  80% (60/75)\u001b[K\rremote: Counting objects:  81% (61/75)\u001b[K\rremote: Counting objects:  82% (62/75)\u001b[K\rremote: Counting objects:  84% (63/75)\u001b[K\rremote: Counting objects:  85% (64/75)\u001b[K\rremote: Counting objects:  86% (65/75)\u001b[K\rremote: Counting objects:  88% (66/75)\u001b[K\rremote: Counting objects:  89% (67/75)\u001b[K\rremote: Counting objects:  90% (68/75)\u001b[K\rremote: Counting objects:  92% (69/75)\u001b[K\rremote: Counting objects:  93% (70/75)\u001b[K\rremote: Counting objects:  94% (71/75)\u001b[K\rremote: Counting objects:  96% (72/75)\u001b[K\rremote: Counting objects:  97% (73/75)\u001b[K\rremote: Counting objects:  98% (74/75)\u001b[K\rremote: Counting objects: 100% (75/75)\u001b[K\rremote: Counting objects: 100% (75/75), done.\u001b[K\r\nremote: Compressing objects:   2% (1/45)\u001b[K\rremote: Compressing objects:   4% (2/45)\u001b[K\rremote: Compressing objects:   6% (3/45)\u001b[K\rremote: Compressing objects:   8% (4/45)\u001b[K\rremote: Compressing objects:  11% (5/45)\u001b[K\rremote: Compressing objects:  13% (6/45)\u001b[K\rremote: Compressing objects:  15% (7/45)\u001b[K\rremote: Compressing objects:  17% (8/45)\u001b[K\rremote: Compressing objects:  20% (9/45)\u001b[K\rremote: Compressing objects:  22% (10/45)\u001b[K\rremote: Compressing objects:  24% (11/45)\u001b[K\rremote: Compressing objects:  26% (12/45)\u001b[K\rremote: Compressing objects:  28% (13/45)\u001b[K\rremote: Compressing objects:  31% (14/45)\u001b[K\rremote: Compressing objects:  33% (15/45)\u001b[K\rremote: Compressing objects:  35% (16/45)\u001b[K\rremote: Compressing objects:  37% (17/45)\u001b[K\rremote: Compressing objects:  40% (18/45)\u001b[K\rremote: Compressing objects:  42% (19/45)\u001b[K\rremote: Compressing objects:  44% (20/45)\u001b[K\rremote: Compressing objects:  46% (21/45)\u001b[K\rremote: Compressing objects:  48% (22/45)\u001b[K\rremote: Compressing objects:  51% (23/45)\u001b[K\rremote: Compressing objects:  53% (24/45)\u001b[K\rremote: Compressing objects:  55% (25/45)\u001b[K\rremote: Compressing objects:  57% (26/45)\u001b[K\rremote: Compressing objects:  60% (27/45)\u001b[K\rremote: Compressing objects:  62% (28/45)\u001b[K\rremote: Compressing objects:  64% (29/45)\u001b[K\rremote: Compressing objects:  66% (30/45)\u001b[K\rremote: Compressing objects:  68% (31/45)\u001b[K\rremote: Compressing objects:  71% (32/45)\u001b[K\rremote: Compressing objects:  73% (33/45)\u001b[K\rremote: Compressing objects:  75% (34/45)\u001b[K\rremote: Compressing objects:  77% (35/45)\u001b[K\rremote: Compressing objects:  80% (36/45)\u001b[K\rremote: Compressing objects:  82% (37/45)\u001b[K\rremote: Compressing objects:  84% (38/45)\u001b[K\rremote: Compressing objects:  86% (39/45)\u001b[K\rremote: Compressing objects:  88% (40/45)\u001b[K\rremote: Compressing objects:  91% (41/45)\u001b[K\rremote: Compressing objects:  93% (42/45)\u001b[K\rremote: Compressing objects:  95% (43/45)\u001b[K\rremote: Compressing objects:  97% (44/45)\u001b[K\rremote: Compressing objects: 100% (45/45)\u001b[K\rremote: Compressing objects: 100% (45/45), done.\u001b[K\r\nReceiving objects:   0% (1/173)\rReceiving objects:   1% (2/173)\rReceiving objects:   2% (4/173)\rReceiving objects:   3% (6/173)\rReceiving objects:   4% (7/173)\rReceiving objects:   5% (9/173)\rReceiving objects:   6% (11/173)\rReceiving objects:   7% (13/173)\rReceiving objects:   8% (14/173)\rReceiving objects:   9% (16/173)\rReceiving objects:  10% (18/173)\rReceiving objects:  11% (20/173)\rReceiving objects:  12% (21/173)\rReceiving objects:  13% (23/173)\rReceiving objects:  14% (25/173)\rReceiving objects:  15% (26/173)\rReceiving objects:  16% (28/173)\rReceiving objects:  17% (30/173)\rReceiving objects:  18% (32/173)\rReceiving objects:  19% (33/173)\rReceiving objects:  20% (35/173)\rReceiving objects:  21% (37/173)\rReceiving objects:  22% (39/173)\rReceiving objects:  23% (40/173)\rReceiving objects:  24% (42/173)\rReceiving objects:  25% (44/173)\rReceiving objects:  26% (45/173)\rReceiving objects:  27% (47/173)\rReceiving objects:  28% (49/173)\rReceiving objects:  29% (51/173)\rReceiving objects:  30% (52/173)\rReceiving objects:  31% (54/173)\rReceiving objects:  32% (56/173)\rReceiving objects:  33% (58/173)\rReceiving objects:  34% (59/173)\rReceiving objects:  35% (61/173)\rReceiving objects:  36% (63/173)\rReceiving objects:  37% (65/173)\rReceiving objects:  38% (66/173)\rReceiving objects:  39% (68/173)\rReceiving objects:  40% (70/173)\rremote: Total 173 (delta 45), reused 30 (delta 30), pack-reused 98 (from 1)\u001b[K\r\nReceiving objects:  41% (71/173)\rReceiving objects:  42% (73/173)\rReceiving objects:  43% (75/173)\rReceiving objects:  44% (77/173)\rReceiving objects:  45% (78/173)\rReceiving objects:  46% (80/173)\rReceiving objects:  47% (82/173)\rReceiving objects:  48% (84/173)\rReceiving objects:  49% (85/173)\rReceiving objects:  50% (87/173)\rReceiving objects:  51% (89/173)\rReceiving objects:  52% (90/173)\rReceiving objects:  53% (92/173)\rReceiving objects:  54% (94/173)\rReceiving objects:  55% (96/173)\rReceiving objects:  56% (97/173)\rReceiving objects:  57% (99/173)\rReceiving objects:  58% (101/173)\rReceiving objects:  59% (103/173)\rReceiving objects:  60% (104/173)\rReceiving objects:  61% (106/173)\rReceiving objects:  62% (108/173)\rReceiving objects:  63% (109/173)\rReceiving objects:  64% (111/173)\rReceiving objects:  65% (113/173)\rReceiving objects:  66% (115/173)\rReceiving objects:  67% (116/173)\rReceiving objects:  68% (118/173)\rReceiving objects:  69% (120/173)\rReceiving objects:  70% (122/173)\rReceiving objects:  71% (123/173)\rReceiving objects:  72% (125/173)\rReceiving objects:  73% (127/173)\rReceiving objects:  74% (129/173)\rReceiving objects:  75% (130/173)\rReceiving objects:  76% (132/173)\rReceiving objects:  77% (134/173)\rReceiving objects:  78% (135/173)\rReceiving objects:  79% (137/173)\rReceiving objects:  80% (139/173)\rReceiving objects:  81% (141/173)\rReceiving objects:  82% (142/173)\rReceiving objects:  83% (144/173)\rReceiving objects:  84% (146/173)\rReceiving objects:  85% (148/173)\rReceiving objects:  86% (149/173)\rReceiving objects:  87% (151/173)\rReceiving objects:  88% (153/173)\rReceiving objects:  89% (154/173)\rReceiving objects:  90% (156/173)\rReceiving objects:  91% (158/173)\rReceiving objects:  92% (160/173)\rReceiving objects:  93% (161/173)\rReceiving objects:  94% (163/173)\rReceiving objects:  95% (165/173)\rReceiving objects:  96% (167/173)\rReceiving objects:  97% (168/173)\rReceiving objects:  98% (170/173)\rReceiving objects:  99% (172/173)\rReceiving objects: 100% (173/173)\rReceiving objects: 100% (173/173), 432.68 KiB | 27.04 MiB/s, done.\r\nResolving deltas:   0% (0/88)\rResolving deltas:   1% (1/88)\rResolving deltas:   2% (2/88)\rResolving deltas:   3% (3/88)\rResolving deltas:   4% (4/88)\rResolving deltas:   5% (5/88)\rResolving deltas:   6% (6/88)\rResolving deltas:   7% (7/88)\rResolving deltas:   9% (8/88)\rResolving deltas:  10% (9/88)\rResolving deltas:  11% (10/88)\rResolving deltas:  12% (11/88)\rResolving deltas:  13% (12/88)\rResolving deltas:  14% (13/88)\rResolving deltas:  15% (14/88)\rResolving deltas:  17% (15/88)\rResolving deltas:  18% (16/88)\rResolving deltas:  19% (17/88)\rResolving deltas:  20% (18/88)\rResolving deltas:  21% (19/88)\rResolving deltas:  22% (20/88)\rResolving deltas:  23% (21/88)\rResolving deltas:  25% (22/88)\rResolving deltas:  26% (23/88)\rResolving deltas:  27% (24/88)\rResolving deltas:  28% (25/88)\rResolving deltas:  29% (26/88)\rResolving deltas:  30% (27/88)\rResolving deltas:  31% (28/88)\rResolving deltas:  32% (29/88)\rResolving deltas:  34% (30/88)\rResolving deltas:  35% (31/88)\rResolving deltas:  36% (32/88)\rResolving deltas:  37% (33/88)\rResolving deltas:  38% (34/88)\rResolving deltas:  39% (35/88)\rResolving deltas:  40% (36/88)\rResolving deltas:  42% (37/88)\rResolving deltas:  43% (38/88)\rResolving deltas:  44% (39/88)\rResolving deltas:  45% (40/88)\rResolving deltas:  46% (41/88)\rResolving deltas:  47% (42/88)\rResolving deltas:  48% (43/88)\rResolving deltas:  50% (44/88)\rResolving deltas:  51% (45/88)\rResolving deltas:  52% (46/88)\rResolving deltas:  53% (47/88)\rResolving deltas:  54% (48/88)\rResolving deltas:  55% (49/88)\rResolving deltas:  56% (50/88)\rResolving deltas:  57% (51/88)\rResolving deltas:  59% (52/88)\rResolving deltas:  60% (53/88)\rResolving deltas:  61% (54/88)\rResolving deltas:  62% (55/88)\rResolving deltas:  63% (56/88)\rResolving deltas:  64% (57/88)\rResolving deltas:  65% (58/88)\rResolving deltas:  67% (59/88)\rResolving deltas:  68% (60/88)\rResolving deltas:  69% (61/88)\rResolving deltas:  70% (62/88)\rResolving deltas:  71% (63/88)\rResolving deltas:  72% (64/88)\rResolving deltas:  73% (65/88)\rResolving deltas:  75% (66/88)\rResolving deltas:  76% (67/88)\rResolving deltas:  77% (68/88)\rResolving deltas:  78% (69/88)\rResolving deltas:  79% (70/88)\rResolving deltas:  80% (71/88)\rResolving deltas:  81% (72/88)\rResolving deltas:  82% (73/88)\rResolving deltas:  84% (74/88)\rResolving deltas:  85% (75/88)\rResolving deltas:  86% (76/88)\rResolving deltas:  87% (77/88)\rResolving deltas:  88% (78/88)\rResolving deltas:  89% (79/88)\rResolving deltas:  90% (80/88)\rResolving deltas:  92% (81/88)\rResolving deltas:  93% (82/88)\rResolving deltas:  94% (83/88)\rResolving deltas:  95% (84/88)\rResolving deltas:  96% (85/88)\rResolving deltas:  97% (86/88)\rResolving deltas:  98% (87/88)\rResolving deltas: 100% (88/88)\rResolving deltas: 100% (88/88), done.\r\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!git clone https://github.com/princeton-nlp/MeZO.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f56b04",
   "metadata": {},
   "source": [
    "### Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b00da7",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb27579",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3af0e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/MeZO/large_models\n/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/MeZO/large_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c741174",
   "metadata": {},
   "source": [
    "### Step 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ecfd32",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddc14c2",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c1d9ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Traceback (most recent call last):\r\n  File \"/content/MeZO/large_models/run.py\", line 9, in <module>\r\n    import tasks\r\n  File \"/content/MeZO/large_models/tasks.py\", line 5, in <module>\r\n    from datasets import load_dataset\r\nModuleNotFoundError: No module named 'datasets'\r\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python run.py \\\n",
    "    --task RTE \\\n",
    "    --output_dir result/output \\\n",
    "    --num_train 10 \\\n",
    "    --num_dev 10 \\\n",
    "    --num_eval 10 \\\n",
    "    --trainer zo \\\n",
    "    --train_set_seed 1234 \\\n",
    "    --train_as_classification \\\n",
    "    --num_train_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d6bf0a",
   "metadata": {},
   "source": [
    "### Step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c82310d",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249bbece",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0868223e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The error\n",
    "# ```ImportError: cannot import name 'default_hp_search_backend' from 'transformers.integrations'```\n",
    "# comes from using a newer version of the `transformers` package. To solve it, I install the version indicted in the repository readme (for the case of large_models)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7b7063",
   "metadata": {},
   "source": [
    "### Step 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187fdcf7",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d186f951",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f3e1a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Traceback (most recent call last):\r\n  File \"/content/MeZO/large_models/run.py\", line 9, in <module>\r\n    import tasks\r\n  File \"/content/MeZO/large_models/tasks.py\", line 5, in <module>\r\n    from datasets import load_dataset\r\nModuleNotFoundError: No module named 'datasets'\r\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python run.py \\\n",
    "    --task RTE \\\n",
    "    --output_dir result/output \\\n",
    "    --num_train 10 \\\n",
    "    --num_dev 10 \\\n",
    "    --num_eval 10 \\\n",
    "    --trainer zo \\\n",
    "    --train_set_seed 1234 \\\n",
    "    --train_as_classification \\\n",
    "    --num_train_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd72f13",
   "metadata": {},
   "source": [
    "### Step 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d18099",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cb7946",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef94620b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The train script assumes GPU as running device, so I patch it to remove that assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53beaadf",
   "metadata": {},
   "source": [
    "### Step 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d201c5c1",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22d6195",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e0b6b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        self.args = args\n        self.task = task\n        self.model, self.tokenizer = self.load_model()\n\n\n    def load_model(self):\n        \"\"\"\n        Load HuggingFace models\n        \"\"\"\n        with count_time(\"Loading model with FP%d\" % (16 if self.args.load_float16 else 32)):\n            # free_in_GB = int(torch.cuda.mem_get_info()[0]/1024**3)\n            config = AutoConfig.from_pretrained(self.args.model_name)\n            if self.args.untie_emb:\n                # Untie embeddings/LM head\n                logger.warn(\"Untie embeddings and LM head\")\n                config.tie_word_embeddings = False\n            if self.args.head_tuning:\n                # Head tuning\n                from ht_opt import OPTForCausalLM"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/MeZO/large_models/run.py. CHANGING:\n",
    "\n",
    "\n",
    "        Load HuggingFace models\n",
    "        \"\"\"\n",
    "        with count_time(\"Loading model with FP%d\" % (16 if self.args.load_float16 else 32)):\n",
    "            free_in_GB = int(torch.cuda.mem_get_info()[0]/1024**3)\n",
    "            config = AutoConfig.from_pretrained(self.args.model_name)\n",
    "            if self.args.untie_emb:\n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "        Load HuggingFace models\n",
    "        \"\"\"\n",
    "        with count_time(\"Loading model with FP%d\" % (16 if self.args.load_float16 else 32)):\n",
    "            # free_in_GB = int(torch.cuda.mem_get_info()[0]/1024**3)\n",
    "            config = AutoConfig.from_pretrained(self.args.model_name)\n",
    "            if self.args.untie_emb:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804f9381",
   "metadata": {},
   "source": [
    "### Step 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a70943",
   "metadata": {},
   "source": [
    "**Thought**: EXECUTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28505fe3",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93498704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n                torch_dtype = torch.float32\n                if self.args.load_float16:\n                    torch_dtype = torch.float16\n                elif self.args.load_bfloat16:\n                    torch_dtype = torch.bfloat16\n                model = AutoModelForCausalLM.from_pretrained(\n                    self.args.model_name,\n                    config=config,\n                    device_map='auto',\n                    torch_dtype=torch_dtype,\n                    # max_memory={i: f'{free_in_GB-5}GB' for i in range(torch.cuda.device_count())},\n                    load_in_8bit=self.args.load_int8,\n                )\n            model.eval()\n\n        # Load tokenizer\n        tokenizer = AutoTokenizer.from_pretrained(self.args.model_name, use_fast=False)\n\n        # HF tokenizer bug fix"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDITING `/content/MeZO/large_models/run.py. CHANGING:\n",
    "\n",
    "\n",
    "                    config=config,\n",
    "                    device_map='auto',\n",
    "                    torch_dtype=torch_dtype,\n",
    "                    max_memory={i: f'{free_in_GB-5}GB' for i in range(torch.cuda.device_count())},\n",
    "                    load_in_8bit=self.args.load_int8,\n",
    "                )\n",
    "\n",
    "\n",
    "TO:\n",
    "\n",
    "\n",
    "                    config=config,\n",
    "                    device_map='auto',\n",
    "                    torch_dtype=torch_dtype,\n",
    "                    # max_memory={i: f'{free_in_GB-5}GB' for i in range(torch.cuda.device_count())},\n",
    "                    load_in_8bit=self.args.load_int8,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dacc87",
   "metadata": {},
   "source": [
    "### Step 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd6127a",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0982a4a7",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dfd037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Traceback (most recent call last):\r\n  File \"/content/MeZO/large_models/run.py\", line 9, in <module>\r\n    import tasks\r\n  File \"/content/MeZO/large_models/tasks.py\", line 5, in <module>\r\n    from datasets import load_dataset\r\nModuleNotFoundError: No module named 'datasets'\r\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python run.py \\\n",
    "    --task RTE \\\n",
    "    --output_dir result/output \\\n",
    "    --num_train 10 \\\n",
    "    --num_dev 10 \\\n",
    "    --num_eval 10 \\\n",
    "    --trainer zo \\\n",
    "    --train_set_seed 1234 \\\n",
    "    --train_as_classification \\\n",
    "    --num_train_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91712db",
   "metadata": {},
   "source": [
    "### Step 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d9a7a7",
   "metadata": {},
   "source": [
    "Execution Start time: 05:11:58, End time: 05:12:05 (duration: 6 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e11ea5b",
   "metadata": {},
   "source": [
    "**Thought**: It seems like the `datasets` module is not installed, which is required for loading datasets from HuggingFace. I will install the `datasets` library using pip to resolve this issue.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e075ff4e",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4ae6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting datasets\n  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from datasets) (1.25.2)\nCollecting pyarrow>=15.0.0 (from datasets)\n  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\nCollecting dill<0.3.9,>=0.3.0 (from datasets)\n  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from datasets) (2.0.3)\nCollecting requests>=2.32.2 (from datasets)\n  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/site-packages (from datasets) (4.66.4)\nCollecting xxhash (from datasets)\n  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nCollecting multiprocess (from datasets)\n  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2023.6.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/site-packages (from datasets) (3.8.3)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/site-packages (from datasets) (0.23.3)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from datasets) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (2.1.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.0.7)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.6.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDownloading datasets-2.21.0-py3-none-any.whl (527 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/527.3 kB ? eta -:--:--\n\n\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/116.3 kB ? eta -:--:--\n\n\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/39.9 MB ? eta -:--:--\n\n\n\n\n\n\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/64.9 kB ? eta -:--:--\n\n\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/134.8 kB ? eta -:--:--\n\n\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/194.1 kB ? eta -:--:--\n\n\u001b[?25hInstalling collected packages: xxhash, requests, pyarrow, dill, multiprocess, datasets\n  Attempting uninstall: requests\n    Found existing installation: requests 2.31.0\n    Uninstalling requests-2.31.0:\n      Successfully uninstalled requests-2.31.0\nSuccessfully installed datasets-2.21.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 requests-2.32.3 xxhash-3.5.0\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812b0053",
   "metadata": {},
   "source": [
    "### Step 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faa9706",
   "metadata": {},
   "source": [
    "Execution Start time: 05:12:08, End time: 05:12:14 (duration: 6 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c994ed",
   "metadata": {},
   "source": [
    "**Thought**: The `datasets` library has been successfully installed. Now, I will attempt to run the training script again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab8399b",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b00a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2024-08-22 05:12:11,796 - INFO - PyTorch version 2.2.1+cpu available.\nTraceback (most recent call last):\n  File \"/content/MeZO/large_models/run.py\", line 24, in <module>\n    from trainer import OurTrainer\n  File \"/content/MeZO/large_models/trainer.py\", line 43, in <module>\n    from transformers.integrations import (  # isort: split\nImportError: cannot import name 'default_hp_search_backend' from 'transformers.integrations' (/usr/local/lib/python3.10/site-packages/transformers/integrations/__init__.py)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python run.py \\\n",
    "    --task RTE \\\n",
    "    --output_dir result/output \\\n",
    "    --num_train 10 \\\n",
    "    --num_dev 10 \\\n",
    "    --num_eval 10 \\\n",
    "    --trainer zo \\\n",
    "    --train_set_seed 1234 \\\n",
    "    --train_as_classification \\\n",
    "    --num_train_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3998832a",
   "metadata": {},
   "source": [
    "### Step 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c54c804",
   "metadata": {},
   "source": [
    "Execution Start time: 05:12:16, End time: 05:12:24 (duration: 7 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb45fa3",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the current version of the `transformers` library does not have the `default_hp_search_backend` in `transformers.integrations`. This is likely due to a version mismatch. I will install a compatible version of the `transformers` library as specified in the repository's requirements or documentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb97cece",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231c6ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting transformers==4.11.3\n  Downloading transformers-4.11.3-py3-none-any.whl.metadata (53 kB)\n\u001b[?25l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/53.7 kB ? eta -:--:--\n\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (3.15.1)\nRequirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (0.23.3)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (1.25.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (2023.12.25)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (2.32.3)\nCollecting sacremoses (from transformers==4.11.3)\n  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\nCollecting tokenizers<0.11,>=0.10.1 (from transformers==4.11.3)\n  Downloading tokenizers-0.10.3.tar.gz (212 kB)\n\u001b[?25l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/212.7 kB ? eta -:--:--\n\n\u001b[?25h  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\n\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.0.17->transformers==4.11.3) (2023.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.0.17->transformers==4.11.3) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.11.3) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.11.3) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.11.3) (2.0.7)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.11.3) (2024.6.2)\nRequirement already satisfied: click in /usr/local/lib/python3.10/site-packages (from sacremoses->transformers==4.11.3) (8.1.7)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/site-packages (from sacremoses->transformers==4.11.3) (1.4.2)\nDownloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/2.9 MB ? eta -:--:--\n\n\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/897.5 kB ? eta -:--:--\n\n\u001b[?25hBuilding wheels for collected packages: tokenizers\n  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l-\b \berror\n  error: subprocess-exited-with-error\n  \n  × Building wheel for tokenizers (pyproject.toml) did not run successfully.\n  │ exit code: 1\n  ╰─> [51 lines of output]\n      running bdist_wheel\n      running build\n      running build_py\n      creating build\n      creating build/lib.linux-x86_64-cpython-310\n      creating build/lib.linux-x86_64-cpython-310/tokenizers\n      copying py_src/tokenizers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/models\n      copying py_src/tokenizers/models/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/models\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/decoders\n      copying py_src/tokenizers/decoders/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/decoders\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/normalizers\n      copying py_src/tokenizers/normalizers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/normalizers\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/pre_tokenizers\n      copying py_src/tokenizers/pre_tokenizers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/pre_tokenizers\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/processors\n      copying py_src/tokenizers/processors/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/processors\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/trainers\n      copying py_src/tokenizers/trainers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/trainers\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/base_tokenizer.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/bert_wordpiece.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/byte_level_bpe.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/char_level_bpe.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/sentencepiece_bpe.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/sentencepiece_unigram.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/tools\n      copying py_src/tokenizers/tools/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/tools\n      copying py_src/tokenizers/tools/visualizer.py -> build/lib.linux-x86_64-cpython-310/tokenizers/tools\n      copying py_src/tokenizers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers\n      copying py_src/tokenizers/models/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/models\n      copying py_src/tokenizers/decoders/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/decoders\n      copying py_src/tokenizers/normalizers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/normalizers\n      copying py_src/tokenizers/pre_tokenizers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/pre_tokenizers\n      copying py_src/tokenizers/processors/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/processors\n      copying py_src/tokenizers/trainers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/trainers\n      copying py_src/tokenizers/tools/visualizer-styles.css -> build/lib.linux-x86_64-cpython-310/tokenizers/tools\n      running build_ext\n      running build_rust\n      error: can't find Rust compiler\n      \n      If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n      \n      To update pip, run:\n      \n          pip install --upgrade pip\n      \n      and then retry package installation.\n      \n      If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\n  ERROR: Failed building wheel for tokenizers\n\u001b[?25hFailed to build tokenizers\nERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install transformers==4.11.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45945ff1",
   "metadata": {},
   "source": [
    "### Step 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e768a519",
   "metadata": {},
   "source": [
    "Execution Start time: 05:12:26, End time: 05:12:39 (duration: 12 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0d091e",
   "metadata": {},
   "source": [
    "**Thought**: The installation of the `transformers` library failed due to a missing Rust compiler, which is required to build the `tokenizers` package from source. To resolve this, I will install the Rust compiler using `rustup`, which is the recommended way to download and update the Rust compiler toolchain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cf8da7",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201f301b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "info: downloading installer\ninfo: profile set to 'default'\ninfo: default host triple is x86_64-unknown-linux-gnu\ninfo: syncing channel updates for 'stable-x86_64-unknown-linux-gnu'\ninfo: latest update on 2024-08-08, rust version 1.80.1 (3f5fd8dd4 2024-08-06)\ninfo: downloading component 'cargo'\ninfo: downloading component 'clippy'\ninfo: downloading component 'rust-docs'\ninfo: downloading component 'rust-std'\ninfo: downloading component 'rustc'\ninfo: downloading component 'rustfmt'\ninfo: installing component 'cargo'\ninfo: installing component 'clippy'\ninfo: installing component 'rust-docs'\n\n  4.8 MiB /  15.8 MiB ( 30 %)   0 B/s in  1s ETA: Unknown\n                                                         \n  5.8 MiB /  15.8 MiB ( 37 %)   4.8 MiB/s in  2s ETA:  2s\n                                                         \n 10.6 MiB /  15.8 MiB ( 67 %)   2.9 MiB/s in  3s ETA:  1s\n                                                         \n 15.8 MiB /  15.8 MiB (100 %)   3.5 MiB/s in  3s ETA:  0s\ninfo: installing component 'rust-std'\n\n 13.8 MiB /  26.7 MiB ( 52 %)   0 B/s in  1s ETA: Unknown\n                                                         \n 26.7 MiB /  26.7 MiB (100 %)  13.8 MiB/s in  1s ETA:  0s\ninfo: installing component 'rustc'\n\n 15.2 MiB /  65.0 MiB ( 23 %)   0 B/s in  1s ETA: Unknown\n                                                         \n 30.4 MiB /  65.0 MiB ( 47 %)  15.2 MiB/s in  2s ETA:  2s\n                                                         \n 44.3 MiB /  65.0 MiB ( 68 %)  15.2 MiB/s in  3s ETA:  1s\n                                                         \n 58.7 MiB /  65.0 MiB ( 90 %)  14.8 MiB/s in  4s ETA:  0s\n                                                         \n 65.0 MiB /  65.0 MiB (100 %)  14.7 MiB/s in  4s ETA:  0s\ninfo: installing component 'rustfmt'\ninfo: default toolchain set to 'stable-x86_64-unknown-linux-gnu'\n\n  stable-x86_64-unknown-linux-gnu installed - rustc 1.80.1 (3f5fd8dd4 2024-08-06)\n\n\nRust is installed now. Great!\n\nTo get started you may need to restart your current shell.\nThis would reload your PATH environment variable to include\nCargo's bin directory ($HOME/.cargo/bin).\n\nTo configure your current shell, you need to source\nthe corresponding env file under $HOME/.cargo.\n\nThis is usually done by running one of the following (note the leading DOT):\n. \"$HOME/.cargo/env\"            # For sh/bash/zsh/ash/dash/pdksh\nsource \"$HOME/.cargo/env.fish\"  # For fish\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2964435",
   "metadata": {},
   "source": [
    "### Step 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc66003",
   "metadata": {},
   "source": [
    "Execution Start time: 05:12:40, End time: 05:12:41 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d839943c",
   "metadata": {},
   "source": [
    "**Thought**: Rust has been successfully installed. Now, I will source the environment to ensure that the Rust compiler is available in the current shell session. After that, I will attempt to install the `transformers` library again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7550645b",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaec6ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/usr/bin/sh: 1: source: not found\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!source $HOME/.cargo/env && pip install transformers==4.11.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c878c240",
   "metadata": {},
   "source": [
    "### Step 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30623d9",
   "metadata": {},
   "source": [
    "Execution Start time: 05:12:42, End time: 05:13:33 (duration: 50 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591dc58c",
   "metadata": {},
   "source": [
    "**Thought**: The `source` command is not available in the current shell environment. Instead, I will use the `.` command to source the environment file, which is equivalent to `source` in bash.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73acda09",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9530b1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting transformers==4.11.3\n  Downloading transformers-4.11.3-py3-none-any.whl.metadata (53 kB)\n\u001b[?25l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/53.7 kB ? eta -:--:--\n\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (3.15.1)\nRequirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (0.23.3)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (1.25.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (2023.12.25)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (2.32.3)\nCollecting sacremoses (from transformers==4.11.3)\n  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\nCollecting tokenizers<0.11,>=0.10.1 (from transformers==4.11.3)\n  Downloading tokenizers-0.10.3.tar.gz (212 kB)\n\u001b[?25l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/212.7 kB ? eta -:--:--\n\n\u001b[?25h  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\n\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.0.17->transformers==4.11.3) (2023.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.0.17->transformers==4.11.3) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.11.3) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.11.3) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.11.3) (2.0.7)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.11.3) (2024.6.2)\nRequirement already satisfied: click in /usr/local/lib/python3.10/site-packages (from sacremoses->transformers==4.11.3) (8.1.7)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/site-packages (from sacremoses->transformers==4.11.3) (1.4.2)\nDownloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/2.9 MB ? eta -:--:--\n\n\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/897.5 kB ? eta -:--:--\n\n\u001b[?25hBuilding wheels for collected packages: tokenizers\n  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \berror\n  error: subprocess-exited-with-error\n  \n  × Building wheel for tokenizers (pyproject.toml) did not run successfully.\n  │ exit code: 1\n  ╰─> [550 lines of output]\n      running bdist_wheel\n      running build\n      running build_py\n      creating build\n      creating build/lib.linux-x86_64-cpython-310\n      creating build/lib.linux-x86_64-cpython-310/tokenizers\n      copying py_src/tokenizers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/models\n      copying py_src/tokenizers/models/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/models\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/decoders\n      copying py_src/tokenizers/decoders/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/decoders\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/normalizers\n      copying py_src/tokenizers/normalizers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/normalizers\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/pre_tokenizers\n      copying py_src/tokenizers/pre_tokenizers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/pre_tokenizers\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/processors\n      copying py_src/tokenizers/processors/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/processors\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/trainers\n      copying py_src/tokenizers/trainers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/trainers\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/base_tokenizer.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/bert_wordpiece.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/byte_level_bpe.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/char_level_bpe.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/sentencepiece_bpe.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/sentencepiece_unigram.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/tools\n      copying py_src/tokenizers/tools/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/tools\n      copying py_src/tokenizers/tools/visualizer.py -> build/lib.linux-x86_64-cpython-310/tokenizers/tools\n      copying py_src/tokenizers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers\n      copying py_src/tokenizers/models/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/models\n      copying py_src/tokenizers/decoders/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/decoders\n      copying py_src/tokenizers/normalizers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/normalizers\n      copying py_src/tokenizers/pre_tokenizers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/pre_tokenizers\n      copying py_src/tokenizers/processors/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/processors\n      copying py_src/tokenizers/trainers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/trainers\n      copying py_src/tokenizers/tools/visualizer-styles.css -> build/lib.linux-x86_64-cpython-310/tokenizers/tools\n      running build_ext\n      running build_rust\n          Updating crates.io index\n          Updating git repository `https://github.com/pyo3/rust-numpy/`\n          Updating git repository `https://github.com/n1t0/rayon-cond`\n           Locking 135 packages to latest compatible versions\n            Adding base64 v0.13.1 (latest: v0.22.1)\n            Adding bitflags v1.3.2 (latest: v2.6.0)\n            Adding cfg-if v0.1.10 (latest: v1.0.0)\n            Adding clap v2.34.0 (latest: v4.5.16)\n            Adding ctor v0.1.26 (latest: v0.2.8)\n            Adding darling v0.10.2 (latest: v0.20.10)\n            Adding darling_core v0.10.2 (latest: v0.20.10)\n            Adding darling_macro v0.10.2 (latest: v0.20.10)\n            Adding derive_builder v0.9.0 (latest: v0.20.0)\n            Adding derive_builder_core v0.9.0 (latest: v0.20.0)\n            Adding encode_unicode v0.3.6 (latest: v1.0.0)\n            Adding env_logger v0.7.1 (latest: v0.11.5)\n            Adding getrandom v0.1.16 (latest: v0.2.15)\n            Adding hermit-abi v0.1.19 (latest: v0.4.0)\n            Adding humantime v1.3.0 (latest: v2.1.0)\n            Adding indicatif v0.15.0 (latest: v0.17.8)\n            Adding indoc v0.3.6 (latest: v2.0.5)\n            Adding indoc-impl v0.3.6 (latest: v0.3.7)\n            Adding inventory v0.1.11 (latest: v0.3.15)\n            Adding itertools v0.8.2 (latest: v0.13.0)\n            Adding itertools v0.9.0 (latest: v0.13.0)\n            Adding linux-raw-sys v0.4.14 (latest: v0.6.5)\n            Adding matrixmultiply v0.2.4 (latest: v0.3.9)\n            Adding ndarray v0.13.1 (latest: v0.16.1)\n            Adding num-complex v0.2.4 (latest: v0.4.6)\n            Adding number_prefix v0.3.0 (latest: v0.4.0)\n            Adding parking_lot v0.11.2 (latest: v0.12.3)\n            Adding parking_lot_core v0.8.6 (latest: v0.9.10)\n            Adding paste v0.1.18 (latest: v1.0.15)\n            Adding pyo3 v0.12.4 (latest: v0.22.2)\n            Adding pyo3-derive-backend v0.12.4 (latest: v0.12.5)\n            Adding pyo3cls v0.12.4 (latest: v0.12.5)\n            Adding quick-error v1.2.3 (latest: v2.0.1)\n            Adding rand v0.7.3 (latest: v0.8.5)\n            Adding rand_chacha v0.2.2 (latest: v0.3.1)\n            Adding rand_core v0.5.1 (latest: v0.6.4)\n            Adding rand_hc v0.2.0 (latest: v0.3.2)\n            Adding redox_syscall v0.2.16 (latest: v0.5.3)\n            Adding regex-syntax v0.6.29 (latest: v0.8.4)\n            Adding strsim v0.8.0 (latest: v0.11.1)\n            Adding strsim v0.9.3 (latest: v0.11.1)\n            Adding syn v1.0.109 (latest: v2.0.75)\n            Adding textwrap v0.11.0 (latest: v0.16.1)\n            Adding unindent v0.1.11 (latest: v0.2.3)\n            Adding wasi v0.9.0+wasi-snapshot-preview1 (latest: v0.13.2+wasi-0.2.1)\n            Adding windows-sys v0.52.0 (latest: v0.59.0)\n       Downloading crates ...\n        Downloaded termcolor v1.4.1\n        Downloaded atty v0.2.14\n        Downloaded windows-targets v0.52.6\n        Downloaded zerocopy-derive v0.7.35\n        Downloaded windows_x86_64_gnullvm v0.52.6\n        Downloaded windows_i686_gnullvm v0.52.6\n        Downloaded unicode-width v0.1.13\n        Downloaded windows_x86_64_msvc v0.52.6\n        Downloaded windows_x86_64_gnu v0.52.6\n        Downloaded windows_i686_msvc v0.52.6\n        Downloaded winapi v0.3.9\n        Downloaded strsim v0.8.0\n        Downloaded rustix v0.38.34\n        Downloaded spm_precompiled v0.1.4\n        Downloaded onig_sys v69.8.1\n        Downloaded libc v0.2.158\n        Downloaded windows-sys v0.59.0\n        Downloaded regex-automata v0.4.7\n        Downloaded windows-sys v0.52.0\n        Downloaded winapi-i686-pc-windows-gnu v0.4.0\n        Downloaded winapi-x86_64-pc-windows-gnu v0.4.0\n        Downloaded linux-raw-sys v0.4.14\n        Downloaded ndarray v0.16.1\n        Downloaded ndarray v0.13.1\n        Downloaded regex-syntax v0.8.4\n        Downloaded regex-syntax v0.6.29\n        Downloaded regex v1.10.6\n        Downloaded once_cell v1.19.0\n        Downloaded num-traits v0.2.19\n        Downloaded num-complex v0.4.6\n        Downloaded nom v7.1.3\n        Downloaded memchr v2.7.4\n        Downloaded matrixmultiply v0.3.9\n        Downloaded lock_api v0.4.12\n        Downloaded getrandom v0.1.16\n        Downloaded esaxx-rs v0.1.10\n        Downloaded derive_builder v0.9.0\n        Downloaded crossbeam-epoch v0.9.18\n        Downloaded console v0.15.8\n        Downloaded clap v2.34.0\n        Downloaded aho-corasick v1.1.3\n        Downloaded smallvec v1.13.2\n        Downloaded shlex v1.3.0\n        Downloaded serde_json v1.0.125\n        Downloaded serde_derive v1.0.208\n        Downloaded serde v1.0.208\n        Downloaded scopeguard v1.2.0\n        Downloaded redox_syscall v0.2.16\n        Downloaded rayon-core v1.12.1\n        Downloaded rayon v1.10.0\n        Downloaded rand_chacha v0.2.2\n        Downloaded rand v0.7.3\n        Downloaded quick-error v1.2.3\n        Downloaded pyo3-derive-backend v0.12.4\n        Downloaded pyo3 v0.12.4\n        Downloaded proc-macro2 v1.0.86\n        Downloaded proc-macro-hack v0.5.20+deprecated\n        Downloaded ppv-lite86 v0.2.20\n        Downloaded portable-atomic-util v0.2.2\n        Downloaded portable-atomic v1.7.0\n        Downloaded pkg-config v0.3.30\n        Downloaded paste-impl v0.1.18\n        Downloaded paste v0.1.18\n        Downloaded parking_lot_core v0.8.6\n        Downloaded parking_lot v0.11.2\n        Downloaded onig v6.4.0\n        Downloaded num-integer v0.1.46\n        Downloaded num-complex v0.2.4\n        Downloaded minimal-lexical v0.2.1\n        Downloaded matrixmultiply v0.2.4\n        Downloaded log v0.4.22\n        Downloaded itoa v1.0.11\n        Downloaded itertools v0.9.0\n        Downloaded itertools v0.8.2\n        Downloaded inventory-impl v0.1.11\n        Downloaded instant v0.1.13\n        Downloaded indoc v0.3.6\n        Downloaded indicatif v0.15.0\n        Downloaded ident_case v1.0.1\n        Downloaded hermit-abi v0.1.19\n        Downloaded fastrand v2.1.0\n        Downloaded errno v0.3.9\n        Downloaded env_logger v0.7.1\n        Downloaded encode_unicode v0.3.6\n        Downloaded either v1.13.0\n        Downloaded ctor v0.1.26\n        Downloaded crossbeam-utils v0.8.20\n        Downloaded crossbeam-deque v0.8.5\n        Downloaded cfg-if v1.0.0\n        Downloaded cc v1.1.13\n        Downloaded bitflags v2.6.0\n        Downloaded base64 v0.13.1\n        Downloaded ansi_term v0.12.1\n        Downloaded ryu v1.0.18\n        Downloaded rawpointer v0.2.1\n        Downloaded rand_hc v0.2.0\n        Downloaded rand_core v0.5.1\n        Downloaded quote v1.0.36\n        Downloaded darling_core v0.10.2\n        Downloaded windows_i686_gnu v0.52.6\n        Downloaded windows_aarch64_msvc v0.52.6\n        Downloaded windows_aarch64_gnullvm v0.52.6\n        Downloaded syn v2.0.75\n        Downloaded zerocopy v0.7.35\n        Downloaded winapi-util v0.1.9\n        Downloaded wasi v0.9.0+wasi-snapshot-preview1\n        Downloaded vec_map v0.8.2\n        Downloaded unicode_categories v0.1.1\n        Downloaded unicode-segmentation v1.11.0\n        Downloaded unicode-normalization-alignments v0.1.12\n        Downloaded tempfile v3.12.0\n        Downloaded syn v1.0.109\n        Downloaded unindent v0.1.11\n        Downloaded unicode-ident v1.0.12\n        Downloaded textwrap v0.11.0\n        Downloaded strsim v0.9.3\n        Downloaded pyo3cls v0.12.4\n        Downloaded number_prefix v0.3.0\n        Downloaded lazy_static v1.5.0\n        Downloaded inventory v0.1.11\n        Downloaded indoc-impl v0.3.6\n        Downloaded humantime v1.3.0\n        Downloaded ghost v0.1.17\n        Downloaded fnv v1.0.7\n        Downloaded derive_builder_core v0.9.0\n        Downloaded darling_macro v0.10.2\n        Downloaded darling v0.10.2\n        Downloaded cfg-if v0.1.10\n        Downloaded byteorder v1.5.0\n        Downloaded bitflags v1.3.2\n        Downloaded autocfg v1.3.0\n      cargo rustc --lib --message-format=json-render-diagnostics --manifest-path Cargo.toml --release -v --features pyo3/extension-module --crate-type cdylib --\n      warning: unused manifest key: target.x86_64-apple-darwin.rustflags\n         Compiling proc-macro2 v1.0.86\n         Compiling unicode-ident v1.0.12\n         Compiling autocfg v1.3.0\n         Compiling syn v1.0.109\n         Compiling libc v0.2.158\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/syn-1.0.109/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"clone-impls\"' --cfg 'feature=\"default\"' --cfg 'feature=\"derive\"' --cfg 'feature=\"extra-traits\"' --cfg 'feature=\"full\"' --cfg 'feature=\"parsing\"' --cfg 'feature=\"printing\"' --cfg 'feature=\"proc-macro\"' --cfg 'feature=\"quote\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"clone-impls\", \"default\", \"derive\", \"extra-traits\", \"fold\", \"full\", \"parsing\", \"printing\", \"proc-macro\", \"quote\", \"test\", \"visit\", \"visit-mut\"))' -C metadata=4cf12c30d28f810d -C extra-filename=-4cf12c30d28f810d --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/syn-4cf12c30d28f810d -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/proc-macro2-1.0.86/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --cfg 'feature=\"proc-macro\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"nightly\", \"proc-macro\", \"span-locations\"))' -C metadata=dce65f3895ff3b4f -C extra-filename=-dce65f3895ff3b4f --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/proc-macro2-dce65f3895ff3b4f -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name autocfg --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/autocfg-1.3.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=7883da62ebd09624 -C extra-filename=-7883da62ebd09624 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name unicode_ident --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/unicode-ident-1.0.12/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=64225dcd6d908b0e -C extra-filename=-64225dcd6d908b0e --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/libc-0.2.158/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --cfg 'feature=\"extra_traits\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"align\", \"const-extern-fn\", \"default\", \"extra_traits\", \"rustc-dep-of-std\", \"rustc-std-workspace-core\", \"std\", \"use_std\"))' -C metadata=4b261129cdecb68b -C extra-filename=-4b261129cdecb68b --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/libc-4b261129cdecb68b -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling cfg-if v1.0.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name cfg_if --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/cfg-if-1.0.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"compiler_builtins\", \"core\", \"rustc-dep-of-std\"))' -C metadata=069aa8d3b6fffb61 -C extra-filename=-069aa8d3b6fffb61 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling shlex v1.3.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name shlex --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/shlex-1.3.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"std\"))' -C metadata=29f68b1250937afa -C extra-filename=-29f68b1250937afa --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling memchr v2.7.4\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name memchr --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/memchr-2.7.4/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"alloc\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"alloc\", \"compiler_builtins\", \"core\", \"default\", \"libc\", \"logging\", \"rustc-dep-of-std\", \"std\", \"use_std\"))' -C metadata=98aad6ebcffcf120 -C extra-filename=-98aad6ebcffcf120 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling num-traits v0.2.19\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/num-traits-0.2.19/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --cfg 'feature=\"i128\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"i128\", \"libm\", \"std\"))' -C metadata=48a8ac98b389200f -C extra-filename=-48a8ac98b389200f --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/num-traits-48a8ac98b389200f -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern autocfg=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libautocfg-7883da62ebd09624.rlib --cap-lints allow`\n           Running `/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/syn-4cf12c30d28f810d/build-script-build`\n           Running `/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/proc-macro2-dce65f3895ff3b4f/build-script-build`\n           Running `/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/libc-4b261129cdecb68b/build-script-build`\n         Compiling cc v1.1.13\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name cc --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/cc-1.1.13/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"jobserver\", \"parallel\"))' -C metadata=94c042aa09c605a0 -C extra-filename=-94c042aa09c605a0 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern shlex=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libshlex-29f68b1250937afa.rmeta --cap-lints allow`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name libc --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/libc-0.2.158/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"extra_traits\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"align\", \"const-extern-fn\", \"default\", \"extra_traits\", \"rustc-dep-of-std\", \"rustc-std-workspace-core\", \"std\", \"use_std\"))' -C metadata=8262032f1e05c089 -C extra-filename=-8262032f1e05c089 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow --cfg freebsd11 --cfg libc_priv_mod_use --cfg libc_union --cfg libc_const_size_of --cfg libc_align --cfg libc_int128 --cfg libc_core_cvoid --cfg libc_packedN --cfg libc_cfg_target_vendor --cfg libc_non_exhaustive --cfg libc_long_array --cfg libc_ptr_addr_of --cfg libc_underscore_const_names --cfg libc_const_extern_fn --check-cfg 'cfg(emscripten_new_stat_abi)' --check-cfg 'cfg(espidf_time64)' --check-cfg 'cfg(freebsd10)' --check-cfg 'cfg(freebsd11)' --check-cfg 'cfg(freebsd12)' --check-cfg 'cfg(freebsd13)' --check-cfg 'cfg(freebsd14)' --check-cfg 'cfg(freebsd15)' --check-cfg 'cfg(libc_align)' --check-cfg 'cfg(libc_cfg_target_vendor)' --check-cfg 'cfg(libc_const_extern_fn)' --check-cfg 'cfg(libc_const_extern_fn_unstable)' --check-cfg 'cfg(libc_const_size_of)' --check-cfg 'cfg(libc_core_cvoid)' --check-cfg 'cfg(libc_deny_warnings)' --check-cfg 'cfg(libc_int128)' --check-cfg 'cfg(libc_long_array)' --check-cfg 'cfg(libc_non_exhaustive)' --check-cfg 'cfg(libc_packedN)' --check-cfg 'cfg(libc_priv_mod_use)' --check-cfg 'cfg(libc_ptr_addr_of)' --check-cfg 'cfg(libc_thread_local)' --check-cfg 'cfg(libc_underscore_const_names)' --check-cfg 'cfg(libc_union)' --check-cfg 'cfg(target_os,values(\"switch\",\"aix\",\"ohos\",\"hurd\",\"visionos\"))' --check-cfg 'cfg(target_env,values(\"illumos\",\"wasi\",\"aix\",\"ohos\"))' --check-cfg 'cfg(target_arch,values(\"loongarch64\",\"mips32r6\",\"mips64r6\",\"csky\"))'`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name proc_macro2 --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/proc-macro2-1.0.86/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --cfg 'feature=\"proc-macro\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"nightly\", \"proc-macro\", \"span-locations\"))' -C metadata=a53443ded1dfcef2 -C extra-filename=-a53443ded1dfcef2 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern unicode_ident=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libunicode_ident-64225dcd6d908b0e.rmeta --cap-lints allow --cfg wrap_proc_macro --check-cfg 'cfg(fuzzing)' --check-cfg 'cfg(no_is_available)' --check-cfg 'cfg(no_literal_byte_character)' --check-cfg 'cfg(no_literal_c_string)' --check-cfg 'cfg(no_source_text)' --check-cfg 'cfg(proc_macro_span)' --check-cfg 'cfg(procmacro2_backtrace)' --check-cfg 'cfg(procmacro2_nightly_testing)' --check-cfg 'cfg(procmacro2_semver_exempt)' --check-cfg 'cfg(randomize_layout)' --check-cfg 'cfg(span_locations)' --check-cfg 'cfg(super_unstable)' --check-cfg 'cfg(wrap_proc_macro)'`\n         Compiling crossbeam-utils v0.8.20\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/crossbeam-utils-0.8.20/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no --warn=unexpected_cfgs -C debug-assertions=off --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"loom\", \"nightly\", \"std\"))' -C metadata=83731834fd1d61fd -C extra-filename=-83731834fd1d61fd --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/crossbeam-utils-83731834fd1d61fd -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling quote v1.0.36\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name quote --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/quote-1.0.36/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --cfg 'feature=\"proc-macro\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"proc-macro\"))' -C metadata=4c220be45cf63bba -C extra-filename=-4c220be45cf63bba --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern proc_macro2=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libproc_macro2-a53443ded1dfcef2.rmeta --cap-lints allow`\n         Compiling proc-macro-hack v0.5.20+deprecated\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/proc-macro-hack-0.5.20+deprecated/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=431824022aa8ad7f -C extra-filename=-431824022aa8ad7f --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/proc-macro-hack-431824022aa8ad7f -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name syn --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/syn-1.0.109/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"clone-impls\"' --cfg 'feature=\"default\"' --cfg 'feature=\"derive\"' --cfg 'feature=\"extra-traits\"' --cfg 'feature=\"full\"' --cfg 'feature=\"parsing\"' --cfg 'feature=\"printing\"' --cfg 'feature=\"proc-macro\"' --cfg 'feature=\"quote\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"clone-impls\", \"default\", \"derive\", \"extra-traits\", \"fold\", \"full\", \"parsing\", \"printing\", \"proc-macro\", \"quote\", \"test\", \"visit\", \"visit-mut\"))' -C metadata=3a67f5675a1f15d1 -C extra-filename=-3a67f5675a1f15d1 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern proc_macro2=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libproc_macro2-a53443ded1dfcef2.rmeta --extern quote=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libquote-4c220be45cf63bba.rmeta --extern unicode_ident=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libunicode_ident-64225dcd6d908b0e.rmeta --cap-lints allow --cfg syn_disable_nightly_tests`\n         Compiling syn v2.0.75\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name syn --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/syn-2.0.75/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"clone-impls\"' --cfg 'feature=\"default\"' --cfg 'feature=\"derive\"' --cfg 'feature=\"parsing\"' --cfg 'feature=\"printing\"' --cfg 'feature=\"proc-macro\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"clone-impls\", \"default\", \"derive\", \"extra-traits\", \"fold\", \"full\", \"parsing\", \"printing\", \"proc-macro\", \"test\", \"visit\", \"visit-mut\"))' -C metadata=a7527cdbfb662196 -C extra-filename=-a7527cdbfb662196 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern proc_macro2=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libproc_macro2-a53443ded1dfcef2.rmeta --extern quote=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libquote-4c220be45cf63bba.rmeta --extern unicode_ident=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libunicode_ident-64225dcd6d908b0e.rmeta --cap-lints allow`\n           Running `/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/proc-macro-hack-431824022aa8ad7f/build-script-build`\n           Running `/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/crossbeam-utils-83731834fd1d61fd/build-script-build`\n           Running `/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/num-traits-48a8ac98b389200f/build-script-build`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name num_traits --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/num-traits-0.2.19/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"i128\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"i128\", \"libm\", \"std\"))' -C metadata=04f24cb06d3c2fbe -C extra-filename=-04f24cb06d3c2fbe --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow --cfg has_total_cmp`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name crossbeam_utils --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/crossbeam-utils-0.8.20/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --warn=unexpected_cfgs --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"loom\", \"nightly\", \"std\"))' -C metadata=5fe1a42cb8431226 -C extra-filename=-5fe1a42cb8431226 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow --check-cfg 'cfg(crossbeam_no_atomic,crossbeam_sanitize_thread)'`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name proc_macro_hack --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/proc-macro-hack-0.5.20+deprecated/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type proc-macro --emit=dep-info,link -C prefer-dynamic -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=b49030fd20457a17 -C extra-filename=-b49030fd20457a17 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern proc_macro --cap-lints allow`\n         Compiling smallvec v1.13.2\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name smallvec --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/smallvec-1.13.2/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"arbitrary\", \"const_generics\", \"const_new\", \"debugger_visualizer\", \"drain_filter\", \"drain_keep_rest\", \"may_dangle\", \"serde\", \"specialization\", \"union\", \"write\"))' -C metadata=54b60278d2fd4af7 -C extra-filename=-54b60278d2fd4af7 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling fnv v1.0.7\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name fnv --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/fnv-1.0.7/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"std\"))' -C metadata=444c02e4b5242b37 -C extra-filename=-444c02e4b5242b37 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling ident_case v1.0.1\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name ident_case --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/ident_case-1.0.1/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=f9ebc3e0d26d6510 -C extra-filename=-f9ebc3e0d26d6510 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling getrandom v0.1.16\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/getrandom-0.1.16/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"bindgen\", \"compiler_builtins\", \"core\", \"dummy\", \"js-sys\", \"log\", \"rustc-dep-of-std\", \"std\", \"stdweb\", \"test-in-browser\", \"wasm-bindgen\"))' -C metadata=df0ecdf682f41f96 -C extra-filename=-df0ecdf682f41f96 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/getrandom-df0ecdf682f41f96 -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling strsim v0.9.3\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name strsim --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/strsim-0.9.3/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=c5ed5f0f3eec67ae -C extra-filename=-c5ed5f0f3eec67ae --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling crossbeam-epoch v0.9.18\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name crossbeam_epoch --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/crossbeam-epoch-0.9.18/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"alloc\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"alloc\", \"default\", \"loom\", \"loom-crate\", \"nightly\", \"std\"))' -C metadata=1162c98467d8fcf6 -C extra-filename=-1162c98467d8fcf6 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern crossbeam_utils=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libcrossbeam_utils-5fe1a42cb8431226.rmeta --cap-lints allow`\n         Compiling aho-corasick v1.1.3\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name aho_corasick --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/aho-corasick-1.1.3/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"perf-literal\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"logging\", \"perf-literal\", \"std\"))' -C metadata=3ab56ade417ee174 -C extra-filename=-3ab56ade417ee174 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern memchr=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libmemchr-98aad6ebcffcf120.rmeta --cap-lints allow`\n           Running `/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/getrandom-df0ecdf682f41f96/build-script-build`\n         Compiling lock_api v0.4.12\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/lock_api-0.4.12/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"atomic_usize\"' --cfg 'feature=\"default\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"arc_lock\", \"atomic_usize\", \"default\", \"nightly\", \"owning_ref\", \"serde\"))' -C metadata=73b0bf1b17677ff7 -C extra-filename=-73b0bf1b17677ff7 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/lock_api-73b0bf1b17677ff7 -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern autocfg=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libautocfg-7883da62ebd09624.rlib --cap-lints allow`\n         Compiling rayon-core v1.12.1\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rayon-core-1.12.1/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"web_spin_lock\"))' -C metadata=2202607f5aa42298 -C extra-filename=-2202607f5aa42298 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/rayon-core-2202607f5aa42298 -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling serde v1.0.208\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/serde-1.0.208/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --cfg 'feature=\"derive\"' --cfg 'feature=\"rc\"' --cfg 'feature=\"serde_derive\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"alloc\", \"default\", \"derive\", \"rc\", \"serde_derive\", \"std\", \"unstable\"))' -C metadata=54947cb5889e94c1 -C extra-filename=-54947cb5889e94c1 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/serde-54947cb5889e94c1 -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling unicode-width v0.1.13\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name unicode_width --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/unicode-width-0.1.13/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"compiler_builtins\", \"core\", \"default\", \"no_std\", \"rustc-dep-of-std\", \"std\"))' -C metadata=147e4b4395044b58 -C extra-filename=-147e4b4395044b58 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling darling_core v0.10.2\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name darling_core --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/darling_core-0.10.2/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"strsim\"' --cfg 'feature=\"suggestions\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"diagnostics\", \"strsim\", \"suggestions\"))' -C metadata=79999c50edd86e3a -C extra-filename=-79999c50edd86e3a --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern fnv=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libfnv-444c02e4b5242b37.rmeta --extern ident_case=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libident_case-f9ebc3e0d26d6510.rmeta --extern proc_macro2=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libproc_macro2-a53443ded1dfcef2.rmeta --extern quote=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libquote-4c220be45cf63bba.rmeta --extern strsim=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libstrsim-c5ed5f0f3eec67ae.rmeta --extern syn=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libsyn-3a67f5675a1f15d1.rmeta --cap-lints allow`\n         Compiling parking_lot_core v0.8.6\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/parking_lot_core-0.8.6/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"backtrace\", \"deadlock_detection\", \"nightly\", \"petgraph\", \"thread-id\"))' -C metadata=e36b8b1d7602edbf -C extra-filename=-e36b8b1d7602edbf --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/parking_lot_core-e36b8b1d7602edbf -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling zerocopy-derive v0.7.35\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name zerocopy_derive --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/zerocopy-derive-0.7.35/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type proc-macro --emit=dep-info,link -C prefer-dynamic -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=c32dc9592cf4c364 -C extra-filename=-c32dc9592cf4c364 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern proc_macro2=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libproc_macro2-a53443ded1dfcef2.rlib --extern quote=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libquote-4c220be45cf63bba.rlib --extern syn=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libsyn-a7527cdbfb662196.rlib --extern proc_macro --cap-lints allow`\n         Compiling rawpointer v0.2.1\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name rawpointer --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rawpointer-0.2.1/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=25344ca069a916c0 -C extra-filename=-25344ca069a916c0 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling byteorder v1.5.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name byteorder --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/byteorder-1.5.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"i128\", \"std\"))' -C metadata=701a861a4d770fe6 -C extra-filename=-701a861a4d770fe6 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling pkg-config v0.3.30\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name pkg_config --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/pkg-config-0.3.30/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=e2aa6d146c930417 -C extra-filename=-e2aa6d146c930417 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling either v1.13.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name either --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/either-1.13.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"use_std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"serde\", \"use_std\"))' -C metadata=cbb2d4c5b75a4107 -C extra-filename=-cbb2d4c5b75a4107 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling regex-syntax v0.8.4\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name regex_syntax --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/regex-syntax-0.8.4/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --cfg 'feature=\"unicode\"' --cfg 'feature=\"unicode-age\"' --cfg 'feature=\"unicode-bool\"' --cfg 'feature=\"unicode-case\"' --cfg 'feature=\"unicode-gencat\"' --cfg 'feature=\"unicode-perl\"' --cfg 'feature=\"unicode-script\"' --cfg 'feature=\"unicode-segment\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"arbitrary\", \"default\", \"std\", \"unicode\", \"unicode-age\", \"unicode-bool\", \"unicode-case\", \"unicode-gencat\", \"unicode-perl\", \"unicode-script\", \"unicode-segment\"))' -C metadata=189987509e6b620a -C extra-filename=-189987509e6b620a --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling zerocopy v0.7.35\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name zerocopy --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/zerocopy-0.7.35/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"byteorder\"' --cfg 'feature=\"default\"' --cfg 'feature=\"derive\"' --cfg 'feature=\"simd\"' --cfg 'feature=\"zerocopy-derive\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"__internal_use_only_features_that_work_on_stable\", \"alloc\", \"byteorder\", \"default\", \"derive\", \"simd\", \"simd-nightly\", \"zerocopy-derive\"))' -C metadata=038baa6cbd8eb075 -C extra-filename=-038baa6cbd8eb075 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern byteorder=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libbyteorder-701a861a4d770fe6.rmeta --extern zerocopy_derive=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libzerocopy_derive-c32dc9592cf4c364.so --cap-lints allow`\n         Compiling onig_sys v69.8.1\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/onig_sys-69.8.1/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"bindgen\", \"default\", \"generate\", \"posix-api\", \"print-debug\"))' -C metadata=178fcc49a4a36512 -C extra-filename=-178fcc49a4a36512 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/onig_sys-178fcc49a4a36512 -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern cc=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libcc-94c042aa09c605a0.rlib --extern pkg_config=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libpkg_config-e2aa6d146c930417.rlib --cap-lints allow`\n         Compiling darling_macro v0.10.2\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name darling_macro --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/darling_macro-0.10.2/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type proc-macro --emit=dep-info,link -C prefer-dynamic -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=7c5d1759444ecb34 -C extra-filename=-7c5d1759444ecb34 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern darling_core=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libdarling_core-79999c50edd86e3a.rlib --extern quote=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libquote-4c220be45cf63bba.rlib --extern syn=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libsyn-3a67f5675a1f15d1.rlib --extern proc_macro --cap-lints allow`\n           Running `/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/parking_lot_core-e36b8b1d7602edbf/build-script-build`\n         Compiling serde_derive v1.0.208\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name serde_derive --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/serde_derive-1.0.208/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type proc-macro --emit=dep-info,link -C prefer-dynamic -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"deserialize_in_place\"))' -C metadata=cdab2c8bee3f7a78 -C extra-filename=-cdab2c8bee3f7a78 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern proc_macro2=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libproc_macro2-a53443ded1dfcef2.rlib --extern quote=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libquote-4c220be45cf63bba.rlib --extern syn=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libsyn-a7527cdbfb662196.rlib --extern proc_macro --cap-lints allow`\n           Running `/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/serde-54947cb5889e94c1/build-script-build`\n           Running `/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/rayon-core-2202607f5aa42298/build-script-build`\n           Running `/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/lock_api-73b0bf1b17677ff7/build-script-build`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name getrandom --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/getrandom-0.1.16/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"bindgen\", \"compiler_builtins\", \"core\", \"dummy\", \"js-sys\", \"log\", \"rustc-dep-of-std\", \"std\", \"stdweb\", \"test-in-browser\", \"wasm-bindgen\"))' -C metadata=8d6eb4053cea3593 -C extra-filename=-8d6eb4053cea3593 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern cfg_if=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libcfg_if-069aa8d3b6fffb61.rmeta --extern libc=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/liblibc-8262032f1e05c089.rmeta --cap-lints allow`\n         Compiling crossbeam-deque v0.8.5\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name crossbeam_deque --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/crossbeam-deque-0.8.5/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"std\"))' -C metadata=743c0f6ed6fe6fe8 -C extra-filename=-743c0f6ed6fe6fe8 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern crossbeam_epoch=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libcrossbeam_epoch-1162c98467d8fcf6.rmeta --extern crossbeam_utils=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libcrossbeam_utils-5fe1a42cb8431226.rmeta --cap-lints allow`\n         Compiling matrixmultiply v0.3.9\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/matrixmultiply-0.3.9/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"cgemm\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"cgemm\", \"constconf\", \"default\", \"num_cpus\", \"once_cell\", \"std\", \"thread-tree\", \"threading\"))' -C metadata=e7d43178843fe7b3 -C extra-filename=-e7d43178843fe7b3 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/matrixmultiply-e7d43178843fe7b3 -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern autocfg=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libautocfg-7883da62ebd09624.rlib --cap-lints allow`\n         Compiling regex-automata v0.4.7\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name regex_automata --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/regex-automata-0.4.7/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"alloc\"' --cfg 'feature=\"dfa-onepass\"' --cfg 'feature=\"hybrid\"' --cfg 'feature=\"meta\"' --cfg 'feature=\"nfa-backtrack\"' --cfg 'feature=\"nfa-pikevm\"' --cfg 'feature=\"nfa-thompson\"' --cfg 'feature=\"perf-inline\"' --cfg 'feature=\"perf-literal\"' --cfg 'feature=\"perf-literal-multisubstring\"' --cfg 'feature=\"perf-literal-substring\"' --cfg 'feature=\"std\"' --cfg 'feature=\"syntax\"' --cfg 'feature=\"unicode\"' --cfg 'feature=\"unicode-age\"' --cfg 'feature=\"unicode-bool\"' --cfg 'feature=\"unicode-case\"' --cfg 'feature=\"unicode-gencat\"' --cfg 'feature=\"unicode-perl\"' --cfg 'feature=\"unicode-script\"' --cfg 'feature=\"unicode-segment\"' --cfg 'feature=\"unicode-word-boundary\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"alloc\", \"default\", \"dfa\", \"dfa-build\", \"dfa-onepass\", \"dfa-search\", \"hybrid\", \"internal-instrument\", \"internal-instrument-pikevm\", \"logging\", \"meta\", \"nfa\", \"nfa-backtrack\", \"nfa-pikevm\", \"nfa-thompson\", \"perf\", \"perf-inline\", \"perf-literal\", \"perf-literal-multisubstring\", \"perf-literal-substring\", \"std\", \"syntax\", \"unicode\", \"unicode-age\", \"unicode-bool\", \"unicode-case\", \"unicode-gencat\", \"unicode-perl\", \"unicode-script\", \"unicode-segment\", \"unicode-word-boundary\"))' -C metadata=ed7cef13f17ea6d9 -C extra-filename=-ed7cef13f17ea6d9 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern aho_corasick=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libaho_corasick-3ab56ade417ee174.rmeta --extern memchr=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libmemchr-98aad6ebcffcf120.rmeta --extern regex_syntax=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libregex_syntax-189987509e6b620a.rmeta --cap-lints allow`\n         Compiling num-complex v0.2.4\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/num-complex-0.2.4/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"i128\", \"rand\", \"serde\", \"std\"))' -C metadata=23abeec020ef583f -C extra-filename=-23abeec020ef583f --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/num-complex-23abeec020ef583f -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern autocfg=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libautocfg-7883da62ebd09624.rlib --cap-lints allow`\n         Compiling instant v0.1.13\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name instant --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/instant-0.1.13/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"inaccurate\", \"js-sys\", \"now\", \"stdweb\", \"wasm-bindgen\", \"wasm-bindgen_rs\", \"web-sys\"))' -C metadata=30811dddb1d7c14f -C extra-filename=-30811dddb1d7c14f --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern cfg_if=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libcfg_if-069aa8d3b6fffb61.rmeta --cap-lints allow`\n         Compiling unindent v0.1.11\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name unindent --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/unindent-0.1.11/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=c0ca27a732d0d4f2 -C extra-filename=-c0ca27a732d0d4f2 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling inventory v0.1.11\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/inventory-0.1.11/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=1d913761fc71377a -C extra-filename=-1d913761fc71377a --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/inventory-1d913761fc71377a -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling scopeguard v1.2.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name scopeguard --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/scopeguard-1.2.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"use_std\"))' -C metadata=ac8773d0c8be3a88 -C extra-filename=-ac8773d0c8be3a88 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name lock_api --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/lock_api-0.4.12/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"atomic_usize\"' --cfg 'feature=\"default\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"arc_lock\", \"atomic_usize\", \"default\", \"nightly\", \"owning_ref\", \"serde\"))' -C metadata=f83b6d9ef7c7f49f -C extra-filename=-f83b6d9ef7c7f49f --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern scopeguard=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libscopeguard-ac8773d0c8be3a88.rmeta --cap-lints allow --cfg has_const_fn_trait_bound`\n           Running `/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/inventory-1d913761fc71377a/build-script-build`\n         Compiling indoc-impl v0.3.6\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name indoc_impl --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/indoc-impl-0.3.6/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type proc-macro --emit=dep-info,link -C prefer-dynamic -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"unstable\"))' -C metadata=31120b60d9d8ff7a -C extra-filename=-31120b60d9d8ff7a --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern proc_macro_hack=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libproc_macro_hack-b49030fd20457a17.so --extern proc_macro2=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libproc_macro2-a53443ded1dfcef2.rlib --extern quote=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libquote-4c220be45cf63bba.rlib --extern syn=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libsyn-3a67f5675a1f15d1.rlib --extern unindent=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libunindent-c0ca27a732d0d4f2.rlib --extern proc_macro --cap-lints allow`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name parking_lot_core --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/parking_lot_core-0.8.6/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"backtrace\", \"deadlock_detection\", \"nightly\", \"petgraph\", \"thread-id\"))' -C metadata=36d039a4665525ee -C extra-filename=-36d039a4665525ee --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern cfg_if=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libcfg_if-069aa8d3b6fffb61.rmeta --extern instant=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libinstant-30811dddb1d7c14f.rmeta --extern libc=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/liblibc-8262032f1e05c089.rmeta --extern smallvec=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libsmallvec-54b60278d2fd4af7.rmeta --cap-lints allow`\n         Compiling regex v1.10.6\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name regex --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/regex-1.10.6/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"perf\"' --cfg 'feature=\"perf-backtrack\"' --cfg 'feature=\"perf-cache\"' --cfg 'feature=\"perf-dfa\"' --cfg 'feature=\"perf-inline\"' --cfg 'feature=\"perf-literal\"' --cfg 'feature=\"perf-onepass\"' --cfg 'feature=\"std\"' --cfg 'feature=\"unicode\"' --cfg 'feature=\"unicode-age\"' --cfg 'feature=\"unicode-bool\"' --cfg 'feature=\"unicode-case\"' --cfg 'feature=\"unicode-gencat\"' --cfg 'feature=\"unicode-perl\"' --cfg 'feature=\"unicode-script\"' --cfg 'feature=\"unicode-segment\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"logging\", \"pattern\", \"perf\", \"perf-backtrack\", \"perf-cache\", \"perf-dfa\", \"perf-dfa-full\", \"perf-inline\", \"perf-literal\", \"perf-onepass\", \"std\", \"unicode\", \"unicode-age\", \"unicode-bool\", \"unicode-case\", \"unicode-gencat\", \"unicode-perl\", \"unicode-script\", \"unicode-segment\", \"unstable\", \"use_std\"))' -C metadata=883f132f16a8c2d0 -C extra-filename=-883f132f16a8c2d0 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern aho_corasick=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libaho_corasick-3ab56ade417ee174.rmeta --extern memchr=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libmemchr-98aad6ebcffcf120.rmeta --extern regex_automata=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libregex_automata-ed7cef13f17ea6d9.rmeta --extern regex_syntax=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libregex_syntax-189987509e6b620a.rmeta --cap-lints allow`\n           Running `/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/num-complex-23abeec020ef583f/build-script-build`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name serde --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/serde-1.0.208/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"derive\"' --cfg 'feature=\"rc\"' --cfg 'feature=\"serde_derive\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"alloc\", \"default\", \"derive\", \"rc\", \"serde_derive\", \"std\", \"unstable\"))' -C metadata=05c41278f1822ffa -C extra-filename=-05c41278f1822ffa --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern serde_derive=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libserde_derive-cdab2c8bee3f7a78.so --cap-lints allow --check-cfg 'cfg(no_core_cstr)' --check-cfg 'cfg(no_core_num_saturating)' --check-cfg 'cfg(no_core_try_from)' --check-cfg 'cfg(no_diagnostic_namespace)' --check-cfg 'cfg(no_float_copysign)' --check-cfg 'cfg(no_num_nonzero_signed)' --check-cfg 'cfg(no_relaxed_trait_bounds)' --check-cfg 'cfg(no_serde_derive)' --check-cfg 'cfg(no_std_atomic)' --check-cfg 'cfg(no_std_atomic64)' --check-cfg 'cfg(no_systemtime_checked_add)' --check-cfg 'cfg(no_target_has_atomic)'`\n         Compiling darling v0.10.2\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name darling --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/darling-0.10.2/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --cfg 'feature=\"suggestions\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"diagnostics\", \"suggestions\"))' -C metadata=ad121ed9a36f1792 -C extra-filename=-ad121ed9a36f1792 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern darling_core=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libdarling_core-79999c50edd86e3a.rmeta --extern darling_macro=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libdarling_macro-7c5d1759444ecb34.so --cap-lints allow`\n           Running `/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/matrixmultiply-e7d43178843fe7b3/build-script-build`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name rayon_core --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rayon-core-1.12.1/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"web_spin_lock\"))' -C metadata=42d3d4e7963916c1 -C extra-filename=-42d3d4e7963916c1 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern crossbeam_deque=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libcrossbeam_deque-743c0f6ed6fe6fe8.rmeta --extern crossbeam_utils=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libcrossbeam_utils-5fe1a42cb8431226.rmeta --cap-lints allow`\n         Compiling rand_core v0.5.1\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name rand_core --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rand_core-0.5.1/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"alloc\"' --cfg 'feature=\"getrandom\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"alloc\", \"getrandom\", \"serde\", \"serde1\", \"std\"))' -C metadata=eb45dce8fe3856ac -C extra-filename=-eb45dce8fe3856ac --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern getrandom=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libgetrandom-8d6eb4053cea3593.rmeta --cap-lints allow`\n           Running `/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/onig_sys-178fcc49a4a36512/build-script-build`\n         Compiling ctor v0.1.26\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name ctor --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/ctor-0.1.26/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type proc-macro --emit=dep-info,link -C prefer-dynamic -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=957f2898f4676888 -C extra-filename=-957f2898f4676888 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern quote=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libquote-4c220be45cf63bba.rlib --extern syn=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libsyn-3a67f5675a1f15d1.rlib --extern proc_macro --cap-lints allow`\n         Compiling inventory-impl v0.1.11\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name inventory_impl --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/inventory-impl-0.1.11/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type proc-macro --emit=dep-info,link -C prefer-dynamic -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=f31c599e4feebe61 -C extra-filename=-f31c599e4feebe61 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern proc_macro2=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libproc_macro2-a53443ded1dfcef2.rlib --extern quote=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libquote-4c220be45cf63bba.rlib --extern syn=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libsyn-3a67f5675a1f15d1.rlib --extern proc_macro --cap-lints allow`\n         Compiling ppv-lite86 v0.2.20\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name ppv_lite86 --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/ppv-lite86-0.2.20/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"simd\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"no_simd\", \"simd\", \"std\"))' -C metadata=2c7f0af64e74045b -C extra-filename=-2c7f0af64e74045b --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern zerocopy=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libzerocopy-038baa6cbd8eb075.rmeta --cap-lints allow`\n         Compiling ghost v0.1.17\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name ghost --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/ghost-0.1.17/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type proc-macro --emit=dep-info,link -C prefer-dynamic -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=ef121d9eb7fe0345 -C extra-filename=-ef121d9eb7fe0345 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern proc_macro2=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libproc_macro2-a53443ded1dfcef2.rlib --extern quote=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libquote-4c220be45cf63bba.rlib --extern syn=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libsyn-a7527cdbfb662196.rlib --extern proc_macro --cap-lints allow`\n         Compiling pyo3-derive-backend v0.12.4\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name pyo3_derive_backend --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/pyo3-derive-backend-0.12.4/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=fb5705e5a6f8418d -C extra-filename=-fb5705e5a6f8418d --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern proc_macro2=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libproc_macro2-a53443ded1dfcef2.rmeta --extern quote=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libquote-4c220be45cf63bba.rmeta --extern syn=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libsyn-3a67f5675a1f15d1.rmeta --cap-lints allow`\n         Compiling paste-impl v0.1.18\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name paste_impl --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/paste-impl-0.1.18/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type proc-macro --emit=dep-info,link -C prefer-dynamic -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=af92603fbf9be662 -C extra-filename=-af92603fbf9be662 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern proc_macro_hack=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libproc_macro_hack-b49030fd20457a17.so --extern proc_macro --cap-lints allow`\n         Compiling num-integer v0.1.46\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name num_integer --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/num-integer-0.1.46/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"i128\", \"std\"))' -C metadata=6b9813fd539b3b7a -C extra-filename=-6b9813fd539b3b7a --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern num_traits=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libnum_traits-04f24cb06d3c2fbe.rmeta --cap-lints allow`\n         Compiling esaxx-rs v0.1.10\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/esaxx-rs-0.1.10/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"cc\"' --cfg 'feature=\"cpp\"' --cfg 'feature=\"default\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"cc\", \"cpp\", \"default\"))' -C metadata=c586f188cc80cffc -C extra-filename=-c586f188cc80cffc --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/esaxx-rs-c586f188cc80cffc -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern cc=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libcc-94c042aa09c605a0.rlib --cap-lints allow`\n         Compiling atty v0.2.14\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name atty --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/atty-0.2.14/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=824de6acfdffc9b1 -C extra-filename=-824de6acfdffc9b1 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern libc=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/liblibc-8262032f1e05c089.rmeta --cap-lints allow`\n         Compiling serde_json v1.0.125\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/serde_json-1.0.125/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"alloc\", \"arbitrary_precision\", \"default\", \"float_roundtrip\", \"indexmap\", \"preserve_order\", \"raw_value\", \"std\", \"unbounded_depth\"))' -C metadata=72c53176e9434372 -C extra-filename=-72c53176e9434372 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/serde_json-72c53176e9434372 -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling pyo3 v0.12.4\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/pyo3-0.12.4/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"ctor\"' --cfg 'feature=\"default\"' --cfg 'feature=\"extension-module\"' --cfg 'feature=\"indoc\"' --cfg 'feature=\"inventory\"' --cfg 'feature=\"macros\"' --cfg 'feature=\"paste\"' --cfg 'feature=\"pyo3cls\"' --cfg 'feature=\"unindent\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"ctor\", \"default\", \"extension-module\", \"hashbrown\", \"indoc\", \"inventory\", \"macros\", \"nightly\", \"num-bigint\", \"num-complex\", \"paste\", \"pyo3cls\", \"python3\", \"unindent\"))' -C metadata=34b558ef4d8979ef -C extra-filename=-34b558ef4d8979ef --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/pyo3-34b558ef4d8979ef -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling bitflags v1.3.2\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name bitflags --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/bitflags-1.3.2/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"compiler_builtins\", \"core\", \"default\", \"example_generated\", \"rustc-dep-of-std\"))' -C metadata=572d549a09e7b042 -C extra-filename=-572d549a09e7b042 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling lazy_static v1.5.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name lazy_static --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/lazy_static-1.5.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"spin\", \"spin_no_std\"))' -C metadata=b8cd5ec493744c98 -C extra-filename=-b8cd5ec493744c98 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling derive_builder v0.9.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_mod --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/derive_builder-0.9.0/build/mod.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"compiletest_rs\", \"env_logger\", \"log\", \"logging\", \"nightlytests\", \"skeptic\", \"skeptic_tests\"))' -C metadata=f6af66a5f4111def -C extra-filename=-f6af66a5f4111def --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/derive_builder-f6af66a5f4111def -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling minimal-lexical v0.2.1\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name minimal_lexical --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/minimal-lexical-0.2.1/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"alloc\", \"compact\", \"default\", \"lint\", \"nightly\", \"std\"))' -C metadata=88cdc20278564cf1 -C extra-filename=-88cdc20278564cf1 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n           Running `/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/derive_builder-f6af66a5f4111def/build-script-mod`\n         Compiling pyo3cls v0.12.4\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name pyo3cls --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/pyo3cls-0.12.4/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type proc-macro --emit=dep-info,link -C prefer-dynamic -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=5a5ad2657c3500fc -C extra-filename=-5a5ad2657c3500fc --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern pyo3_derive_backend=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libpyo3_derive_backend-fb5705e5a6f8418d.rlib --extern quote=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libquote-4c220be45cf63bba.rlib --extern syn=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libsyn-3a67f5675a1f15d1.rlib --extern proc_macro --cap-lints allow`\n         Compiling nom v7.1.3\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name nom --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/nom-7.1.3/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"alloc\"' --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"alloc\", \"default\", \"docsrs\", \"std\"))' -C metadata=8a084cf9e98aef2c -C extra-filename=-8a084cf9e98aef2c --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern memchr=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libmemchr-98aad6ebcffcf120.rmeta --extern minimal_lexical=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libminimal_lexical-88cdc20278564cf1.rmeta --cap-lints allow`\n           Running `/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/pyo3-34b558ef4d8979ef/build-script-build`\n         Compiling console v0.15.8\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name console --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/console-0.15.8/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"ansi-parsing\"' --cfg 'feature=\"default\"' --cfg 'feature=\"unicode-width\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"ansi-parsing\", \"default\", \"unicode-width\", \"windows-console-colors\"))' -C metadata=7389811a5688094b -C extra-filename=-7389811a5688094b --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern lazy_static=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/liblazy_static-b8cd5ec493744c98.rmeta --extern libc=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/liblibc-8262032f1e05c089.rmeta --extern unicode_width=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libunicode_width-147e4b4395044b58.rmeta --cap-lints allow`\n           Running `/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/serde_json-72c53176e9434372/build-script-build`\n           Running `/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/esaxx-rs-c586f188cc80cffc/build-script-build`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name inventory --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/inventory-0.1.11/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=01732795dcd7bb16 -C extra-filename=-01732795dcd7bb16 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern ctor=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libctor-957f2898f4676888.so --extern ghost=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libghost-ef121d9eb7fe0345.so --extern inventory_impl=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libinventory_impl-f31c599e4feebe61.so --cap-lints allow`\n         Compiling paste v0.1.18\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name paste --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/paste-0.1.18/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=77cfc92c5c84a892 -C extra-filename=-77cfc92c5c84a892 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern paste_impl=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libpaste_impl-af92603fbf9be662.so --extern proc_macro_hack=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libproc_macro_hack-b49030fd20457a17.so --cap-lints allow`\n         Compiling rand_chacha v0.2.2\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name rand_chacha --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rand_chacha-0.2.2/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"simd\", \"std\"))' -C metadata=d82021d0089f938a -C extra-filename=-d82021d0089f938a --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern ppv_lite86=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libppv_lite86-2c7f0af64e74045b.rmeta --extern rand_core=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/librand_core-eb45dce8fe3856ac.rmeta --cap-lints allow`\n         Compiling rayon v1.10.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name rayon --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rayon-1.10.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"web_spin_lock\"))' -C metadata=8704d374a66af352 -C extra-filename=-8704d374a66af352 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern either=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libeither-cbb2d4c5b75a4107.rmeta --extern rayon_core=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/librayon_core-42d3d4e7963916c1.rmeta --cap-lints allow`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name matrixmultiply --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/matrixmultiply-0.3.9/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"cgemm\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"cgemm\", \"constconf\", \"default\", \"num_cpus\", \"once_cell\", \"std\", \"thread-tree\", \"threading\"))' -C metadata=fef05d356b78315f -C extra-filename=-fef05d356b78315f --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern rawpointer=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/librawpointer-25344ca069a916c0.rmeta --cap-lints allow`\n         Compiling derive_builder_core v0.9.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name derive_builder_core --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/derive_builder_core-0.9.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"log\", \"logging\"))' -C metadata=e7db7296aa9f92b1 -C extra-filename=-e7db7296aa9f92b1 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern darling=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libdarling-ad121ed9a36f1792.rmeta --extern proc_macro2=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libproc_macro2-a53443ded1dfcef2.rmeta --extern quote=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libquote-4c220be45cf63bba.rmeta --extern syn=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libsyn-3a67f5675a1f15d1.rmeta --cap-lints allow`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name num_complex --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/num-complex-0.2.4/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"i128\", \"rand\", \"serde\", \"std\"))' -C metadata=88ee02742315b6d2 -C extra-filename=-88ee02742315b6d2 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern num_traits=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libnum_traits-04f24cb06d3c2fbe.rmeta --cap-lints allow --cfg has_i128 --cfg has_const_fn`\n         Compiling parking_lot v0.11.2\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name parking_lot --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/parking_lot-0.11.2/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"arc_lock\", \"deadlock_detection\", \"default\", \"nightly\", \"owning_ref\", \"send_guard\", \"serde\", \"stdweb\", \"wasm-bindgen\"))' -C metadata=6389d506bd23dbb3 -C extra-filename=-6389d506bd23dbb3 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern instant=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libinstant-30811dddb1d7c14f.rmeta --extern lock_api=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/liblock_api-f83b6d9ef7c7f49f.rmeta --extern parking_lot_core=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libparking_lot_core-36d039a4665525ee.rmeta --cap-lints allow`\n         Compiling indoc v0.3.6\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name indoc --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/indoc-0.3.6/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"unstable\"))' -C metadata=5ae5844505251d95 -C extra-filename=-5ae5844505251d95 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern indoc_impl=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libindoc_impl-31120b60d9d8ff7a.so --extern proc_macro_hack=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libproc_macro_hack-b49030fd20457a17.so --cap-lints allow`\n         Compiling itertools v0.8.2\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name itertools --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/itertools-0.8.2/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"use_std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"use_std\"))' -C metadata=23c94ea62554b785 -C extra-filename=-23c94ea62554b785 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern either=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libeither-cbb2d4c5b75a4107.rmeta --cap-lints allow`\n         Compiling textwrap v0.11.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name textwrap --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/textwrap-0.11.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"hyphenation\", \"term_size\"))' -C metadata=b47182d7f07a0e83 -C extra-filename=-b47182d7f07a0e83 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern unicode_width=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libunicode_width-147e4b4395044b58.rmeta --cap-lints allow`\n         Compiling num-complex v0.4.6\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name num_complex --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/num-complex-0.4.6/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"bytecheck\", \"bytemuck\", \"default\", \"libm\", \"rand\", \"rkyv\", \"serde\", \"std\"))' -C metadata=4bce8ee361c8cbb5 -C extra-filename=-4bce8ee361c8cbb5 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern num_traits=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libnum_traits-04f24cb06d3c2fbe.rmeta --cap-lints allow`\n         Compiling once_cell v1.19.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name once_cell --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/once_cell-1.19.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"alloc\"' --cfg 'feature=\"default\"' --cfg 'feature=\"race\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"alloc\", \"atomic-polyfill\", \"critical-section\", \"default\", \"parking_lot\", \"portable-atomic\", \"race\", \"std\", \"unstable\"))' -C metadata=9e854eed23737148 -C extra-filename=-9e854eed23737148 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name unindent --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/unindent-0.1.11/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=78354fc3d1163902 -C extra-filename=-78354fc3d1163902 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling vec_map v0.8.2\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name vec_map --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/vec_map-0.8.2/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"eders\", \"serde\"))' -C metadata=fa32a1cd80d7fcbc -C extra-filename=-fa32a1cd80d7fcbc --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling itoa v1.0.11\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name itoa --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/itoa-1.0.11/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"no-panic\"))' -C metadata=5ca595a7c9604cbf -C extra-filename=-5ca595a7c9604cbf --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling log v0.4.22\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name log --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/log-0.4.22/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"kv\", \"kv_serde\", \"kv_std\", \"kv_sval\", \"kv_unstable\", \"kv_unstable_serde\", \"kv_unstable_std\", \"kv_unstable_sval\", \"max_level_debug\", \"max_level_error\", \"max_level_info\", \"max_level_off\", \"max_level_trace\", \"max_level_warn\", \"release_max_level_debug\", \"release_max_level_error\", \"release_max_level_info\", \"release_max_level_off\", \"release_max_level_trace\", \"release_max_level_warn\", \"serde\", \"std\", \"sval\", \"sval_ref\", \"value-bag\"))' -C metadata=fee03072b48908b6 -C extra-filename=-fee03072b48908b6 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling ndarray v0.13.1\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name build_script_build --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/ndarray-0.13.1/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"approx\", \"blas\", \"blas-src\", \"cblas-sys\", \"docs\", \"rayon\", \"serde\", \"serde-1\", \"test\", \"test-blas-openblas-sys\"))' -C metadata=ffd10b2b084df0ee -C extra-filename=-ffd10b2b084df0ee --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/ndarray-ffd10b2b084df0ee -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling unicode-segmentation v1.11.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name unicode_segmentation --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/unicode-segmentation-1.11.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"no_std\"))' -C metadata=f9cb403ccf9aafe3 -C extra-filename=-f9cb403ccf9aafe3 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling strsim v0.8.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name strsim --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/strsim-0.8.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=25f5b22ddc7add9b -C extra-filename=-25f5b22ddc7add9b --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling number_prefix v0.3.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name number_prefix --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/number_prefix-0.3.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"std\"))' -C metadata=47262f61f1c6a366 -C extra-filename=-47262f61f1c6a366 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling ansi_term v0.12.1\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name ansi_term --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/ansi_term-0.12.1/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"derive_serde_style\", \"serde\"))' -C metadata=9697d021dfc96b25 -C extra-filename=-9697d021dfc96b25 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling base64 v0.13.1\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name base64 --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/base64-0.13.1/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"alloc\", \"default\", \"std\"))' -C metadata=835cd68726307e54 -C extra-filename=-835cd68726307e54 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling ryu v1.0.18\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name ryu --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/ryu-1.0.18/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"no-panic\", \"small\"))' -C metadata=9fe2ffa6563da72a -C extra-filename=-9fe2ffa6563da72a --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling quick-error v1.2.3\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name quick_error --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/quick-error-1.2.3/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=83540d84ed6b4b87 -C extra-filename=-83540d84ed6b4b87 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name serde_json --edition=2021 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/serde_json-1.0.125/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"alloc\", \"arbitrary_precision\", \"default\", \"float_roundtrip\", \"indexmap\", \"preserve_order\", \"raw_value\", \"std\", \"unbounded_depth\"))' -C metadata=fa4c0abbd1ca3397 -C extra-filename=-fa4c0abbd1ca3397 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern itoa=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libitoa-5ca595a7c9604cbf.rmeta --extern memchr=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libmemchr-98aad6ebcffcf120.rmeta --extern ryu=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libryu-9fe2ffa6563da72a.rmeta --extern serde=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libserde-05c41278f1822ffa.rmeta --cap-lints allow --cfg limb_width_64 --check-cfg 'cfg(limb_width_32)' --check-cfg 'cfg(limb_width_64)'`\n         Compiling humantime v1.3.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name humantime --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/humantime-1.3.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=b17c493756e9b200 -C extra-filename=-b17c493756e9b200 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern quick_error=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libquick_error-83540d84ed6b4b87.rmeta --cap-lints allow`\n         Compiling spm_precompiled v0.1.4\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name spm_precompiled --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/spm_precompiled-0.1.4/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=43a0b58c3a4cde0e -C extra-filename=-43a0b58c3a4cde0e --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern base64=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libbase64-835cd68726307e54.rmeta --extern nom=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libnom-8a084cf9e98aef2c.rmeta --extern serde=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libserde-05c41278f1822ffa.rmeta --extern unicode_segmentation=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libunicode_segmentation-f9cb403ccf9aafe3.rmeta --cap-lints allow`\n         Compiling clap v2.34.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name clap --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/clap-2.34.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"ansi_term\"' --cfg 'feature=\"atty\"' --cfg 'feature=\"color\"' --cfg 'feature=\"default\"' --cfg 'feature=\"strsim\"' --cfg 'feature=\"suggestions\"' --cfg 'feature=\"vec_map\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"ansi_term\", \"atty\", \"clippy\", \"color\", \"debug\", \"default\", \"doc\", \"nightly\", \"no_cargo\", \"strsim\", \"suggestions\", \"term_size\", \"unstable\", \"vec_map\", \"wrap_help\", \"yaml\", \"yaml-rust\"))' -C metadata=8b73c3ff02b2061a -C extra-filename=-8b73c3ff02b2061a --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern ansi_term=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libansi_term-9697d021dfc96b25.rmeta --extern atty=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libatty-824de6acfdffc9b1.rmeta --extern bitflags=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libbitflags-572d549a09e7b042.rmeta --extern strsim=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libstrsim-25f5b22ddc7add9b.rmeta --extern textwrap=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libtextwrap-b47182d7f07a0e83.rmeta --extern unicode_width=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libunicode_width-147e4b4395044b58.rmeta --extern vec_map=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libvec_map-fa32a1cd80d7fcbc.rmeta --cap-lints allow`\n         Compiling indicatif v0.15.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name indicatif --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/indicatif-0.15.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"improved_unicode\", \"rayon\", \"unicode-segmentation\", \"unicode-width\", \"with_rayon\"))' -C metadata=92e5708bfdd21479 -C extra-filename=-92e5708bfdd21479 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern console=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libconsole-7389811a5688094b.rmeta --extern lazy_static=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/liblazy_static-b8cd5ec493744c98.rmeta --extern number_prefix=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libnumber_prefix-47262f61f1c6a366.rmeta --extern regex=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libregex-883f132f16a8c2d0.rmeta --cap-lints allow`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name onig_sys --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/onig_sys-69.8.1/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"bindgen\", \"default\", \"generate\", \"posix-api\", \"print-debug\"))' -C metadata=b402d193bca184db -C extra-filename=-b402d193bca184db --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow -L native=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/onig_sys-1f90ae5af86047a8/out -l static=onig`\n         Compiling onig v6.4.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name onig --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/onig-6.4.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"generate\", \"posix-api\", \"print-debug\", \"std-pattern\"))' -C metadata=ac548520ce4f12f7 -C extra-filename=-ac548520ce4f12f7 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern bitflags=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libbitflags-572d549a09e7b042.rmeta --extern once_cell=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libonce_cell-9e854eed23737148.rmeta --extern onig_sys=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libonig_sys-b402d193bca184db.rmeta --cap-lints allow -L native=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/onig_sys-1f90ae5af86047a8/out`\n           Running `/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/ndarray-ffd10b2b084df0ee/build-script-build`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name pyo3 --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/pyo3-0.12.4/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"ctor\"' --cfg 'feature=\"default\"' --cfg 'feature=\"extension-module\"' --cfg 'feature=\"indoc\"' --cfg 'feature=\"inventory\"' --cfg 'feature=\"macros\"' --cfg 'feature=\"paste\"' --cfg 'feature=\"pyo3cls\"' --cfg 'feature=\"unindent\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"ctor\", \"default\", \"extension-module\", \"hashbrown\", \"indoc\", \"inventory\", \"macros\", \"nightly\", \"num-bigint\", \"num-complex\", \"paste\", \"pyo3cls\", \"python3\", \"unindent\"))' -C metadata=95ae9d9bf4e7ce9e -C extra-filename=-95ae9d9bf4e7ce9e --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern ctor=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libctor-957f2898f4676888.so --extern indoc=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libindoc-5ae5844505251d95.rmeta --extern inventory=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libinventory-01732795dcd7bb16.rmeta --extern libc=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/liblibc-8262032f1e05c089.rmeta --extern parking_lot=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libparking_lot-6389d506bd23dbb3.rmeta --extern paste=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libpaste-77cfc92c5c84a892.rmeta --extern pyo3cls=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libpyo3cls-5a5ad2657c3500fc.so --extern unindent=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libunindent-78354fc3d1163902.rmeta --cap-lints allow --cfg Py_3_5 --cfg Py_3_6 --cfg Py_3_7 --cfg Py_3_8 --cfg Py_3_9 --cfg Py_3_10 --cfg Py_3 --cfg 'py_sys_config=\"WITH_THREAD\"'`\n         Compiling ndarray v0.16.1\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name ndarray --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/ndarray-0.16.1/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"approx\", \"blas\", \"default\", \"docs\", \"matrixmultiply-threading\", \"portable-atomic-critical-section\", \"rayon\", \"serde\", \"serde-1\", \"std\", \"test\"))' -C metadata=9cdbb7e178a9dab9 -C extra-filename=-9cdbb7e178a9dab9 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern matrixmultiply=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libmatrixmultiply-fef05d356b78315f.rmeta --extern num_complex=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libnum_complex-4bce8ee361c8cbb5.rmeta --extern num_integer=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libnum_integer-6b9813fd539b3b7a.rmeta --extern num_traits=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libnum_traits-04f24cb06d3c2fbe.rmeta --extern rawpointer=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/librawpointer-25344ca069a916c0.rmeta --cap-lints allow`\n         Compiling rayon-cond v0.1.0 (https://github.com/n1t0/rayon-cond#c56e4f1d)\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name rayon_cond --edition=2015 /root/.cargo/git/checkouts/rayon-cond-7ecee7da88eb486f/c56e4f1/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=825f7e3b4ea61273 -C extra-filename=-825f7e3b4ea61273 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern either=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libeither-cbb2d4c5b75a4107.rmeta --extern itertools=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libitertools-23c94ea62554b785.rmeta --extern rayon=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/librayon-8704d374a66af352.rmeta --cap-lints allow`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name derive_builder --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/derive_builder-0.9.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type proc-macro --emit=dep-info,link -C prefer-dynamic -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"compiletest_rs\", \"env_logger\", \"log\", \"logging\", \"nightlytests\", \"skeptic\", \"skeptic_tests\"))' -C metadata=1858b2f9319ccc12 -C extra-filename=-1858b2f9319ccc12 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern darling=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libdarling-ad121ed9a36f1792.rlib --extern derive_builder_core=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libderive_builder_core-e7db7296aa9f92b1.rlib --extern proc_macro2=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libproc_macro2-a53443ded1dfcef2.rlib --extern quote=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libquote-4c220be45cf63bba.rlib --extern syn=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libsyn-3a67f5675a1f15d1.rlib --extern proc_macro --cap-lints allow`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name esaxx_rs --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/esaxx-rs-0.1.10/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"cc\"' --cfg 'feature=\"cpp\"' --cfg 'feature=\"default\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"cc\", \"cpp\", \"default\"))' -C metadata=37214f5b5ebdfb66 -C extra-filename=-37214f5b5ebdfb66 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow -L native=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/esaxx-rs-f18adfc51d566de3/out -l static=esaxx -l stdc++`\n         Compiling rand v0.7.3\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name rand --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rand-0.7.3/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"alloc\"' --cfg 'feature=\"default\"' --cfg 'feature=\"getrandom\"' --cfg 'feature=\"getrandom_package\"' --cfg 'feature=\"libc\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"alloc\", \"default\", \"getrandom\", \"getrandom_package\", \"libc\", \"log\", \"nightly\", \"packed_simd\", \"rand_pcg\", \"serde1\", \"simd_support\", \"small_rng\", \"std\", \"stdweb\", \"wasm-bindgen\"))' -C metadata=d987cd4e82b0315f -C extra-filename=-d987cd4e82b0315f --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern getrandom_package=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libgetrandom-8d6eb4053cea3593.rmeta --extern libc=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/liblibc-8262032f1e05c089.rmeta --extern rand_chacha=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/librand_chacha-d82021d0089f938a.rmeta --extern rand_core=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/librand_core-eb45dce8fe3856ac.rmeta --cap-lints allow`\n         Compiling itertools v0.9.0\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name itertools --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/itertools-0.9.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"use_std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"use_std\"))' -C metadata=120219380b521206 -C extra-filename=-120219380b521206 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern either=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libeither-cbb2d4c5b75a4107.rmeta --cap-lints allow`\n         Compiling matrixmultiply v0.2.4\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name matrixmultiply --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/matrixmultiply-0.2.4/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"std\"))' -C metadata=e81e1bb8e77793e1 -C extra-filename=-e81e1bb8e77793e1 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern rawpointer=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/librawpointer-25344ca069a916c0.rmeta --cap-lints allow`\n         Compiling unicode-normalization-alignments v0.1.12\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name unicode_normalization_alignments --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/unicode-normalization-alignments-0.1.12/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=63bf7301ba1e71a4 -C extra-filename=-63bf7301ba1e71a4 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern smallvec=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libsmallvec-54b60278d2fd4af7.rmeta --cap-lints allow`\n         Compiling regex-syntax v0.6.29\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name regex_syntax --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/regex-syntax-0.6.29/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"unicode\"' --cfg 'feature=\"unicode-age\"' --cfg 'feature=\"unicode-bool\"' --cfg 'feature=\"unicode-case\"' --cfg 'feature=\"unicode-gencat\"' --cfg 'feature=\"unicode-perl\"' --cfg 'feature=\"unicode-script\"' --cfg 'feature=\"unicode-segment\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"unicode\", \"unicode-age\", \"unicode-bool\", \"unicode-case\", \"unicode-gencat\", \"unicode-perl\", \"unicode-script\", \"unicode-segment\"))' -C metadata=436fa53568c5ecc4 -C extra-filename=-436fa53568c5ecc4 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling cfg-if v0.1.10\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name cfg_if --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/cfg-if-0.1.10/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"compiler_builtins\", \"core\", \"rustc-dep-of-std\"))' -C metadata=61526e3ac0a5d11a -C extra-filename=-61526e3ac0a5d11a --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling termcolor v1.4.1\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name termcolor --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/termcolor-1.4.1/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=91817b919abf241e -C extra-filename=-91817b919abf241e --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling unicode_categories v0.1.1\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name unicode_categories --edition=2015 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/unicode_categories-0.1.1/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=6cc42eb7547cbd30 -C extra-filename=-6cc42eb7547cbd30 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --cap-lints allow`\n         Compiling env_logger v0.7.1\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name env_logger --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/env_logger-0.7.1/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"atty\"' --cfg 'feature=\"default\"' --cfg 'feature=\"humantime\"' --cfg 'feature=\"regex\"' --cfg 'feature=\"termcolor\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"atty\", \"default\", \"humantime\", \"regex\", \"termcolor\"))' -C metadata=25532a30b2e55d0b -C extra-filename=-25532a30b2e55d0b --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern atty=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libatty-824de6acfdffc9b1.rmeta --extern humantime=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libhumantime-b17c493756e9b200.rmeta --extern log=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/liblog-fee03072b48908b6.rmeta --extern regex=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libregex-883f132f16a8c2d0.rmeta --extern termcolor=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libtermcolor-91817b919abf241e.rmeta --cap-lints allow`\n         Compiling numpy v0.11.0 (https://github.com/pyo3/rust-numpy/?rev=e331befa27fede78d4662edf08fa0508db39be01#e331befa)\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name numpy --edition=2018 /root/.cargo/git/checkouts/rust-numpy-ea13d29ac0b89ab6/e331bef/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"python3\"))' -C metadata=8dbec0b557a24b63 -C extra-filename=-8dbec0b557a24b63 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern cfg_if=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libcfg_if-61526e3ac0a5d11a.rmeta --extern libc=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/liblibc-8262032f1e05c089.rmeta --extern ndarray=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libndarray-9cdbb7e178a9dab9.rmeta --extern num_complex=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libnum_complex-88ee02742315b6d2.rmeta --extern num_traits=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libnum_traits-04f24cb06d3c2fbe.rmeta --extern pyo3=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libpyo3-95ae9d9bf4e7ce9e.rmeta --cap-lints allow`\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name ndarray --edition=2018 /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/ndarray-0.13.1/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"approx\", \"blas\", \"blas-src\", \"cblas-sys\", \"docs\", \"rayon\", \"serde\", \"serde-1\", \"test\", \"test-blas-openblas-sys\"))' -C metadata=8406a1ca7eb20aaa -C extra-filename=-8406a1ca7eb20aaa --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern matrixmultiply=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libmatrixmultiply-e81e1bb8e77793e1.rmeta --extern num_complex=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libnum_complex-88ee02742315b6d2.rmeta --extern num_integer=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libnum_integer-6b9813fd539b3b7a.rmeta --extern num_traits=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libnum_traits-04f24cb06d3c2fbe.rmeta --extern rawpointer=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/librawpointer-25344ca069a916c0.rmeta --cap-lints allow`\n         Compiling tokenizers v0.10.1 (/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/tokenizers-lib)\n           Running `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name tokenizers --edition=2018 tokenizers-lib/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"indicatif\"' --cfg 'feature=\"progressbar\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"indicatif\", \"progressbar\"))' -C metadata=b8be9f1c19456a03 -C extra-filename=-b8be9f1c19456a03 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern clap=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libclap-8b73c3ff02b2061a.rmeta --extern derive_builder=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libderive_builder-1858b2f9319ccc12.so --extern esaxx_rs=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libesaxx_rs-37214f5b5ebdfb66.rmeta --extern indicatif=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libindicatif-92e5708bfdd21479.rmeta --extern itertools=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libitertools-120219380b521206.rmeta --extern lazy_static=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/liblazy_static-b8cd5ec493744c98.rmeta --extern log=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/liblog-fee03072b48908b6.rmeta --extern onig=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libonig-ac548520ce4f12f7.rmeta --extern rand=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/librand-d987cd4e82b0315f.rmeta --extern rayon=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/librayon-8704d374a66af352.rmeta --extern rayon_cond=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/librayon_cond-825f7e3b4ea61273.rmeta --extern regex=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libregex-883f132f16a8c2d0.rmeta --extern regex_syntax=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libregex_syntax-436fa53568c5ecc4.rmeta --extern serde=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libserde-05c41278f1822ffa.rmeta --extern serde_json=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libserde_json-fa4c0abbd1ca3397.rmeta --extern spm_precompiled=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libspm_precompiled-43a0b58c3a4cde0e.rmeta --extern unicode_normalization_alignments=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libunicode_normalization_alignments-63bf7301ba1e71a4.rmeta --extern unicode_segmentation=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libunicode_segmentation-f9cb403ccf9aafe3.rmeta --extern unicode_categories=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libunicode_categories-6cc42eb7547cbd30.rmeta -L native=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/esaxx-rs-f18adfc51d566de3/out -L native=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/onig_sys-1f90ae5af86047a8/out`\n      warning: `#[macro_use]` only has an effect on `extern crate` and modules\n        --> tokenizers-lib/src/utils/mod.rs:24:1\n         |\n      24 | #[macro_use]\n         | ^^^^^^^^^^^^\n         |\n         = note: `#[warn(unused_attributes)]` on by default\n      \n      warning: `#[macro_use]` only has an effect on `extern crate` and modules\n        --> tokenizers-lib/src/utils/mod.rs:35:1\n         |\n      35 | #[macro_use]\n         | ^^^^^^^^^^^^\n      \n      warning: variable does not need to be mutable\n         --> tokenizers-lib/src/models/unigram/model.rs:280:21\n          |\n      280 |                 let mut target_node = &mut best_path_ends_at[key_pos];\n          |                     ----^^^^^^^^^^^\n          |                     |\n          |                     help: remove this `mut`\n          |\n          = note: `#[warn(unused_mut)]` on by default\n      \n      warning: variable does not need to be mutable\n         --> tokenizers-lib/src/models/unigram/model.rs:297:21\n          |\n      297 |                 let mut target_node = &mut best_path_ends_at[starts_at + mblen];\n          |                     ----^^^^^^^^^^^\n          |                     |\n          |                     help: remove this `mut`\n      \n      warning: variable does not need to be mutable\n         --> tokenizers-lib/src/pre_tokenizers/byte_level.rs:175:59\n          |\n      175 |     encoding.process_tokens_with_offsets_mut(|(i, (token, mut offsets))| {\n          |                                                           ----^^^^^^^\n          |                                                           |\n          |                                                           help: remove this `mut`\n      \n      warning: fields `bos_id` and `eos_id` are never read\n        --> tokenizers-lib/src/models/unigram/lattice.rs:59:5\n         |\n      53 | pub struct Lattice<'a> {\n         |            ------- fields in this struct\n      ...\n      59 |     bos_id: usize,\n         |     ^^^^^^\n      60 |     eos_id: usize,\n         |     ^^^^^^\n         |\n         = note: `Lattice` has a derived impl for the trait `Debug`, but this is intentionally ignored during dead code analysis\n         = note: `#[warn(dead_code)]` on by default\n      \n      error: casting `&T` to `&mut T` is undefined behavior, even if the reference is unused, consider instead using an `UnsafeCell`\n         --> tokenizers-lib/src/models/bpe/trainer.rs:517:47\n          |\n      513 |                     let w = &words[*i] as *const _ as *mut _;\n          |                             -------------------------------- casting happend here\n      ...\n      517 |                         let word: &mut Word = &mut (*w);\n          |                                               ^^^^^^^^^\n          |\n          = note: for more information, visit <https://doc.rust-lang.org/book/ch15-05-interior-mutability.html>\n          = note: `#[deny(invalid_reference_casting)]` on by default\n      \n      warning: `tokenizers` (lib) generated 6 warnings\n      error: could not compile `tokenizers` (lib) due to 1 previous error; 6 warnings emitted\n      \n      Caused by:\n        process didn't exit successfully: `/root/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/bin/rustc --crate-name tokenizers --edition=2018 tokenizers-lib/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"indicatif\"' --cfg 'feature=\"progressbar\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"indicatif\", \"progressbar\"))' -C metadata=b8be9f1c19456a03 -C extra-filename=-b8be9f1c19456a03 --out-dir /tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps -C strip=debuginfo -L dependency=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps --extern clap=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libclap-8b73c3ff02b2061a.rmeta --extern derive_builder=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libderive_builder-1858b2f9319ccc12.so --extern esaxx_rs=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libesaxx_rs-37214f5b5ebdfb66.rmeta --extern indicatif=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libindicatif-92e5708bfdd21479.rmeta --extern itertools=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libitertools-120219380b521206.rmeta --extern lazy_static=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/liblazy_static-b8cd5ec493744c98.rmeta --extern log=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/liblog-fee03072b48908b6.rmeta --extern onig=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libonig-ac548520ce4f12f7.rmeta --extern rand=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/librand-d987cd4e82b0315f.rmeta --extern rayon=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/librayon-8704d374a66af352.rmeta --extern rayon_cond=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/librayon_cond-825f7e3b4ea61273.rmeta --extern regex=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libregex-883f132f16a8c2d0.rmeta --extern regex_syntax=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libregex_syntax-436fa53568c5ecc4.rmeta --extern serde=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libserde-05c41278f1822ffa.rmeta --extern serde_json=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libserde_json-fa4c0abbd1ca3397.rmeta --extern spm_precompiled=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libspm_precompiled-43a0b58c3a4cde0e.rmeta --extern unicode_normalization_alignments=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libunicode_normalization_alignments-63bf7301ba1e71a4.rmeta --extern unicode_segmentation=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libunicode_segmentation-f9cb403ccf9aafe3.rmeta --extern unicode_categories=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/deps/libunicode_categories-6cc42eb7547cbd30.rmeta -L native=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/esaxx-rs-f18adfc51d566de3/out -L native=/tmp/pip-install-jqdkt9sg/tokenizers_9cb5cfdb7be24897b88de47450b2fd84/target/release/build/onig_sys-1f90ae5af86047a8/out` (exit status: 1)\n      warning: build failed, waiting for other jobs to finish...\n      error: `cargo rustc --lib --message-format=json-render-diagnostics --manifest-path Cargo.toml --release -v --features pyo3/extension-module --crate-type cdylib --` failed with code 101\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\n  ERROR: Failed building wheel for tokenizers\n\u001b[?25hFailed to build tokenizers\nERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!. $HOME/.cargo/env && pip install transformers==4.11.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3906e03a",
   "metadata": {},
   "source": [
    "### Step 19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c492432",
   "metadata": {},
   "source": [
    "Execution Start time: 05:13:37, End time: 05:13:44 (duration: 7 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b4c806",
   "metadata": {},
   "source": [
    "**Thought**: The installation of the `transformers` library is failing due to an issue with building the `tokenizers` package. This might be related to the specific version of `tokenizers` required by `transformers==4.11.3`. I will try installing a compatible version of `tokenizers` separately to see if it resolves the issue.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3579d22a",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d40928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting tokenizers==0.10.3\n  Downloading tokenizers-0.10.3.tar.gz (212 kB)\n\u001b[?25l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/212.7 kB ? eta -:--:--\n\n\u001b[?25h  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\n\u001b[?25hBuilding wheels for collected packages: tokenizers\n  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l-\b \berror\n  error: subprocess-exited-with-error\n  \n  × Building wheel for tokenizers (pyproject.toml) did not run successfully.\n  │ exit code: 1\n  ╰─> [51 lines of output]\n      running bdist_wheel\n      running build\n      running build_py\n      creating build\n      creating build/lib.linux-x86_64-cpython-310\n      creating build/lib.linux-x86_64-cpython-310/tokenizers\n      copying py_src/tokenizers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/models\n      copying py_src/tokenizers/models/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/models\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/decoders\n      copying py_src/tokenizers/decoders/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/decoders\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/normalizers\n      copying py_src/tokenizers/normalizers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/normalizers\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/pre_tokenizers\n      copying py_src/tokenizers/pre_tokenizers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/pre_tokenizers\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/processors\n      copying py_src/tokenizers/processors/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/processors\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/trainers\n      copying py_src/tokenizers/trainers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/trainers\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/base_tokenizer.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/bert_wordpiece.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/byte_level_bpe.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/char_level_bpe.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/sentencepiece_bpe.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/sentencepiece_unigram.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/tools\n      copying py_src/tokenizers/tools/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/tools\n      copying py_src/tokenizers/tools/visualizer.py -> build/lib.linux-x86_64-cpython-310/tokenizers/tools\n      copying py_src/tokenizers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers\n      copying py_src/tokenizers/models/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/models\n      copying py_src/tokenizers/decoders/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/decoders\n      copying py_src/tokenizers/normalizers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/normalizers\n      copying py_src/tokenizers/pre_tokenizers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/pre_tokenizers\n      copying py_src/tokenizers/processors/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/processors\n      copying py_src/tokenizers/trainers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/trainers\n      copying py_src/tokenizers/tools/visualizer-styles.css -> build/lib.linux-x86_64-cpython-310/tokenizers/tools\n      running build_ext\n      running build_rust\n      error: can't find Rust compiler\n      \n      If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n      \n      To update pip, run:\n      \n          pip install --upgrade pip\n      \n      and then retry package installation.\n      \n      If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\n  ERROR: Failed building wheel for tokenizers\n\u001b[?25hFailed to build tokenizers\nERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install tokenizers==0.10.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28948b1",
   "metadata": {},
   "source": [
    "### Step 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7214a733",
   "metadata": {},
   "source": [
    "Execution Start time: 05:13:46, End time: 05:13:51 (duration: 5 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154448b6",
   "metadata": {},
   "source": [
    "**Thought**: It seems there is still an issue with the Rust compiler not being recognized. I will try to update pip first, as suggested, to see if it resolves the issue with installing the `tokenizers` package.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f0d6af",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119bd1e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Requirement already satisfied: pip in /usr/local/lib/python3.10/site-packages (24.0)\nCollecting pip\n  Downloading pip-24.2-py3-none-any.whl.metadata (3.6 kB)\nDownloading pip-24.2-py3-none-any.whl (1.8 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/1.8 MB ? eta -:--:--\n\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 24.0\n    Uninstalling pip-24.0:\n      Successfully uninstalled pip-24.0\nSuccessfully installed pip-24.2\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a89d2dc",
   "metadata": {},
   "source": [
    "### Step 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217da7a3",
   "metadata": {},
   "source": [
    "Execution Start time: 05:13:53, End time: 05:13:59 (duration: 5 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdad07d9",
   "metadata": {},
   "source": [
    "**Thought**: Now that pip has been upgraded, I will attempt to install the `transformers` library again, which should also handle the `tokenizers` dependency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905c3df6",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fa1fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting transformers==4.11.3\n  Downloading transformers-4.11.3-py3-none-any.whl.metadata (53 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (3.15.1)\nRequirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (0.23.3)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (1.25.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (2023.12.25)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (2.32.3)\nCollecting sacremoses (from transformers==4.11.3)\n  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\nCollecting tokenizers<0.11,>=0.10.1 (from transformers==4.11.3)\n  Downloading tokenizers-0.10.3.tar.gz (212 kB)\n  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\n\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/site-packages (from transformers==4.11.3) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.0.17->transformers==4.11.3) (2023.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.0.17->transformers==4.11.3) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.11.3) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.11.3) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.11.3) (2.0.7)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.11.3) (2024.6.2)\nRequirement already satisfied: click in /usr/local/lib/python3.10/site-packages (from sacremoses->transformers==4.11.3) (8.1.7)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/site-packages (from sacremoses->transformers==4.11.3) (1.4.2)\nDownloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/2.9 MB ? eta -:--:--\n\u001b[2K   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.3/2.9 MB ? eta -:--:--\n\n\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n\u001b[?25l   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/897.5 kB ? eta -:--:--\n\n\u001b[?25hBuilding wheels for collected packages: tokenizers\n  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l-\b \berror\n  error: subprocess-exited-with-error\n  \n  × Building wheel for tokenizers (pyproject.toml) did not run successfully.\n  │ exit code: 1\n  ╰─> [51 lines of output]\n      running bdist_wheel\n      running build\n      running build_py\n      creating build\n      creating build/lib.linux-x86_64-cpython-310\n      creating build/lib.linux-x86_64-cpython-310/tokenizers\n      copying py_src/tokenizers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/models\n      copying py_src/tokenizers/models/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/models\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/decoders\n      copying py_src/tokenizers/decoders/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/decoders\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/normalizers\n      copying py_src/tokenizers/normalizers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/normalizers\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/pre_tokenizers\n      copying py_src/tokenizers/pre_tokenizers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/pre_tokenizers\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/processors\n      copying py_src/tokenizers/processors/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/processors\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/trainers\n      copying py_src/tokenizers/trainers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/trainers\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/base_tokenizer.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/bert_wordpiece.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/byte_level_bpe.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/char_level_bpe.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/sentencepiece_bpe.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      copying py_src/tokenizers/implementations/sentencepiece_unigram.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n      creating build/lib.linux-x86_64-cpython-310/tokenizers/tools\n      copying py_src/tokenizers/tools/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/tools\n      copying py_src/tokenizers/tools/visualizer.py -> build/lib.linux-x86_64-cpython-310/tokenizers/tools\n      copying py_src/tokenizers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers\n      copying py_src/tokenizers/models/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/models\n      copying py_src/tokenizers/decoders/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/decoders\n      copying py_src/tokenizers/normalizers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/normalizers\n      copying py_src/tokenizers/pre_tokenizers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/pre_tokenizers\n      copying py_src/tokenizers/processors/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/processors\n      copying py_src/tokenizers/trainers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/trainers\n      copying py_src/tokenizers/tools/visualizer-styles.css -> build/lib.linux-x86_64-cpython-310/tokenizers/tools\n      running build_ext\n      running build_rust\n      error: can't find Rust compiler\n      \n      If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n      \n      To update pip, run:\n      \n          pip install --upgrade pip\n      \n      and then retry package installation.\n      \n      If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\n  ERROR: Failed building wheel for tokenizers\n\u001b[?25hFailed to build tokenizers\nERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (tokenizers)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install transformers==4.11.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5e7390",
   "metadata": {},
   "source": [
    "### Step 22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e318cf34",
   "metadata": {},
   "source": [
    "Execution Start time: 05:14:00, End time: 05:14:02 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac75c07",
   "metadata": {},
   "source": [
    "**Thought**: The issue with the Rust compiler persists. It seems that the Rust environment is not being recognized. I will attempt to install the `transformers` library without the `tokenizers` dependency by using a more recent version of `transformers` that might have pre-built wheels for `tokenizers`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d820b80f",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7c000e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Requirement already satisfied: transformers in /usr/local/lib/python3.10/site-packages (4.40.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/site-packages (from transformers) (0.23.3)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from transformers) (1.25.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (2.0.7)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2db2576",
   "metadata": {},
   "source": [
    "### Step 23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9ce899",
   "metadata": {},
   "source": [
    "Execution Start time: 05:14:04, End time: 05:14:11 (duration: 6 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb255fb",
   "metadata": {},
   "source": [
    "**Thought**: The `transformers` library is already installed, and it seems to be a more recent version. I will now attempt to run the training script again to see if the issue has been resolved.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8d96e6",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77819aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2024-08-22 05:14:08,462 - INFO - PyTorch version 2.2.1+cpu available.\nTraceback (most recent call last):\n  File \"/content/MeZO/large_models/run.py\", line 24, in <module>\n    from trainer import OurTrainer\n  File \"/content/MeZO/large_models/trainer.py\", line 43, in <module>\n    from transformers.integrations import (  # isort: split\nImportError: cannot import name 'default_hp_search_backend' from 'transformers.integrations' (/usr/local/lib/python3.10/site-packages/transformers/integrations/__init__.py)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python run.py \\\n",
    "    --task RTE \\\n",
    "    --output_dir result/output \\\n",
    "    --num_train 10 \\\n",
    "    --num_dev 10 \\\n",
    "    --num_eval 10 \\\n",
    "    --trainer zo \\\n",
    "    --train_set_seed 1234 \\\n",
    "    --train_as_classification \\\n",
    "    --num_train_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea77558",
   "metadata": {},
   "source": [
    "### Step 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afce0602",
   "metadata": {},
   "source": [
    "Execution Start time: 05:14:12, End time: 05:14:13 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b691abe",
   "metadata": {},
   "source": [
    "**Thought**: The error persists due to the `default_hp_search_backend` import, which is not present in the current version of `transformers`. I will modify the `trainer.py` file to remove or replace this import with a compatible alternative.\n",
    "\n",
    "First, I'll open the `trainer.py` file to locate the problematic import.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f986a33",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cd278b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/MeZO/large_models/trainer.py (893 lines total)]\n1:# coding=utf-8\n2:# Copyright 2020-present the HuggingFace Inc. team.\n3:#\n4:# Licensed under the Apache License, Version 2.0 (the \"License\");\n5:# you may not use this file except in compliance with the License.\n6:# You may obtain a copy of the License at\n7:#\n8:#     http://www.apache.org/licenses/LICENSE-2.0\n9:#\n10:# Unless required by applicable law or agreed to in writing, software\n11:# distributed under the License is distributed on an \"AS IS\" BASIS,\n12:# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n13:# See the License for the specific language governing permissions and\n14:# limitations under the License.\n15:\"\"\"\n16:The Trainer class, to easily train a 🤗 Transformers from scratch or finetune it on a new task.\n17:\"\"\"\n18:\n19:import contextlib\n20:import functools\n21:import glob\n22:import inspect\n23:import math\n24:import os\n25:import random\n26:import re\n27:import shutil\n28:import sys\n29:import time\n30:import warnings\n31:from collections.abc import Mapping\n32:from pathlib import Path\n33:from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n34:import copy\n35:from metrics import f1\n36:import numpy as np\n37:\n38:from tqdm.auto import tqdm\n39:from transformers import Trainer\n40:from sklearn.linear_model import LinearRegression, LogisticRegression, LogisticRegressionCV\n41:\n42:# Integrations must be imported before ML frameworks:\n43:from transformers.integrations import (  # isort: split\n44:    default_hp_search_backend,\n45:    get_reporting_integration_callbacks,\n46:    hp_params,\n47:    is_fairscale_available,\n48:    is_optuna_available,\n49:    is_ray_tune_available,\n50:    is_sigopt_available,\n51:    is_wandb_available,\n52:    run_hp_search_optuna,\n53:    run_hp_search_ray,\n54:    run_hp_search_sigopt,\n55:    run_hp_search_wandb,\n56:)\n57:\n58:import numpy as np\n59:import torch\n60:import torch.distributed as dist\n61:from packaging import version\n62:from torch import nn\n63:from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n64:from torch.utils.data.distributed import DistributedSampler\n65:\n66:from huggingface_hub import Repository\n67:\n68:from transformers import __version__\n69:from transformers.configuration_utils import PretrainedConfig\n70:from transformers.data.data_collator import DataCollator, DataCollatorWithPadding, default_data_collator\n71:from transformers.debug_utils import DebugOption, DebugUnderflowOverflow\n72:from transformers.deepspeed import deepspeed_init, is_deepspeed_zero3_enabled\n73:from transformers.dependency_versions_check import dep_version_check\n74:from transformers.modelcard import TrainingSummary\n75:from transformers.modeling_utils import PreTrainedModel, load_sharded_checkpoint, unwrap_model\n76:from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES, MODEL_MAPPING_NAMES\n77:from transformers.optimization import Adafactor, get_scheduler\n78:from transformers.pytorch_utils import ALL_LAYERNORM_LAYERS, is_torch_greater_or_equal_than_1_10, is_torch_less_than_1_11\n79:from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n80:from transformers.trainer_callback import (\n81:    CallbackHandler,\n82:    DefaultFlowCallback,\n83:    PrinterCallback,\n84:    ProgressCallback,\n85:    TrainerCallback,\n86:    TrainerControl,\n87:    TrainerState,\n88:)\n89:from transformers.trainer_pt_utils import (\n90:    DistributedLengthGroupedSampler,\n91:    DistributedSamplerWithLoop,\n92:    DistributedTensorGatherer,\n93:    IterableDatasetShard,\n94:    LabelSmoother,\n95:    LengthGroupedSampler,\n96:    SequentialDistributedSampler,\n97:    ShardSampler,\n98:    distributed_broadcast_scalars,\n99:    distributed_concat,\n100:    find_batch_size,\n(793 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open trainer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e77ad5",
   "metadata": {},
   "source": [
    "### Step 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c391948d",
   "metadata": {},
   "source": [
    "Execution Start time: 05:14:15, End time: 05:14:16 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f8635a",
   "metadata": {},
   "source": [
    "**Thought**: The problematic import is located at line 44. I will remove the `default_hp_search_backend` import from the `transformers.integrations` module and attempt to run the script again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfeaae1",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78acf6fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/MeZO/large_models/trainer.py (892 lines total)]\n1:# coding=utf-8\n2:# Copyright 2020-present the HuggingFace Inc. team.\n3:#\n4:# Licensed under the Apache License, Version 2.0 (the \"License\");\n5:# you may not use this file except in compliance with the License.\n6:# You may obtain a copy of the License at\n7:#\n8:#     http://www.apache.org/licenses/LICENSE-2.0\n9:#\n10:# Unless required by applicable law or agreed to in writing, software\n11:# distributed under the License is distributed on an \"AS IS\" BASIS,\n12:# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n13:# See the License for the specific language governing permissions and\n14:# limitations under the License.\n15:\"\"\"\n16:The Trainer class, to easily train a 🤗 Transformers from scratch or finetune it on a new task.\n17:\"\"\"\n18:\n19:import contextlib\n20:import functools\n21:import glob\n22:import inspect\n23:import math\n24:import os\n25:import random\n26:import re\n27:import shutil\n28:import sys\n29:import time\n30:import warnings\n31:from collections.abc import Mapping\n32:from pathlib import Path\n33:from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n34:import copy\n35:from metrics import f1\n36:import numpy as np\n37:\n38:from tqdm.auto import tqdm\n39:from transformers import Trainer\n40:from sklearn.linear_model import LinearRegression, LogisticRegression, LogisticRegressionCV\n41:\n42:# Integrations must be imported before ML frameworks:\n43:from transformers.integrations import (  # isort: split\n44:    get_reporting_integration_callbacks,\n45:    hp_params,\n46:    is_fairscale_available,\n47:    is_optuna_available,\n48:    is_ray_tune_available,\n49:    is_sigopt_available,\n50:    is_wandb_available,\n51:    run_hp_search_optuna,\n52:    run_hp_search_ray,\n53:    run_hp_search_sigopt,\n54:    run_hp_search_wandb,\n55:)\n56:\n57:import numpy as np\n58:import torch\n59:import torch.distributed as dist\n60:from packaging import version\n61:from torch import nn\n62:from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n63:from torch.utils.data.distributed import DistributedSampler\n64:\n65:from huggingface_hub import Repository\n66:\n67:from transformers import __version__\n68:from transformers.configuration_utils import PretrainedConfig\n69:from transformers.data.data_collator import DataCollator, DataCollatorWithPadding, default_data_collator\n70:from transformers.debug_utils import DebugOption, DebugUnderflowOverflow\n71:from transformers.deepspeed import deepspeed_init, is_deepspeed_zero3_enabled\n72:from transformers.dependency_versions_check import dep_version_check\n73:from transformers.modelcard import TrainingSummary\n74:from transformers.modeling_utils import PreTrainedModel, load_sharded_checkpoint, unwrap_model\n75:from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES, MODEL_MAPPING_NAMES\n76:from transformers.optimization import Adafactor, get_scheduler\n77:from transformers.pytorch_utils import ALL_LAYERNORM_LAYERS, is_torch_greater_or_equal_than_1_10, is_torch_less_than_1_11\n78:from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n79:from transformers.trainer_callback import (\n80:    CallbackHandler,\n81:    DefaultFlowCallback,\n82:    PrinterCallback,\n83:    ProgressCallback,\n84:    TrainerCallback,\n85:    TrainerControl,\n86:    TrainerState,\n87:)\n88:from transformers.trainer_pt_utils import (\n89:    DistributedLengthGroupedSampler,\n90:    DistributedSamplerWithLoop,\n91:    DistributedTensorGatherer,\n92:    IterableDatasetShard,\n93:    LabelSmoother,\n94:    LengthGroupedSampler,\n95:    SequentialDistributedSampler,\n96:    ShardSampler,\n97:    distributed_broadcast_scalars,\n98:    distributed_concat,\n99:    find_batch_size,\n100:    get_module_class_from_name,\n(792 more lines below)\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 43:56\n",
    "from transformers.integrations import (  # isort: split\n",
    "    get_reporting_integration_callbacks,\n",
    "    hp_params,\n",
    "    is_fairscale_available,\n",
    "    is_optuna_available,\n",
    "    is_ray_tune_available,\n",
    "    is_sigopt_available,\n",
    "    is_wandb_available,\n",
    "    run_hp_search_optuna,\n",
    "    run_hp_search_ray,\n",
    "    run_hp_search_sigopt,\n",
    "    run_hp_search_wandb,\n",
    ")\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d197f473",
   "metadata": {},
   "source": [
    "### Step 26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe785b61",
   "metadata": {},
   "source": [
    "Execution Start time: 05:14:18, End time: 05:14:23 (duration: 5 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640316c6",
   "metadata": {},
   "source": [
    "**Thought**: The import statement for `default_hp_search_backend` has been removed. Now, I will attempt to run the training script again to see if the issue has been resolved.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd88a46",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f435b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2024-08-22 05:14:21,298 - INFO - PyTorch version 2.2.1+cpu available.\nTraceback (most recent call last):\n  File \"/content/MeZO/large_models/run.py\", line 24, in <module>\n    from trainer import OurTrainer\n  File \"/content/MeZO/large_models/trainer.py\", line 43, in <module>\n    from transformers.integrations import (  # isort: split\nImportError: cannot import name 'is_fairscale_available' from 'transformers.integrations' (/usr/local/lib/python3.10/site-packages/transformers/integrations/__init__.py)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python run.py \\\n",
    "    --task RTE \\\n",
    "    --output_dir result/output \\\n",
    "    --num_train 10 \\\n",
    "    --num_dev 10 \\\n",
    "    --num_eval 10 \\\n",
    "    --trainer zo \\\n",
    "    --train_set_seed 1234 \\\n",
    "    --train_as_classification \\\n",
    "    --num_train_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a154d826",
   "metadata": {},
   "source": [
    "### Step 27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3027ab24",
   "metadata": {},
   "source": [
    "Execution Start time: 05:14:26, End time: 05:14:27 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a5334f",
   "metadata": {},
   "source": [
    "**Thought**: The error now indicates that `is_fairscale_available` is also not present in the `transformers.integrations` module. I will remove this import as well and try running the script again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a023233e",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e913e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\n\nERRORS:\n- F821 undefined name 'is_fairscale_available'\n\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /content/MeZO/large_models/trainer.py (890 lines total)]\n(37 more lines above)\n38:from tqdm.auto import tqdm\n39:from transformers import Trainer\n40:from sklearn.linear_model import LinearRegression, LogisticRegression, LogisticRegressionCV\n41:\n42:# Integrations must be imported before ML frameworks:\n43:from transformers.integrations import (  # isort: split\n44:    get_reporting_integration_callbacks,\n45:    hp_params,\n46:    is_optuna_available,\n47:    is_ray_tune_available,\n48:    is_sigopt_available,\n49:    is_wandb_available,\n50:    run_hp_search_optuna,\n51:    run_hp_search_ray,\n52:    run_hp_search_sigopt,\n53:    run_hp_search_wandb,\n54:)\n55:import numpy as np\n56:import torch\n57:import torch.distributed as dist\n58:from packaging import version\n59:from torch import nn\n(831 more lines below)\n-------------------------------------------------\n\nThis is the original code before your edit\n-------------------------------------------------\n[File: /content/MeZO/large_models/trainer.py (892 lines total)]\n(37 more lines above)\n38:from tqdm.auto import tqdm\n39:from transformers import Trainer\n40:from sklearn.linear_model import LinearRegression, LogisticRegression, LogisticRegressionCV\n41:\n42:# Integrations must be imported before ML frameworks:\n43:from transformers.integrations import (  # isort: split\n44:    get_reporting_integration_callbacks,\n45:    hp_params,\n46:    is_fairscale_available,\n47:    is_optuna_available,\n48:    is_ray_tune_available,\n49:    is_sigopt_available,\n50:    is_wandb_available,\n51:    run_hp_search_optuna,\n52:    run_hp_search_ray,\n53:    run_hp_search_sigopt,\n54:    run_hp_search_wandb,\n55:)\n56:\n57:import numpy as np\n58:import torch\n59:import torch.distributed as dist\n60:from packaging import version\n61:from torch import nn\n(831 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 43:56\n",
    "from transformers.integrations import (  # isort: split\n",
    "    get_reporting_integration_callbacks,\n",
    "    hp_params,\n",
    "    is_optuna_available,\n",
    "    is_ray_tune_available,\n",
    "    is_sigopt_available,\n",
    "    is_wandb_available,\n",
    "    run_hp_search_optuna,\n",
    "    run_hp_search_ray,\n",
    "    run_hp_search_sigopt,\n",
    "    run_hp_search_wandb,\n",
    ")\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2cd59e",
   "metadata": {},
   "source": [
    "### Step 28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427a7e5c",
   "metadata": {},
   "source": [
    "Execution Start time: 05:14:28, End time: 05:14:28 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f07b60",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that `is_fairscale_available` is used elsewhere in the code. Instead of removing it from the import, I will comment out its usage in the code to prevent errors. Let's search for `is_fairscale_available` in the file to locate its usage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0b07cc",
   "metadata": {},
   "source": [
    "**Action (search_file)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c552ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No file open. Use the open command first.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_file is_fairscale_available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602620c7",
   "metadata": {},
   "source": [
    "### Step 29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00337eed",
   "metadata": {},
   "source": [
    "Execution Start time: 05:14:30, End time: 05:14:30 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90142b4",
   "metadata": {},
   "source": [
    "**Thought**: I will search for the term `is_fairscale_available` in the `trainer.py` file to locate where it is used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9be7ce",
   "metadata": {},
   "source": [
    "**Action (search_file)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14f317a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Found 2 matches for \"is_fairscale_available\" in /content/MeZO/large_models/trainer.py:\nLine 46:    is_fairscale_available,\nLine 177:if is_fairscale_available():\nEnd of matches for \"is_fairscale_available\" in /content/MeZO/large_models/trainer.py\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_file is_fairscale_available trainer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9d7ce7",
   "metadata": {},
   "source": [
    "### Step 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a4d2aa",
   "metadata": {},
   "source": [
    "Execution Start time: 05:14:31, End time: 05:14:32 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f326c7",
   "metadata": {},
   "source": [
    "**Thought**: The `is_fairscale_available` function is used at line 177. I will comment out this line to prevent errors and then try running the script again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7c8aa8",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8116d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/MeZO/large_models/trainer.py (892 lines total)]\n(126 more lines above)\n127:    find_executable_batch_size,\n128:    get_last_checkpoint,\n129:    has_length,\n130:    number_of_arguments,\n131:    seed_worker,\n132:    set_seed,\n133:    speed_metrics,\n134:)\n135:from transformers.training_args import OptimizerNames, ParallelMode, TrainingArguments\n136:from transformers.utils import (\n137:    CONFIG_NAME,\n138:    WEIGHTS_INDEX_NAME,\n139:    WEIGHTS_NAME,\n140:    find_labels,\n141:    get_full_repo_name,\n142:    is_apex_available,\n143:    is_datasets_available,\n144:    is_in_notebook,\n145:    is_ipex_available,\n146:    is_sagemaker_dp_enabled,\n147:    is_sagemaker_mp_enabled,\n148:    is_torch_tensorrt_fx_available,\n149:    is_torch_tpu_available,\n150:    is_torchdynamo_available,\n151:    logging,\n152:)\n153:from transformers.utils.generic import ContextManagers\n154:\n155:\n156:_is_native_cpu_amp_available = is_torch_greater_or_equal_than_1_10\n157:\n158:DEFAULT_CALLBACKS = [DefaultFlowCallback]\n159:DEFAULT_PROGRESS_CALLBACK = ProgressCallback\n160:\n161:if is_in_notebook():\n162:    from .utils.notebook import NotebookProgressCallback\n163:\n164:    DEFAULT_PROGRESS_CALLBACK = NotebookProgressCallback\n165:\n166:if is_apex_available():\n167:    from apex import amp\n168:\n169:if is_datasets_available():\n170:    import datasets\n171:\n172:if is_torch_tpu_available(check_device=False):\n173:    import torch_xla.core.xla_model as xm\n174:    import torch_xla.debug.metrics as met\n175:    import torch_xla.distributed.parallel_loader as pl\n176:\n177:# if is_fairscale_available():\n178:    dep_version_check(\"fairscale\")\n179:    import fairscale\n180:    from fairscale.nn.data_parallel import FullyShardedDataParallel as FullyShardedDDP\n181:    from fairscale.nn.data_parallel import ShardedDataParallel as ShardedDDP\n182:    from fairscale.nn.wrap import auto_wrap\n183:    from fairscale.optim import OSS\n184:    from fairscale.optim.grad_scaler import ShardedGradScaler\n185:\n186:\n187:if is_sagemaker_mp_enabled():\n188:    import smdistributed.modelparallel.torch as smp\n189:    from smdistributed.modelparallel import __version__ as SMP_VERSION\n190:\n191:    IS_SAGEMAKER_MP_POST_1_10 = version.parse(SMP_VERSION) >= version.parse(\"1.10\")\n192:\n193:    from .trainer_pt_utils import smp_forward_backward, smp_forward_only, smp_gather, smp_nested_concat\n194:else:\n195:    IS_SAGEMAKER_MP_POST_1_10 = False\n196:\n197:\n198:if TYPE_CHECKING:\n199:    import optuna\n200:\n201:logger = logging.get_logger(__name__)\n202:\n203:\n204:# Name of the files used for checkpointing\n205:TRAINING_ARGS_NAME = \"training_args.bin\"\n206:TRAINER_STATE_NAME = \"trainer_state.json\"\n207:OPTIMIZER_NAME = \"optimizer.pt\"\n208:SCHEDULER_NAME = \"scheduler.pt\"\n209:SCALER_NAME = \"scaler.pt\"\n210:\n211:\n212:class OurTrainer(Trainer):\n213:\n214:    from transformers.trainer_pt_utils import _get_learning_rate, log_metrics, metrics_format, save_metrics, save_state\n215:\n216:    def _inner_training_loop(\n217:        self, batch_size=None, args=None, resume_from_checkpoint=None, trial=None, ignore_keys_for_eval=None\n218:    ):\n219:        \"\"\"\n220:        We overload the original training loop to add linear probing and MeZO. Search key word \"MeZO added\"\n221:        for those updates.\n222:        \"\"\"\n223:        self._train_batch_size = batch_size\n224:        # Data loader and number of training steps\n225:        train_dataloader = self.get_train_dataloader()\n226:\n(666 more lines below)\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 177:177\n",
    "# if is_fairscale_available():\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e2ea2c",
   "metadata": {},
   "source": [
    "### Step 31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ebcb15",
   "metadata": {},
   "source": [
    "Execution Start time: 05:14:35, End time: 05:14:40 (duration: 5 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d44af66",
   "metadata": {},
   "source": [
    "**Thought**: The line using `is_fairscale_available` has been commented out. Now, I will attempt to run the training script again to see if the issue has been resolved.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffb4268",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93232880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2024-08-22 05:14:38,563 - INFO - PyTorch version 2.2.1+cpu available.\nTraceback (most recent call last):\n  File \"/content/MeZO/large_models/run.py\", line 24, in <module>\n    from trainer import OurTrainer\n  File \"/content/MeZO/large_models/trainer.py\", line 43, in <module>\n    from transformers.integrations import (  # isort: split\nImportError: cannot import name 'is_fairscale_available' from 'transformers.integrations' (/usr/local/lib/python3.10/site-packages/transformers/integrations/__init__.py)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python run.py \\\n",
    "    --task RTE \\\n",
    "    --output_dir result/output \\\n",
    "    --num_train 10 \\\n",
    "    --num_dev 10 \\\n",
    "    --num_eval 10 \\\n",
    "    --trainer zo \\\n",
    "    --train_set_seed 1234 \\\n",
    "    --train_as_classification \\\n",
    "    --num_train_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ede523",
   "metadata": {},
   "source": [
    "### Step 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68d9a75",
   "metadata": {},
   "source": [
    "Execution Start time: 05:14:43, End time: 05:14:44 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469ea77f",
   "metadata": {},
   "source": [
    "**Thought**: It seems that commenting out the usage of `is_fairscale_available` was not sufficient. I will remove the import statement for `is_fairscale_available` from the `transformers.integrations` module in the `trainer.py` file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaaa17b",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982ee14a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/MeZO/large_models/trainer.py (890 lines total)]\n1:# coding=utf-8\n2:# Copyright 2020-present the HuggingFace Inc. team.\n3:#\n4:# Licensed under the Apache License, Version 2.0 (the \"License\");\n5:# you may not use this file except in compliance with the License.\n6:# You may obtain a copy of the License at\n7:#\n8:#     http://www.apache.org/licenses/LICENSE-2.0\n9:#\n10:# Unless required by applicable law or agreed to in writing, software\n11:# distributed under the License is distributed on an \"AS IS\" BASIS,\n12:# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n13:# See the License for the specific language governing permissions and\n14:# limitations under the License.\n15:\"\"\"\n16:The Trainer class, to easily train a 🤗 Transformers from scratch or finetune it on a new task.\n17:\"\"\"\n18:\n19:import contextlib\n20:import functools\n21:import glob\n22:import inspect\n23:import math\n24:import os\n25:import random\n26:import re\n27:import shutil\n28:import sys\n29:import time\n30:import warnings\n31:from collections.abc import Mapping\n32:from pathlib import Path\n33:from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n34:import copy\n35:from metrics import f1\n36:import numpy as np\n37:\n38:from tqdm.auto import tqdm\n39:from transformers import Trainer\n40:from sklearn.linear_model import LinearRegression, LogisticRegression, LogisticRegressionCV\n41:\n42:# Integrations must be imported before ML frameworks:\n43:from transformers.integrations import (  # isort: split\n44:    get_reporting_integration_callbacks,\n45:    hp_params,\n46:    is_optuna_available,\n47:    is_ray_tune_available,\n48:    is_sigopt_available,\n49:    is_wandb_available,\n50:    run_hp_search_optuna,\n51:    run_hp_search_ray,\n52:    run_hp_search_sigopt,\n53:    run_hp_search_wandb,\n54:)\n55:import numpy as np\n56:import torch\n57:import torch.distributed as dist\n58:from packaging import version\n59:from torch import nn\n60:from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n61:from torch.utils.data.distributed import DistributedSampler\n62:\n63:from huggingface_hub import Repository\n64:\n65:from transformers import __version__\n66:from transformers.configuration_utils import PretrainedConfig\n67:from transformers.data.data_collator import DataCollator, DataCollatorWithPadding, default_data_collator\n68:from transformers.debug_utils import DebugOption, DebugUnderflowOverflow\n69:from transformers.deepspeed import deepspeed_init, is_deepspeed_zero3_enabled\n70:from transformers.dependency_versions_check import dep_version_check\n71:from transformers.modelcard import TrainingSummary\n72:from transformers.modeling_utils import PreTrainedModel, load_sharded_checkpoint, unwrap_model\n73:from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES, MODEL_MAPPING_NAMES\n74:from transformers.optimization import Adafactor, get_scheduler\n75:from transformers.pytorch_utils import ALL_LAYERNORM_LAYERS, is_torch_greater_or_equal_than_1_10, is_torch_less_than_1_11\n76:from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n77:from transformers.trainer_callback import (\n78:    CallbackHandler,\n79:    DefaultFlowCallback,\n80:    PrinterCallback,\n81:    ProgressCallback,\n82:    TrainerCallback,\n83:    TrainerControl,\n84:    TrainerState,\n85:)\n86:from transformers.trainer_pt_utils import (\n87:    DistributedLengthGroupedSampler,\n88:    DistributedSamplerWithLoop,\n89:    DistributedTensorGatherer,\n90:    IterableDatasetShard,\n91:    LabelSmoother,\n92:    LengthGroupedSampler,\n93:    SequentialDistributedSampler,\n94:    ShardSampler,\n95:    distributed_broadcast_scalars,\n96:    distributed_concat,\n97:    find_batch_size,\n98:    get_module_class_from_name,\n99:    get_parameter_names,\n100:    nested_concat,\n(790 more lines below)\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 43:56\n",
    "from transformers.integrations import (  # isort: split\n",
    "    get_reporting_integration_callbacks,\n",
    "    hp_params,\n",
    "    is_optuna_available,\n",
    "    is_ray_tune_available,\n",
    "    is_sigopt_available,\n",
    "    is_wandb_available,\n",
    "    run_hp_search_optuna,\n",
    "    run_hp_search_ray,\n",
    "    run_hp_search_sigopt,\n",
    "    run_hp_search_wandb,\n",
    ")\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6122bb",
   "metadata": {},
   "source": [
    "### Step 33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c0f9ee",
   "metadata": {},
   "source": [
    "Execution Start time: 05:14:46, End time: 05:14:51 (duration: 5 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e465347",
   "metadata": {},
   "source": [
    "**Thought**: The import statement for `is_fairscale_available` has been removed. Now, I will attempt to run the training script again to see if the issue has been resolved.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b78722e",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7dab5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2024-08-22 05:14:49,415 - INFO - PyTorch version 2.2.1+cpu available.\n/usr/local/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n  warnings.warn(\nTraceback (most recent call last):\n  File \"/content/MeZO/large_models/run.py\", line 24, in <module>\n    from trainer import OurTrainer\n  File \"/content/MeZO/large_models/trainer.py\", line 75, in <module>\n    from transformers.pytorch_utils import ALL_LAYERNORM_LAYERS, is_torch_greater_or_equal_than_1_10, is_torch_less_than_1_11\nImportError: cannot import name 'is_torch_greater_or_equal_than_1_10' from 'transformers.pytorch_utils' (/usr/local/lib/python3.10/site-packages/transformers/pytorch_utils.py)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python run.py \\\n",
    "    --task RTE \\\n",
    "    --output_dir result/output \\\n",
    "    --num_train 10 \\\n",
    "    --num_dev 10 \\\n",
    "    --num_eval 10 \\\n",
    "    --trainer zo \\\n",
    "    --train_set_seed 1234 \\\n",
    "    --train_as_classification \\\n",
    "    --num_train_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0173b68",
   "metadata": {},
   "source": [
    "### Step 34"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1095d5",
   "metadata": {},
   "source": [
    "Execution Start time: 05:14:53, End time: 05:14:53 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc64a956",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that `is_torch_greater_or_equal_than_1_10` is not present in the `transformers.pytorch_utils` module. I will open the `trainer.py` file to locate this import and remove or replace it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfdadc9",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7112328b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/MeZO/large_models/trainer.py (890 lines total)]\n(59 more lines above)\n60:from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n61:from torch.utils.data.distributed import DistributedSampler\n62:\n63:from huggingface_hub import Repository\n64:\n65:from transformers import __version__\n66:from transformers.configuration_utils import PretrainedConfig\n67:from transformers.data.data_collator import DataCollator, DataCollatorWithPadding, default_data_collator\n68:from transformers.debug_utils import DebugOption, DebugUnderflowOverflow\n69:from transformers.deepspeed import deepspeed_init, is_deepspeed_zero3_enabled\n70:from transformers.dependency_versions_check import dep_version_check\n71:from transformers.modelcard import TrainingSummary\n72:from transformers.modeling_utils import PreTrainedModel, load_sharded_checkpoint, unwrap_model\n73:from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES, MODEL_MAPPING_NAMES\n74:from transformers.optimization import Adafactor, get_scheduler\n75:from transformers.pytorch_utils import ALL_LAYERNORM_LAYERS, is_torch_greater_or_equal_than_1_10, is_torch_less_than_1_11\n76:from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n77:from transformers.trainer_callback import (\n78:    CallbackHandler,\n79:    DefaultFlowCallback,\n80:    PrinterCallback,\n81:    ProgressCallback,\n82:    TrainerCallback,\n83:    TrainerControl,\n84:    TrainerState,\n85:)\n86:from transformers.trainer_pt_utils import (\n87:    DistributedLengthGroupedSampler,\n88:    DistributedSamplerWithLoop,\n89:    DistributedTensorGatherer,\n90:    IterableDatasetShard,\n91:    LabelSmoother,\n92:    LengthGroupedSampler,\n93:    SequentialDistributedSampler,\n94:    ShardSampler,\n95:    distributed_broadcast_scalars,\n96:    distributed_concat,\n97:    find_batch_size,\n98:    get_module_class_from_name,\n99:    get_parameter_names,\n100:    nested_concat,\n101:    nested_detach,\n102:    nested_numpify,\n103:    nested_truncate,\n104:    nested_xla_mesh_reduce,\n105:    reissue_pt_warnings,\n106:)\n107:from transformers.trainer_utils import (\n108:    PREFIX_CHECKPOINT_DIR,\n109:    BestRun,\n110:    EvalLoopOutput,\n111:    EvalPrediction,\n112:    FSDPOption,\n113:    HPSearchBackend,\n114:    HubStrategy,\n115:    IntervalStrategy,\n116:    PredictionOutput,\n117:    RemoveColumnsCollator,\n118:    ShardedDDPOption,\n119:    TrainerMemoryTracker,\n120:    TrainOutput,\n121:    default_compute_objective,\n122:    default_hp_space,\n123:    denumpify_detensorize,\n124:    enable_full_determinism,\n125:    find_executable_batch_size,\n126:    get_last_checkpoint,\n127:    has_length,\n128:    number_of_arguments,\n129:    seed_worker,\n130:    set_seed,\n131:    speed_metrics,\n132:)\n133:from transformers.training_args import OptimizerNames, ParallelMode, TrainingArguments\n134:from transformers.utils import (\n135:    CONFIG_NAME,\n136:    WEIGHTS_INDEX_NAME,\n137:    WEIGHTS_NAME,\n138:    find_labels,\n139:    get_full_repo_name,\n140:    is_apex_available,\n141:    is_datasets_available,\n142:    is_in_notebook,\n143:    is_ipex_available,\n144:    is_sagemaker_dp_enabled,\n145:    is_sagemaker_mp_enabled,\n146:    is_torch_tensorrt_fx_available,\n147:    is_torch_tpu_available,\n148:    is_torchdynamo_available,\n149:    logging,\n150:)\n151:from transformers.utils.generic import ContextManagers\n152:\n153:\n154:_is_native_cpu_amp_available = is_torch_greater_or_equal_than_1_10\n155:\n156:DEFAULT_CALLBACKS = [DefaultFlowCallback]\n157:DEFAULT_PROGRESS_CALLBACK = ProgressCallback\n158:\n159:if is_in_notebook():\n(731 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open trainer.py 75"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9594470",
   "metadata": {},
   "source": [
    "### Step 35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51021cef",
   "metadata": {},
   "source": [
    "Execution Start time: 05:14:56, End time: 05:14:57 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f99f19",
   "metadata": {},
   "source": [
    "**Thought**: The import statement for `is_torch_greater_or_equal_than_1_10` is located at line 75. I will remove this import and any related usage in the code to prevent errors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4ec23c",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a85bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\n\nERRORS:\n- F821 undefined name 'is_torch_greater_or_equal_than_1_10'\n\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /content/MeZO/large_models/trainer.py (890 lines total)]\n(69 more lines above)\n69:from transformers.deepspeed import deepspeed_init, is_deepspeed_zero3_enabled\n70:from transformers.dependency_versions_check import dep_version_check\n71:from transformers.modelcard import TrainingSummary\n72:from transformers.modeling_utils import PreTrainedModel, load_sharded_checkpoint, unwrap_model\n73:from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES, MODEL_MAPPING_NAMES\n74:from transformers.optimization import Adafactor, get_scheduler\n75:from transformers.pytorch_utils import ALL_LAYERNORM_LAYERS, is_torch_less_than_1_11\n76:from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n77:from transformers.trainer_callback import (\n78:    CallbackHandler,\n79:    DefaultFlowCallback,\n(811 more lines below)\n-------------------------------------------------\n\nThis is the original code before your edit\n-------------------------------------------------\n[File: /content/MeZO/large_models/trainer.py (890 lines total)]\n(70 more lines above)\n70:from transformers.dependency_versions_check import dep_version_check\n71:from transformers.modelcard import TrainingSummary\n72:from transformers.modeling_utils import PreTrainedModel, load_sharded_checkpoint, unwrap_model\n73:from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES, MODEL_MAPPING_NAMES\n74:from transformers.optimization import Adafactor, get_scheduler\n75:from transformers.pytorch_utils import ALL_LAYERNORM_LAYERS, is_torch_greater_or_equal_than_1_10, is_torch_less_than_1_11\n76:from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n77:from transformers.trainer_callback import (\n78:    CallbackHandler,\n79:    DefaultFlowCallback,\n80:    PrinterCallback,\n(810 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 75:75\n",
    "from transformers.pytorch_utils import ALL_LAYERNORM_LAYERS, is_torch_less_than_1_11\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8efdfa",
   "metadata": {},
   "source": [
    "### Step 36"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2616f27",
   "metadata": {},
   "source": [
    "Execution Start time: 05:15:02, End time: 05:15:02 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3635e4f9",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that `is_torch_greater_or_equal_than_1_10` is used elsewhere in the code. I will search for `is_torch_greater_or_equal_than_1_10` in the `trainer.py` file to locate its usage and comment it out.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0ed8c7",
   "metadata": {},
   "source": [
    "**Action (search_file)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76964d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Found 2 matches for \"is_torch_greater_or_equal_than_1_10\" in /content/MeZO/large_models/trainer.py:\nLine 75:from transformers.pytorch_utils import ALL_LAYERNORM_LAYERS, is_torch_greater_or_equal_than_1_10, is_torch_less_than_1_11\nLine 154:_is_native_cpu_amp_available = is_torch_greater_or_equal_than_1_10\nEnd of matches for \"is_torch_greater_or_equal_than_1_10\" in /content/MeZO/large_models/trainer.py\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_file is_torch_greater_or_equal_than_1_10 trainer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd33de8",
   "metadata": {},
   "source": [
    "### Step 37"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38187f86",
   "metadata": {},
   "source": [
    "Execution Start time: 05:15:04, End time: 05:15:05 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db48d7a9",
   "metadata": {},
   "source": [
    "**Thought**: The `is_torch_greater_or_equal_than_1_10` function is used at line 154. I will comment out this line to prevent errors and then try running the script again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf54581",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09c586e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/MeZO/large_models/trainer.py (890 lines total)]\n(103 more lines above)\n104:    nested_xla_mesh_reduce,\n105:    reissue_pt_warnings,\n106:)\n107:from transformers.trainer_utils import (\n108:    PREFIX_CHECKPOINT_DIR,\n109:    BestRun,\n110:    EvalLoopOutput,\n111:    EvalPrediction,\n112:    FSDPOption,\n113:    HPSearchBackend,\n114:    HubStrategy,\n115:    IntervalStrategy,\n116:    PredictionOutput,\n117:    RemoveColumnsCollator,\n118:    ShardedDDPOption,\n119:    TrainerMemoryTracker,\n120:    TrainOutput,\n121:    default_compute_objective,\n122:    default_hp_space,\n123:    denumpify_detensorize,\n124:    enable_full_determinism,\n125:    find_executable_batch_size,\n126:    get_last_checkpoint,\n127:    has_length,\n128:    number_of_arguments,\n129:    seed_worker,\n130:    set_seed,\n131:    speed_metrics,\n132:)\n133:from transformers.training_args import OptimizerNames, ParallelMode, TrainingArguments\n134:from transformers.utils import (\n135:    CONFIG_NAME,\n136:    WEIGHTS_INDEX_NAME,\n137:    WEIGHTS_NAME,\n138:    find_labels,\n139:    get_full_repo_name,\n140:    is_apex_available,\n141:    is_datasets_available,\n142:    is_in_notebook,\n143:    is_ipex_available,\n144:    is_sagemaker_dp_enabled,\n145:    is_sagemaker_mp_enabled,\n146:    is_torch_tensorrt_fx_available,\n147:    is_torch_tpu_available,\n148:    is_torchdynamo_available,\n149:    logging,\n150:)\n151:from transformers.utils.generic import ContextManagers\n152:\n153:\n154:# _is_native_cpu_amp_available = is_torch_greater_or_equal_than_1_10\n155:\n156:DEFAULT_CALLBACKS = [DefaultFlowCallback]\n157:DEFAULT_PROGRESS_CALLBACK = ProgressCallback\n158:\n159:if is_in_notebook():\n160:    from .utils.notebook import NotebookProgressCallback\n161:\n162:    DEFAULT_PROGRESS_CALLBACK = NotebookProgressCallback\n163:\n164:if is_apex_available():\n165:    from apex import amp\n166:\n167:if is_datasets_available():\n168:    import datasets\n169:\n170:if is_torch_tpu_available(check_device=False):\n171:    import torch_xla.core.xla_model as xm\n172:    import torch_xla.debug.metrics as met\n173:    import torch_xla.distributed.parallel_loader as pl\n174:\n175:# if is_fairscale_available():\n176:    dep_version_check(\"fairscale\")\n177:    import fairscale\n178:    from fairscale.nn.data_parallel import FullyShardedDataParallel as FullyShardedDDP\n179:    from fairscale.nn.data_parallel import ShardedDataParallel as ShardedDDP\n180:    from fairscale.nn.wrap import auto_wrap\n181:    from fairscale.optim import OSS\n182:    from fairscale.optim.grad_scaler import ShardedGradScaler\n183:\n184:\n185:if is_sagemaker_mp_enabled():\n186:    import smdistributed.modelparallel.torch as smp\n187:    from smdistributed.modelparallel import __version__ as SMP_VERSION\n188:\n189:    IS_SAGEMAKER_MP_POST_1_10 = version.parse(SMP_VERSION) >= version.parse(\"1.10\")\n190:\n191:    from .trainer_pt_utils import smp_forward_backward, smp_forward_only, smp_gather, smp_nested_concat\n192:else:\n193:    IS_SAGEMAKER_MP_POST_1_10 = False\n194:\n195:\n196:if TYPE_CHECKING:\n197:    import optuna\n198:\n199:logger = logging.get_logger(__name__)\n200:\n201:\n202:# Name of the files used for checkpointing\n203:TRAINING_ARGS_NAME = \"training_args.bin\"\n(687 more lines below)\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 154:154\n",
    "# _is_native_cpu_amp_available = is_torch_greater_or_equal_than_1_10\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf5ecf4",
   "metadata": {},
   "source": [
    "### Step 38"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf0ad2e",
   "metadata": {},
   "source": [
    "Execution Start time: 05:15:08, End time: 05:15:13 (duration: 5 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5bfa06",
   "metadata": {},
   "source": [
    "**Thought**: The line using `is_torch_greater_or_equal_than_1_10` has been commented out. Now, I will attempt to run the training script again to see if the issue has been resolved.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30336f6",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7358fc90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2024-08-22 05:15:11,427 - INFO - PyTorch version 2.2.1+cpu available.\n/usr/local/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n  warnings.warn(\nTraceback (most recent call last):\n  File \"/content/MeZO/large_models/run.py\", line 24, in <module>\n    from trainer import OurTrainer\n  File \"/content/MeZO/large_models/trainer.py\", line 75, in <module>\n    from transformers.pytorch_utils import ALL_LAYERNORM_LAYERS, is_torch_greater_or_equal_than_1_10, is_torch_less_than_1_11\nImportError: cannot import name 'is_torch_greater_or_equal_than_1_10' from 'transformers.pytorch_utils' (/usr/local/lib/python3.10/site-packages/transformers/pytorch_utils.py)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python run.py \\\n",
    "    --task RTE \\\n",
    "    --output_dir result/output \\\n",
    "    --num_train 10 \\\n",
    "    --num_dev 10 \\\n",
    "    --num_eval 10 \\\n",
    "    --trainer zo \\\n",
    "    --train_set_seed 1234 \\\n",
    "    --train_as_classification \\\n",
    "    --num_train_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a1511c",
   "metadata": {},
   "source": [
    "### Step 39"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c487558",
   "metadata": {},
   "source": [
    "Execution Start time: 05:15:16, End time: 05:15:16 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c98105b",
   "metadata": {},
   "source": [
    "**Thought**: It seems I forgot to remove the import statement for `is_torch_greater_or_equal_than_1_10`. I will remove it from the import line in the `trainer.py` file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c24d265",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5da7b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/MeZO/large_models/trainer.py (890 lines total)]\n(24 more lines above)\n25:import random\n26:import re\n27:import shutil\n28:import sys\n29:import time\n30:import warnings\n31:from collections.abc import Mapping\n32:from pathlib import Path\n33:from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n34:import copy\n35:from metrics import f1\n36:import numpy as np\n37:\n38:from tqdm.auto import tqdm\n39:from transformers import Trainer\n40:from sklearn.linear_model import LinearRegression, LogisticRegression, LogisticRegressionCV\n41:\n42:# Integrations must be imported before ML frameworks:\n43:from transformers.integrations import (  # isort: split\n44:    get_reporting_integration_callbacks,\n45:    hp_params,\n46:    is_optuna_available,\n47:    is_ray_tune_available,\n48:    is_sigopt_available,\n49:    is_wandb_available,\n50:    run_hp_search_optuna,\n51:    run_hp_search_ray,\n52:    run_hp_search_sigopt,\n53:    run_hp_search_wandb,\n54:)\n55:import numpy as np\n56:import torch\n57:import torch.distributed as dist\n58:from packaging import version\n59:from torch import nn\n60:from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n61:from torch.utils.data.distributed import DistributedSampler\n62:\n63:from huggingface_hub import Repository\n64:\n65:from transformers import __version__\n66:from transformers.configuration_utils import PretrainedConfig\n67:from transformers.data.data_collator import DataCollator, DataCollatorWithPadding, default_data_collator\n68:from transformers.debug_utils import DebugOption, DebugUnderflowOverflow\n69:from transformers.deepspeed import deepspeed_init, is_deepspeed_zero3_enabled\n70:from transformers.dependency_versions_check import dep_version_check\n71:from transformers.modelcard import TrainingSummary\n72:from transformers.modeling_utils import PreTrainedModel, load_sharded_checkpoint, unwrap_model\n73:from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES, MODEL_MAPPING_NAMES\n74:from transformers.optimization import Adafactor, get_scheduler\n75:from transformers.pytorch_utils import ALL_LAYERNORM_LAYERS, is_torch_less_than_1_11\n76:from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n77:from transformers.trainer_callback import (\n78:    CallbackHandler,\n79:    DefaultFlowCallback,\n80:    PrinterCallback,\n81:    ProgressCallback,\n82:    TrainerCallback,\n83:    TrainerControl,\n84:    TrainerState,\n85:)\n86:from transformers.trainer_pt_utils import (\n87:    DistributedLengthGroupedSampler,\n88:    DistributedSamplerWithLoop,\n89:    DistributedTensorGatherer,\n90:    IterableDatasetShard,\n91:    LabelSmoother,\n92:    LengthGroupedSampler,\n93:    SequentialDistributedSampler,\n94:    ShardSampler,\n95:    distributed_broadcast_scalars,\n96:    distributed_concat,\n97:    find_batch_size,\n98:    get_module_class_from_name,\n99:    get_parameter_names,\n100:    nested_concat,\n101:    nested_detach,\n102:    nested_numpify,\n103:    nested_truncate,\n104:    nested_xla_mesh_reduce,\n105:    reissue_pt_warnings,\n106:)\n107:from transformers.trainer_utils import (\n108:    PREFIX_CHECKPOINT_DIR,\n109:    BestRun,\n110:    EvalLoopOutput,\n111:    EvalPrediction,\n112:    FSDPOption,\n113:    HPSearchBackend,\n114:    HubStrategy,\n115:    IntervalStrategy,\n116:    PredictionOutput,\n117:    RemoveColumnsCollator,\n118:    ShardedDDPOption,\n119:    TrainerMemoryTracker,\n120:    TrainOutput,\n121:    default_compute_objective,\n122:    default_hp_space,\n123:    denumpify_detensorize,\n124:    enable_full_determinism,\n(766 more lines below)\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 75:75\n",
    "from transformers.pytorch_utils import ALL_LAYERNORM_LAYERS, is_torch_less_than_1_11\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ccc1a7",
   "metadata": {},
   "source": [
    "### Step 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a7ec5c",
   "metadata": {},
   "source": [
    "Execution Start time: 05:15:19, End time: 05:15:24 (duration: 5 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fdb68a",
   "metadata": {},
   "source": [
    "**Thought**: The import statement for `is_torch_greater_or_equal_than_1_10` has been removed. Now, I will attempt to run the training script again to see if the issue has been resolved.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7065650a",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dd414b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2024-08-22 05:15:22,522 - INFO - PyTorch version 2.2.1+cpu available.\n/usr/local/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n  warnings.warn(\nTraceback (most recent call last):\n  File \"/content/MeZO/large_models/run.py\", line 24, in <module>\n    from trainer import OurTrainer\n  File \"/content/MeZO/large_models/trainer.py\", line 75, in <module>\n    from transformers.pytorch_utils import ALL_LAYERNORM_LAYERS, is_torch_less_than_1_11\nImportError: cannot import name 'is_torch_less_than_1_11' from 'transformers.pytorch_utils' (/usr/local/lib/python3.10/site-packages/transformers/pytorch_utils.py)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python run.py \\\n",
    "    --task RTE \\\n",
    "    --output_dir result/output \\\n",
    "    --num_train 10 \\\n",
    "    --num_dev 10 \\\n",
    "    --num_eval 10 \\\n",
    "    --trainer zo \\\n",
    "    --train_set_seed 1234 \\\n",
    "    --train_as_classification \\\n",
    "    --num_train_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89293794",
   "metadata": {},
   "source": [
    "### Step 41"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84983077",
   "metadata": {},
   "source": [
    "Execution Start time: 05:15:26, End time: 05:15:27 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca71a3b",
   "metadata": {},
   "source": [
    "**Thought**: The error now indicates that `is_torch_less_than_1_11` is also not present in the `transformers.pytorch_utils` module. I will remove this import from the `trainer.py` file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86876b12",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb1e979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\n\nERRORS:\n- F821 undefined name 'is_torch_less_than_1_11'\n\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /content/MeZO/large_models/trainer.py (890 lines total)]\n(69 more lines above)\n69:from transformers.deepspeed import deepspeed_init, is_deepspeed_zero3_enabled\n70:from transformers.dependency_versions_check import dep_version_check\n71:from transformers.modelcard import TrainingSummary\n72:from transformers.modeling_utils import PreTrainedModel, load_sharded_checkpoint, unwrap_model\n73:from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES, MODEL_MAPPING_NAMES\n74:from transformers.optimization import Adafactor, get_scheduler\n75:from transformers.pytorch_utils import ALL_LAYERNORM_LAYERS\n76:from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n77:from transformers.trainer_callback import (\n78:    CallbackHandler,\n79:    DefaultFlowCallback,\n(811 more lines below)\n-------------------------------------------------\n\nThis is the original code before your edit\n-------------------------------------------------\n[File: /content/MeZO/large_models/trainer.py (890 lines total)]\n(70 more lines above)\n70:from transformers.dependency_versions_check import dep_version_check\n71:from transformers.modelcard import TrainingSummary\n72:from transformers.modeling_utils import PreTrainedModel, load_sharded_checkpoint, unwrap_model\n73:from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES, MODEL_MAPPING_NAMES\n74:from transformers.optimization import Adafactor, get_scheduler\n75:from transformers.pytorch_utils import ALL_LAYERNORM_LAYERS, is_torch_less_than_1_11\n76:from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n77:from transformers.trainer_callback import (\n78:    CallbackHandler,\n79:    DefaultFlowCallback,\n80:    PrinterCallback,\n(810 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 75:75\n",
    "from transformers.pytorch_utils import ALL_LAYERNORM_LAYERS\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a2a1d6",
   "metadata": {},
   "source": [
    "### Step 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbfabbc",
   "metadata": {},
   "source": [
    "Execution Start time: 05:15:31, End time: 05:15:31 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5c420c",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that `is_torch_less_than_1_11` is used elsewhere in the code. I will search for `is_torch_less_than_1_11` in the `trainer.py` file to locate its usage and comment it out.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018ef29f",
   "metadata": {},
   "source": [
    "**Action (search_file)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f7a97b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Found 2 matches for \"is_torch_less_than_1_11\" in /content/MeZO/large_models/trainer.py:\nLine 75:from transformers.pytorch_utils import ALL_LAYERNORM_LAYERS, is_torch_less_than_1_11\nLine 470:                if is_torch_less_than_1_11 or not is_random_sampler:\nEnd of matches for \"is_torch_less_than_1_11\" in /content/MeZO/large_models/trainer.py\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_file is_torch_less_than_1_11 trainer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c2fd75",
   "metadata": {},
   "source": [
    "### Step 43"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3423684d",
   "metadata": {},
   "source": [
    "Execution Start time: 05:15:33, End time: 05:15:34 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ce8d87",
   "metadata": {},
   "source": [
    "**Thought**: The `is_torch_less_than_1_11` function is used at line 470. I will comment out this line to prevent errors and then try running the script again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bca89a6",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89344c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\n\nERRORS:\n- E999 IndentationError: unexpected indent\n\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /content/MeZO/large_models/trainer.py (890 lines total)]\n(464 more lines above)\n464:        # Skip the first epochs_trained epochs to get the random state of the dataloader at the right point.\n465:        if not args.ignore_data_skip:\n466:            for epoch in range(epochs_trained):\n467:                is_random_sampler = hasattr(train_dataloader, \"sampler\") and isinstance(\n468:                    train_dataloader.sampler, RandomSampler\n469:                )\n470:# if is_torch_less_than_1_11 or not is_random_sampler:\n471:                    # We just need to begin an iteration to create the randomization of the sampler.\n472:                    # That was before PyTorch 1.11 however...\n473:                    for _ in train_dataloader:\n474:                        break\n(416 more lines below)\n-------------------------------------------------\n\nThis is the original code before your edit\n-------------------------------------------------\n[File: /content/MeZO/large_models/trainer.py (890 lines total)]\n(465 more lines above)\n465:        if not args.ignore_data_skip:\n466:            for epoch in range(epochs_trained):\n467:                is_random_sampler = hasattr(train_dataloader, \"sampler\") and isinstance(\n468:                    train_dataloader.sampler, RandomSampler\n469:                )\n470:                if is_torch_less_than_1_11 or not is_random_sampler:\n471:                    # We just need to begin an iteration to create the randomization of the sampler.\n472:                    # That was before PyTorch 1.11 however...\n473:                    for _ in train_dataloader:\n474:                        break\n475:                else:\n(415 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 470:470\n",
    "# if is_torch_less_than_1_11 or not is_random_sampler:\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed05fe7",
   "metadata": {},
   "source": [
    "### Step 44"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d4d700",
   "metadata": {},
   "source": [
    "Execution Start time: 05:15:35, End time: 05:15:36 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be8b825",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates an indentation issue. Instead of commenting out the line, I will replace the condition with `True` to maintain the indentation structure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f9185c",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f72e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\n\nERRORS:\n- E999 IndentationError: unindent does not match any outer indentation level\n\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /content/MeZO/large_models/trainer.py (890 lines total)]\n(464 more lines above)\n464:        # Skip the first epochs_trained epochs to get the random state of the dataloader at the right point.\n465:        if not args.ignore_data_skip:\n466:            for epoch in range(epochs_trained):\n467:                is_random_sampler = hasattr(train_dataloader, \"sampler\") and isinstance(\n468:                    train_dataloader.sampler, RandomSampler\n469:                )\n470:if True:\n471:                    # We just need to begin an iteration to create the randomization of the sampler.\n472:                    # That was before PyTorch 1.11 however...\n473:                    for _ in train_dataloader:\n474:                        break\n(416 more lines below)\n-------------------------------------------------\n\nThis is the original code before your edit\n-------------------------------------------------\n[File: /content/MeZO/large_models/trainer.py (890 lines total)]\n(465 more lines above)\n465:        if not args.ignore_data_skip:\n466:            for epoch in range(epochs_trained):\n467:                is_random_sampler = hasattr(train_dataloader, \"sampler\") and isinstance(\n468:                    train_dataloader.sampler, RandomSampler\n469:                )\n470:                if is_torch_less_than_1_11 or not is_random_sampler:\n471:                    # We just need to begin an iteration to create the randomization of the sampler.\n472:                    # That was before PyTorch 1.11 however...\n473:                    for _ in train_dataloader:\n474:                        break\n475:                else:\n(415 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 470:470\n",
    "if True:\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e5225f",
   "metadata": {},
   "source": [
    "### Step 45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c9b5be",
   "metadata": {},
   "source": [
    "Execution Start time: 05:15:38, End time: 05:15:39 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c17416",
   "metadata": {},
   "source": [
    "**Thought**: The indentation error suggests that the line was not properly aligned with the surrounding code. I will adjust the indentation to match the existing code structure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df56418a",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cf98ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/MeZO/large_models/trainer.py (890 lines total)]\n(419 more lines above)\n420:\n421:            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n422:            logger.info(f\"  Continuing training from epoch {epochs_trained}\")\n423:            logger.info(f\"  Continuing training from global step {self.state.global_step}\")\n424:            if not args.ignore_data_skip:\n425:                logger.info(\n426:                    f\"  Will skip the first {epochs_trained} epochs then the first {steps_trained_in_current_epoch} \"\n427:                    \"batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` \"\n428:                    \"flag to your launch command, but you will resume the training on data already seen by your model.\"\n429:                )\n430:                if self.is_local_process_zero() and not args.disable_tqdm:\n431:                    steps_trained_progress_bar = tqdm(total=steps_trained_in_current_epoch)\n432:                    steps_trained_progress_bar.set_description(\"Skipping the first batches\")\n433:\n434:        # Update the references\n435:        self.callback_handler.model = self.model\n436:        self.callback_handler.optimizer = self.optimizer\n437:        self.callback_handler.lr_scheduler = self.lr_scheduler\n438:        self.callback_handler.train_dataloader = train_dataloader\n439:        if self.hp_name is not None and self._trial is not None:\n440:            # use self._trial because the SigOpt/Optuna hpo only call `_hp_search_setup(trial)` instead of passing trial\n441:            # parameter to Train when using DDP.\n442:            self.state.trial_name = self.hp_name(self._trial)\n443:        if trial is not None:\n444:            assignments = trial.assignments if self.hp_search_backend == HPSearchBackend.SIGOPT else trial\n445:            self.state.trial_params = hp_params(assignments)\n446:        else:\n447:            self.state.trial_params = None\n448:        # This should be the same if the state has been saved but in case the training arguments changed, it's safer\n449:        # to set this after the load.\n450:        self.state.max_steps = max_steps\n451:        self.state.num_train_epochs = num_train_epochs\n452:        self.state.is_local_process_zero = self.is_local_process_zero()\n453:        self.state.is_world_process_zero = self.is_world_process_zero()\n454:\n455:        # tr_loss is a tensor to avoid synchronization of TPUs through .item()\n456:        tr_loss = torch.tensor(0.0).to(args.device)\n457:        # _total_loss_scalar is updated everytime .item() has to be called on tr_loss and stores the sum of all losses\n458:        self._total_loss_scalar = 0.0\n459:        self._globalstep_last_logged = self.state.global_step\n460:        model.zero_grad()\n461:\n462:        self.control = self.callback_handler.on_train_begin(args, self.state, self.control)\n463:\n464:        # Skip the first epochs_trained epochs to get the random state of the dataloader at the right point.\n465:        if not args.ignore_data_skip:\n466:            for epoch in range(epochs_trained):\n467:                is_random_sampler = hasattr(train_dataloader, \"sampler\") and isinstance(\n468:                    train_dataloader.sampler, RandomSampler\n469:                )\n470:                if True:\n471:                    # We just need to begin an iteration to create the randomization of the sampler.\n472:                    # That was before PyTorch 1.11 however...\n473:                    for _ in train_dataloader:\n474:                        break\n475:                else:\n476:                    # Otherwise we need to call the whooooole sampler cause there is some random operation added\n477:                    # AT THE VERY END!\n478:                    _ = list(train_dataloader.sampler)\n479:\n480:        for epoch in range(epochs_trained, num_train_epochs):\n481:            if isinstance(train_dataloader, DataLoader) and isinstance(train_dataloader.sampler, DistributedSampler):\n482:                train_dataloader.sampler.set_epoch(epoch)\n483:            elif hasattr(train_dataloader, \"dataset\") and isinstance(train_dataloader.dataset, IterableDatasetShard):\n484:                train_dataloader.dataset.set_epoch(epoch)\n485:\n486:            if is_torch_tpu_available():\n487:                parallel_loader = pl.ParallelLoader(train_dataloader, [args.device]).per_device_loader(args.device)\n488:                epoch_iterator = parallel_loader\n489:            else:\n490:                epoch_iterator = train_dataloader\n491:\n492:            # Reset the past mems state at the beginning of each epoch if necessary.\n493:            if args.past_index >= 0:\n494:                self._past = None\n495:\n496:            steps_in_epoch = (\n497:                len(epoch_iterator)\n498:                if len_dataloader is not None\n499:                else args.max_steps * args.gradient_accumulation_steps\n500:            )\n501:            self.control = self.callback_handler.on_epoch_begin(args, self.state, self.control)\n502:\n503:            if epoch == epochs_trained and resume_from_checkpoint is not None and steps_trained_in_current_epoch == 0:\n504:                self._load_rng_state(resume_from_checkpoint)\n505:\n506:            step = -1\n507:            for step, inputs in enumerate(epoch_iterator):\n508:\n509:                # Skip past any already trained steps if resuming training\n510:                if steps_trained_in_current_epoch > 0:\n511:                    steps_trained_in_current_epoch -= 1\n512:                    if steps_trained_progress_bar is not None:\n513:                        steps_trained_progress_bar.update(1)\n514:                    if steps_trained_in_current_epoch == 0:\n515:                        self._load_rng_state(resume_from_checkpoint)\n516:                    continue\n517:                elif steps_trained_progress_bar is not None:\n518:                    steps_trained_progress_bar.close()\n519:                    steps_trained_progress_bar = None\n(371 more lines below)\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 470:470\n",
    "                if True:\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7fef50",
   "metadata": {},
   "source": [
    "### Step 46"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498975f8",
   "metadata": {},
   "source": [
    "Execution Start time: 05:15:41, End time: 05:15:47 (duration: 5 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51611608",
   "metadata": {},
   "source": [
    "**Thought**: The condition using `is_torch_less_than_1_11` has been replaced with `True`. Now, I will attempt to run the training script again to see if the issue has been resolved.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541c9465",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d750f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2024-08-22 05:15:44,972 - INFO - PyTorch version 2.2.1+cpu available.\n/usr/local/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n  warnings.warn(\nTraceback (most recent call last):\n  File \"/content/MeZO/large_models/run.py\", line 24, in <module>\n    from trainer import OurTrainer\n  File \"/content/MeZO/large_models/trainer.py\", line 75, in <module>\n    from transformers.pytorch_utils import ALL_LAYERNORM_LAYERS, is_torch_less_than_1_11\nImportError: cannot import name 'is_torch_less_than_1_11' from 'transformers.pytorch_utils' (/usr/local/lib/python3.10/site-packages/transformers/pytorch_utils.py)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python run.py \\\n",
    "    --task RTE \\\n",
    "    --output_dir result/output \\\n",
    "    --num_train 10 \\\n",
    "    --num_dev 10 \\\n",
    "    --num_eval 10 \\\n",
    "    --trainer zo \\\n",
    "    --train_set_seed 1234 \\\n",
    "    --train_as_classification \\\n",
    "    --num_train_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd94fe14",
   "metadata": {},
   "source": [
    "### Step 47"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e4a374",
   "metadata": {},
   "source": [
    "Execution Start time: 05:15:49, End time: 05:15:50 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2ce429",
   "metadata": {},
   "source": [
    "**Thought**: It seems I forgot to remove the import statement for `is_torch_less_than_1_11`. I will remove it from the import line in the `trainer.py` file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0833de",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ff9892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/MeZO/large_models/trainer.py (890 lines total)]\n(24 more lines above)\n25:import random\n26:import re\n27:import shutil\n28:import sys\n29:import time\n30:import warnings\n31:from collections.abc import Mapping\n32:from pathlib import Path\n33:from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n34:import copy\n35:from metrics import f1\n36:import numpy as np\n37:\n38:from tqdm.auto import tqdm\n39:from transformers import Trainer\n40:from sklearn.linear_model import LinearRegression, LogisticRegression, LogisticRegressionCV\n41:\n42:# Integrations must be imported before ML frameworks:\n43:from transformers.integrations import (  # isort: split\n44:    get_reporting_integration_callbacks,\n45:    hp_params,\n46:    is_optuna_available,\n47:    is_ray_tune_available,\n48:    is_sigopt_available,\n49:    is_wandb_available,\n50:    run_hp_search_optuna,\n51:    run_hp_search_ray,\n52:    run_hp_search_sigopt,\n53:    run_hp_search_wandb,\n54:)\n55:import numpy as np\n56:import torch\n57:import torch.distributed as dist\n58:from packaging import version\n59:from torch import nn\n60:from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n61:from torch.utils.data.distributed import DistributedSampler\n62:\n63:from huggingface_hub import Repository\n64:\n65:from transformers import __version__\n66:from transformers.configuration_utils import PretrainedConfig\n67:from transformers.data.data_collator import DataCollator, DataCollatorWithPadding, default_data_collator\n68:from transformers.debug_utils import DebugOption, DebugUnderflowOverflow\n69:from transformers.deepspeed import deepspeed_init, is_deepspeed_zero3_enabled\n70:from transformers.dependency_versions_check import dep_version_check\n71:from transformers.modelcard import TrainingSummary\n72:from transformers.modeling_utils import PreTrainedModel, load_sharded_checkpoint, unwrap_model\n73:from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES, MODEL_MAPPING_NAMES\n74:from transformers.optimization import Adafactor, get_scheduler\n75:from transformers.pytorch_utils import ALL_LAYERNORM_LAYERS\n76:from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n77:from transformers.trainer_callback import (\n78:    CallbackHandler,\n79:    DefaultFlowCallback,\n80:    PrinterCallback,\n81:    ProgressCallback,\n82:    TrainerCallback,\n83:    TrainerControl,\n84:    TrainerState,\n85:)\n86:from transformers.trainer_pt_utils import (\n87:    DistributedLengthGroupedSampler,\n88:    DistributedSamplerWithLoop,\n89:    DistributedTensorGatherer,\n90:    IterableDatasetShard,\n91:    LabelSmoother,\n92:    LengthGroupedSampler,\n93:    SequentialDistributedSampler,\n94:    ShardSampler,\n95:    distributed_broadcast_scalars,\n96:    distributed_concat,\n97:    find_batch_size,\n98:    get_module_class_from_name,\n99:    get_parameter_names,\n100:    nested_concat,\n101:    nested_detach,\n102:    nested_numpify,\n103:    nested_truncate,\n104:    nested_xla_mesh_reduce,\n105:    reissue_pt_warnings,\n106:)\n107:from transformers.trainer_utils import (\n108:    PREFIX_CHECKPOINT_DIR,\n109:    BestRun,\n110:    EvalLoopOutput,\n111:    EvalPrediction,\n112:    FSDPOption,\n113:    HPSearchBackend,\n114:    HubStrategy,\n115:    IntervalStrategy,\n116:    PredictionOutput,\n117:    RemoveColumnsCollator,\n118:    ShardedDDPOption,\n119:    TrainerMemoryTracker,\n120:    TrainOutput,\n121:    default_compute_objective,\n122:    default_hp_space,\n123:    denumpify_detensorize,\n124:    enable_full_determinism,\n(766 more lines below)\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 75:75\n",
    "from transformers.pytorch_utils import ALL_LAYERNORM_LAYERS\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9627c3e6",
   "metadata": {},
   "source": [
    "### Step 48"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7167644",
   "metadata": {},
   "source": [
    "Execution Start time: 05:15:53, End time: 05:15:59 (duration: 5 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db44fe0",
   "metadata": {},
   "source": [
    "**Thought**: The import statement for `is_torch_less_than_1_11` has been removed. Now, I will attempt to run the training script again to see if the issue has been resolved.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af9da00",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8951aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2024-08-22 05:15:56,606 - INFO - PyTorch version 2.2.1+cpu available.\n/usr/local/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n  warnings.warn(\nTraceback (most recent call last):\n  File \"/content/MeZO/large_models/run.py\", line 24, in <module>\n    from trainer import OurTrainer\n  File \"/content/MeZO/large_models/trainer.py\", line 107, in <module>\n    from transformers.trainer_utils import (\nImportError: cannot import name 'ShardedDDPOption' from 'transformers.trainer_utils' (/usr/local/lib/python3.10/site-packages/transformers/trainer_utils.py)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python run.py \\\n",
    "    --task RTE \\\n",
    "    --output_dir result/output \\\n",
    "    --num_train 10 \\\n",
    "    --num_dev 10 \\\n",
    "    --num_eval 10 \\\n",
    "    --trainer zo \\\n",
    "    --train_set_seed 1234 \\\n",
    "    --train_as_classification \\\n",
    "    --num_train_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b1b9be",
   "metadata": {},
   "source": [
    "### Step 49"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678e1c09",
   "metadata": {},
   "source": [
    "Execution Start time: 05:16:01, End time: 05:16:01 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c07488",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that `ShardedDDPOption` is not present in the `transformers.trainer_utils` module. I will open the `trainer.py` file to locate this import and remove or replace it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb3735a",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee24801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/MeZO/large_models/trainer.py (890 lines total)]\n(91 more lines above)\n92:    LengthGroupedSampler,\n93:    SequentialDistributedSampler,\n94:    ShardSampler,\n95:    distributed_broadcast_scalars,\n96:    distributed_concat,\n97:    find_batch_size,\n98:    get_module_class_from_name,\n99:    get_parameter_names,\n100:    nested_concat,\n101:    nested_detach,\n102:    nested_numpify,\n103:    nested_truncate,\n104:    nested_xla_mesh_reduce,\n105:    reissue_pt_warnings,\n106:)\n107:from transformers.trainer_utils import (\n108:    PREFIX_CHECKPOINT_DIR,\n109:    BestRun,\n110:    EvalLoopOutput,\n111:    EvalPrediction,\n112:    FSDPOption,\n113:    HPSearchBackend,\n114:    HubStrategy,\n115:    IntervalStrategy,\n116:    PredictionOutput,\n117:    RemoveColumnsCollator,\n118:    ShardedDDPOption,\n119:    TrainerMemoryTracker,\n120:    TrainOutput,\n121:    default_compute_objective,\n122:    default_hp_space,\n123:    denumpify_detensorize,\n124:    enable_full_determinism,\n125:    find_executable_batch_size,\n126:    get_last_checkpoint,\n127:    has_length,\n128:    number_of_arguments,\n129:    seed_worker,\n130:    set_seed,\n131:    speed_metrics,\n132:)\n133:from transformers.training_args import OptimizerNames, ParallelMode, TrainingArguments\n134:from transformers.utils import (\n135:    CONFIG_NAME,\n136:    WEIGHTS_INDEX_NAME,\n137:    WEIGHTS_NAME,\n138:    find_labels,\n139:    get_full_repo_name,\n140:    is_apex_available,\n141:    is_datasets_available,\n142:    is_in_notebook,\n143:    is_ipex_available,\n144:    is_sagemaker_dp_enabled,\n145:    is_sagemaker_mp_enabled,\n146:    is_torch_tensorrt_fx_available,\n147:    is_torch_tpu_available,\n148:    is_torchdynamo_available,\n149:    logging,\n150:)\n151:from transformers.utils.generic import ContextManagers\n152:\n153:\n154:# _is_native_cpu_amp_available = is_torch_greater_or_equal_than_1_10\n155:\n156:DEFAULT_CALLBACKS = [DefaultFlowCallback]\n157:DEFAULT_PROGRESS_CALLBACK = ProgressCallback\n158:\n159:if is_in_notebook():\n160:    from .utils.notebook import NotebookProgressCallback\n161:\n162:    DEFAULT_PROGRESS_CALLBACK = NotebookProgressCallback\n163:\n164:if is_apex_available():\n165:    from apex import amp\n166:\n167:if is_datasets_available():\n168:    import datasets\n169:\n170:if is_torch_tpu_available(check_device=False):\n171:    import torch_xla.core.xla_model as xm\n172:    import torch_xla.debug.metrics as met\n173:    import torch_xla.distributed.parallel_loader as pl\n174:\n175:# if is_fairscale_available():\n176:    dep_version_check(\"fairscale\")\n177:    import fairscale\n178:    from fairscale.nn.data_parallel import FullyShardedDataParallel as FullyShardedDDP\n179:    from fairscale.nn.data_parallel import ShardedDataParallel as ShardedDDP\n180:    from fairscale.nn.wrap import auto_wrap\n181:    from fairscale.optim import OSS\n182:    from fairscale.optim.grad_scaler import ShardedGradScaler\n183:\n184:\n185:if is_sagemaker_mp_enabled():\n186:    import smdistributed.modelparallel.torch as smp\n187:    from smdistributed.modelparallel import __version__ as SMP_VERSION\n188:\n189:    IS_SAGEMAKER_MP_POST_1_10 = version.parse(SMP_VERSION) >= version.parse(\"1.10\")\n190:\n191:    from .trainer_pt_utils import smp_forward_backward, smp_forward_only, smp_gather, smp_nested_concat\n(699 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open trainer.py 107"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e91232c",
   "metadata": {},
   "source": [
    "### Step 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f18c6b",
   "metadata": {},
   "source": [
    "Execution Start time: 05:16:05, End time: 05:16:06 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae21f790",
   "metadata": {},
   "source": [
    "**Thought**: The `ShardedDDPOption` is imported at line 118. I will remove this import from the `trainer.py` file to prevent errors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd7d23d",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ec891e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\n\nERRORS:\n- F821 undefined name 'ShardedDDPOption'\n- F821 undefined name 'ShardedDDPOption'\n- F821 undefined name 'ShardedDDPOption'\n\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /content/MeZO/large_models/trainer.py (889 lines total)]\n(101 more lines above)\n101:    nested_detach,\n102:    nested_numpify,\n103:    nested_truncate,\n104:    nested_xla_mesh_reduce,\n105:    reissue_pt_warnings,\n106:)\n107:from transformers.trainer_utils import (\n108:    PREFIX_CHECKPOINT_DIR,\n109:    BestRun,\n110:    EvalLoopOutput,\n111:    EvalPrediction,\n112:    FSDPOption,\n113:    HPSearchBackend,\n114:    HubStrategy,\n115:    IntervalStrategy,\n116:    PredictionOutput,\n117:    RemoveColumnsCollator,\n118:    TrainerMemoryTracker,\n119:    TrainOutput,\n120:    default_compute_objective,\n121:    default_hp_space,\n122:    denumpify_detensorize,\n123:    enable_full_determinism,\n124:    find_executable_batch_size,\n125:    get_last_checkpoint,\n126:    has_length,\n127:    number_of_arguments,\n128:    seed_worker,\n129:    set_seed,\n130:    speed_metrics,\n131:)\n132:from transformers.training_args import OptimizerNames, ParallelMode, TrainingArguments\n133:from transformers.utils import (\n134:    CONFIG_NAME,\n135:    WEIGHTS_INDEX_NAME,\n(754 more lines below)\n-------------------------------------------------\n\nThis is the original code before your edit\n-------------------------------------------------\n[File: /content/MeZO/large_models/trainer.py (890 lines total)]\n(101 more lines above)\n102:    nested_numpify,\n103:    nested_truncate,\n104:    nested_xla_mesh_reduce,\n105:    reissue_pt_warnings,\n106:)\n107:from transformers.trainer_utils import (\n108:    PREFIX_CHECKPOINT_DIR,\n109:    BestRun,\n110:    EvalLoopOutput,\n111:    EvalPrediction,\n112:    FSDPOption,\n113:    HPSearchBackend,\n114:    HubStrategy,\n115:    IntervalStrategy,\n116:    PredictionOutput,\n117:    RemoveColumnsCollator,\n118:    ShardedDDPOption,\n119:    TrainerMemoryTracker,\n120:    TrainOutput,\n121:    default_compute_objective,\n122:    default_hp_space,\n123:    denumpify_detensorize,\n124:    enable_full_determinism,\n125:    find_executable_batch_size,\n126:    get_last_checkpoint,\n127:    has_length,\n128:    number_of_arguments,\n129:    seed_worker,\n130:    set_seed,\n131:    speed_metrics,\n132:)\n133:from transformers.training_args import OptimizerNames, ParallelMode, TrainingArguments\n134:from transformers.utils import (\n135:    CONFIG_NAME,\n136:    WEIGHTS_INDEX_NAME,\n137:    WEIGHTS_NAME,\n(753 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 107:132\n",
    "from transformers.trainer_utils import (\n",
    "    PREFIX_CHECKPOINT_DIR,\n",
    "    BestRun,\n",
    "    EvalLoopOutput,\n",
    "    EvalPrediction,\n",
    "    FSDPOption,\n",
    "    HPSearchBackend,\n",
    "    HubStrategy,\n",
    "    IntervalStrategy,\n",
    "    PredictionOutput,\n",
    "    RemoveColumnsCollator,\n",
    "    TrainerMemoryTracker,\n",
    "    TrainOutput,\n",
    "    default_compute_objective,\n",
    "    default_hp_space,\n",
    "    denumpify_detensorize,\n",
    "    enable_full_determinism,\n",
    "    find_executable_batch_size,\n",
    "    get_last_checkpoint,\n",
    "    has_length,\n",
    "    number_of_arguments,\n",
    "    seed_worker,\n",
    "    set_seed,\n",
    "    speed_metrics,\n",
    ")\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdc015f",
   "metadata": {},
   "source": [
    "### Step 51"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719d4944",
   "metadata": {},
   "source": [
    "Execution Start time: 05:16:08, End time: 05:16:08 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbaa23f",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that `ShardedDDPOption` is used elsewhere in the code. I will search for `ShardedDDPOption` in the `trainer.py` file to locate its usage and comment it out.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039e597a",
   "metadata": {},
   "source": [
    "**Action (search_file)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65daea2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Found 4 matches for \"ShardedDDPOption\" in /content/MeZO/large_models/trainer.py:\nLine 118:    ShardedDDPOption,\nLine 349:            and self.sharded_ddp != ShardedDDPOption.SIMPLE\nLine 844:            ShardedDDPOption.ZERO_DP_2 in self.args.sharded_ddp\nLine 845:            or ShardedDDPOption.ZERO_DP_3 in self.args.sharded_ddp\nEnd of matches for \"ShardedDDPOption\" in /content/MeZO/large_models/trainer.py\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_file ShardedDDPOption trainer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c529c6cc",
   "metadata": {},
   "source": [
    "### Step 52"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2d544f",
   "metadata": {},
   "source": [
    "Execution Start time: 05:16:16, End time: 05:16:16 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a762aa44",
   "metadata": {},
   "source": [
    "**Thought**: Exit due to context window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d2364e",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87389c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Exited"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exit_context"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
