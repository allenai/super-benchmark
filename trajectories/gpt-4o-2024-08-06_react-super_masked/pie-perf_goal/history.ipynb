{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69779533",
   "metadata": {},
   "source": [
    "### Query\n",
    "Evaluate the generations of my code improving model which are provided in https://drive.google.com/file/d/1izs1iF5cd_NAZsOaZvrrQF3NAsoP8lHf/view?usp=sharing (v1 vs v0). Once evaluated, report the result problem_id and input_acc for each problem of the dataset, as a json list of dictionaries structured as follows: [{\"problem_id\": \"\", \"input_acc\": 0.0}] (replace \"\" and 0.0 with the actual values).\n",
    "\n",
    "Additional instructions:\n",
    "1. Set \"num_trials\": 2 in the evaluation configuration file to reduce computation time.\n",
    "2. Load only the first 10 rows of the dataset.\n",
    "\n",
    "Git repository: https://github.com/madaan/pie-perf\n",
    "\n",
    "I have already executed some of the required steps. Now, your goal is to complete the remaining steps and submit the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a150ca",
   "metadata": {},
   "source": [
    "### Prompt\n",
    "<details>\n",
    "[{'role': 'system', 'content': 'Interact in a jupyter notebook to solve the request.\\nOutput a thought and an action.\\nThought can reason about the current situation, and action allows you to interact with the Jupyter notebook. There are three types of actions:\\n(1) execute, which executes a notebook cell with the command under the \"content\" field. This could be a bash command (beginning with !), a python command, or magic command (beginning with %).\\n(2) edit, which allows you to replace existing lines in a file with new lines. To replace lines, you will need to provide the lines as they appear before the replacement, and the lines to replace them with.\\nIn the \"content\" field, format should be ```{filename}\\n<<<BEFORE_EDIT>>>\\nlines before edit\\nlines before edit\\n<<<AFTER_EDIT>>>\\nlines after edit\\nlines after edit```\\n  - Include the entire lines WITH indentation. After editing, make sure indentation is correct.\\n(3) submit, which submits the final answer (or null if there\\'s no answer to submit), formatted as JSON (make sure to use double quotes for keys and values).\\nAlways output exactly one \"thought\" and one \"action\", and encapsulate the content with <content>...</content> tags.\\nI will provide the cell output (observation) for you.\\n\\nTips:\\n- When a github repository is given, you should start by cloning it. You can read the readme file and browse the repo for installation and usage instructions.\\n- Avoid using conda, instead use pip for package management.\\n- Each execution has a 5-minute timeout. After this period, you can decide whether to interrupt the kernel or continue waiting.\\n  You should interrupt if: (1) you are prompted with an interactive message (e.g., \"Do you want to continue? [Y/n]\"). In that case, you can bypass it by executing echo \"y\" | command, or (2) the execution is taking longer than expected because the wrong task is being executed, such as a different model or incorrect number of epochs.\\n  You should continue to wait if the current execution is correct and proceeding as expected.\\n\\nHere are some examples.\\nGit repository: EleutherAI/lm-evaluation-harness\\nQuery: Evaluate EleutherAI/gpt-j-6B on hellaswag. What\\'s the score?\\nThought: First, I need to clone the GitHub repository \\'EleutherAI/lm-evaluation-harness\\'.\\nAction: execute\\n<content>\\n!git clone https://github.com/EleutherAI/lm-evaluation-harness.git\\n</content>\\nObservation: ```\\nCloning into \\'lm-evaluation-harness\\'...\\n```\\nThought: The repository has been cloned. Next, I should check the README file for installation and usage instructions to proceed with evaluating EleutherAI/gpt-j-6B on hellaswag.\\nAction: execute\\n<content>\\n!cat lm-evaluation-harness/README.md\\n</content>\\nObservation: ```# Language Model Evaluation Harness ... truncated ...\\n```\\nThought: The README provides detailed instructions for installing the `lm-eval` package and evaluating models like GPT-J-6B on tasks like hellaswag. I need to install the package and its dependencies first.\\nAction: execute\\n<content>\\n!pip install -e lm-evaluation-harness/.\\n</content>\\nObservation: ```\\nObtaining file:///home/nora/lm-evaluation-harness\\n  Installing build dependencies ... truncated ...\\n```\\nThought: The `lm-eval` package and its dependencies have been installed successfully. Now, I can proceed with evaluating the EleutherAI/gpt-j-6B model on the hellaswag task as per the instructions in the README.\\nAction: execute\\n<content>\\n!lm_eval --model hf --model_args pretrained=EleutherAI/gpt-j-6B --tasks hellaswag --device cuda:0 --batch_size 8\\n</content>\\nObservation: ```\\n{\\n  \"hellaswag\": {\\n    \"acc\": 0.24603658536585363,\\n    \"acc_norm\": 0.34436059706446426,\\n  },  ... truncated ...\\n```\\nThought: The `lm-eval` was executed successfully, providing the metrics I needed. The `acc` metric is 0.2460. The `acc_norm` metric is 0.3444\\nAction: submit\\n<content>\\n{\"acc\": 0.2460, \"acc_norm\": 0.3444}\\n</content>\\n\\nQuery: use pandas to read the file \\'data.csv\\' and display the first 2 rows.\\nThought: First, I need to read the file \\'data.csv\\' using the pandas library.\\nAction: execute\\n<content>\\nimport pandas as pd\\npd.read_csv(\\'data.csv\\').head()\\n</content>\\nObservation: ```\\n    name\\n  0  Alice\\n   1    Bob\\n  2  Charlie\\n```\\nThought: The file \\'data.csv\\' has been read successfully and the first 2 rows have been displayed.\\nAction: submit\\n<content>\\n[\"Alice\", \"Bob\"]\\n</content>\\n\\nQuery: Change the model name in the \\'config.json\\' file from \\'roberta-base\\' to \\'roberta-large\\'.\\nThought: First, I need to read the \\'config.json\\' file to find the location where I can replace the model name.\\nAction: execute\\n<content>\\n!cat config.json\\n</content>\\nObservation: ```\\n{\\n    \"model\": \"roberta-base\",\\n    \"batch_size\": 8,\\n    \"epochs\": 10\\n}\\n```\\nThought: The file has been read successfully. Now, I can replace the model name in the \\'config.json\\' file.\\nAction: edit\\n<content>\\nconfig.json\\n<<<BEFORE_EDIT>>>\\n{\\n    \"model\": \"roberta-base\",\\n    \"batch_size\": 8,\\n<<<AFTER_EDIT>>>\\n{\\n    \"model\": \"roberta-large\",\\n    \"batch_size\": 8,\\n</content>\\nObservation: \"editing file config.json\"\\nThought: The model name has been replaced in the \\'config.json\\' file.\\nAction: submit\\n<content>\\nnull\\n</content>\\n'}, {'role': 'user', 'content': 'Query: Evaluate the generations of my code improving model which are provided in https://drive.google.com/file/d/1izs1iF5cd_NAZsOaZvrrQF3NAsoP8lHf/view?usp=sharing (v1 vs v0). Once evaluated, report the result problem_id and input_acc for each problem of the dataset, as a json list of dictionaries structured as follows: [{\"problem_id\": \"\", \"input_acc\": 0.0}] (replace \"\" and 0.0 with the actual values).\\n\\nAdditional instructions:\\n1. Set \"num_trials\": 2 in the evaluation configuration file to reduce computation time.\\n2. Load only the first 10 rows of the dataset.\\n\\nGit repository: https://github.com/madaan/pie-perf\\n\\nI have already executed some of the required steps. Now, your goal is to complete the remaining steps and submit the answer.'}]\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a76ef7",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150c5775",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:21, End time: 17:48:22 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd096d6a",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5e1fec",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdfd4c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5613df10",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d79e37b",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:22, End time: 17:48:22 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfb4ef3",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddf98e0",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea81d1e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Clone repository"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00790fd",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cb2a7f",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:22, End time: 17:48:33 (duration: 11 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fda3b63",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106cdcaa",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79a1f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cloning into 'pie-perf'...\nremote: Enumerating objects: 1282, done.\u001b[K\nremote: Counting objects: 100% (45/45)\u001b[K\nremote: Counting objects: 100% (45/45), done.\u001b[K\nremote: Compressing objects: 100% (34/34)\u001b[K\nremote: Compressing objects: 100% (34/34), done.\u001b[K\nremote: Total 1282 (delta 25), reused 26 (delta 11), pack-reused 1237 (from 1)\u001b[K\nReceiving objects: 100% (1282/1282), 478.99 MiB | 63.65 MiB/s\nReceiving objects: 100% (1282/1282), 490.04 MiB | 61.90 MiB/s, done.\nResolving deltas: 100% (830/830)\nResolving deltas: 100% (830/830), done.\nUpdating files: 100% (1010/1010)\nUpdating files: 100% (1010/1010), done.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!git clone https://github.com/madaan/pie-perf.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bb66b3",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683fde76",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:33, End time: 17:48:33 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d234712f",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c3f51b",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b658c8d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9066c1",
   "metadata": {},
   "source": [
    "### Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74993d6b",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:33, End time: 17:48:34 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71175487",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aba015",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00592e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #### Download data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3211a3ed",
   "metadata": {},
   "source": [
    "### Step 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd08846",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:34, End time: 17:48:37 (duration: 3 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9f4d42",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed37618",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83cb55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Requirement already satisfied: gdown in /usr/local/lib/python3.10/site-packages (5.1.0)\nCollecting gdown\n  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/site-packages (from gdown) (4.12.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from gdown) (3.15.1)\nRequirement already satisfied: requests[socks] in /usr/local/lib/python3.10/site-packages (from gdown) (2.31.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from gdown) (4.66.4)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown) (2.0.7)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.6.2)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\nDownloading gdown-5.2.0-py3-none-any.whl (18 kB)\nInstalling collected packages: gdown\n  Attempting uninstall: gdown\n    Found existing installation: gdown 5.1.0\n    Uninstalling gdown-5.1.0:\n      Successfully uninstalled gdown-5.1.0\nSuccessfully installed gdown-5.2.0\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use gdown to download files\n",
    "!pip install gdown --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f47800",
   "metadata": {},
   "source": [
    "### Step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e137f7",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:37, End time: 17:48:48 (duration: 11 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3098a17",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfd7391",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d3f344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\nRetrieving folder contents\n/content/pie-perf/data\nProcessing file 1zI7lobzGjmM8Qa59hs72QYRU1-HfB3sQ generated_programs.jsonl\nProcessing file 1izs1iF5cd_NAZsOaZvrrQF3NAsoP8lHf improvement_pairs_additional_metadata.csv\nProcessing file 1aNUEmIyTs0NqPbGMFPfVWb8ExRgIc1gj problem_tests.json\nRetrieving folder contents completed\nBuilding directory structure\nBuilding directory structure completed\nDownloading...\nFrom: https://drive.google.com/uc?id=1zI7lobzGjmM8Qa59hs72QYRU1-HfB3sQ\nTo: /content/pie-perf/data/my_data/generated_programs.jsonl\n\n  0%|          | 0.00/163k [00:00<?, ?B/s]\n\nDownloading...\nFrom: https://drive.google.com/uc?id=1izs1iF5cd_NAZsOaZvrrQF3NAsoP8lHf\nTo: /content/pie-perf/data/my_data/improvement_pairs_additional_metadata.csv\n\n  0%|          | 0.00/147k [00:00<?, ?B/s]\n\nDownloading...\nFrom: https://drive.google.com/uc?id=1aNUEmIyTs0NqPbGMFPfVWb8ExRgIc1gj\nTo: /content/pie-perf/data/my_data/problem_tests.json\n\n  0%|          | 0.00/1.30k [00:00<?, ?B/s]\n\nDownload completed\n['my_data/generated_programs.jsonl',\n 'my_data/improvement_pairs_additional_metadata.csv',\n 'my_data/problem_tests.json']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "from pathlib import Path\n",
    "\n",
    "Path(\"/content/pie-perf/data\").mkdir(exist_ok=True)\n",
    "\n",
    "%cd /content/pie-perf/data\n",
    "gdown.download_folder(\"https://drive.google.com/drive/folders/1-TKkLWBFYHvWITHyOQt2oyuF5J8SdVl1\", output=\"my_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7faa46",
   "metadata": {},
   "source": [
    "### Step 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa07b5f4",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:48, End time: 17:48:48 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb1656a",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287765f0",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bee74b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #### Install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1003117e",
   "metadata": {},
   "source": [
    "### Step 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38792c3",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:48, End time: 17:48:49 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a5ec87",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da349104",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0ec2f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d606663b",
   "metadata": {},
   "source": [
    "### Step 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1088ec5b",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:49, End time: 17:48:49 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76be1632",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385140b2",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dd021a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #### Generate input files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be09b5c",
   "metadata": {},
   "source": [
    "### Step 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7ded8c",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:49, End time: 17:48:49 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01060d8f",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa9f6a6",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad27622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/pie-perf/data/my_data\n/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/pie-perf/data/my_data\n",
    "\n",
    "generations_filename = \"generations.jsonl\"\n",
    "references_filename = \"references.jsonl\"\n",
    "\n",
    "slow_code_col = \"input\"\n",
    "model_generated_potentially_faster_code_col = \"improved_code\"\n",
    "\n",
    "df = pd.read_json(\"generated_programs.jsonl\", lines=True)\n",
    "df = df.rename(columns={\"original_code\": slow_code_col})\n",
    "gen_df = df.drop(columns=\"problem_id\")\n",
    "gen_df.to_json(generations_filename, orient=\"records\", lines=True)\n",
    "\n",
    "ref_df = df.drop(columns=\"improved_code\").copy()\n",
    "ref_df[\"submission_id_v0\"] = [f\"s{i}\" for i in range(len(ref_df))]\n",
    "ref_df.to_json(references_filename, orient=\"records\", lines=True)\n",
    "\n",
    "# Test cases\n",
    "with open(\"problem_tests.json\") as f:\n",
    "    tests = json.load(f)\n",
    "tests_dir = Path(\"test_cases\")\n",
    "tests_dir.mkdir(exist_ok=True)\n",
    "for test_dict in tests:\n",
    "    pid_dir = tests_dir / test_dict[\"pid\"]\n",
    "    pid_dir.mkdir(exist_ok=True)\n",
    "    for i, test in enumerate(test_dict[\"tests\"]):\n",
    "        with open(pid_dir / f\"input.{i}.txt\", \"w\") as f:\n",
    "             f.write(test[\"input\"])\n",
    "        with open(pid_dir / f\"output.{i}.txt\", \"w\") as f:\n",
    "            f.write(test[\"output\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b460883",
   "metadata": {},
   "source": [
    "### Step 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3501df00",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:49, End time: 17:48:49 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678457b3",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35345e51",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484933d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #### Generate evaluation configuration file\n",
    "# \n",
    "# Note: choose `num_problems_to_evaluate = 2` to speedup the evaluation process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55b3eca",
   "metadata": {},
   "source": [
    "### Step 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54e2cde",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:49, End time: 17:48:49 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb60aeb",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18f18bd",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0150801e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation config file\n",
    "data_dir = \"data/my_data\"\n",
    "eval_config_dict = {\n",
    "    \"model_generated_outputs_path\": f\"{data_dir}/{generations_filename}\",\n",
    "    \"inputs_outputs_basepath\": f\"{data_dir}/test_cases\",\n",
    "    \"reference_file_path\": f\"{data_dir}/{references_filename}\",\n",
    "    \"output_report_file_path\": f\"{data_dir}/{generations_filename}.report\",\n",
    "    \"num_problems_to_evaluate\": 2,\n",
    "    \"num_trials\": 2,\n",
    "    \"ignore_first_k\": 0,\n",
    "    \"max_time_per_run\": 10,\n",
    "    \"temp_dir\": None,\n",
    "    \"model_generated_potentially_faster_code_col\": model_generated_potentially_faster_code_col,\n",
    "    \"slow_code_col\": slow_code_col,\n",
    "    #\"reference_code_col\": \"improved_code\",\n",
    "    \"reference_code_col\": model_generated_potentially_faster_code_col,\n",
    "    \"is_prompt_based\": False,\n",
    "    \"cpu_number\": 0,\n",
    "    \"return_if_acc_below\": 1.0,\n",
    "    \"language\": \"cpp\",\n",
    "}\n",
    "eval_config_file = \"eval_config.yaml\"\n",
    "yaml.dump(eval_config_dict, open(eval_config_file, \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57263bd9",
   "metadata": {},
   "source": [
    "### Step 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cc2ca0",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:49, End time: 17:48:49 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dee8c5",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633c1303",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b46815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All metrics are NaN or zero, which means that the problems are not being run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae4e4c5",
   "metadata": {},
   "source": [
    "### Step 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e7a1f9",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:49, End time: 17:48:49 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb883dd1",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f7519c",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efdb944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Error/Traceback: SyntaxError: invalid syntax (1232779528.py, line 1)  Cell In[17], line 1\n    This is because of a bug in the source code. There is a configuration variable called `cflags` that must be a string, but the default value is `None`. This raises an error during excecution which is handled and sent to the logger as a warning. To solve this, I add `clang = \"\"` to the configuration file.\n                    ^\nSyntaxError: invalid syntax\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "This is because of a bug in the source code. There is a configuration variable called `cflags` that must be a string, but the default value is `None`. This raises an error during excecution which is handled and sent to the logger as a warning. To solve this, I add `clang = \"\"` to the configuration file.\n",
    "\n",
    "I also set `num_problems_to_evaluate = 1` to test the code faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658a3102",
   "metadata": {},
   "source": [
    "### Step 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97957f2f",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:49, End time: 17:48:50 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbf5991",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd89c5e",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a0d645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/pie-perf/data/my_data\n/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/pie-perf/data/my_data\n",
    "eval_config_dict[\"cflags\"] = \"\"\n",
    "yaml.dump(eval_config_dict, open(eval_config_file, \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d19a3e",
   "metadata": {},
   "source": [
    "### Step 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a32705",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:50, End time: 17:48:50 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f913f1",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2623ae95",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c868a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The code crashes becauste it tries to log information that is not in the provided data. Specifically, the fields `cpu_time_v0` and `cpu_time_v1`. I add dummy values to those columns as a workaround for this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51533a0",
   "metadata": {},
   "source": [
    "### Step 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2681aa1e",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:50, End time: 17:48:50 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18920fb2",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8841ccc",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08de188e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/pie-perf/data/my_data\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/pie-perf/data/my_data\n",
    "ref_df[\"cpu_time_v0\"] = 1\n",
    "ref_df[\"cpu_time_v1\"] = 1\n",
    "ref_df.to_json(references_filename, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c722dc",
   "metadata": {},
   "source": [
    "### Step 19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a473749",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:50, End time: 17:48:50 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1b4f92",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce04cac",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6098799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the metrics again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfd394b",
   "metadata": {},
   "source": [
    "### Step 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bc25d4",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:50, End time: 17:48:50 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fe666d",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da08202d",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72892fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now it seems to be running properly. I run the script for all the problems and report the metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbed36e",
   "metadata": {},
   "source": [
    "### Step 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec9ef91",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:50, End time: 17:48:50 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05690d9",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022940f2",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4911eb6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/pie-perf/data/my_data\n/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/pie-perf/data/my_data\n",
    "eval_config_dict[\"num_problems_to_evaluate\"] = 10\n",
    "yaml.dump(eval_config_dict, open(eval_config_file, \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31bb70e",
   "metadata": {},
   "source": [
    "### Step 22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a3dc6d",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:52, End time: 17:48:52 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af43b706",
   "metadata": {},
   "source": [
    "**Thought**: Now that the configuration file is set up correctly, I can proceed to run the evaluation script to generate the required metrics for the first 10 problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7339372f",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61527aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/pie-perf\npython: can't open file '/content/pie-perf/evaluate.py': [Errno 2] No such file or directory\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/pie-perf\n",
    "!python evaluate.py --config data/my_data/eval_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aad700",
   "metadata": {},
   "source": [
    "### Step 23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7685978d",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:53, End time: 17:48:54 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2ac72a",
   "metadata": {},
   "source": [
    "**Thought**: It seems that the `evaluate.py` script does not exist in the `/content/pie-perf` directory. I should check the repository structure to find the correct script or command to run the evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c140801a",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd7f50a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " CITATION.cff  '[PIE]_KNN_Prompt.ipynb'   docs\t\t     src\n README.md      data\t\t\t  requirements.txt\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ls /content/pie-perf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae46455",
   "metadata": {},
   "source": [
    "### Step 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591e5c4a",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:55, End time: 17:48:55 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c889274",
   "metadata": {},
   "source": [
    "**Thought**: The `evaluate.py` script is not present in the root directory of the repository. It might be located in the `src` directory. I should check the contents of the `src` directory to find the evaluation script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e9f15f",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2c2ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__init__.py  codenet_eval  make_splits.py  utils\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ls /content/pie-perf/src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d039e12a",
   "metadata": {},
   "source": [
    "### Step 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7769291",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:57, End time: 17:48:57 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd97ee0",
   "metadata": {},
   "source": [
    "**Thought**: The `src` directory contains a `codenet_eval` subdirectory, which might contain the evaluation script. I should check the contents of the `codenet_eval` directory to find the correct script for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db31bc0",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747a8e14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__init__.py   eval_submissions\trun_eval.py\ncpp_examples  evalconfig.py\tsandbox.py\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ls /content/pie-perf/src/codenet_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7019749f",
   "metadata": {},
   "source": [
    "### Step 26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44a85da",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:59, End time: 17:49:00 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1021b65",
   "metadata": {},
   "source": [
    "**Thought**: The `run_eval.py` script in the `src/codenet_eval` directory is likely the script used for evaluation. I should run this script with the appropriate configuration file to perform the evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6479118e",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e660e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Traceback (most recent call last):\n  File \"/content/pie-perf/src/codenet_eval/run_eval.py\", line 23, in <module>\n    from src.codenet_eval.evalconfig import EvaluationConfig\nModuleNotFoundError: No module named 'src'\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python /content/pie-perf/src/codenet_eval/run_eval.py --config /content/pie-perf/data/my_data/eval_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af45148b",
   "metadata": {},
   "source": [
    "### Step 27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8538112b",
   "metadata": {},
   "source": [
    "Execution Start time: 17:49:02, End time: 17:49:03 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c4a867",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the `src` module is not found, which suggests that the current working directory might not be set correctly for the script to locate the module. I should set the `PYTHONPATH` to include the `src` directory and then try running the script again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aefece4",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb605fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Traceback (most recent call last):\n  File \"/content/pie-perf/src/codenet_eval/run_eval.py\", line 23, in <module>\n    from src.codenet_eval.evalconfig import EvaluationConfig\nModuleNotFoundError: No module named 'src'\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!PYTHONPATH=/content/pie-perf/src python /content/pie-perf/src/codenet_eval/run_eval.py --config /content/pie-perf/data/my_data/eval_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3712d13d",
   "metadata": {},
   "source": [
    "### Step 28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aa0142",
   "metadata": {},
   "source": [
    "Execution Start time: 17:49:04, End time: 17:49:05 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c078fa9d",
   "metadata": {},
   "source": [
    "**Thought**: The error persists, indicating that the `src` module is still not found. This might be due to the way the script is structured or the directory structure. I should try modifying the `PYTHONPATH` to include the parent directory of `src` and attempt to run the script again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0321af15",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233e9393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "usage: run_eval.py [-h] [--reference_file_path REFERENCE_FILE_PATH]\n                   [--model_generated_outputs_path MODEL_GENERATED_OUTPUTS_PATH]\n                   [--language LANGUAGE]\n                   [--model_generated_output_col MODEL_GENERATED_OUTPUT_COL]\n                   [--model_generated_potentially_faster_code_col MODEL_GENERATED_POTENTIALLY_FASTER_CODE_COL]\n                   [--output_report_file_path OUTPUT_REPORT_FILE_PATH]\n                   [--cpp_results_path CPP_RESULTS_PATH]\n                   [--reference_code_col REFERENCE_CODE_COL]\n                   [--slow_code_col SLOW_CODE_COL] [--num_trials NUM_TRIALS]\n                   [--ignore_first_k IGNORE_FIRST_K]\n                   [--inputs_outputs_basepath INPUTS_OUTPUTS_BASEPATH]\n                   [--max_time_per_run MAX_TIME_PER_RUN]\n                   [--num_problems_to_evaluate NUM_PROBLEMS_TO_EVALUATE]\n                   [--temp_dir TEMP_DIR] [--cflags CFLAGS] [--is_prompt_based]\n                   [--cpu_number CPU_NUMBER]\n                   [--return_if_acc_below RETURN_IF_ACC_BELOW]\n                   [--eval_config EVAL_CONFIG]\nrun_eval.py: error: unrecognized arguments: --config /content/pie-perf/data/my_data/eval_config.yaml\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!PYTHONPATH=/content/pie-perf python /content/pie-perf/src/codenet_eval/run_eval.py --config /content/pie-perf/data/my_data/eval_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f12f3e1",
   "metadata": {},
   "source": [
    "### Step 29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878e8e02",
   "metadata": {},
   "source": [
    "Execution Start time: 17:49:11, End time: 17:49:12 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebe773d",
   "metadata": {},
   "source": [
    "**Thought**: The `run_eval.py` script does not accept a `--config` argument directly. Instead, it seems to require individual arguments for each configuration parameter. I need to extract the parameters from the `eval_config.yaml` file and pass them as command-line arguments to the script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fea49",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb58c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "usage: run_eval.py [-h] [--reference_file_path REFERENCE_FILE_PATH]\n                   [--model_generated_outputs_path MODEL_GENERATED_OUTPUTS_PATH]\n                   [--language LANGUAGE]\n                   [--model_generated_output_col MODEL_GENERATED_OUTPUT_COL]\n                   [--model_generated_potentially_faster_code_col MODEL_GENERATED_POTENTIALLY_FASTER_CODE_COL]\n                   [--output_report_file_path OUTPUT_REPORT_FILE_PATH]\n                   [--cpp_results_path CPP_RESULTS_PATH]\n                   [--reference_code_col REFERENCE_CODE_COL]\n                   [--slow_code_col SLOW_CODE_COL] [--num_trials NUM_TRIALS]\n                   [--ignore_first_k IGNORE_FIRST_K]\n                   [--inputs_outputs_basepath INPUTS_OUTPUTS_BASEPATH]\n                   [--max_time_per_run MAX_TIME_PER_RUN]\n                   [--num_problems_to_evaluate NUM_PROBLEMS_TO_EVALUATE]\n                   [--temp_dir TEMP_DIR] [--cflags CFLAGS] [--is_prompt_based]\n                   [--cpu_number CPU_NUMBER]\n                   [--return_if_acc_below RETURN_IF_ACC_BELOW]\n                   [--eval_config EVAL_CONFIG]\nrun_eval.py: error: argument --cflags: expected one argument\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the evaluation configuration from the YAML file\n",
    "with open('/content/pie-perf/data/my_data/eval_config.yaml', 'r') as file:\n",
    "    eval_config = yaml.safe_load(file)\n",
    "\n",
    "# Prepare the command-line arguments based on the configuration\n",
    "args = [\n",
    "    f\"--reference_file_path {eval_config['reference_file_path']}\",\n",
    "    f\"--model_generated_outputs_path {eval_config['model_generated_outputs_path']}\",\n",
    "    f\"--language {eval_config['language']}\",\n",
    "    f\"--model_generated_potentially_faster_code_col {eval_config['model_generated_potentially_faster_code_col']}\",\n",
    "    f\"--output_report_file_path {eval_config['output_report_file_path']}\",\n",
    "    f\"--reference_code_col {eval_config['reference_code_col']}\",\n",
    "    f\"--slow_code_col {eval_config['slow_code_col']}\",\n",
    "    f\"--num_trials {eval_config['num_trials']}\",\n",
    "    f\"--ignore_first_k {eval_config['ignore_first_k']}\",\n",
    "    f\"--inputs_outputs_basepath {eval_config['inputs_outputs_basepath']}\",\n",
    "    f\"--max_time_per_run {eval_config['max_time_per_run']}\",\n",
    "    f\"--num_problems_to_evaluate {eval_config['num_problems_to_evaluate']}\",\n",
    "    f\"--cflags {eval_config['cflags']}\",\n",
    "    f\"--cpu_number {eval_config['cpu_number']}\",\n",
    "    f\"--return_if_acc_below {eval_config['return_if_acc_below']}\"\n",
    "]\n",
    "\n",
    "# Join the arguments into a single string\n",
    "args_str = ' '.join(args)\n",
    "\n",
    "# Run the evaluation script with the prepared arguments\n",
    "!PYTHONPATH=/content/pie-perf python /content/pie-perf/src/codenet_eval/run_eval.py {args_str}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da118b2e",
   "metadata": {},
   "source": [
    "### Step 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e086b40d",
   "metadata": {},
   "source": [
    "Execution Start time: 17:49:14, End time: 17:50:41 (duration: 86 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418b1e71",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the `--cflags` argument is not being recognized correctly, likely due to the way it is being passed. I should ensure that the `cflags` argument is properly formatted and try running the script again without the `--cflags` argument, as it might not be necessary for this evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ce8769",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3f0900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\nwriting programs:   0%|                                  | 0/10 [00:00<?, ?it/s]\nwriting programs: 100%|| 10/10 [00:00<00:00, 2044.51it/s]\n\nrunning programs:   0%|                                  | 0/10 [00:00<?, ?it/s]\nrunning programs:  10%|                       | 1/10 [00:04<00:36,  4.04s/it]\nrunning programs:  20%|                    | 2/10 [00:07<00:28,  3.51s/it]\nrunning programs:  30%|                  | 3/10 [00:10<00:23,  3.41s/it]\nrunning programs:  40%|               | 4/10 [00:14<00:21,  3.63s/it]\nrunning programs:  50%|             | 5/10 [00:18<00:18,  3.66s/it]\nrunning programs:  60%|          | 6/10 [00:21<00:14,  3.58s/it]\nrunning programs:  70%|       | 7/10 [00:46<00:32, 10.69s/it]\nrunning programs:  80%|     | 8/10 [01:00<00:23, 11.57s/it]\nrunning programs:  90%|  | 9/10 [01:13<00:12, 12.03s/it]\nrunning programs: 100%|| 10/10 [01:26<00:00, 12.22s/it]\nrunning programs: 100%|| 10/10 [01:26<00:00,  8.60s/it]\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\n100%|| 10/10 [00:00<00:00, 8055.13it/s]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the --cflags argument and try running the script again\n",
    "args.remove(f\"--cflags {eval_config['cflags']}\")\n",
    "\n",
    "# Join the arguments into a single string\n",
    "args_str = ' '.join(args)\n",
    "\n",
    "# Run the evaluation script with the prepared arguments\n",
    "!PYTHONPATH=/content/pie-perf python /content/pie-perf/src/codenet_eval/run_eval.py {args_str}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9ce8de",
   "metadata": {},
   "source": [
    "### Step 31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc00abb9",
   "metadata": {},
   "source": [
    "Execution Start time: 17:50:43, End time: 17:50:44 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba50a0f",
   "metadata": {},
   "source": [
    "**Thought**: The evaluation script has run successfully, and the results should be stored in the specified output report file. I need to read this file and extract the `problem_id` and `input_acc` for each problem to format the results as requested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a07c6aa",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a05e3d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Error/Traceback: JSONDecodeError: Extra data: line 2 column 1 (char 14950)---------------------------------------------------------------------------JSONDecodeError                           Traceback (most recent call last)Cell In[33], line 4\n      2 output_report_file_path = eval_config['output_report_file_path']\n      3 with open(output_report_file_path, 'r') as file:\n----> 4     evaluation_results = json.load(file)\n      6 # Extract the problem_id and input_acc for each problem\n      7 results = [{\"problem_id\": result[\"problem_id\"], \"input_acc\": result[\"input_acc\"]} for result in evaluation_results]\nFile /usr/local/lib/python3.10/json/__init__.py:293, in load(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n    274 def load(fp, *, cls=None, object_hook=None, parse_float=None,\n    275         parse_int=None, parse_constant=None, object_pairs_hook=None, **kw):\n    276     \"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\n    277     a JSON document) to a Python object.\n    278 \n   (...)\n    291     kwarg; otherwise ``JSONDecoder`` is used.\n    292     \"\"\"\n--> 293     return loads(fp.read(),\n    294         cls=cls, object_hook=object_hook,\n    295         parse_float=parse_float, parse_int=parse_int,\n    296         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\nFile /usr/local/lib/python3.10/json/__init__.py:346, in loads(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n    341     s = s.decode(detect_encoding(s), 'surrogatepass')\n    343 if (cls is None and object_hook is None and\n    344         parse_int is None and parse_float is None and\n    345         parse_constant is None and object_pairs_hook is None and not kw):\n--> 346     return _default_decoder.decode(s)\n    347 if cls is None:\n    348     cls = JSONDecoder\nFile /usr/local/lib/python3.10/json/decoder.py:340, in JSONDecoder.decode(self, s, _w)\n    338 end = _w(s, end).end()\n    339 if end != len(s):\n--> 340     raise JSONDecodeError(\"Extra data\", s, end)\n    341 return obj\nJSONDecodeError: Extra data: line 2 column 1 (char 14950)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the evaluation results from the output report file\n",
    "output_report_file_path = eval_config['output_report_file_path']\n",
    "with open(output_report_file_path, 'r') as file:\n",
    "    evaluation_results = json.load(file)\n",
    "\n",
    "# Extract the problem_id and input_acc for each problem\n",
    "results = [{\"problem_id\": result[\"problem_id\"], \"input_acc\": result[\"input_acc\"]} for result in evaluation_results]\n",
    "\n",
    "# Display the results\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3c115a",
   "metadata": {},
   "source": [
    "### Step 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eb9a35",
   "metadata": {},
   "source": [
    "Execution Start time: 17:50:46, End time: 17:50:46 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5c0ce9",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the output report file contains extra data, which suggests that it might not be a valid JSON file. It could be a JSONL (JSON Lines) file where each line is a separate JSON object. I should read the file line by line and parse each line as a JSON object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cae0078",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219ee668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'problem_id': 'p00849', 'input_acc': 0.0},\n {'problem_id': 'p00849', 'input_acc': 0.0},\n {'problem_id': 'p00849', 'input_acc': 0.0},\n {'problem_id': 'p00849', 'input_acc': 0.0},\n {'problem_id': 'p00849', 'input_acc': 1.0},\n {'problem_id': 'p00849', 'input_acc': 1.0},\n {'problem_id': 'p00849', 'input_acc': 1.0},\n {'problem_id': 'p00849', 'input_acc': 1.0},\n {'problem_id': 'p00849', 'input_acc': 1.0},\n {'problem_id': 'p00849', 'input_acc': 1.0}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the output report file line by line and parse each line as a JSON object\n",
    "results = []\n",
    "with open(output_report_file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        result = json.loads(line)\n",
    "        results.append({\"problem_id\": result[\"problem_id\"], \"input_acc\": result[\"input_acc\"]})\n",
    "\n",
    "# Display the results\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d94d67",
   "metadata": {},
   "source": [
    "### Step 33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc6b458",
   "metadata": {},
   "source": [
    "Execution Start time: 17:50:50, End time: 17:50:50 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713c2ba6",
   "metadata": {},
   "source": [
    "**Thought**: The evaluation results have been successfully extracted, showing the `problem_id` and `input_acc` for each problem. The results are ready to be submitted in the requested format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b8fa90",
   "metadata": {},
   "source": [
    "**Action (submit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50c0278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[{'problem_id': 'p00849', 'input_acc': 0.0}, {'problem_id': 'p00849', 'input_acc': 0.0}, {'problem_id': 'p00849', 'input_acc': 0.0}, {'problem_id': 'p00849', 'input_acc': 0.0}, {'problem_id': 'p00849', 'input_acc': 1.0}, {'problem_id': 'p00849', 'input_acc': 1.0}, {'problem_id': 'p00849', 'input_acc': 1.0}, {'problem_id': 'p00849', 'input_acc': 1.0}, {'problem_id': 'p00849', 'input_acc': 1.0}, {'problem_id': 'p00849', 'input_acc': 1.0}]"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
