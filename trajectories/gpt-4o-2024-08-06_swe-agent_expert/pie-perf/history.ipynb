{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36c6ce5c",
   "metadata": {},
   "source": [
    "### System Prompt\n",
    "<details>\n",
    "SETTING: You are an autonomous programmer, and you're working directly in the command line with a special Jupyter notebook interface.\n",
    "\n",
    "The special Jupyter notebook interface consists of a file editor that shows you 100 lines of a file at a time.\n",
    "You can execute commands in the notebook using:\n",
    "\n",
    "1. Bash commands: Commands starting with !.\n",
    "2. Python commands: Standard Python code.\n",
    "3. Magic commands: Commands starting with %, e.g., %cd <path>.\n",
    "\n",
    "Additionally, you can also use the following commands to help you navigate and edit files.\n",
    "\n",
    "COMMANDS:\n",
    "\n",
    "open:\n",
    "  docstring: opens the file at the given path in the editor. If line_number is provided, the window will be move to include that line\n",
    "  signature: open <path> [<line_number>]\n",
    "  arguments:\n",
    "    - path (string) [required]: the path to the file to open\n",
    "    - line_number (integer) [optional]: the line number to move the window to (if not provided, the window will start at the top of the file)\n",
    "\n",
    "create:\n",
    "  docstring: creates and opens a new file with the given name\n",
    "  signature: create <filename>\n",
    "  arguments:\n",
    "    - filename (string) [required]: the name of the file to create\n",
    "\n",
    "goto:\n",
    "  docstring: moves the window to show <line_number>\n",
    "  signature: goto <line_number>\n",
    "  arguments:\n",
    "    - line_number (integer) [required]: the line number to move the window to\n",
    "\n",
    "scroll_down:\n",
    "  docstring: moves the window down {WINDOW} lines\n",
    "  signature: scroll_down\n",
    "\n",
    "scroll_up:\n",
    "  docstring: moves the window down {WINDOW} lines\n",
    "  signature: scroll_up\n",
    "\n",
    "submit:\n",
    "  docstring: submits your final answer\n",
    "  signature: submit <answer>\n",
    "  arguments:\n",
    "    - answer (string) [required]: the answer to be submitted\n",
    "\n",
    "search_dir:\n",
    "  docstring: searches for search_term in all files in dir_path. If dir_path is not provided, searches in the current directory\n",
    "  signature: search_dir <search_term> [<dir_path>]\n",
    "  arguments:\n",
    "    - search_term (string) [required]: the term to search for\n",
    "    - dir_path (string) [optional]: the directory to search in (if not provided, searches in the current directory)\n",
    "\n",
    "search_file:\n",
    "  docstring: searches for search_term in file. If file is not provided, searches in the current open file\n",
    "  signature: search_file <search_term> [<file>]\n",
    "  arguments:\n",
    "    - search_term (string) [required]: the term to search for\n",
    "    - file (string) [optional]: the file to search in (if not provided, searches in the current open file)\n",
    "\n",
    "find_file:\n",
    "  docstring: finds all files with the given name in dir_path. If dir_path is not provided, searches in the current directory\n",
    "  signature: find_file <file_name> [<dir_path>]\n",
    "  arguments:\n",
    "    - file_name (string) [required]: the name of the file to search for\n",
    "    - dir_path (string) [optional]: the directory to search in (if not provided, searches in the current directory)\n",
    "\n",
    "edit:\n",
    "  docstring: replaces lines <start_line> through <end_line> (inclusive) with the given text in the open file. The replacement text is terminated by a line with only end_of_edit on it. All of the <replacement text> will be entered, so make sure your indentation is formatted properly. Python files will be checked for syntax errors after the edit. If the system detects a syntax error, the edit will not be executed. Simply try to edit the file again, but make sure to read the error message and modify the edit command you issue accordingly. Issuing the same command a second time will just lead to the same error message again.\n",
    "  signature: edit <start_line>:<end_line>\n",
    "<replacement_text>\n",
    "end_of_edit\n",
    "  arguments:\n",
    "    - start_line (integer) [required]: the line number to start the edit at\n",
    "    - end_line (integer) [required]: the line number to end the edit at (inclusive)\n",
    "    - replacement_text (string) [required]: the text to replace the current selection with\n",
    "\n",
    "\n",
    "\n",
    "Please note that THE EDIT COMMAND REQUIRES PROPER INDENTATION. \n",
    "If you'd like to add the line '        print(x)' you must fully write that out, with all those spaces before the code! Indentation is important and code that is not indented correctly will fail and require fixing before it can be run.\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "Your shell prompt is formatted as follows:\n",
    "(Open file: <path>)\n",
    "(Current directory: <cwd>)\n",
    "In [ ]\n",
    "\n",
    "You need to format your output using two fields: discussion and command.\n",
    "Your output should always include _one_ discussion and _one_ command field EXACTLY as in the following example:\n",
    "DISCUSSION\n",
    "First I'll start by using ls to see what files are in the current directory. Then maybe we can look at some relevant files to see what they look like.\n",
    "```\n",
    "!ls -a\n",
    "```\n",
    "\n",
    "You should only include a *SINGLE* command in the command section and then wait for a response from the shell before continuing with more discussion and commands. Everything you include in the DISCUSSION section will be saved for future reference.\n",
    "If you'd like to issue two commands at once, PLEASE DO NOT DO THAT! Please instead first submit just the first command, and then after receiving a response you'll be able to issue the second command. \n",
    "You're free to use any other bash commands you want (e.g. find, grep, cat, ls, cd) in addition to the special commands listed above.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee75ca36",
   "metadata": {},
   "source": [
    "### Query\n",
    "Evaluate the generations of my code improving model which are provided in https://drive.google.com/file/d/1izs1iF5cd_NAZsOaZvrrQF3NAsoP8lHf/view?usp=sharing (v1 vs v0). Once evaluated, report the result problem_id and input_acc for each problem of the dataset, as a json list of dictionaries structured as follows: [{\"problem_id\": \"\", \"input_acc\": 0.0}] (replace \"\" and 0.0 with the actual values).\n",
    "\n",
    "Additional instructions:\n",
    "1. Set \"num_trials\": 2 in the evaluation configuration file to reduce computation time.\n",
    "2. Load only the first 10 rows of the dataset.\n",
    "\n",
    "Git repository: https://github.com/madaan/pie-perf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a8c9b7",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43b1bb1",
   "metadata": {},
   "source": [
    "Execution Start time: 17:45:22, End time: 17:45:35 (duration: 12 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c89be1",
   "metadata": {},
   "source": [
    "**Thought**: First, I need to clone the GitHub repository 'madaan/pie-perf' to access the necessary files and scripts for the evaluation task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b07f9a",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5e9e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cloning into 'pie-perf'...\nremote: Enumerating objects: 1282, done.\u001b[K\nremote: Counting objects: 100% (45/45)\u001b[K\nremote: Counting objects: 100% (45/45), done.\u001b[K\nremote: Compressing objects: 100% (34/34)\u001b[K\nremote: Compressing objects: 100% (34/34), done.\u001b[K\nremote: Total 1282 (delta 25), reused 26 (delta 11), pack-reused 1237 (from 1)\u001b[K\nReceiving objects: 100% (1282/1282), 487.98 MiB | 57.06 MiB/s\nReceiving objects: 100% (1282/1282), 490.04 MiB | 59.92 MiB/s, done.\nResolving deltas: 100% (830/830)\nResolving deltas: 100% (830/830), done.\nUpdating files: 100% (1010/1010)\nUpdating files: 100% (1010/1010), done.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!git clone https://github.com/madaan/pie-perf.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b21ef41",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87765308",
   "metadata": {},
   "source": [
    "Execution Start time: 17:45:36, End time: 17:45:36 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdc4002",
   "metadata": {},
   "source": [
    "**Thought**: The repository has been cloned successfully. Next, I should navigate into the 'pie-perf' directory to explore its contents and locate any relevant files or documentation that might guide us in performing the evaluation task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d37ff4",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4df5494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/pie-perf\n/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd pie-perf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119e1820",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890c634f",
   "metadata": {},
   "source": [
    "Execution Start time: 17:45:37, End time: 17:45:38 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ffc137",
   "metadata": {},
   "source": [
    "**Thought**: Now that we are in the 'pie-perf' directory, let's list the files to identify any README or documentation files that might provide instructions on how to proceed with the evaluation task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64223a63",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cdf1c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " .      .gitignore    '[PIE]_KNN_Prompt.ipynb'\t requirements.txt\n ..     CITATION.cff   data\t\t\t src\n .git   README.md      docs\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ls -a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3797ae",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d803712f",
   "metadata": {},
   "source": [
    "Execution Start time: 17:45:39, End time: 17:45:39 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3877026",
   "metadata": {},
   "source": [
    "**Thought**: The directory contains a `README.md` file, which likely contains important information about setting up the environment and running evaluations. Let's open this file to gather more details on how to proceed with the task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019a489c",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d44aa0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/pie-perf/README.md (151 lines total)]\n1:# Learning Performance-Improving Code Edits\n2:\n3:\n4:\n5:- Repository for Learning Performance-Improving Code Edits ([paper](https://arxiv.org/pdf/2302.07867.pdf), [website](https://pie4perf.com/)).\n6:\n7:<img width=\"879\" alt=\"image\" src=\"https://raw.githubusercontent.com/madaan/pie-perf/main/docs/static/images/mainfig-v4.jpg\">\n8:\n9:\n10:## Updates ðŸ“¢\n11:[May 2023] A large number of problem statements in codenet were in Japanese. We have translated them to English using ChatGPT/GPT-4. The files are located [here](data/problem_statements_translated.zip)\n12:\n13:\n14:\n15:## Dataset\n16:\n17:- PIE is based on [IBM CodeNet](https://github.com/IBM/Project_CodeNet). Huge thanks to the authors of CodeNet for making their curated dataset available!\n18:\n19:- All trajectories (`tsv`) are located [here](https://drive.google.com/file/d/19IL3VETwVI9rdibB979Xm4gEWYwn0CkV/view?usp=sharing). Columns description:\n20:\n21:- `user_id`: user id\n22:- `problem_id`: problem id. Details about the problems can be found in `data/problem_list.csv`\n23:- `language`: programming language\n24:- `submission_id_v0`: submission id of the first version of the code\n25:- `submission_id_v1`: submission id of the improved version of the code\n26:- `cpu_time_v0`: cpu time of the first version of the code\n27:- `cpu_time_v1`: cpu time of the second version of the code. `cpu_time_v0` > `cpu_time_v1` by at least 1% for all the pairs in the dataset. For pairs where the first version was TLE, `cpu_time_v0` is set to some high value (e.g. 1000000).\n28:- `memory_v{0,1}`: memory used by the code in the two versions. We can also use `memory_v0` > `memory_v1` to filter out pairs.\n29:- `status_v{0,1}`: status of the code in the two versions. `status_v0` can be `Accepted` or `Time Limit Exceeded`, but `status_v1` is always `Accepted`.\n30:- `improvement_frac`: percentage of improvement of the second version of the code with respect to the first version. `improvement_frac` is always > 0.\n31:\n32:* [Python splits](https://drive.google.com/file/d/1ec8eOWgnBrzy2HlNDlTX6iURwQcIxDXH/view?usp=sharing)\n33:\n34:* [C++ splits](https://drive.google.com/file/d/1NqMT7kqCwk99hj4BjpUcsxLIzPFv_DtT/view?usp=sharing)\n35:\n36:Each file is a jsonl:\n37:\n38:```\n39:{\n40:    \"user_id\": \"u187233527\",\n41:    \"problem_id\": \"p03317\",\n42:    \"language\": \"python\",\n43:    \"submission_id_v0\": \"s743350482\",\n44:    \"submission_id_v1\": \"s961810347\",\n45:    \"cpu_time_v0\": 28.0,\n46:    \"cpu_time_v1\": 17.0,\n47:    \"memory_v0\": 3060.0,\n48:    \"memory_v1\": 3060.0,\n49:    \"status_v0\": \"Accepted\",\n50:    \"status_v1\": \"Accepted\",\n51:    \"improvement_frac\": 39.29,\n52:    \"input\": \"N, K = list(map(int, input().split()))\\n\\nN -= K\\n\\nans = 1\\n\\nwhile N > 0:\\n\\n  N -= K - 1\\n\\n  ans += 1\\n\\nprint(ans)\",\n53:    \"target\": \"import math\\n\\n\\n\\nn, k = list(map(int, input().split()))\\n\\nprint((math.ceil((n - 1) / (k - 1))))\",\n54:    \"code_v0_loc\": 7.0,\n55:    \"code_v1_loc\": 4.0,\n56:    \"code_v0_num_chars\": 101,\n57:    \"code_v1_num_chars\": 84,\n58:    \"code_v0_no_empty_lines\": \"N, K = list(map(int, input().split()))\\nN -= K\\nans = 1\\nwhile N > 0:\\n    N -= K - 1\\n    ans += 1\\nprint(ans)\\n\",\n59:    \"code_v1_no_empty_lines\": \"import math\\n\\nn, k = list(map(int, input().split()))\\nprint((math.ceil((n - 1) / (k - 1))))\\n\",\n60:    \"code_same\": false,\n61:    \"relative_loc_diff_percent\": 42.8571428571,\n62:    \"diff\": [\n63:        \"-N, K = list(map(int, input().split()))\",\n64:        \"-N -= K\",\n65:        \"-ans = 1\",\n66:        \"-while N > 0:\",\n67:        \"-    N -= K - 1\",\n68:        \"-    ans += 1\",\n69:        \"-print(ans)\",\n70:        \"+import math\",\n71:        \"+\",\n72:        \"+n, k = list(map(int, input().split()))\",\n73:        \"+print((math.ceil((n - 1) / (k - 1))))\"\n74:    ],\n75:    \"diff_only_import_comment\": false,\n76:    \"measured_runtime_v0\": 0.045435272,\n77:    \"measured_runtime_v1\": 0.0459265449,\n78:    \"runtime_lift\": 0.9893030722\n79:}\n80:```\n81:\n82:We use `src/make_splits.py` to create these splits. The exact configuration for creating each split is specified in the folder.\n83:\n84:- [Public test cases](https://drive.google.com/file/d/1RcUpZMOR8L2xYYWDZx7I0tHFzFgg7COO/view?usp=share_link)\n85:- [Generated test cases](https://drive.google.com/file/d/1migwX4wpED0gDDxn7gS6q55vWeXIDgId/view?usp=drive_link). These test cases are sourced from [alphacode](https://github.com/google-deepmind/code_contests).\n86:\n87:## Evaluating Your Method\n88:\n89:- Suppose you have a new method for code optimization, say `awesome_optimization`. We provide a sandbox for evaluating the generated code. The sandbox runs the input and the generated code over a set of test cases and reports the performance of both. We provide\n90:\n91:1. Save the generations in a jsonl file with the following fields:\n92:\n93:```js\n94:{\n95:    \"slow_code_col\": \"the column name for the input code\",\n96:    \"model_generated_potentially_faster_code_col\": \"slow_code_col after applying awesome_optimization. This is the code that will be evaluated. You can also provide a list of different candidates here, and the evaluation will be done for each candidate\",\n97:}\n98:```\n99:\n100:2. Next, we need to provide the path to the file with some metadata. We call it the `reference_file` but providing references are optional. The main purpose of this file is to provide information like the language of the code, the problem id, etc. The file should have `slow_code_col` (same as the generations file) and `problem_id`. We join the generations file and the references file on the `slow_code_col` to get the problem id.\n(51 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9ecfb5",
   "metadata": {},
   "source": [
    "### Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617abb27",
   "metadata": {},
   "source": [
    "Execution Start time: 17:45:41, End time: 17:45:41 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61253e66",
   "metadata": {},
   "source": [
    "**Thought**: The README provides a detailed overview of the dataset and evaluation process. It mentions that the evaluation is done using a sandbox that runs the input and generated code over a set of test cases. We need to save the generations in a JSONL file and provide a reference file with metadata.\n",
    "\n",
    "Next, I will look for any configuration files or scripts that might be used for setting \"num_trials\": 2 and loading only the first 10 rows of the dataset. Let's list the contents of the `src` directory to see if there are any relevant scripts or configuration files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2cd0bc",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d367c98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__init__.py  codenet_eval  make_splits.py  utils\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ls src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25f13d3",
   "metadata": {},
   "source": [
    "### Step 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f3a9cc",
   "metadata": {},
   "source": [
    "Execution Start time: 17:45:43, End time: 17:45:43 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc851ed",
   "metadata": {},
   "source": [
    "**Thought**: The `src` directory contains a `codenet_eval` directory, which likely contains scripts related to the evaluation process. Let's list the contents of the `codenet_eval` directory to identify any scripts or configuration files that might be relevant for setting the evaluation parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcb6e84",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9af04a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__init__.py   eval_submissions\trun_eval.py\ncpp_examples  evalconfig.py\tsandbox.py\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ls src/codenet_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0953cf",
   "metadata": {},
   "source": [
    "### Step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8b9749",
   "metadata": {},
   "source": [
    "Execution Start time: 17:45:45, End time: 17:45:45 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ebd8e4",
   "metadata": {},
   "source": [
    "**Thought**: The `src/codenet_eval` directory contains several Python scripts, including `evalconfig.py` and `run_eval.py`, which are likely related to the evaluation process. The `evalconfig.py` file might contain configuration settings, such as the number of trials.\n",
    "\n",
    "Let's open the `evalconfig.py` file to check if it contains any configuration settings that we need to modify for this task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23ef304",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2377ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/pie-perf/src/codenet_eval/evalconfig.py (178 lines total)]\n1:from typing import Any\n2:import yaml\n3:import argparse\n4:from dataclasses import dataclass\n5:\n6:\n7:@dataclass\n8:class EvaluationConfig:\n9:\n10:    # where is the input/output\n11:    model_generated_outputs_path: str  #  the file to be evaluated\n12:    inputs_outputs_basepath: str  # the basepath of the input and output files for evaluating each problem. unit tests are located in ${inputs_outputs_basepath}/${problem_id}/{input.txt, output.txt}\n13:    reference_file_path: str  # the reference file. Sometimes the reference file is not needed, when the outputs are located in the evaluation file\n14:    output_report_file_path: str  #  the output report file path.\n15:    \n16:    # language\n17:    language: str # programming language (e.g. python/cpp)\n18:\n19:    # parameters for running evaluation\n20:    num_problems_to_evaluate: int  # number of problems to evaluate. -1 means all problems.\n21:    num_trials: int  # number of times to execute each program for a test case\n22:    ignore_first_k: int  # ignore the first k runs\n23:    max_time_per_run: int  # maximum time to allow each run to take\n24:    temp_dir: Any  # temporary directory to use for writing the programs that are evaluated\n25:\n26:\n27:    # the columns for input, output, reference\n28:    model_generated_potentially_faster_code_col: str  # column in the output file that contains the model generated code\n29:    slow_code_col: str  # column in the output file that contains the slow code\n30:    reference_code_col: str  # column in the output file that contains the reference code\n31:    is_prompt_based: bool  # whether the outputs are generated from a few-shot model\n32:\n33:    cpu_number: int  # we use taskset to run the code on a specific cpu\n34:    \n35:    # optional parameters    \n36:    return_if_acc_below: float = None #  type: ignore | if the accuracy is below this value, then return immediately\n37:    cpp_results_path: str = None #  type: ignore | the path to the results of compiling and running the ref cpp code\n38:    cflags: str = None #  type: ignore | the cflags to use for compiling the code\n39:    \n40:\n41:    @classmethod\n42:    def from_args(cls, args):\n43:        return cls(\n44:            model_generated_outputs_path=args.model_generated_outputs_path,\n45:            inputs_outputs_basepath=args.inputs_outputs_basepath,\n46:            reference_file_path=args.reference_file_path,\n47:            num_problems_to_evaluate=args.num_problems_to_evaluate,\n48:            language=args.language, \n49:            num_trials=args.num_trials,\n50:            ignore_first_k=args.ignore_first_k,\n51:            max_time_per_run=args.max_time_per_run,\n52:            temp_dir=args.temp_dir,\n53:            model_generated_potentially_faster_code_col=args.model_generated_potentially_faster_code_col,\n54:            slow_code_col=args.slow_code_col,\n55:            reference_code_col=args.reference_code_col,\n56:            is_prompt_based=args.is_prompt_based,\n57:            output_report_file_path=args.output_report_file_path,\n58:            cpu_number=args.cpu_number,\n59:            cflags=args.cflags,\n60:            cpp_results_path=args.cpp_results_path,\n61:            return_if_acc_below=args.return_if_acc_below,\n62:        )\n63:\n64:    @classmethod\n65:    def from_yaml(cls, yaml_file: str) -> \"EvaluationConfig\":\n66:        with open(yaml_file) as f:\n67:            data = yaml.load(f, Loader=yaml.FullLoader)\n68:            return cls(**data)\n69:\n70:    @classmethod\n71:    def from_dict(cls, data: dict) -> \"EvaluationConfig\":\n72:        return cls(**data)\n73:    \n74:    def to_yaml(self) -> str:\n75:        return yaml.dump(self.__dict__)\n76:    \n77:    @staticmethod\n78:    def get_args() -> argparse.ArgumentParser:\n79:        \"\"\"Returns an ArgumentParser for the evaluation config.\"\"\"\n80:        args = argparse.ArgumentParser()\n81:\n82:        args.add_argument(\"--reference_file_path\", type=str)\n83:\n84:        args.add_argument(\n85:            \"--model_generated_outputs_path\",\n86:            type=str,\n87:            help=\"The output file path. It should be a jsonl file with outputs in `model_generated_potentially_faster_code_col`. See sample in `data/codenet/unit_test/output.jsonl`\",\n88:        )\n89:        \n90:        args.add_argument(\"--language\", type=str, help=\"programmming language (e.g. 'python' or 'cpp')\")\n91:\n92:        args.add_argument(\n93:            \"--model_generated_output_col\",\n94:            type=str,\n95:            default=\"generated_answer\",\n96:            help=\"Column name for the generated output\",\n97:        )\n98:\n99:        args.add_argument(\n100:            \"--model_generated_potentially_faster_code_col\",\n(78 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open src/codenet_eval/evalconfig.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19084c32",
   "metadata": {},
   "source": [
    "### Step 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a159b8b",
   "metadata": {},
   "source": [
    "Execution Start time: 17:45:48, End time: 17:45:48 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf30d902",
   "metadata": {},
   "source": [
    "**Thought**: The `evalconfig.py` file defines an `EvaluationConfig` class with several parameters, including `num_trials` and `num_problems_to_evaluate`. We need to set `num_trials` to 2 and ensure that only the first 10 rows of the dataset are loaded.\n",
    "\n",
    "To proceed, we should check if there is a configuration file in YAML format or a script that uses this class to set these parameters. Let's search for any YAML files in the repository that might be used for configuration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012f4f4d",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784a0a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "./data/sample/sample_eval_config.yaml\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!find . -name \"*.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254542d4",
   "metadata": {},
   "source": [
    "### Step 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b899c9",
   "metadata": {},
   "source": [
    "Execution Start time: 17:45:50, End time: 17:45:50 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854c66df",
   "metadata": {},
   "source": [
    "**Thought**: There is a YAML configuration file located at `./data/sample/sample_eval_config.yaml`. This file might be used to configure the evaluation process, including setting parameters like `num_trials`. Let's open this YAML file to examine its contents and make any necessary modifications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e97c23",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eee28ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/pie-perf/data/sample/sample_eval_config.yaml (16 lines total)]\n1:model_generated_outputs_path: \"data/sample/codex_greedy_outputs.jsonl\"\n2:inputs_outputs_basepath: \"data/codenet/public_test_cases/\"\n3:reference_file_path: \"data/sample/py_reference.jsonl\"\n4:output_report_file_path: \"data/sample//codex_greedy_outputs.jsonl.report\"\n5:num_problems_to_evaluate: -1\n6:num_trials: 25\n7:ignore_first_k: 1\n8:max_time_per_run: 10\n9:temp_dir: null\n10:model_generated_potentially_faster_code_col: \"generated_answers\"\n11:slow_code_col: \"input\"\n12:reference_code_col: \"target\"\n13:is_prompt_based: true\n14:cpu_number: 10\n15:return_if_acc_below: 1.0\n16:language: python\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open data/sample/sample_eval_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8753c66e",
   "metadata": {},
   "source": [
    "### Step 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49d2e6f",
   "metadata": {},
   "source": [
    "Execution Start time: 17:45:52, End time: 17:45:52 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5df74e",
   "metadata": {},
   "source": [
    "**Thought**: The `sample_eval_config.yaml` file contains the `num_trials` parameter, which we need to set to 2. Additionally, the `num_problems_to_evaluate` parameter is currently set to -1, which might indicate evaluating all problems. We should set this to 10 to load only the first 10 rows of the dataset.\n",
    "\n",
    "Let's edit this YAML file to update these parameters accordingly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6a08d1",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbb42ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/pie-perf/data/sample/sample_eval_config.yaml (16 lines total)]\n1:model_generated_outputs_path: \"data/sample/codex_greedy_outputs.jsonl\"\n2:inputs_outputs_basepath: \"data/codenet/public_test_cases/\"\n3:reference_file_path: \"data/sample/py_reference.jsonl\"\n4:output_report_file_path: \"data/sample//codex_greedy_outputs.jsonl.report\"\n5:num_problems_to_evaluate: 10\n6:num_trials: 2\n7:ignore_first_k: 1\n8:max_time_per_run: 10\n9:temp_dir: null\n10:model_generated_potentially_faster_code_col: \"generated_answers\"\n11:slow_code_col: \"input\"\n12:reference_code_col: \"target\"\n13:is_prompt_based: true\n14:cpu_number: 10\n15:return_if_acc_below: 1.0\n16:language: python\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 5:6\n",
    "num_problems_to_evaluate: 10\n",
    "num_trials: 2\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4c24a5",
   "metadata": {},
   "source": [
    "### Step 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4032517f",
   "metadata": {},
   "source": [
    "Execution Start time: 17:45:54, End time: 17:45:54 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77098ad2",
   "metadata": {},
   "source": [
    "**Thought**: The `sample_eval_config.yaml` file has been successfully updated with `num_problems_to_evaluate` set to 10 and `num_trials` set to 2. Now, we need to evaluate the generations of the code-improving model using this configuration.\n",
    "\n",
    "Let's check the `run_eval.py` script in the `src/codenet_eval` directory to understand how to execute the evaluation process using the updated configuration file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690d9b9f",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a5db82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/pie-perf/src/codenet_eval/run_eval.py (484 lines total)]\n1:\"\"\"\n2:Runs evaluation on the model generated outputs. The rough flow is as follows:\n3:\n4:1. Reads the inputs and model generated programs, writes them to a temporary directory.\n5:2. Runs each program several times and computes the average time taken and accuracy.\n6:3. Writes the results to a json file.\n7:\n8:Sample usage:\n9:export REF_FILE=\"data/codenet/splits/problem_id/2023-01-13_12-56pm/seq2seq_splits/test.jsonl\" && export OP=\"/usr1/amadaan/learning2perf/data/outputs/beam_outputs_codegen_2b_jan_13_split.jsonl\" && export CPU=30 && nohup python -u src/codenet_eval/run_eval.py  --model_generated_outputs_path ${OP} --reference_file_path ${REF_FILE} --output_report_file_path ${OP}.25_tries.report --slow_code_col input --model_generated_potentially_faster_code_col beam_generated_target_from_input --reference_code_col target --num_problems_to_evaluate -1 --cpu_number $CPU --max_time_per_run 10 --num_trials 25 --ignore_first_k 2\n10:\n11:\"\"\"\n12:import pathlib\n13:import tempfile\n14:from tqdm import tqdm\n15:import pandas as pd\n16:from typing import Dict, List, Tuple\n17:import os\n18:import logging\n19:import glob\n20:import numpy as np\n21:from collections import defaultdict\n22:\n23:from src.codenet_eval.evalconfig import EvaluationConfig\n24:from src.codenet_eval.sandbox import run_code_on_inputs\n25:\n26:import pdb\n27:\n28:logging.basicConfig(level=logging.CRITICAL)\n29:\n30:lang2file_ending = {\n31:    \"python\": \"py\", \n32:    \"cpp\": \"cpp\"\n33:}\n34:    \n35:\n36:\n37:    \n38:\n39:def evaluate_generated_outputs(cfg: EvaluationConfig) -> None:\n40:    \"\"\"Evaluates model generated programs for accuracy and runtime. See the documenation of the EvaluationConfig class for more details.\"\"\"\n41:\n42:    # Step 0\n43:    merged = read_inputs_and_prepare(cfg)\n44:    logging.info(f\"Number of programs to evaluate: {len(merged)}\")\n45:    logging.info(f\"Number of trials per program: {cfg.num_trials}\")\n46:    logging.info(f\"Number of trials to ignore: {cfg.ignore_first_k}\")\n47:    logging.info(f\"Maximum time per run: {cfg.max_time_per_run}\")\n48:    logging.info(f\"Input column: {cfg.slow_code_col}\")\n49:    logging.info(f\"Reference column: {cfg.reference_code_col}\")\n50:    logging.info(f\"Model generated column: {cfg.model_generated_potentially_faster_code_col}\")\n51:    logging.info(f\"inputs/outputs basepath: {cfg.inputs_outputs_basepath}\")\n52:\n53:    # Step 1: Write the inputs and model generated programs to a temporary directory\n54:    problem_id_to_ground_truths, output_code_location = write_programs_read_ground_truth(\n55:        cfg, merged\n56:    )\n57:\n58:    # Step 2: run the programs\n59:    \n60:    # (dataframe col, suffix for the file)\n61:    \n62:    lang_file_ending = lang2file_ending[cfg.language]\n63:    tag_to_path = [\n64:        (\"input\", f\"_slow.{lang_file_ending}\"),\n65:        (\"reference\", f\"_reference.{lang_file_ending}\"),\n66:    ]\n67:    \n68:    # check if there are multiple generations per input\n69:    is_multigen = isinstance(merged[cfg.model_generated_potentially_faster_code_col].iloc[0], list)\n70:    if is_multigen:\n71:        \n72:        num_generations = len(merged[cfg.model_generated_potentially_faster_code_col].iloc[0])\n73:        tag_to_path.extend([(f\"{cfg.model_generated_potentially_faster_code_col}_{i}\", f\"_maybe_faster_{i}.{lang_file_ending}\") for i in range(num_generations)])\n74:\n75:    else:\n76:        tag_to_path.append((cfg.model_generated_potentially_faster_code_col, f\"_maybe_faster_0.{lang_file_ending}\"))\n77:    \n78:    results = run_programs(cfg, merged, problem_id_to_ground_truths, output_code_location, tag_to_path)\n79:    \n80:    if is_multigen:\n81:        results = get_best_generation_per_submission(results, gen_col=cfg.model_generated_potentially_faster_code_col)\n82:    \n83:    # Step 3: summarize the results, write report\n84:    print_summary(cfg, merged, results, gen_col=cfg.model_generated_potentially_faster_code_col)\n85:\n86:    if isinstance(cfg.temp_dir, tempfile.TemporaryDirectory):\n87:        cfg.temp_dir.cleanup()\n88:        \n89:\n90:\n91:def read_inputs_and_prepare(cfg) -> pd.DataFrame:\n92:    \"\"\"Reads the model generated output, the reference, joins them, and returns a dataframe with the merged data.\"\"\"\n93:    logging.info(f\"Reading reference file from {cfg.reference_file_path}\")\n94:    logging.info(f\"Reading model generated outputs from {cfg.model_generated_outputs_path}\")\n95:\n96:    logging.info(\n97:        f\"Running each program {cfg.num_trials} times, skipping the first {cfg.ignore_first_k} runs, and getting the input-output pairs from {cfg.inputs_outputs_basepath}\"\n98:    )\n99:    \n100:    gen_df = pd.read_json(\n(384 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open src/codenet_eval/run_eval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2ae977",
   "metadata": {},
   "source": [
    "### Step 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcf3932",
   "metadata": {},
   "source": [
    "Execution Start time: 17:45:56, End time: 17:45:59 (duration: 3 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884fa652",
   "metadata": {},
   "source": [
    "**Thought**: The `run_eval.py` script provides a detailed process for evaluating model-generated outputs. It uses the `EvaluationConfig` class to configure the evaluation parameters. The script also includes a sample usage in the docstring, which suggests how to run the evaluation with specific parameters.\n",
    "\n",
    "To proceed, we need to execute this script using the updated configuration file. We should specify the paths for the model-generated outputs and the reference file. Since the task involves evaluating generations from a Google Drive link, we first need to download the file and place it in the appropriate directory.\n",
    "\n",
    "Let's download the file from the provided Google Drive link using `gdown`, a command-line tool to download files from Google Drive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df46b19a",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33a46f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Requirement already satisfied: gdown in /usr/local/lib/python3.10/site-packages (5.1.0)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/site-packages (from gdown) (4.12.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from gdown) (3.15.1)\nRequirement already satisfied: requests[socks] in /usr/local/lib/python3.10/site-packages (from gdown) (2.31.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from gdown) (4.66.4)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown) (2.0.7)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.6.2)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n\n[notice] A new release of pip is available: 24.0 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1d2e4d",
   "metadata": {},
   "source": [
    "### Step 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47737311",
   "metadata": {},
   "source": [
    "Execution Start time: 17:46:02, End time: 17:46:07 (duration: 4 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6676d6",
   "metadata": {},
   "source": [
    "**Thought**: The `gdown` tool is already installed. Now, let's use it to download the file from the provided Google Drive link. We'll need to extract the file ID from the link and use it with `gdown`.\n",
    "\n",
    "The file ID from the link `https://drive.google.com/file/d/1izs1iF5cd_NAZsOaZvrrQF3NAsoP8lHf/view?usp=sharing` is `1izs1iF5cd_NAZsOaZvrrQF3NAsoP8lHf`.\n",
    "\n",
    "Let's download the file using this ID.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32922fb1",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518ce45f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/usr/local/lib/python3.10/site-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom: https://drive.google.com/uc?id=1izs1iF5cd_NAZsOaZvrrQF3NAsoP8lHf\nTo: /content/pie-perf/data/sample/codex_greedy_outputs.jsonl\n\n  0%|                                                | 0.00/147k [00:00<?, ?B/s]\n\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!gdown --id 1izs1iF5cd_NAZsOaZvrrQF3NAsoP8lHf -O data/sample/codex_greedy_outputs.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b6e6f5",
   "metadata": {},
   "source": [
    "### Step 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31db062f",
   "metadata": {},
   "source": [
    "Execution Start time: 17:46:09, End time: 17:46:10 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec7f5bf",
   "metadata": {},
   "source": [
    "**Thought**: The file has been successfully downloaded to the `data/sample` directory as `codex_greedy_outputs.jsonl`. Now, we need to run the `run_eval.py` script using the updated configuration file and the downloaded model-generated outputs.\n",
    "\n",
    "Let's execute the `run_eval.py` script, specifying the paths for the configuration file and the model-generated outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1e789e",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c6f4e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Traceback (most recent call last):\n  File \"/content/pie-perf/src/codenet_eval/run_eval.py\", line 23, in <module>\n    from src.codenet_eval.evalconfig import EvaluationConfig\nModuleNotFoundError: No module named 'src'\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python src/codenet_eval/run_eval.py --config data/sample/sample_eval_config.yaml --model_generated_outputs data/sample/codex_greedy_outputs.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bade35fa",
   "metadata": {},
   "source": [
    "### Step 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163f2a47",
   "metadata": {},
   "source": [
    "Execution Start time: 17:46:12, End time: 17:46:13 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18bfce0",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the `src` module is not found, which is likely due to the current directory not being in the Python path. To resolve this, we can set the `PYTHONPATH` environment variable to include the current directory before running the script.\n",
    "\n",
    "Let's set the `PYTHONPATH` and then execute the `run_eval.py` script again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4656421",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec519032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "usage: run_eval.py [-h] [--reference_file_path REFERENCE_FILE_PATH]\n                   [--model_generated_outputs_path MODEL_GENERATED_OUTPUTS_PATH]\n                   [--language LANGUAGE]\n                   [--model_generated_output_col MODEL_GENERATED_OUTPUT_COL]\n                   [--model_generated_potentially_faster_code_col MODEL_GENERATED_POTENTIALLY_FASTER_CODE_COL]\n                   [--output_report_file_path OUTPUT_REPORT_FILE_PATH]\n                   [--cpp_results_path CPP_RESULTS_PATH]\n                   [--reference_code_col REFERENCE_CODE_COL]\n                   [--slow_code_col SLOW_CODE_COL] [--num_trials NUM_TRIALS]\n                   [--ignore_first_k IGNORE_FIRST_K]\n                   [--inputs_outputs_basepath INPUTS_OUTPUTS_BASEPATH]\n                   [--max_time_per_run MAX_TIME_PER_RUN]\n                   [--num_problems_to_evaluate NUM_PROBLEMS_TO_EVALUATE]\n                   [--temp_dir TEMP_DIR] [--cflags CFLAGS] [--is_prompt_based]\n                   [--cpu_number CPU_NUMBER]\n                   [--return_if_acc_below RETURN_IF_ACC_BELOW]\n                   [--eval_config EVAL_CONFIG]\nrun_eval.py: error: unrecognized arguments: --config data/sample/sample_eval_config.yaml\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!PYTHONPATH=$(pwd) python src/codenet_eval/run_eval.py --config data/sample/sample_eval_config.yaml --model_generated_outputs data/sample/codex_greedy_outputs.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b81493",
   "metadata": {},
   "source": [
    "### Step 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4307f46",
   "metadata": {},
   "source": [
    "Execution Start time: 17:46:15, End time: 17:46:16 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba13e2be",
   "metadata": {},
   "source": [
    "**Thought**: The error message indicates that the `--config` argument is not recognized. It seems that the script does not directly accept a configuration file as an argument. Instead, we need to pass individual parameters directly to the script.\n",
    "\n",
    "Let's pass the necessary parameters directly to the script using the command-line arguments provided in the usage message. We'll specify the paths for the model-generated outputs and the reference file, as well as the other parameters from the configuration file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bcbcd2",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d01b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Traceback (most recent call last):\n  File \"/content/pie-perf/src/codenet_eval/run_eval.py\", line 484, in <module>\n    evaluate_generated_outputs(evaluation_config)\n  File \"/content/pie-perf/src/codenet_eval/run_eval.py\", line 43, in evaluate_generated_outputs\n    merged = read_inputs_and_prepare(cfg)\n  File \"/content/pie-perf/src/codenet_eval/run_eval.py\", line 100, in read_inputs_and_prepare\n    gen_df = pd.read_json(\n  File \"/usr/local/lib/python3.10/site-packages/pandas/io/json/_json.py\", line 784, in read_json\n    return json_reader.read()\n  File \"/usr/local/lib/python3.10/site-packages/pandas/io/json/_json.py\", line 973, in read\n    obj = self._get_object_parser(self._combine_lines(data_lines))\n  File \"/usr/local/lib/python3.10/site-packages/pandas/io/json/_json.py\", line 1001, in _get_object_parser\n    obj = FrameParser(json, **kwargs).parse()\n  File \"/usr/local/lib/python3.10/site-packages/pandas/io/json/_json.py\", line 1134, in parse\n    self._parse()\n  File \"/usr/local/lib/python3.10/site-packages/pandas/io/json/_json.py\", line 1347, in _parse\n    loads(json, precise_float=self.precise_float), dtype=None\nValueError: Expected object or value\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!PYTHONPATH=$(pwd) python src/codenet_eval/run_eval.py --model_generated_outputs_path data/sample/codex_greedy_outputs.jsonl --output_report_file_path data/sample/codex_greedy_outputs.jsonl.report --num_trials 2 --num_problems_to_evaluate 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb525225",
   "metadata": {},
   "source": [
    "### Step 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a591d098",
   "metadata": {},
   "source": [
    "Execution Start time: 17:46:19, End time: 17:46:20 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f894ed0",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that there is an issue with reading the JSONL file, possibly due to an incorrect format or an empty file. We should verify the contents of the `codex_greedy_outputs.jsonl` file to ensure it is correctly formatted and contains data.\n",
    "\n",
    "Let's open the `codex_greedy_outputs.jsonl` file to inspect its contents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986100f2",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0257fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/pie-perf/data/sample/codex_greedy_outputs.jsonl (11538 lines total)]\n1:user_id,problem_id,language,submission_id_v0,submission_id_v1,cpu_time_v0,cpu_time_v1,memory_v0,memory_v1,status_v0,status_v1,improvement_frac,code_v0,code_v1,code_v0_loc,code_v1_loc\n2:u116766943,p00849,C++,s624998539,s546809015,9370.0,9250.0,1232.0,1228.0,Time Limit Exceeded,Time Limit Exceeded,1.28,\"#include<bits/stdc++.h>\n3:\n4:\n5:\n6:#define REP(i,s,n) for(int i=s;i<n;i++)\n7:\n8:#define rep(i,n) REP(i,0,n)\n9:\n10:\n11:\n12:using namespace std;\n13:\n14:\n15:\n16:const int IINF = INT_MAX;\n17:\n18:\n19:\n20:int H,W,mini,field[10][10],prev[10][10],mindist[10][10],nfield[10][10],mindist2[10][10],m1[10][10],m2[10][10];\n21:\n22:vector<int> sp[2];\n23:\n24:bool visited[10][10];\n25:\n26:int dx[] = {0,1,0,-1};\n27:\n28:int dy[] = {1,0,-1,0};\n29:\n30:bool X[10], Y[10];\n31:\n32:\n33:\n34:bool isValid(int x,int y) { return 0 <= x && x < W && 0 <= y && y < H; }\n35:\n36:\n37:\n38:void simulate_min(int dist[10][10],int f[10][10],int s){\n39:\n40:  rep(i,H) rep(j,W) dist[i][j] = IINF;\n41:\n42:  dist[s/W][s%W] = 0;\n43:\n44:  deque<int> deq;\n45:\n46:  deq.push_back(s);\n47:\n48:  while( !deq.empty() ){\n49:\n50:    int cur = deq.front(); deq.pop_front();\n51:\n52:    int x = cur % W, y = cur / W;\n53:\n54:    rep(i,4){\n55:\n56:      int nx = x + dx[i], ny = y + dy[i];\n57:\n58:      if( !isValid(nx,ny) ) continue;\n59:\n60:      if( f[ny][nx] != 0 && f[ny][nx] != f[s/W][s%W] ) continue;\n61:\n62:      if( dist[ny][nx] == IINF ) {\n63:\n64:        dist[ny][nx] = dist[y][x] + 1;\n65:\n66:        deq.push_back(nx+ny*W);\n67:\n68:      }\n69:\n70:    }\n71:\n72:  }\n73:\n74:}\n75:\n76:\n77:\n78:\n79:\n80:void dfs(int x,int y,int cost,int f[10][10],int ep,deque<int> &path,int F,int S,int prev){\n81:\n82:\n83:\n84:  if( x + y * W == ep ) {\n85:\n86:    rep(i,H) rep(j,W) nfield[i][j] = field[i][j];\n87:\n88:    rep(i,path.size()) nfield[path[i]/W][path[i]%W] = 1;\n89:\n90:    simulate_min(mindist2,nfield,sp[S][0]);\n91:\n92:    if( mindist2[sp[S][1]/W][sp[S][1]%W] == IINF ) return;\n93:\n94:    mini = min(mini,(int)path.size()-1+mindist2[sp[S][1]/W][sp[S][1]%W]);\n95:\n96:    return;\n97:\n98:  }\n99:\n100:  if( cost >= mini ) return;\n(11438 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open data/sample/codex_greedy_outputs.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf6fece",
   "metadata": {},
   "source": [
    "### Step 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f498088b",
   "metadata": {},
   "source": [
    "Execution Start time: 17:46:22, End time: 17:46:22 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ebb202",
   "metadata": {},
   "source": [
    "**Thought**: The `codex_greedy_outputs.jsonl` file appears to be in CSV format rather than JSONL, which is likely causing the error when attempting to read it as JSON. We need to convert this file into a proper JSONL format where each line is a valid JSON object.\n",
    "\n",
    "To proceed, let's write a script to convert this CSV file into a JSONL file. We'll then use the converted file for evaluation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dadc417",
   "metadata": {},
   "source": [
    "**Action (create)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28462339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "File 'convert_csv_to_jsonl.py' created.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create convert_csv_to_jsonl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a339b2c1",
   "metadata": {},
   "source": [
    "### Step 19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e8d3a0",
   "metadata": {},
   "source": [
    "Execution Start time: 17:46:26, End time: 17:46:26 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238903d2",
   "metadata": {},
   "source": [
    "**Thought**: Let's write a script in `convert_csv_to_jsonl.py` to read the CSV file and convert it into a JSONL format. We'll assume the CSV has a header and convert each row into a JSON object.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78f772c",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d14480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/commands/edit_linting.sh: line 64: $start_line: substring expression < 0\nError: CalledProcessError: Command 'b\"source /content/commands/_env_vars.sh && source /content/commands/defaults.sh && source /content/commands/search.sh && source /content/commands/edit_linting.sh && edit 0:0 << 'end_of_edit'\\nimport csv\\nimport json\\n\\n# Define the input and output file paths\\ncsv_file_path = 'data/sample/codex_greedy_outputs.jsonl'\\njsonl_file_path = 'data/sample/codex_greedy_outputs_converted.jsonl'\\n\\n# Open the CSV file and read its contents\\nwith open(csv_file_path, mode='r', newline='', encoding='utf-8') as csv_file:\\n    csv_reader = csv.DictReader(csv_file)\\n    \\n    # Open the JSONL file for writing\\n    with open(jsonl_file_path, mode='w', encoding='utf-8') as jsonl_file:\\n        for row in csv_reader:\\n            # Convert each row to a JSON object and write it to the JSONL file\\n            json.dump(row, jsonl_file)\\n            jsonl_file.write('\\\\n')\\nend_of_edit\\n\"' returned non-zero exit status 1.---------------------------------------------------------------------------CalledProcessError                        Traceback (most recent call last)Cell In[51], line 1\n----> 1 get_ipython().run_cell_magic('bash', '', \"source /content/commands/_env_vars.sh && source /content/commands/defaults.sh && source /content/commands/search.sh && source /content/commands/edit_linting.sh && edit 0:0 << 'end_of_edit'\\nimport csv\\nimport json\\n\\n# Define the input and output file paths\\ncsv_file_path = 'data/sample/codex_greedy_outputs.jsonl'\\njsonl_file_path = 'data/sample/codex_greedy_outputs_converted.jsonl'\\n\\n# Open the CSV file and read its contents\\nwith open(csv_file_path, mode='r', newline='', encoding='utf-8') as csv_file:\\n    csv_reader = csv.DictReader(csv_file)\\n    \\n    # Open the JSONL file for writing\\n    with open(jsonl_file_path, mode='w', encoding='utf-8') as jsonl_file:\\n        for row in csv_reader:\\n            # Convert each row to a JSON object and write it to the JSONL file\\n            json.dump(row, jsonl_file)\\n            jsonl_file.write('\\\\n')\\nend_of_edit\\n\")\nFile /usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2541, in InteractiveShell.run_cell_magic(self, magic_name, line, cell)\n   2539 with self.builtin_trap:\n   2540     args = (magic_arg_s, cell)\n-> 2541     result = fn(*args, **kwargs)\n   2543 # The code below prevents the output from being displayed\n   2544 # when using magics with decorator @output_can_be_silenced\n   2545 # when the last Python token in the expression is a ';'.\n   2546 if getattr(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, False):\nFile /usr/local/lib/python3.10/site-packages/IPython/core/magics/script.py:155, in ScriptMagics._make_script_magic.<locals>.named_script_magic(line, cell)\n    153 else:\n    154     line = script\n--> 155 return self.shebang(line, cell)\nFile /usr/local/lib/python3.10/site-packages/IPython/core/magics/script.py:315, in ScriptMagics.shebang(self, line, cell)\n    310 if args.raise_error and p.returncode != 0:\n    311     # If we get here and p.returncode is still None, we must have\n    312     # killed it but not yet seen its return code. We don't wait for it,\n    313     # in case it's stuck in uninterruptible sleep. -9 = SIGKILL\n    314     rc = p.returncode or -9\n--> 315     raise CalledProcessError(rc, cell)\nCalledProcessError: Command 'b\"source /content/commands/_env_vars.sh && source /content/commands/defaults.sh && source /content/commands/search.sh && source /content/commands/edit_linting.sh && edit 0:0 << 'end_of_edit'\\nimport csv\\nimport json\\n\\n# Define the input and output file paths\\ncsv_file_path = 'data/sample/codex_greedy_outputs.jsonl'\\njsonl_file_path = 'data/sample/codex_greedy_outputs_converted.jsonl'\\n\\n# Open the CSV file and read its contents\\nwith open(csv_file_path, mode='r', newline='', encoding='utf-8') as csv_file:\\n    csv_reader = csv.DictReader(csv_file)\\n    \\n    # Open the JSONL file for writing\\n    with open(jsonl_file_path, mode='w', encoding='utf-8') as jsonl_file:\\n        for row in csv_reader:\\n            # Convert each row to a JSON object and write it to the JSONL file\\n            json.dump(row, jsonl_file)\\n            jsonl_file.write('\\\\n')\\nend_of_edit\\n\"' returned non-zero exit status 1."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 0:0\n",
    "import csv\n",
    "import json\n",
    "\n",
    "# Define the input and output file paths\n",
    "csv_file_path = 'data/sample/codex_greedy_outputs.jsonl'\n",
    "jsonl_file_path = 'data/sample/codex_greedy_outputs_converted.jsonl'\n",
    "\n",
    "# Open the CSV file and read its contents\n",
    "with open(csv_file_path, mode='r', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    \n",
    "    # Open the JSONL file for writing\n",
    "    with open(jsonl_file_path, mode='w', encoding='utf-8') as jsonl_file:\n",
    "        for row in csv_reader:\n",
    "            # Convert each row to a JSON object and write it to the JSONL file\n",
    "            json.dump(row, jsonl_file)\n",
    "            jsonl_file.write('\\n')\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6e865c",
   "metadata": {},
   "source": [
    "### Step 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390f09d5",
   "metadata": {},
   "source": [
    "Execution Start time: 17:46:30, End time: 17:46:30 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee9f5ee",
   "metadata": {},
   "source": [
    "**Thought**: It seems there was an error with the edit command, possibly due to incorrect line numbers or a misconfiguration. Let's try creating the script again, ensuring that we specify the correct line numbers for the edit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7466c5",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc15e414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/commands/edit_linting.sh: line 64: $start_line: substring expression < 0\nError: CalledProcessError: Command 'b\"source /content/commands/_env_vars.sh && source /content/commands/defaults.sh && source /content/commands/search.sh && source /content/commands/edit_linting.sh && edit 0:0 << 'end_of_edit'\\nimport csv\\nimport json\\n\\n# Define the input and output file paths\\ncsv_file_path = 'data/sample/codex_greedy_outputs.jsonl'\\njsonl_file_path = 'data/sample/codex_greedy_outputs_converted.jsonl'\\n\\n# Open the CSV file and read its contents\\nwith open(csv_file_path, mode='r', newline='', encoding='utf-8') as csv_file:\\n    csv_reader = csv.DictReader(csv_file)\\n    \\n    # Open the JSONL file for writing\\n    with open(jsonl_file_path, mode='w', encoding='utf-8') as jsonl_file:\\n        for row in csv_reader:\\n            # Convert each row to a JSON object and write it to the JSONL file\\n            json.dump(row, jsonl_file)\\n            jsonl_file.write('\\\\n')\\nend_of_edit\\n\"' returned non-zero exit status 1.---------------------------------------------------------------------------CalledProcessError                        Traceback (most recent call last)Cell In[53], line 1\n----> 1 get_ipython().run_cell_magic('bash', '', \"source /content/commands/_env_vars.sh && source /content/commands/defaults.sh && source /content/commands/search.sh && source /content/commands/edit_linting.sh && edit 0:0 << 'end_of_edit'\\nimport csv\\nimport json\\n\\n# Define the input and output file paths\\ncsv_file_path = 'data/sample/codex_greedy_outputs.jsonl'\\njsonl_file_path = 'data/sample/codex_greedy_outputs_converted.jsonl'\\n\\n# Open the CSV file and read its contents\\nwith open(csv_file_path, mode='r', newline='', encoding='utf-8') as csv_file:\\n    csv_reader = csv.DictReader(csv_file)\\n    \\n    # Open the JSONL file for writing\\n    with open(jsonl_file_path, mode='w', encoding='utf-8') as jsonl_file:\\n        for row in csv_reader:\\n            # Convert each row to a JSON object and write it to the JSONL file\\n            json.dump(row, jsonl_file)\\n            jsonl_file.write('\\\\n')\\nend_of_edit\\n\")\nFile /usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2541, in InteractiveShell.run_cell_magic(self, magic_name, line, cell)\n   2539 with self.builtin_trap:\n   2540     args = (magic_arg_s, cell)\n-> 2541     result = fn(*args, **kwargs)\n   2543 # The code below prevents the output from being displayed\n   2544 # when using magics with decorator @output_can_be_silenced\n   2545 # when the last Python token in the expression is a ';'.\n   2546 if getattr(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, False):\nFile /usr/local/lib/python3.10/site-packages/IPython/core/magics/script.py:155, in ScriptMagics._make_script_magic.<locals>.named_script_magic(line, cell)\n    153 else:\n    154     line = script\n--> 155 return self.shebang(line, cell)\nFile /usr/local/lib/python3.10/site-packages/IPython/core/magics/script.py:315, in ScriptMagics.shebang(self, line, cell)\n    310 if args.raise_error and p.returncode != 0:\n    311     # If we get here and p.returncode is still None, we must have\n    312     # killed it but not yet seen its return code. We don't wait for it,\n    313     # in case it's stuck in uninterruptible sleep. -9 = SIGKILL\n    314     rc = p.returncode or -9\n--> 315     raise CalledProcessError(rc, cell)\nCalledProcessError: Command 'b\"source /content/commands/_env_vars.sh && source /content/commands/defaults.sh && source /content/commands/search.sh && source /content/commands/edit_linting.sh && edit 0:0 << 'end_of_edit'\\nimport csv\\nimport json\\n\\n# Define the input and output file paths\\ncsv_file_path = 'data/sample/codex_greedy_outputs.jsonl'\\njsonl_file_path = 'data/sample/codex_greedy_outputs_converted.jsonl'\\n\\n# Open the CSV file and read its contents\\nwith open(csv_file_path, mode='r', newline='', encoding='utf-8') as csv_file:\\n    csv_reader = csv.DictReader(csv_file)\\n    \\n    # Open the JSONL file for writing\\n    with open(jsonl_file_path, mode='w', encoding='utf-8') as jsonl_file:\\n        for row in csv_reader:\\n            # Convert each row to a JSON object and write it to the JSONL file\\n            json.dump(row, jsonl_file)\\n            jsonl_file.write('\\\\n')\\nend_of_edit\\n\"' returned non-zero exit status 1."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 0:0\n",
    "import csv\n",
    "import json\n",
    "\n",
    "# Define the input and output file paths\n",
    "csv_file_path = 'data/sample/codex_greedy_outputs.jsonl'\n",
    "jsonl_file_path = 'data/sample/codex_greedy_outputs_converted.jsonl'\n",
    "\n",
    "# Open the CSV file and read its contents\n",
    "with open(csv_file_path, mode='r', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    \n",
    "    # Open the JSONL file for writing\n",
    "    with open(jsonl_file_path, mode='w', encoding='utf-8') as jsonl_file:\n",
    "        for row in csv_reader:\n",
    "            # Convert each row to a JSON object and write it to the JSONL file\n",
    "            json.dump(row, jsonl_file)\n",
    "            jsonl_file.write('\\n')\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49b57ab",
   "metadata": {},
   "source": [
    "### Step 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0917bea4",
   "metadata": {},
   "source": [
    "Execution Start time: 17:46:32, End time: 17:46:32 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58888f0e",
   "metadata": {},
   "source": [
    "**Thought**: It seems there was an issue with the line numbers specified for the edit command. Let's try creating the script again, ensuring that we specify the correct line numbers for the edit. This time, I'll ensure the file is open before editing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1a7f9c",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06e37b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/pie-perf/convert_csv_to_jsonl.py (1 lines total)]\n1:\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open convert_csv_to_jsonl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2158ae1a",
   "metadata": {},
   "source": [
    "### Step 22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893b3b88",
   "metadata": {},
   "source": [
    "Execution Start time: 17:46:37, End time: 17:46:37 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e1404c",
   "metadata": {},
   "source": [
    "**Thought**: The `convert_csv_to_jsonl.py` file is currently empty. Let's add the script to convert the CSV file to JSONL format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcce8cb",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f10f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/pie-perf/convert_csv_to_jsonl.py (17 lines total)]\n1:import csv\n2:import json\n3:\n4:# Define the input and output file paths\n5:csv_file_path = 'data/sample/codex_greedy_outputs.jsonl'\n6:jsonl_file_path = 'data/sample/codex_greedy_outputs_converted.jsonl'\n7:\n8:# Open the CSV file and read its contents\n9:with open(csv_file_path, mode='r', newline='', encoding='utf-8') as csv_file:\n10:    csv_reader = csv.DictReader(csv_file)\n11:    \n12:    # Open the JSONL file for writing\n13:    with open(jsonl_file_path, mode='w', encoding='utf-8') as jsonl_file:\n14:        for row in csv_reader:\n15:            # Convert each row to a JSON object and write it to the JSONL file\n16:            json.dump(row, jsonl_file)\n17:            jsonl_file.write('\\n')\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 1:1\n",
    "import csv\n",
    "import json\n",
    "\n",
    "# Define the input and output file paths\n",
    "csv_file_path = 'data/sample/codex_greedy_outputs.jsonl'\n",
    "jsonl_file_path = 'data/sample/codex_greedy_outputs_converted.jsonl'\n",
    "\n",
    "# Open the CSV file and read its contents\n",
    "with open(csv_file_path, mode='r', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    \n",
    "    # Open the JSONL file for writing\n",
    "    with open(jsonl_file_path, mode='w', encoding='utf-8') as jsonl_file:\n",
    "        for row in csv_reader:\n",
    "            # Convert each row to a JSON object and write it to the JSONL file\n",
    "            json.dump(row, jsonl_file)\n",
    "            jsonl_file.write('\\n')\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03877c5f",
   "metadata": {},
   "source": [
    "### Step 23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec8004e",
   "metadata": {},
   "source": [
    "Execution Start time: 17:46:39, End time: 17:46:39 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c0fa6f",
   "metadata": {},
   "source": [
    "**Thought**: The script to convert the CSV file to JSONL format has been successfully added. Now, let's execute this script to perform the conversion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fa9503",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a9e802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python convert_csv_to_jsonl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641892b8",
   "metadata": {},
   "source": [
    "### Step 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19255a4f",
   "metadata": {},
   "source": [
    "Execution Start time: 17:46:42, End time: 17:46:43 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca91a5aa",
   "metadata": {},
   "source": [
    "**Thought**: The script executed successfully, indicating that the CSV file has been converted to JSONL format. The converted file should now be located at `data/sample/codex_greedy_outputs_converted.jsonl`.\n",
    "\n",
    "Let's proceed by running the `run_eval.py` script using the converted JSONL file for the model-generated outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fc1939",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33127ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3653, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 147, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 176, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'input'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/content/pie-perf/src/codenet_eval/run_eval.py\", line 484, in <module>\n    evaluate_generated_outputs(evaluation_config)\n  File \"/content/pie-perf/src/codenet_eval/run_eval.py\", line 43, in evaluate_generated_outputs\n    merged = read_inputs_and_prepare(cfg)\n  File \"/content/pie-perf/src/codenet_eval/run_eval.py\", line 114, in read_inputs_and_prepare\n    gen_df[\"slower_program\"] = gen_df[cfg.slow_code_col].apply(lambda x: x.strip())\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/frame.py\", line 3761, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3655, in get_loc\n    raise KeyError(key) from err\nKeyError: 'input'\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!PYTHONPATH=$(pwd) python src/codenet_eval/run_eval.py --model_generated_outputs_path data/sample/codex_greedy_outputs_converted.jsonl --output_report_file_path data/sample/codex_greedy_outputs_converted.jsonl.report --num_trials 2 --num_problems_to_evaluate 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fc06f2",
   "metadata": {},
   "source": [
    "### Step 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8facd8",
   "metadata": {},
   "source": [
    "Execution Start time: 17:46:45, End time: 17:46:49 (duration: 3 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e68cf2e",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the script is trying to access a column named `'input'`, which does not exist in the converted JSONL file. This suggests that the column names in the JSONL file might not match the expected column names in the script.\n",
    "\n",
    "Let's open the first few lines of the converted JSONL file to inspect the column names and verify their correctness.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c067d41",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e570d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"user_id\": \"u116766943\", \"problem_id\": \"p00849\", \"language\": \"C++\", \"submission_id_v0\": \"s624998539\", \"submission_id_v1\": \"s546809015\", \"cpu_time_v0\": \"9370.0\", \"cpu_time_v1\": \"9250.0\", \"memory_v0\": \"1232.0\", \"memory_v1\": \"1228.0\", \"status_v0\": \"Time Limit Exceeded\", \"status_v1\": \"Time Limit Exceeded\", \"improvement_frac\": \"1.28\", \"code_v0\": \"#include<bits/stdc++.h>\\n\\n\\n\\n#define REP(i,s,n) for(int i=s;i<n;i++)\\n\\n#define rep(i,n) REP(i,0,n)\\n\\n\\n\\nusing namespace std;\\n\\n\\n\\nconst int IINF = INT_MAX;\\n\\n\\n\\nint H,W,mini,field[10][10],prev[10][10],mindist[10][10],nfield[10][10],mindist2[10][10],m1[10][10],m2[10][10];\\n\\nvector<int> sp[2];\\n\\nbool visited[10][10];\\n\\nint dx[] = {0,1,0,-1};\\n\\nint dy[] = {1,0,-1,0};\\n\\nbool X[10], Y[10];\\n\\n\\n\\nbool isValid(int x,int y) { return 0 <= x && x < W && 0 <= y && y < H; }\\n\\n\\n\\nvoid simulate_min(int dist[10][10],int f[10][10],int s){\\n\\n  rep(i,H) rep(j,W) dist[i][j] = IINF;\\n\\n  dist[s/W][s%W] = 0;\\n\\n  deque<int> deq;\\n\\n  deq.push_back(s);\\n\\n  while( !deq.empty() ){\\n\\n    int cur = deq.front(); deq.pop_front();\\n\\n    int x = cur % W, y = cur / W;\\n\\n    rep(i,4){\\n\\n      int nx = x + dx[i], ny = y + dy[i];\\n\\n      if( !isValid(nx,ny) ) continue;\\n\\n      if( f[ny][nx] != 0 && f[ny][nx] != f[s/W][s%W] ) continue;\\n\\n      if( dist[ny][nx] == IINF ) {\\n\\n        dist[ny][nx] = dist[y][x] + 1;\\n\\n        deq.push_back(nx+ny*W);\\n\\n      }\\n\\n    }\\n\\n  }\\n\\n}\\n\\n\\n\\n\\n\\nvoid dfs(int x,int y,int cost,int f[10][10],int ep,deque<int> &path,int F,int S,int prev){\\n\\n\\n\\n  if( x + y * W == ep ) {\\n\\n    rep(i,H) rep(j,W) nfield[i][j] = field[i][j];\\n\\n    rep(i,path.size()) nfield[path[i]/W][path[i]%W] = 1;\\n\\n    simulate_min(mindist2,nfield,sp[S][0]);\\n\\n    if( mindist2[sp[S][1]/W][sp[S][1]%W] == IINF ) return;\\n\\n    mini = min(mini,(int)path.size()-1+mindist2[sp[S][1]/W][sp[S][1]%W]);\\n\\n    return;\\n\\n  }\\n\\n  if( cost >= mini ) return;\\n\\n  if( m2[sp[S][1]/W][sp[S][1]%W] + cost >= mini ) return;\\n\\n\\n\\n  int cnt = 0;\\n\\n  rep(i,4) {\\n\\n    int nx = x + dx[i], ny = y + dy[i];\\n\\n    if( !isValid(nx,ny) ) continue;\\n\\n    cnt += visited[ny][nx];\\n\\n }\\n\\n  if( cnt >= 2 ) return;\\n\\n\\n\\n  {\\n\\n    rep(i,4){\\n\\n      int nx = x, ny = y;\\n\\n      nx += dx[i], ny += dy[i];\\n\\n      if( !isValid(nx,ny) ) continue;\\n\\n      if( f[ny][nx] != 0 && f[ny][nx] != f[ep/W][ep%W] ) continue;\\n\\n      if( visited[ny][nx] ) continue;\\n\\n      while(1){\\n\\n        nx += dx[i], ny += dy[i];\\n\\n        if( !isValid(nx,ny) ) break;\\n\\n        if( f[ny][nx] != 0 && f[ny][nx] != f[ep/W][ep%W] ) break;\\n\\n        if( X[nx] || Y[ny] ) break;\\n\\n        if( visited[ny][nx] ) return;\\n\\n      }\\n\\n    }\\n\\n  }\\n\\n\\n\\n\\n\\n  rep(i,4) {\\n\\n    if( i == (prev+2) % 4 ) continue;\\n\\n    int nx = x + dx[i], ny = y + dy[i];\\n\\n    if( !isValid(nx,ny) ) continue;\\n\\n    if( f[ny][nx] != 0 && f[ny][nx] != f[ep/W][ep%W] ) continue;\\n\\n    if( visited[ny][nx] ) continue;\\n\\n    visited[ny][nx] = true;\\n\\n    path.push_back(nx+ny*W);\\n\\n    dfs(nx,ny,cost+1,f,ep,path,F,S,i);\\n\\n    path.pop_back();\\n\\n    visited[ny][nx] = false;\\n\\n  }\\n\\n\\n\\n\\n\\n}\\n\\n\\n\\nint compute(int F,int S){\\n\\n  rep(i,H) rep(j,W) visited[i][j] = false;\\n\\n  deque<int> path;\\n\\n  path.push_back(sp[F][0]);\\n\\n  visited[sp[F][0]/W][sp[F][0]%W] = true;\\n\\n  simulate_min(m1,field,sp[F][0]);\\n\\n  simulate_min(m2,field,sp[S][0]);\\n\\n  dfs(sp[F][0]%W,sp[F][0]/W,0,field,sp[F][1],path,F,S,-IINF);\\n\\n}\\n\\n\\n\\nint main(){\\n\\n  while( cin >> H >> W, H|W ){\\n\\n    rep(i,2) sp[i].clear();\\n\\n    rep(i,10) X[i] = Y[i] = false;\\n\\n    rep(i,H) rep(j,W) {\\n\\n      cin >> field[i][j];\\n\\n      if( field[i][j] == 1 ) X[j] = true, Y[i] = true;\\n\\n      if( field[i][j] == 2 ) sp[0].push_back(j+i*W), X[j] = Y[i] = true;\\n\\n      if( field[i][j] == 3 ) sp[1].push_back(j+i*W), X[j] = Y[i] = true;\\n\\n    }\\n\\n    mini = IINF;\\n\\n\\n\\n    compute(0,1);\\n\\n    if( mini == IINF ) puts(\\\"0\\\");\\n\\n    else cout << mini << endl;\\n\\n  }\\n\\n  return 0;\\n\\n}\", \"code_v1\": \"#include<bits/stdc++.h>\\n\\n\\n\\n#define REP(i,s,n) for(int i=s;i<n;i++)\\n\\n#define rep(i,n) REP(i,0,n)\\n\\n\\n\\nusing namespace std;\\n\\n\\n\\nconst int IINF = INT_MAX;\\n\\n\\n\\nint H,W,mini,field[10][10],prev[10][10],mindist[10][10],nfield[10][10],mindist2[10][10],m1[10][10],m2[10][10];\\n\\nvector<int> sp[2];\\n\\nbool visited[10][10];\\n\\nint dx[] = {0,1,0,-1};\\n\\nint dy[] = {1,0,-1,0};\\n\\nbool X[10], Y[10];\\n\\n\\n\\nbool isValid(int x,int y) { return 0 <= x && x < W && 0 <= y && y < H; }\\n\\n\\n\\nvoid simulate_min(int dist[10][10],int f[10][10],int s){\\n\\n  rep(i,H) rep(j,W) dist[i][j] = IINF;\\n\\n  dist[s/W][s%W] = 0;\\n\\n  deque<int> deq;\\n\\n  deq.push_back(s);\\n\\n  while( !deq.empty() ){\\n\\n    int cur = deq.front(); deq.pop_front();\\n\\n    int x = cur % W, y = cur / W;\\n\\n    rep(i,4){\\n\\n      int nx = x + dx[i], ny = y + dy[i];\\n\\n      if( !isValid(nx,ny) ) continue;\\n\\n      if( f[ny][nx] != 0 && f[ny][nx] != f[s/W][s%W] ) continue;\\n\\n      if( dist[ny][nx] == IINF ) {\\n\\n        dist[ny][nx] = dist[y][x] + 1;\\n\\n        deq.push_back(nx+ny*W);\\n\\n      }\\n\\n    }\\n\\n  }\\n\\n}\\n\\n\\n\\n\\n\\nvoid dfs(int x,int y,int cost,int f[10][10],int ep,deque<int> &path,int F,int S,int prev){\\n\\n\\n\\n  if( cost >= mini ) return;\\n\\n  if( m2[sp[S][1]/W][sp[S][1]%W] + cost >= mini ) return;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  if( x + y * W == ep ) {\\n\\n    rep(i,H) rep(j,W) nfield[i][j] = field[i][j];\\n\\n    rep(i,path.size()) nfield[path[i]/W][path[i]%W] = 1;\\n\\n    simulate_min(mindist2,nfield,sp[S][0]);\\n\\n    if( mindist2[sp[S][1]/W][sp[S][1]%W] == IINF ) return;\\n\\n    mini = min(mini,(int)path.size()-1+mindist2[sp[S][1]/W][sp[S][1]%W]);\\n\\n    return;\\n\\n  }\\n\\n  int cnt = 0;\\n\\n  rep(i,4) {\\n\\n    int nx = x + dx[i], ny = y + dy[i];\\n\\n    if( !isValid(nx,ny) ) continue;\\n\\n    cnt += visited[ny][nx];\\n\\n }\\n\\n  if( cnt >= 2 ) return;\\n\\n\\n\\n\\n\\n  {\\n\\n    rep(i,4){\\n\\n      int nx = x, ny = y;\\n\\n      nx += dx[i], ny += dy[i];\\n\\n      if( !isValid(nx,ny) ) continue;\\n\\n      if( f[ny][nx] != 0 && f[ny][nx] != f[ep/W][ep%W] ) continue;\\n\\n      if( visited[ny][nx] ) continue;\\n\\n      while(1){\\n\\n        nx += dx[i], ny += dy[i];\\n\\n        if( !isValid(nx,ny) ) break;\\n\\n        if( f[ny][nx] != 0 && f[ny][nx] != f[ep/W][ep%W] ) break;\\n\\n        if( X[nx] || Y[ny] ) break;\\n\\n        if( visited[ny][nx] ) return;\\n\\n      }\\n\\n    }\\n\\n  }\\n\\n\\n\\n\\n\\n  rep(i,4) {\\n\\n    if( i == (prev+2) % 4 ) continue;\\n\\n    int nx = x + dx[i], ny = y + dy[i];\\n\\n    if( !isValid(nx,ny) ) continue;\\n\\n    if( f[ny][nx] != 0 && f[ny][nx] != f[ep/W][ep%W] ) continue;\\n\\n    if( visited[ny][nx] ) continue;\\n\\n    visited[ny][nx] = true;\\n\\n    path.push_back(nx+ny*W);\\n\\n    dfs(nx,ny,cost+1,f,ep,path,F,S,i);\\n\\n    path.pop_back();\\n\\n    visited[ny][nx] = false;\\n\\n  }\\n\\n\\n\\n\\n\\n}\\n\\n\\n\\nint compute(int F,int S){\\n\\n  rep(i,H) rep(j,W) visited[i][j] = false;\\n\\n  deque<int> path;\\n\\n  path.push_back(sp[F][0]);\\n\\n  visited[sp[F][0]/W][sp[F][0]%W] = true;\\n\\n  simulate_min(m1,field,sp[F][0]);\\n\\n  simulate_min(m2,field,sp[S][0]);\\n\\n  dfs(sp[F][0]%W,sp[F][0]/W,0,field,sp[F][1],path,F,S,-IINF);\\n\\n}\\n\\n\\n\\nint main(){\\n\\n  while( cin >> H >> W, H|W ){\\n\\n    rep(i,2) sp[i].clear();\\n\\n    rep(i,10) X[i] = Y[i] = false;\\n\\n    rep(i,H) rep(j,W) {\\n\\n      cin >> field[i][j];\\n\\n      if( field[i][j] == 1 ) X[j] = true, Y[i] = true;\\n\\n      if( field[i][j] == 2 ) sp[0].push_back(j+i*W), X[j] = Y[i] = true;\\n\\n      if( field[i][j] == 3 ) sp[1].push_back(j+i*W), X[j] = Y[i] = true;\\n\\n    }\\n\\n    mini = IINF;\\n\\n\\n\\n    compute(0,1);\\n\\n    if( mini == IINF ) puts(\\\"0\\\");\\n\\n    else cout << mini << endl;\\n\\n  }\\n\\n  return 0;\\n\\n}\", \"code_v0_loc\": \"122.0\", \"code_v1_loc\": \"127.0\"}\n{\"user_id\": \"u116766943\", \"problem_id\": \"p00849\", \"language\": \"C++\", \"submission_id_v0\": \"s546809015\", \"submission_id_v1\": \"s829771916\", \"cpu_time_v0\": \"9250.0\", \"cpu_time_v1\": \"8780.0\", \"memory_v0\": \"1228.0\", \"memory_v1\": \"1228.0\", \"status_v0\": \"Time Limit Exceeded\", \"status_v1\": \"Time Limit Exceeded\", \"improvement_frac\": \"5.08\", \"code_v0\": \"#include<bits/stdc++.h>\\n\\n\\n\\n#define REP(i,s,n) for(int i=s;i<n;i++)\\n\\n#define rep(i,n) REP(i,0,n)\\n\\n\\n\\nusing namespace std;\\n\\n\\n\\nconst int IINF = INT_MAX;\\n\\n\\n\\nint H,W,mini,field[10][10],prev[10][10],mindist[10][10],nfield[10][10],mindist2[10][10],m1[10][10],m2[10][10];\\n\\nvector<int> sp[2];\\n\\nbool visited[10][10];\\n\\nint dx[] = {0,1,0,-1};\\n\\nint dy[] = {1,0,-1,0};\\n\\nbool X[10], Y[10];\\n\\n\\n\\nbool isValid(int x,int y) { return 0 <= x && x < W && 0 <= y && y < H; }\\n\\n\\n\\nvoid simulate_min(int dist[10][10],int f[10][10],int s){\\n\\n  rep(i,H) rep(j,W) dist[i][j] = IINF;\\n\\n  dist[s/W][s%W] = 0;\\n\\n  deque<int> deq;\\n\\n  deq.push_back(s);\\n\\n  while( !deq.empty() ){\\n\\n    int cur = deq.front(); deq.pop_front();\\n\\n    int x = cur % W, y = cur / W;\\n\\n    rep(i,4){\\n\\n      int nx = x + dx[i], ny = y + dy[i];\\n\\n      if( !isValid(nx,ny) ) continue;\\n\\n      if( f[ny][nx] != 0 && f[ny][nx] != f[s/W][s%W] ) continue;\\n\\n      if( dist[ny][nx] == IINF ) {\\n\\n        dist[ny][nx] = dist[y][x] + 1;\\n\\n        deq.push_back(nx+ny*W);\\n\\n      }\\n\\n    }\\n\\n  }\\n\\n}\\n\\n\\n\\n\\n\\nvoid dfs(int x,int y,int cost,int f[10][10],int ep,deque<int> &path,int F,int S,int prev){\\n\\n\\n\\n  if( cost >= mini ) return;\\n\\n  if( m2[sp[S][1]/W][sp[S][1]%W] + cost >= mini ) return;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  if( x + y * W == ep ) {\\n\\n    rep(i,H) rep(j,W) nfield[i][j] = field[i][j];\\n\\n    rep(i,path.size()) nfield[path[i]/W][path[i]%W] = 1;\\n\\n    simulate_min(mindist2,nfield,sp[S][0]);\\n\\n    if( mindist2[sp[S][1]/W][sp[S][1]%W] == IINF ) return;\\n\\n    mini = min(mini,(int)path.size()-1+mindist2[sp[S][1]/W][sp[S][1]%W]);\\n\\n    return;\\n\\n  }\\n\\n  int cnt = 0;\\n\\n  rep(i,4) {\\n\\n    int nx = x + dx[i], ny = y + dy[i];\\n\\n    if( !isValid(nx,ny) ) continue;\\n\\n    cnt += visited[ny][nx];\\n\\n }\\n\\n  if( cnt >= 2 ) return;\\n\\n\\n\\n\\n\\n  {\\n\\n    rep(i,4){\\n\\n      int nx = x, ny = y;\\n\\n      nx += dx[i], ny += dy[i];\\n\\n      if( !isValid(nx,ny) ) continue;\\n\\n      if( f[ny][nx] != 0 && f[ny][nx] != f[ep/W][ep%W] ) continue;\\n\\n      if( visited[ny][nx] ) continue;\\n\\n      while(1){\\n\\n        nx += dx[i], ny += dy[i];\\n\\n        if( !isValid(nx,ny) ) break;\\n\\n        if( f[ny][nx] != 0 && f[ny][nx] != f[ep/W][ep%W] ) break;\\n\\n        if( X[nx] || Y[ny] ) break;\\n\\n        if( visited[ny][nx] ) return;\\n\\n      }\\n\\n    }\\n\\n  }\\n\\n\\n\\n\\n\\n  rep(i,4) {\\n\\n    if( i == (prev+2) % 4 ) continue;\\n\\n    int nx = x + dx[i], ny = y + dy[i];\\n\\n    if( !isValid(nx,ny) ) continue;\\n\\n    if( f[ny][nx] != 0 && f[ny][nx] != f[ep/W][ep%W] ) continue;\\n\\n    if( visited[ny][nx] ) continue;\\n\\n    visited[ny][nx] = true;\\n\\n    path.push_back(nx+ny*W);\\n\\n    dfs(nx,ny,cost+1,f,ep,path,F,S,i);\\n\\n    path.pop_back();\\n\\n    visited[ny][nx] = false;\\n\\n  }\\n\\n\\n\\n\\n\\n}\\n\\n\\n\\nint compute(int F,int S){\\n\\n  rep(i,H) rep(j,W) visited[i][j] = false;\\n\\n  deque<int> path;\\n\\n  path.push_back(sp[F][0]);\\n\\n  visited[sp[F][0]/W][sp[F][0]%W] = true;\\n\\n  simulate_min(m1,field,sp[F][0]);\\n\\n  simulate_min(m2,field,sp[S][0]);\\n\\n  dfs(sp[F][0]%W,sp[F][0]/W,0,field,sp[F][1],path,F,S,-IINF);\\n\\n}\\n\\n\\n\\nint main(){\\n\\n  while( cin >> H >> W, H|W ){\\n\\n    rep(i,2) sp[i].clear();\\n\\n    rep(i,10) X[i] = Y[i] = false;\\n\\n    rep(i,H) rep(j,W) {\\n\\n      cin >> field[i][j];\\n\\n      if( field[i][j] == 1 ) X[j] = true, Y[i] = true;\\n\\n      if( field[i][j] == 2 ) sp[0].push_back(j+i*W), X[j] = Y[i] = true;\\n\\n      if( field[i][j] == 3 ) sp[1].push_back(j+i*W), X[j] = Y[i] = true;\\n\\n    }\\n\\n    mini = IINF;\\n\\n\\n\\n    compute(0,1);\\n\\n    if( mini == IINF ) puts(\\\"0\\\");\\n\\n    else cout << mini << endl;\\n\\n  }\\n\\n  return 0;\\n\\n}\", \"code_v1\": \"#include<bits/stdc++.h>\\n\\n\\n\\n#define REP(i,s,n) for(int i=s;i<n;i++)\\n\\n#define rep(i,n) REP(i,0,n)\\n\\n\\n\\nusing namespace std;\\n\\n\\n\\nconst int IINF = INT_MAX;\\n\\n\\n\\nint H,W,mini,field[10][10],prev[10][10],mindist[10][10],nfield[10][10],mindist2[10][10],m1[10][10],m2[10][10];\\n\\nvector<int> sp[2];\\n\\nbool visited[10][10];\\n\\nint dx[] = {0,1,0,-1};\\n\\nint dy[] = {1,0,-1,0};\\n\\nbool X[10], Y[10];\\n\\n\\n\\nbool isValid(int x,int y) { return 0 <= x && x < W && 0 <= y && y < H; }\\n\\n\\n\\nvoid simulate_min(int dist[10][10],int f[10][10],int s){\\n\\n  rep(i,H) rep(j,W) dist[i][j] = IINF;\\n\\n  dist[s/W][s%W] = 0;\\n\\n  deque<int> deq;\\n\\n  deq.push_back(s);\\n\\n  while( !deq.empty() ){\\n\\n    int cur = deq.front(); deq.pop_front();\\n\\n    int x = cur % W, y = cur / W;\\n\\n    if( x+y*W != s && f[y][x] == f[s/W][s%W] ) return;\\n\\n    rep(i,4){\\n\\n      int nx = x + dx[i], ny = y + dy[i];\\n\\n      if( !isValid(nx,ny) ) continue;\\n\\n      if( f[ny][nx] != 0 && f[ny][nx] != f[s/W][s%W] ) continue;\\n\\n      if( dist[ny][nx] == IINF ) {\\n\\n        dist[ny][nx] = dist[y][x] + 1;\\n\\n        deq.push_back(nx+ny*W);\\n\\n      }\\n\\n    }\\n\\n  }\\n\\n}\\n\\n\\n\\n\\n\\nvoid dfs(int x,int y,int cost,int f[10][10],int ep,deque<int> &path,int F,int S,int prev){\\n\\n\\n\\n  if( cost >= mini ) return;\\n\\n  if( m1[ep/W][ep%W] - m1[y][x] + m2[sp[S][1]/W][sp[S][1]%W] + cost >= mini ) return;\\n\\n\\n\\n\\n\\n  if( x + y * W == ep ) {\\n\\n    rep(i,H) rep(j,W) nfield[i][j] = field[i][j];\\n\\n    rep(i,path.size()) nfield[path[i]/W][path[i]%W] = 1;\\n\\n    simulate_min(mindist2,nfield,sp[S][0]);\\n\\n    if( mindist2[sp[S][1]/W][sp[S][1]%W] == IINF ) return;\\n\\n    mini = min(mini,(int)path.size()-1+mindist2[sp[S][1]/W][sp[S][1]%W]);\\n\\n    return;\\n\\n  }\\n\\n  int cnt = 0;\\n\\n  rep(i,4) {\\n\\n    int nx = x + dx[i], ny = y + dy[i];\\n\\n    if( !isValid(nx,ny) ) continue;\\n\\n    cnt += visited[ny][nx];\\n\\n }\\n\\n  if( cnt >= 2 ) return;\\n\\n\\n\\n\\n\\n  {\\n\\n    rep(i,4){\\n\\n      int nx = x, ny = y;\\n\\n      nx += dx[i], ny += dy[i];\\n\\n      if( !isValid(nx,ny) ) continue;\\n\\n      if( f[ny][nx] != 0 && f[ny][nx] != f[ep/W][ep%W] ) continue;\\n\\n      if( visited[ny][nx] ) continue;\\n\\n      while(1){\\n\\n        nx += dx[i], ny += dy[i];\\n\\n        if( !isValid(nx,ny) ) break;\\n\\n        if( f[ny][nx] != 0 && f[ny][nx] != f[ep/W][ep%W] ) break;\\n\\n        if( X[nx] || Y[ny] ) break;\\n\\n        if( visited[ny][nx] ) return;\\n\\n      }\\n\\n    }\\n\\n  }\\n\\n\\n\\n\\n\\n  rep(i,4) {\\n\\n    if( i == (prev+2) % 4 ) continue;\\n\\n    int nx = x + dx[i], ny = y + dy[i];\\n\\n    if( !isValid(nx,ny) ) continue;\\n\\n    if( f[ny][nx] != 0 && f[ny][nx] != f[ep/W][ep%W] ) continue;\\n\\n    if( visited[ny][nx] ) continue;\\n\\n    visited[ny][nx] = true;\\n\\n    path.push_back(nx+ny*W);\\n\\n    dfs(nx,ny,cost+1,f,ep,path,F,S,i);\\n\\n    path.pop_back();\\n\\n    visited[ny][nx] = false;\\n\\n  }\\n\\n\\n\\n\\n\\n}\\n\\n\\n\\nint compute(int F,int S){\\n\\n  rep(i,H) rep(j,W) visited[i][j] = false;\\n\\n  deque<int> path;\\n\\n  path.push_back(sp[F][0]);\\n\\n  visited[sp[F][0]/W][sp[F][0]%W] = true;\\n\\n  simulate_min(m1,field,sp[F][0]);\\n\\n  simulate_min(m2,field,sp[S][0]);\\n\\n  dfs(sp[F][0]%W,sp[F][0]/W,0,field,sp[F][1],path,F,S,-IINF);\\n\\n}\\n\\n\\n\\nint main(){\\n\\n  while( cin >> H >> W, H|W ){\\n\\n    rep(i,2) sp[i].clear();\\n\\n    rep(i,10) X[i] = Y[i] = false;\\n\\n    rep(i,H) rep(j,W) {\\n\\n      cin >> field[i][j];\\n\\n      if( field[i][j] == 1 ) X[j] = true, Y[i] = true;\\n\\n      if( field[i][j] == 2 ) sp[0].push_back(j+i*W), X[j] = Y[i] = true;\\n\\n      if( field[i][j] == 3 ) sp[1].push_back(j+i*W), X[j] = Y[i] = true;\\n\\n    }\\n\\n    mini = IINF;\\n\\n\\n\\n    compute(0,1);\\n\\n    if( mini == IINF ) puts(\\\"0\\\");\\n\\n    else cout << mini << endl;\\n\\n  }\\n\\n  return 0;\\n\\n}\", \"code_v0_loc\": \"127.0\", \"code_v1_loc\": \"125.0\"}\n{\"user_id\": \"u116766943\", \"problem_id\": \"p00849\", \"language\": \"C++\", \"submission_id_v0\": \"s812631681\", \"submission_id_v1\": \"s154463767\", \"cpu_time_v0\": \"8700.0\", \"cpu_time_v1\": \"8030.0\", \"memory_v0\": \"1232.0\", \"memory_v1\": \"1232.0\", \"status_v0\": \"Time Limit Exceeded\", \"status_v1\": \"Time Limit Exceeded\", \"improvement_frac\": \"7.7\", \"code_v0\": \"#include<bits/stdc++.h>\\n\\n\\n\\n#define REP(i,s,n) for(int i=s;i<n;i++)\\n\\n#define rep(i,n) REP(i,0,n)\\n\\n\\n\\nusing namespace std;\\n\\n\\n\\nconst int IINF = INT_MAX;\\n\\n\\n\\nint H,W,mini,field[10][10],prev[10][10],mindist[10][10],nfield[10][10],mindist2[10][10],m1[10][10],m2[10][10];\\n\\nvector<int> sp[2];\\n\\nbool visited[10][10];\\n\\nint dx[] = {0,1,0,-1};\\n\\nint dy[] = {1,0,-1,0};\\n\\nbool X[10], Y[10];\\n\\n\\n\\nbool isValid(int x,int y) { return 0 <= x && x < W && 0 <= y && y < H; }\\n\\n\\n\\nvoid simulate_min(int dist[10][10],int f[10][10],int s){\\n\\n  rep(i,H) rep(j,W) dist[i][j] = IINF;\\n\\n  dist[s/W][s%W] = 0;\\n\\n  deque<int> deq;\\n\\n  deq.push_back(s);\\n\\n  while( !deq.empty() ){\\n\\n    int cur = deq.front(); deq.pop_front();\\n\\n    int x = cur % W, y = cur / W;\\n\\n    if( x+y*W != s && f[y][x] == f[s/W][s%W] ) return;\\n\\n    rep(i,4){\\n\\n      int nx = x + dx[i], ny = y + dy[i];\\n\\n      if( !isValid(nx,ny) ) continue;\\n\\n      if( f[ny][nx] != 0 && f[ny][nx] != f[s/W][s%W] ) continue;\\n\\n      if( dist[ny][nx] == IINF ) {\\n\\n        dist[ny][nx] = dist[y][x] + 1;\\n\\n        deq.push_back(nx+ny*W);\\n\\n      }\\n\\n    }\\n\\n  }\\n\\n}\\n\\n\\n\\n\\n\\nvoid dfs(int x,int y,int cost,int f[10][10],int ep,deque<int> &path,int F,int S,int prev){\\n\\n\\n\\n  if( cost >= mini ) return;\\n\\n  if( m1[ep/W][ep%W] - m1[y][x] + m2[sp[S][1]/W][sp[S][1]%W] + cost >= mini ) return;\\n\\n\\n\\n\\n\\n  if( x + y * W == ep ) {\\n\\n    rep(i,H) rep(j,W) nfield[i][j] = field[i][j];\\n\\n    rep(i,path.size()) nfield[path[i]/W][path[i]%W] = 1;\\n\\n    simulate_min(mindist2,nfield,sp[S][0]);\\n\\n    if( mindist2[sp[S][1]/W][sp[S][1]%W] == IINF ) return;\\n\\n    mini = min(mini,(int)path.size()-1+mindist2[sp[S][1]/W][sp[S][1]%W]);\\n\\n    return;\\n\\n  }\\n\\n  int cnt = 0;\\n\\n  rep(i,4) {\\n\\n    int nx = x + dx[i], ny = y + dy[i];\\n\\n    if( !isValid(nx,ny) ) continue;\\n\\n    cnt += visited[ny][nx];\\n\\n }\\n\\n  if( cnt >= 2 ) return;\\n\\n\\n\\n\\n\\n  {\\n\\n    rep(i,4){\\n\\n      if( i == (prev+2) % 4 ) continue;\\n\\n      int nx = x, ny = y;\\n\\n      nx += dx[i], ny += dy[i];\\n\\n      if( !isValid(nx,ny) ) continue;\\n\\n      if( f[ny][nx] != 0 && f[ny][nx] != f[ep/W][ep%W] ) continue;\\n\\n      if( visited[ny][nx] ) continue;\\n\\n      while(1){\\n\\n        nx += dx[i], ny += dy[i];\\n\\n        if( !isValid(nx,ny) ) break;\\n\\n        if( f[ny][nx] != 0 && f[ny][nx] != f[ep/W][ep%W] ) break;\\n\\n        if( X[nx] || Y[ny] ) break;\\n\\n        if( visited[ny][nx] ) return;\\n\\n      }\\n\\n    }\\n\\n  }\\n\\n\\n\\n\\n\\n  rep(i,4) {\\n\\n    if( i == (prev+2) % 4 ) continue;\\n\\n    int nx = x + dx[i], ny = y + dy[i];\\n\\n    if( !isValid(nx,ny) ) continue;\\n\\n    if( f[ny][nx] != 0 && f[ny][nx] != f[ep/W][ep%W] ) continue;\\n\\n    if( visited[ny][nx] ) continue;\\n\\n    visited[ny][nx] = true;\\n\\n    path.push_back(nx+ny*W);\\n\\n    dfs(nx,ny,cost+1,f,ep,path,F,S,i);\\n\\n    path.pop_back();\\n\\n    visited[ny][nx] = false;\\n\\n  }\\n\\n\\n\\n\\n\\n}\\n\\n\\n\\nint compute(int F,int S){\\n\\n  rep(i,H) rep(j,W) visited[i][j] = false;\\n\\n  deque<int> path;\\n\\n  path.push_back(sp[F][0]);\\n\\n  visited[sp[F][0]/W][sp[F][0]%W] = true;\\n\\n  simulate_min(m1,field,sp[F][0]);\\n\\n  simulate_min(m2,field,sp[S][0]);\\n\\n  dfs(sp[F][0]%W,sp[F][0]/W,0,field,sp[F][1],path,F,S,-IINF);\\n\\n}\\n\\n\\n\\nbool check(int a){\\n\\n  int x = a % W, y = a / W;\\n\\n  return 0 == x || W-1 == x || 0 == y || H-1 == y;\\n\\n}\\n\\n\\n\\nint main(){\\n\\n  while( cin >> H >> W, H|W ){\\n\\n    rep(i,2) sp[i].clear();\\n\\n    rep(i,10) X[i] = Y[i] = false;\\n\\n    rep(i,H) rep(j,W) {\\n\\n      cin >> field[i][j];\\n\\n      if( field[i][j] == 1 ) X[j] = true, Y[i] = true;\\n\\n      if( field[i][j] == 2 ) sp[0].push_back(j+i*W), X[j] = Y[i] = true;\\n\\n      if( field[i][j] == 3 ) sp[1].push_back(j+i*W), X[j] = Y[i] = true;\\n\\n    }\\n\\n    mini = IINF;\\n\\n    if( check(sp[0][1]) && check(sp[0][0]) && check(sp[1][1]) && check(sp[1][0]) ) {\\n\\n      vector<int> vec;\\n\\n      for(int i=0;i<W;i++) if( field[0][i] == 2 || field[0][i] == 3 ) vec.push_back(field[0][i]);\\n\\n      for(int i=1;i<H;i++) if( field[i][W-1] == 2 || field[i][W-1] == 3 ) vec.push_back(field[i][W-1]);\\n\\n      for(int i=W-2;i>=0;i--) if( field[H-1][i] == 2 || field[H-1][i] == 3 ) vec.push_back(field[H-1][i]);\\n\\n      for(int i=H-2;i>=1;i--) if( field[i][0] == 2 || field[i][0] == 3 ) vec.push_back(field[i][0]);\\n\\n      assert(vec.size() == 4);\\n\\n      if( !( vec[0] == vec[1] || vec[0] == vec[3] ) ) {\\n\\n        puts(\\\"0\\\");\\n\\n        continue;\\n\\n      }\\n\\n    }\\n\\n\\n\\n\\n\\n    compute(0,1);\\n\\n    if( mini == IINF ) puts(\\\"0\\\");\\n\\n    else cout << mini << endl;\\n\\n  }\\n\\n  return 0;\\n\\n}\", \"code_v1\": \"#include<bits/stdc++.h>\\n\\n\\n\\n#define REP(i,s,n) for(int i=s;i<n;i++)\\n\\n#define rep(i,n) REP(i,0,n)\\n\\n\\n\\nusing namespace std;\\n\\n\\n\\nconst int IINF = INT_MAX;\\n\\n\\n\\nint H,W,mini,field[10][10],prev[10][10],mindist[10][10],nfield[10][10],mindist2[10][10],m1[10][10],m2[10][10];\\n\\nvector<int> sp[2];\\n\\nbool visited[10][10];\\n\\nint dx[] = {0,1,0,-1};\\n\\nint dy[] = {1,0,-1,0};\\n\\nbool X[10], Y[10];\\n\\n\\n\\nbool isValid(int x,int y) { return 0 <= x && x < W && 0 <= y && y < H; }\\n\\n\\n\\nvoid simulate_min(int dist[10][10],int f[10][10],int s){\\n\\n  rep(i,H) rep(j,W) dist[i][j] = IINF;\\n\\n  dist[s/W][s%W] = 0;\\n\\n  deque<int> deq;\\n\\n  deq.push_back(s);\\n\\n  while( !deq.empty() ){\\n\\n    int cur = deq.front(); deq.pop_front();\\n\\n    int x = cur % W, y = cur / W;\\n\\n    if( x+y*W != s && f[y][x] == f[s/W][s%W] ) return;\\n\\n    rep(i,4){\\n\\n      int nx = x + dx[i], ny = y + dy[i];\\n\\n      if( !isValid(nx,ny) ) continue;\\n\\n      if( f[ny][nx] != 0 && f[ny][nx] != f[s/W][s%W] ) continue;\\n\\n      if( dist[ny][nx] == IINF ) {\\n\\n        dist[ny][nx] = dist[y][x] + 1;\\n\\n        deq.push_back(nx+ny*W);\\n\\n      }\\n\\n    }\\n\\n  }\\n\\n}\\n\\n\\n\\n\\n\\nvoid dfs(int x,int y,int cost,int f[10][10],int ep,deque<int> &path,int F,int S,int prev){\\n\\n\\n\\n  if( cost >= mini ) return;\\n\\n  if( m1[ep/W][ep%W] - m1[y][x] + m2[sp[S][1]/W][sp[S][1]%W] + cost >= mini ) return;\\n\\n\\n\\n\\n\\n  if( x + y * W == ep ) {\\n\\n    rep(i,H) rep(j,W) nfield[i][j] = field[i][j];\\n\\n    rep(i,path.size()) nfield[path[i]/W][path[i]%W] = 1;\\n\\n    simulate_min(mindist2,nfield,sp[S][0]);\\n\\n    if( mindist2[sp[S][1]/W][sp[S][1]%W] == IINF ) return;\\n\\n    mini = min(mini,(int)path.size()-1+mindist2[sp[S][1]/W][sp[S][1]%W]);\\n\\n    return;\\n\\n  }\\n\\n  int cnt = 0;\\n\\n  rep(i,4) {\\n\\n    int nx = x + dx[i], ny = y + dy[i];\\n\\n    if( !isValid(nx,ny) ) continue;\\n\\n    cnt += visited[ny][nx];\\n\\n }\\n\\n  if( cnt >= 2 ) return;\\n\\n\\n\\n\\n\\n  {\\n\\n    rep(i,4){\\n\\n      if( i == (prev+2) % 4 ) continue;\\n\\n      int nx = x, ny = y;\\n\\n      nx += dx[i], ny += dy[i];\\n\\n      if( !isValid(nx,ny) ) continue;\\n\\n      if( f[ny][nx] != 0 && f[ny][nx] != f[ep/W][ep%W] ) continue;\\n\\n      if( visited[ny][nx] ) continue;\\n\\n      while(1){\\n\\n        nx += dx[i], ny += dy[i];\\n\\n        if( !isValid(nx,ny) ) break;\\n\\n        if( f[ny][nx] != 0 && f[ny][nx] != f[ep/W][ep%W] ) break;\\n\\n        if( X[nx] || Y[ny] ) break;\\n\\n        if( visited[ny][nx] ) return;\\n\\n      }\\n\\n    }\\n\\n  }\\n\\n\\n\\n\\n\\n  rep(i,4) {\\n\\n    if( i == (prev+2) % 4 ) continue;\\n\\n    int nx = x + dx[i], ny = y + dy[i];\\n\\n    if( !isValid(nx,ny) ) continue;\\n\\n    if( f[ny][nx] != 0 && f[ny][nx] != f[ep/W][ep%W] ) continue;\\n\\n    if( visited[ny][nx] ) continue;\\n\\n    visited[ny][nx] = true;\\n\\n    path.push_back(nx+ny*W);\\n\\n    dfs(nx,ny,cost+1,f,ep,path,F,S,i);\\n\\n    path.pop_back();\\n\\n    visited[ny][nx] = false;\\n\\n  }\\n\\n\\n\\n\\n\\n}\\n\\n\\n\\nint compute(int F,int S){\\n\\n  rep(i,H) rep(j,W) visited[i][j] = false;\\n\\n  deque<int> path;\\n\\n  path.push_back(sp[F][0]);\\n\\n  visited[sp[F][0]/W][sp[F][0]%W] = true;\\n\\n  simulate_min(m1,field,sp[F][0]);\\n\\n  simulate_min(m2,field,sp[S][0]);\\n\\n  dfs(sp[F][0]%W,sp[F][0]/W,0,field,sp[F][1],path,F,S,-IINF);\\n\\n}\\n\\n\\n\\nbool check(int a){\\n\\n  int x = a % W, y = a / W;\\n\\n  return 0 == x || W-1 == x || 0 == y || H-1 == y;\\n\\n}\\n\\n\\n\\nint main(){\\n\\n  while( cin >> H >> W, H|W ){\\n\\n    rep(i,2) sp[i].clear();\\n\\n    rep(i,10) X[i] = Y[i] = false;\\n\\n    rep(i,H) rep(j,W) {\\n\\n      cin >> field[i][j];\\n\\n      if( field[i][j] == 1 ) X[j] = true, Y[i] = true;\\n\\n      if( field[i][j] == 2 ) sp[0].push_back(j+i*W), X[j] = Y[i] = true;\\n\\n      if( field[i][j] == 3 ) sp[1].push_back(j+i*W), X[j] = Y[i] = true;\\n\\n    }\\n\\n    mini = IINF;\\n\\n    if( check(sp[0][1]) && check(sp[0][0]) && check(sp[1][1]) && check(sp[1][0]) ) {\\n\\n      vector<int> vec;\\n\\n      for(int i=0;i<W;i++) if( field[0][i] == 2 || field[0][i] == 3 ) vec.push_back(field[0][i]);\\n\\n      for(int i=1;i<H;i++) if( field[i][W-1] == 2 || field[i][W-1] == 3 ) vec.push_back(field[i][W-1]);\\n\\n      for(int i=W-2;i>=0;i--) if( field[H-1][i] == 2 || field[H-1][i] == 3 ) vec.push_back(field[H-1][i]);\\n\\n      for(int i=H-2;i>=1;i--) if( field[i][0] == 2 || field[i][0] == 3 ) vec.push_back(field[i][0]);\\n\\n      assert(vec.size() == 4);\\n\\n      if( !( vec[0] == vec[1] || vec[0] == vec[3] ) ) {\\n\\n        puts(\\\"0\\\");\\n\\n        continue;\\n\\n      }\\n\\n    }\\n\\n\\n\\n\\n\\n    compute(0,1);\\n\\n    if( mini == IINF ) puts(\\\"0\\\");\\n\\n    else cout << mini << endl;\\n\\n  }\\n\\n  return 0;\\n\\n}\", \"code_v0_loc\": \"144.0\", \"code_v1_loc\": \"144.0\"}\n{\"user_id\": \"u116766943\", \"problem_id\": \"p00849\", \"language\": \"C++\", \"submission_id_v0\": \"s363969251\", \"submission_id_v1\": \"s061168313\", \"cpu_time_v0\": \"8500.0\", \"cpu_time_v1\": \"920.0\", \"memory_v0\": \"1252.0\", \"memory_v1\": \"12164.0\", \"status_v0\": \"Time Limit Exceeded\", \"status_v1\": \"Accepted\", \"improvement_frac\": \"89.18\", \"code_v0\": \"#include<bits/stdc++.h>\\n\\n\\n\\n#define REP(i,s,n) for(int i=s;i<n;i++)\\n\\n#define rep(i,n) REP(i,0,n)\\n\\n\\n\\nusing namespace std;\\n\\n\\n\\nconst int IINF = INT_MAX;\\n\\n\\n\\nint H,W,mini,field[10][10],prev[10][10],mindist[10][10],nfield[10][10],mindist2[10][10],m1[10][10],m2[10][10];\\n\\nvector<int> sp[2];\\n\\nbool visited[10][10];\\n\\nint dx[] = {0,1,0,-1};\\n\\nint dy[] = {1,0,-1,0};\\n\\nbool X[10], Y[10];\\n\\n\\n\\nbool isValid(int x,int y) { return 0 <= x && x < W && 0 <= y && y < H; }\\n\\n\\n\\nvoid simulate_min(int dist[10][10],int f[10][10],int s){\\n\\n  rep(i,H) rep(j,W) dist[i][j] = IINF;\\n\\n  dist[s/W][s%W] = 0;\\n\\n  deque<int> deq;\\n\\n  deq.push_back(s);\\n\\n  while( !deq.empty() ){\\n\\n    int cur = deq.front(); deq.pop_front();\\n\\n    int x = cur % W, y = cur / W;\\n\\n    if( x+y*W != s && f[y][x] == f[s/W][s%W] ) return;\\n\\n    rep(i,4){\\n\\n      int nx = x + dx[i], ny = y + dy[i];\\n\\n      if( !isValid(nx,ny) ) continue;\\n\\n      if( f[ny][nx] != 0 && f[ny][nx] != f[s/W][s%W] ) continue;\\n\\n      if( dist[ny][nx] == IINF ) {\\n\\n        dist[ny][nx] = dist[y][x] + 1;\\n\\n        deq.push_back(nx+ny*W);\\n\\n      }\\n\\n    }\\n\\n  }\\n\\n}\\n\\n\\n\\n\\n\\nvoid dfs(int x,int y,int cost,int f[10][10],int ep,deque<int> &path,int F,int S,int prev){\\n\\n\\n\\n  if( cost >= mini ) return;\\n\\n  if( m2[sp[S][1]/W][sp[S][1]%W] + cost >= mini ) return;\\n\\n\\n\\n\\n\\n  if( x + y * W == ep ) {\\n\\n    rep(i,H) rep(j,W) nfield[i][j] = field[i][j];\\n\\n    rep(i,path.size()) nfield[path[i]/W][path[i]%W] = 1;\\n\\n    simulate_min(mindist2,nfield,sp[S][0]);\\n\\n    if( mindist2[sp[S][1]/W][sp[S][1]%W] == IINF ) return;\\n\\n    mini = min(mini,(int)path.size()-1+mindist2[sp[S][1]/W][sp[S][1]%W]);\\n\\n    return;\\n\\n  }\\n\\n  int cnt = 0;\\n\\n  rep(i,4) {\\n\\n    int nx = x + dx[i], ny = y + dy[i];\\n\\n    if( !isValid(nx,ny) ) continue;\\n\\n    cnt += visited[ny][nx];\\n\\n }\\n\\n  if( cnt >= 2 ) return;\\n\\n\\n\\n\\n\\n  {\\n\\n    rep(i,4){\\n\\n      if( i == (prev+2) % 4 ) continue;\\n\\n      int nx = x, ny = y;\\n\\n      nx += dx[i], ny += dy[i];\\n\\n      if( !isValid(nx,ny) ) continue;\\n\\n      if( f[ny][nx] != 0 && f[ny][nx] != f[ep/W][ep%W] ) continue;\\n\\n      if( visited[ny][nx] ) continue;\\n\\n      while(1){\\n\\n        nx += dx[i], ny += dy[i];\\n\\n        if( !isValid(nx,ny) ) break;\\n\\n        if( f[ny][nx] != 0 && f[ny][nx] != f[ep/W][ep%W] ) break;\\n\\n        if( X[nx] || Y[ny] ) break;\\n\\n        if( visited[ny][nx] ) return;\\n\\n      }\\n\\n    }\\n\\n  }\\n\\n\\n\\n\\n\\n  rep(i,4) {\\n\\n    if( i == (prev+2) % 4 ) continue;\\n\\n    int nx = x + dx[i], ny = y + dy[i];\\n\\n    if( !isValid(nx,ny) ) continue;\\n\\n    if( f[ny][nx] != 0 && f[ny][nx] != f[ep/W][ep%W] ) continue;\\n\\n    if( visited[ny][nx] ) continue;\\n\\n    visited[ny][nx] = true;\\n\\n    path.push_back(nx+ny*W);\\n\\n    dfs(nx,ny,cost+1,f,ep,path,F,S,i);\\n\\n    path.pop_back();\\n\\n    visited[ny][nx] = false;\\n\\n  }\\n\\n\\n\\n\\n\\n}\\n\\n\\n\\nint compute(int F,int S){\\n\\n  rep(i,H) rep(j,W) visited[i][j] = false;\\n\\n  deque<int> path;\\n\\n  path.push_back(sp[F][0]);\\n\\n  visited[sp[F][0]/W][sp[F][0]%W] = true;\\n\\n  simulate_min(m1,field,sp[F][0]);\\n\\n  simulate_min(m2,field,sp[S][0]);\\n\\n  dfs(sp[F][0]%W,sp[F][0]/W,0,field,sp[F][1],path,F,S,-IINF);\\n\\n}\\n\\n\\n\\nbool check(int a){\\n\\n  int x = a % W, y = a / W;\\n\\n  return 0 == x || W-1 == x || 0 == y || H-1 == y;\\n\\n}\\n\\n\\n\\nint main(){\\n\\n  while( scanf(\\\"%d %d\\\",&H,&W), H|W ){\\n\\n    rep(i,2) sp[i].clear();\\n\\n    rep(i,10) X[i] = Y[i] = false;\\n\\n    int one = 0;\\n\\n    rep(i,H) rep(j,W) {\\n\\n      cin >> field[i][j];\\n\\n      if( field[i][j] == 1 ) X[j] = true, Y[i] = true, one++;\\n\\n      if( field[i][j] == 2 ) sp[0].push_back(j+i*W), X[j] = Y[i] = true;\\n\\n      if( field[i][j] == 3 ) sp[1].push_back(j+i*W), X[j] = Y[i] = true;\\n\\n    }\\n\\n    mini = H*W - one;\\n\\n    if( check(sp[0][1]) && check(sp[0][0]) && check(sp[1][1]) && check(sp[1][0]) ) {\\n\\n      vector<int> vec;\\n\\n      for(int i=0;i<W;i++) if( field[0][i] == 2 || field[0][i] == 3 ) vec.push_back(field[0][i]);\\n\\n      for(int i=1;i<H;i++) if( field[i][W-1] == 2 || field[i][W-1] == 3 ) vec.push_back(field[i][W-1]);\\n\\n      for(int i=W-2;i>=0;i--) if( field[H-1][i] == 2 || field[H-1][i] == 3 ) vec.push_back(field[H-1][i]);\\n\\n      for(int i=H-2;i>=1;i--) if( field[i][0] == 2 || field[i][0] == 3 ) vec.push_back(field[i][0]);\\n\\n      assert(vec.size() == 4);\\n\\n      if( !( vec[0] == vec[1] || vec[0] == vec[3] ) ) {\\n\\n        puts(\\\"0\\\");\\n\\n        continue;\\n\\n      }\\n\\n    }\\n\\n\\n\\n\\n\\n    compute(0,1);\\n\\n    if( mini == IINF ) puts(\\\"0\\\");\\n\\n    else printf(\\\"%d\\\\n\\\",mini);\\n\\n  }\\n\\n  return 0;\\n\\n}\", \"code_v1\": \"#include<bits/stdc++.h>\\n\\n\\n\\n#define REP(i,s,n) for(int i=s;i<n;i++)\\n\\n#define rep(i,n) REP(i,0,n)\\n\\n\\n\\nusing namespace std;\\n\\n\\n\\nconst int IINF = INT_MAX;\\n\\nconst int MAX_W = 9;\\n\\ntypedef pair<int,int> ii;\\n\\n\\n\\n\\n\\nint h,w;\\n\\nint field[10][10],indice[10][10];\\n\\nint dp[2][1<<((MAX_W+1)*2)];\\n\\nint dx[] = {1,0,-1,0};\\n\\nint dy[] = {0,1,0,-1};\\n\\n\\n\\ninline bool isValid(int x,int y) { return 0 <= x && x < w && 0 <= y && y < h; }\\n\\n\\n\\n#define SET(bitmask,index,value) ( bitmask = ( ( bitmask & ~(1<<(2*index)) & ~(1<<(2*index+1)) ) | ( value << (index*2) ) ) )\\n\\n#define GET(bitmask,x) ( ( ( bitmask >> ( 2 * x ) ) & 1 ) | ( ( ( bitmask >> (2*x+1) ) & 1) << 1 ) )\\n\\n#define FIX(bitmask) ((bitmask<<2)&((1<<(2*(w+1)))-1))\\n\\n#define STAR ( field[y][x] == 2 || field[y][x] == 3 )\\n\\n#define update(x,v) ( ( x == -1 ) ? ( x = v ) : ( x = min(x,v) ) )\\n\\n\\n\\n\\n\\nvoid compute(){\\n\\n\\n\\n  set<int> S;\\n\\n  vector<int> state;\\n\\n  rep(bitmask,(1<<((w+1)*2))) {\\n\\n    bool success = true;\\n\\n    for(int i=0;i<(w+1)*2;i+=2){\\n\\n      if( ( ( bitmask >> i ) & 1 ) && ( ( bitmask >> (i+1) ) & 1 ) ) { success = false; break; }\\n\\n    }\\n\\n    if( success ) state.push_back(bitmask), S.insert(bitmask);\\n\\n  }\\n\\n  memset(dp,-1,sizeof(dp));\\n\\n\\n\\n  int _size = state.size();\\n\\n  bool initter = true, phase = false;\\n\\n  int mini = IINF;\\n\\n  int encounter = 0;\\n\\n  rep(y,h){\\n\\n    rep(x,w){\\n\\n      /*\\n\\n      cout << x << \\\",\\\" << y << endl;\\n\\n      rep(i,_size){\\n\\n        if( dp[phase][state[i]] != -1 ) {\\n\\n          bitset<12> BIT(state[i]);\\n\\n          for(int j=0;j<12;j+=2){\\n\\n            cout << BIT[j] << BIT[j+1] << \\\" \\\";\\n\\n          } \\n\\n          cout << \\\" = \\\" << dp[phase][state[i]] << endl;\\n\\n        }\\n\\n      }\\n\\n      */\\n\\n      if( field[y][x] == 2 || field[y][x] == 3 ) ++encounter;\\n\\n      if( initter ) dp[phase][0] = 0;\\n\\n\\n\\n      rep(i,_size){\\n\\n        int bitmask = state[i];\\n\\n\\n\\n        if( dp[phase][bitmask] == -1 ) continue;\\n\\n        bitset<12> bit(bitmask);\\n\\n\\n\\n\\n\\n        assert( S.count(bitmask));\\n\\n        // ?????????\\n\\n        if( field[y][x] == 1 ) {\\n\\n          int nbitmask = bitmask;\\n\\n          SET(nbitmask,x,0);\\n\\n          if( x == w-1 ) nbitmask = FIX(nbitmask);\\n\\n          update(dp[!phase][nbitmask],dp[phase][bitmask]);\\n\\n          continue;\\n\\n        }\\n\\n\\n\\n        int nbitmask;\\n\\n        // 0 0\\n\\n        if( !( ( GET(bitmask,x) == 0 ) && ( GET(bitmask,(x+1)) == 0 ) ) ) goto Label1;\\n\\n        if( STAR ) {\\n\\n          // x 0\\n\\n          if( y+1 < h && field[y+1][x] != 1 ) {\\n\\n            nbitmask = bitmask;\\n\\n            SET(nbitmask,x,((field[y][x]==3)+1));\\n\\n          if( x == w-1 ) nbitmask = FIX(nbitmask);\\n\\n\\n\\n            update(dp[!phase][nbitmask],dp[phase][bitmask]+1);\\n\\n          }\\n\\n          // 0 x\\n\\n          if( x+1 < w && field[y][x+1] != 1 ) {\\n\\n            nbitmask = bitmask;\\n\\n            SET(nbitmask,(x+1),((field[y][x]==3)+1));\\n\\n\\n\\n          if( x == w-1 ) nbitmask = FIX(nbitmask);\\n\\n\\n\\n          update(dp[!phase][nbitmask],dp[phase][bitmask]+1);\\n\\n          }\\n\\n        } else {\\n\\n          // x x\\n\\n          if( x+1 < w && y+1 < h && field[y][x+1] != 1 && field[y+1][x] != 1 ) {\\n\\n            REP(color,1,3){\\n\\n              nbitmask = bitmask;\\n\\n              SET(nbitmask,x,color);\\n\\n              SET(nbitmask,(x+1),color);\\n\\n              if( x == w-1 ) nbitmask = FIX(nbitmask);\\n\\n              update(dp[!phase][nbitmask],(dp[phase][bitmask]+1));\\n\\n            }\\n\\n          }\\n\\n          // 0 0\\n\\n          nbitmask = bitmask;\\n\\n          SET(nbitmask,x,0);\\n\\n          SET(nbitmask,(x+1),0);\\n\\n          if( x == w-1 ) nbitmask = FIX(nbitmask);\\n\\n          update(dp[!phase][nbitmask],dp[phase][bitmask]);\\n\\n        }\\n\\n\\n\\n        continue;\\n\\n      Label1:;\\n\\n        // x 0\\n\\n        // 0 x\\n\\n        int color;\\n\\n        if( !( ( GET(bitmask,x) && ( GET(bitmask,(x+1)) == 0 ) ) || ( GET(bitmask,(x+1)) && ( GET(bitmask,x) == 0 ) ) ) ) goto Label2;\\n\\n        color = GET(bitmask,x) | GET(bitmask,(x+1));\\n\\n        assert( color == 1 || color == 2 );\\n\\n        if( STAR ) {\\n\\n          // 0 0\\n\\n          if( ((color==1)?2:3) == field[y][x] ) {\\n\\n\\n\\n            nbitmask = bitmask;\\n\\n            SET(nbitmask,x,0);\\n\\n            SET(nbitmask,(x+1),0);\\n\\n          if( x == w-1 ) nbitmask = FIX(nbitmask);\\n\\n            update(dp[!phase][nbitmask],dp[phase][bitmask]+1);\\n\\n          }\\n\\n        } else {\\n\\n          // x 0\\n\\n          if( y+1 < h && field[y+1][x] != 1 ) {\\n\\n            nbitmask = bitmask;\\n\\n            SET(nbitmask,x,color);\\n\\n            SET(nbitmask,(x+1),0);\\n\\n          if( x == w-1 ) nbitmask = FIX(nbitmask);\\n\\n\\n\\n\\n\\n            update(dp[!phase][nbitmask],(dp[phase][bitmask]+1));\\n\\n          }\\n\\n          \\n\\n          // 0 x\\n\\n          if( x+1 < w && field[y][x+1] != 1 ) {\\n\\n            nbitmask = bitmask;\\n\\n            SET(nbitmask,x,0);\\n\\n            SET(nbitmask,(x+1),color);\\n\\n\\n\\n          if( x == w-1 ) nbitmask = FIX(nbitmask);\\n\\n            update(dp[!phase][nbitmask],(dp[phase][bitmask]+1));\\n\\n          }\\n\\n        }\\n\\n        continue;\\n\\n      Label2:;\\n\\n        // x x\\n\\n        if( !( GET(bitmask,x) && GET(bitmask,(x+1)) ) ) goto Label3;\\n\\n        if( GET(bitmask,x) != GET(bitmask,(x+1)) ) goto Label3;\\n\\n\\n\\n        color = GET(bitmask,x);\\n\\n        assert( color );\\n\\n        //assert( GET(bitmask,x) == GET(bitmask,(x+1)) );\\n\\n        \\n\\n        // 0 0\\n\\n        if( !STAR ) {\\n\\n          nbitmask = bitmask;\\n\\n          SET(nbitmask,x,0);\\n\\n          SET(nbitmask,(x+1),0);\\n\\n          if( x == w-1 ) nbitmask = FIX(nbitmask);\\n\\n          update(dp[!phase][nbitmask],dp[phase][bitmask]+1);\\n\\n        }\\n\\n      Label3:;\\n\\n\\n\\n      }\\n\\n      if( field[y][x] == 2 || field[y][x] == 3 ) initter = false;\\n\\n      rep(i,_size) dp[phase][state[i]] = -1;\\n\\n      if( encounter >= 4 && dp[!phase][0] != -1 ) {\\n\\n        mini = min(mini,dp[!phase][0]);\\n\\n      }\\n\\n      phase = !phase;\\n\\n    }\\n\\n  }\\n\\n\\n\\n  /*\\n\\n  puts(\\\"final\\\");\\n\\n  rep(i,_size){\\n\\n    if( dp[phase][state[i]] != -1 ) {\\n\\n      bitset<12> BIT(state[i]);\\n\\n      for(int j=0;j<12;j+=2){\\n\\n        cout << BIT[j] << BIT[j+1] << \\\" \\\";\\n\\n      } \\n\\n      cout << \\\" = \\\" << dp[phase][state[i]] << endl;\\n\\n    }\\n\\n  }\\n\\n  */\\n\\n  if( dp[phase][0] != -1 ) mini = min(mini,dp[phase][0]);\\n\\n  if( mini != IINF ) printf(\\\"%d\\\\n\\\",mini-2);\\n\\n  else puts(\\\"0\\\");\\n\\n}\\n\\n\\n\\nint main(){\\n\\n\\n\\n\\n\\n  while( scanf(\\\"%d %d\\\",&h,&w), h|w ){\\n\\n    rep(i,h) rep(j,w) scanf(\\\"%d\\\",&field[i][j]);\\n\\n    compute();\\n\\n  }\\n\\n  return 0;\\n\\n}\", \"code_v0_loc\": \"145.0\", \"code_v1_loc\": \"214.0\"}\n{\"user_id\": \"u116766943\", \"problem_id\": \"p00849\", \"language\": \"C++\", \"submission_id_v0\": \"s061168313\", \"submission_id_v1\": \"s755775167\", \"cpu_time_v0\": \"920.0\", \"cpu_time_v1\": \"420.0\", \"memory_v0\": \"12164.0\", \"memory_v1\": \"9780.0\", \"status_v0\": \"Accepted\", \"status_v1\": \"Accepted\", \"improvement_frac\": \"54.35\", \"code_v0\": \"#include<bits/stdc++.h>\\n\\n\\n\\n#define REP(i,s,n) for(int i=s;i<n;i++)\\n\\n#define rep(i,n) REP(i,0,n)\\n\\n\\n\\nusing namespace std;\\n\\n\\n\\nconst int IINF = INT_MAX;\\n\\nconst int MAX_W = 9;\\n\\ntypedef pair<int,int> ii;\\n\\n\\n\\n\\n\\nint h,w;\\n\\nint field[10][10],indice[10][10];\\n\\nint dp[2][1<<((MAX_W+1)*2)];\\n\\nint dx[] = {1,0,-1,0};\\n\\nint dy[] = {0,1,0,-1};\\n\\n\\n\\ninline bool isValid(int x,int y) { return 0 <= x && x < w && 0 <= y && y < h; }\\n\\n\\n\\n#define SET(bitmask,index,value) ( bitmask = ( ( bitmask & ~(1<<(2*index)) & ~(1<<(2*index+1)) ) | ( value << (index*2) ) ) )\\n\\n#define GET(bitmask,x) ( ( ( bitmask >> ( 2 * x ) ) & 1 ) | ( ( ( bitmask >> (2*x+1) ) & 1) << 1 ) )\\n\\n#define FIX(bitmask) ((bitmask<<2)&((1<<(2*(w+1)))-1))\\n\\n#define STAR ( field[y][x] == 2 || field[y][x] == 3 )\\n\\n#define update(x,v) ( ( x == -1 ) ? ( x = v ) : ( x = min(x,v) ) )\\n\\n\\n\\n\\n\\nvoid compute(){\\n\\n\\n\\n  set<int> S;\\n\\n  vector<int> state;\\n\\n  rep(bitmask,(1<<((w+1)*2))) {\\n\\n    bool success = true;\\n\\n    for(int i=0;i<(w+1)*2;i+=2){\\n\\n      if( ( ( bitmask >> i ) & 1 ) && ( ( bitmask >> (i+1) ) & 1 ) ) { success = false; break; }\\n\\n    }\\n\\n    if( success ) state.push_back(bitmask), S.insert(bitmask);\\n\\n  }\\n\\n  memset(dp,-1,sizeof(dp));\\n\\n\\n\\n  int _size = state.size();\\n\\n  bool initter = true, phase = false;\\n\\n  int mini = IINF;\\n\\n  int encounter = 0;\\n\\n  rep(y,h){\\n\\n    rep(x,w){\\n\\n      /*\\n\\n      cout << x << \\\",\\\" << y << endl;\\n\\n      rep(i,_size){\\n\\n        if( dp[phase][state[i]] != -1 ) {\\n\\n          bitset<12> BIT(state[i]);\\n\\n          for(int j=0;j<12;j+=2){\\n\\n            cout << BIT[j] << BIT[j+1] << \\\" \\\";\\n\\n          } \\n\\n          cout << \\\" = \\\" << dp[phase][state[i]] << endl;\\n\\n        }\\n\\n      }\\n\\n      */\\n\\n      if( field[y][x] == 2 || field[y][x] == 3 ) ++encounter;\\n\\n      if( initter ) dp[phase][0] = 0;\\n\\n\\n\\n      rep(i,_size){\\n\\n        int bitmask = state[i];\\n\\n\\n\\n        if( dp[phase][bitmask] == -1 ) continue;\\n\\n        bitset<12> bit(bitmask);\\n\\n\\n\\n\\n\\n        assert( S.count(bitmask));\\n\\n        // ?????????\\n\\n        if( field[y][x] == 1 ) {\\n\\n          int nbitmask = bitmask;\\n\\n          SET(nbitmask,x,0);\\n\\n          if( x == w-1 ) nbitmask = FIX(nbitmask);\\n\\n          update(dp[!phase][nbitmask],dp[phase][bitmask]);\\n\\n          continue;\\n\\n        }\\n\\n\\n\\n        int nbitmask;\\n\\n        // 0 0\\n\\n        if( !( ( GET(bitmask,x) == 0 ) && ( GET(bitmask,(x+1)) == 0 ) ) ) goto Label1;\\n\\n        if( STAR ) {\\n\\n          // x 0\\n\\n          if( y+1 < h && field[y+1][x] != 1 ) {\\n\\n            nbitmask = bitmask;\\n\\n            SET(nbitmask,x,((field[y][x]==3)+1));\\n\\n          if( x == w-1 ) nbitmask = FIX(nbitmask);\\n\\n\\n\\n            update(dp[!phase][nbitmask],dp[phase][bitmask]+1);\\n\\n          }\\n\\n          // 0 x\\n\\n          if( x+1 < w && field[y][x+1] != 1 ) {\\n\\n            nbitmask = bitmask;\\n\\n            SET(nbitmask,(x+1),((field[y][x]==3)+1));\\n\\n\\n\\n          if( x == w-1 ) nbitmask = FIX(nbitmask);\\n\\n\\n\\n          update(dp[!phase][nbitmask],dp[phase][bitmask]+1);\\n\\n          }\\n\\n        } else {\\n\\n          // x x\\n\\n          if( x+1 < w && y+1 < h && field[y][x+1] != 1 && field[y+1][x] != 1 ) {\\n\\n            REP(color,1,3){\\n\\n              nbitmask = bitmask;\\n\\n              SET(nbitmask,x,color);\\n\\n              SET(nbitmask,(x+1),color);\\n\\n              if( x == w-1 ) nbitmask = FIX(nbitmask);\\n\\n              update(dp[!phase][nbitmask],(dp[phase][bitmask]+1));\\n\\n            }\\n\\n          }\\n\\n          // 0 0\\n\\n          nbitmask = bitmask;\\n\\n          SET(nbitmask,x,0);\\n\\n          SET(nbitmask,(x+1),0);\\n\\n          if( x == w-1 ) nbitmask = FIX(nbitmask);\\n\\n          update(dp[!phase][nbitmask],dp[phase][bitmask]);\\n\\n        }\\n\\n\\n\\n        continue;\\n\\n      Label1:;\\n\\n        // x 0\\n\\n        // 0 x\\n\\n        int color;\\n\\n        if( !( ( GET(bitmask,x) && ( GET(bitmask,(x+1)) == 0 ) ) || ( GET(bitmask,(x+1)) && ( GET(bitmask,x) == 0 ) ) ) ) goto Label2;\\n\\n        color = GET(bitmask,x) | GET(bitmask,(x+1));\\n\\n        assert( color == 1 || color == 2 );\\n\\n        if( STAR ) {\\n\\n          // 0 0\\n\\n          if( ((color==1)?2:3) == field[y][x] ) {\\n\\n\\n\\n            nbitmask = bitmask;\\n\\n            SET(nbitmask,x,0);\\n\\n            SET(nbitmask,(x+1),0);\\n\\n          if( x == w-1 ) nbitmask = FIX(nbitmask);\\n\\n            update(dp[!phase][nbitmask],dp[phase][bitmask]+1);\\n\\n          }\\n\\n        } else {\\n\\n          // x 0\\n\\n          if( y+1 < h && field[y+1][x] != 1 ) {\\n\\n            nbitmask = bitmask;\\n\\n            SET(nbitmask,x,color);\\n\\n            SET(nbitmask,(x+1),0);\\n\\n          if( x == w-1 ) nbitmask = FIX(nbitmask);\\n\\n\\n\\n\\n\\n            update(dp[!phase][nbitmask],(dp[phase][bitmask]+1));\\n\\n          }\\n\\n          \\n\\n          // 0 x\\n\\n          if( x+1 < w && field[y][x+1] != 1 ) {\\n\\n            nbitmask = bitmask;\\n\\n            SET(nbitmask,x,0);\\n\\n            SET(nbitmask,(x+1),color);\\n\\n\\n\\n          if( x == w-1 ) nbitmask = FIX(nbitmask);\\n\\n            update(dp[!phase][nbitmask],(dp[phase][bitmask]+1));\\n\\n          }\\n\\n        }\\n\\n        continue;\\n\\n      Label2:;\\n\\n        // x x\\n\\n        if( !( GET(bitmask,x) && GET(bitmask,(x+1)) ) ) goto Label3;\\n\\n        if( GET(bitmask,x) != GET(bitmask,(x+1)) ) goto Label3;\\n\\n\\n\\n        color = GET(bitmask,x);\\n\\n        assert( color );\\n\\n        //assert( GET(bitmask,x) == GET(bitmask,(x+1)) );\\n\\n        \\n\\n        // 0 0\\n\\n        if( !STAR ) {\\n\\n          nbitmask = bitmask;\\n\\n          SET(nbitmask,x,0);\\n\\n          SET(nbitmask,(x+1),0);\\n\\n          if( x == w-1 ) nbitmask = FIX(nbitmask);\\n\\n          update(dp[!phase][nbitmask],dp[phase][bitmask]+1);\\n\\n        }\\n\\n      Label3:;\\n\\n\\n\\n      }\\n\\n      if( field[y][x] == 2 || field[y][x] == 3 ) initter = false;\\n\\n      rep(i,_size) dp[phase][state[i]] = -1;\\n\\n      if( encounter >= 4 && dp[!phase][0] != -1 ) {\\n\\n        mini = min(mini,dp[!phase][0]);\\n\\n      }\\n\\n      phase = !phase;\\n\\n    }\\n\\n  }\\n\\n\\n\\n  /*\\n\\n  puts(\\\"final\\\");\\n\\n  rep(i,_size){\\n\\n    if( dp[phase][state[i]] != -1 ) {\\n\\n      bitset<12> BIT(state[i]);\\n\\n      for(int j=0;j<12;j+=2){\\n\\n        cout << BIT[j] << BIT[j+1] << \\\" \\\";\\n\\n      } \\n\\n      cout << \\\" = \\\" << dp[phase][state[i]] << endl;\\n\\n    }\\n\\n  }\\n\\n  */\\n\\n  if( dp[phase][0] != -1 ) mini = min(mini,dp[phase][0]);\\n\\n  if( mini != IINF ) printf(\\\"%d\\\\n\\\",mini-2);\\n\\n  else puts(\\\"0\\\");\\n\\n}\\n\\n\\n\\nint main(){\\n\\n\\n\\n\\n\\n  while( scanf(\\\"%d %d\\\",&h,&w), h|w ){\\n\\n    rep(i,h) rep(j,w) scanf(\\\"%d\\\",&field[i][j]);\\n\\n    compute();\\n\\n  }\\n\\n  return 0;\\n\\n}\", \"code_v1\": \"#include<bits/stdc++.h>\\n\\n\\n\\n#define REP(i,s,n) for(int i=s;i<n;i++)\\n\\n#define rep(i,n) REP(i,0,n)\\n\\n\\n\\nusing namespace std;\\n\\n\\n\\nconst int IINF = INT_MAX;\\n\\nconst int MAX_W = 9;\\n\\ntypedef pair<int,int> ii;\\n\\n\\n\\n\\n\\nint h,w;\\n\\nint field[10][10],indice[10][10];\\n\\nint dp[2][1<<((MAX_W+1)*2)];\\n\\nint dx[] = {1,0,-1,0};\\n\\nint dy[] = {0,1,0,-1};\\n\\n\\n\\ninline bool isValid(int x,int y) { return 0 <= x && x < w && 0 <= y && y < h; }\\n\\n\\n\\n#define SET(bitmask,index,value) ( bitmask = ( ( bitmask & ~(1<<(2*index)) & ~(1<<(2*index+1)) ) | ( value << (index*2) ) ) )\\n\\n#define GET(bitmask,x) ( ( ( bitmask >> ( 2 * x ) ) & 1 ) | ( ( ( bitmask >> (2*x+1) ) & 1) << 1 ) )\\n\\n#define FIX(bitmask) ((bitmask<<2)&((1<<(2*(w+1)))-1))\\n\\n#define STAR ( field[y][x] == 2 || field[y][x] == 3 )\\n\\n#define update(x,v) ( ( x == -1 ) ? ( x = v ) : ( x = min(x,v) ) )\\n\\n\\n\\n\\n\\nvoid compute(){\\n\\n\\n\\n  vector<int> state;\\n\\n  rep(bitmask,(1<<((w+1)*2))) {\\n\\n    bool success = true;\\n\\n    for(int i=0;i<(w+1)*2;i+=2) if( ( ( bitmask >> i ) & 1 ) && ( ( bitmask >> (i+1) ) & 1 ) ) { success = false; break; }\\n\\n    if( success ) state.push_back(bitmask);\\n\\n  }\\n\\n  memset(dp,-1,sizeof(dp));\\n\\n\\n\\n  int _size = state.size();\\n\\n  bool initter = true, phase = false;\\n\\n  int mini = IINF;\\n\\n  int encounter = 0;\\n\\n  rep(y,h){\\n\\n    rep(x,w){\\n\\n\\n\\n      if( field[y][x] == 2 || field[y][x] == 3 ) ++encounter;\\n\\n      if( initter ) dp[phase][0] = 0;\\n\\n\\n\\n      rep(i,_size){\\n\\n        int bitmask = state[i];\\n\\n\\n\\n        if( dp[phase][bitmask] == -1 ) continue;\\n\\n\\n\\n        // ?????????\\n\\n        if( field[y][x] == 1 ) {\\n\\n          int nbitmask = bitmask;\\n\\n          SET(nbitmask,x,0);\\n\\n          if( x == w-1 ) nbitmask = FIX(nbitmask);\\n\\n          update(dp[!phase][nbitmask],dp[phase][bitmask]);\\n\\n          continue;\\n\\n        }\\n\\n\\n\\n        int nbitmask;\\n\\n        // 0 0\\n\\n        if( !( ( GET(bitmask,x) == 0 ) && ( GET(bitmask,(x+1)) == 0 ) ) ) goto Label1;\\n\\n        if( STAR ) {\\n\\n          // x 0\\n\\n          if( y+1 < h && field[y+1][x] != 1 ) {\\n\\n            nbitmask = bitmask;\\n\\n            SET(nbitmask,x,((field[y][x]==3)+1));\\n\\n            if( x == w-1 ) nbitmask = FIX(nbitmask);\\n\\n          \\n\\n            update(dp[!phase][nbitmask],dp[phase][bitmask]+1);\\n\\n          }\\n\\n          // 0 x\\n\\n          if( x+1 < w && field[y][x+1] != 1 ) {\\n\\n            nbitmask = bitmask;\\n\\n            SET(nbitmask,(x+1),((field[y][x]==3)+1));\\n\\n\\n\\n            if( x == w-1 ) nbitmask = FIX(nbitmask);\\n\\n\\n\\n            update(dp[!phase][nbitmask],dp[phase][bitmask]+1);\\n\\n          }\\n\\n        } else {\\n\\n          // x x\\n\\n          if( x+1 < w && y+1 < h && field[y][x+1] != 1 && field[y+1][x] != 1 ) {\\n\\n            REP(color,1,3){\\n\\n              nbitmask = bitmask;\\n\\n              SET(nbitmask,x,color);\\n\\n              SET(nbitmask,(x+1),color);\\n\\n              if( x == w-1 ) nbitmask = FIX(nbitmask);\\n\\n              update(dp[!phase][nbitmask],(dp[phase][bitmask]+1));\\n\\n            }\\n\\n          }\\n\\n          // 0 0\\n\\n          nbitmask = bitmask;\\n\\n          SET(nbitmask,x,0);\\n\\n          SET(nbitmask,(x+1),0);\\n\\n          if( x == w-1 ) nbitmask = FIX(nbitmask);\\n\\n          update(dp[!phase][nbitmask],dp[phase][bitmask]);\\n\\n        }\\n\\n\\n\\n        continue;\\n\\n      Label1:;\\n\\n        // x 0\\n\\n        // 0 x\\n\\n        int color;\\n\\n        if( !( ( GET(bitmask,x) && ( GET(bitmask,(x+1)) == 0 ) ) || ( GET(bitmask,(x+1)) && ( GET(bitmask,x) == 0 ) ) ) ) goto Label2;\\n\\n        color = GET(bitmask,x) | GET(bitmask,(x+1));\\n\\n        if( STAR ) {\\n\\n          // 0 0\\n\\n          if( ((color==1)?2:3) == field[y][x] ) {\\n\\n            nbitmask = bitmask;\\n\\n            SET(nbitmask,x,0);\\n\\n            SET(nbitmask,(x+1),0);\\n\\n            if( x == w-1 ) nbitmask = FIX(nbitmask);\\n\\n            update(dp[!phase][nbitmask],dp[phase][bitmask]+1);\\n\\n          }\\n\\n        } else {\\n\\n          // x 0\\n\\n          if( y+1 < h && field[y+1][x] != 1 ) {\\n\\n            nbitmask = bitmask;\\n\\n            SET(nbitmask,x,color);\\n\\n            SET(nbitmask,(x+1),0);\\n\\n            if( x == w-1 ) nbitmask = FIX(nbitmask);\\n\\n\\n\\n\\n\\n            update(dp[!phase][nbitmask],(dp[phase][bitmask]+1));\\n\\n          }\\n\\n          \\n\\n          // 0 x\\n\\n          if( x+1 < w && field[y][x+1] != 1 ) {\\n\\n            nbitmask = bitmask;\\n\\n            SET(nbitmask,x,0);\\n\\n            SET(nbitmask,(x+1),color);\\n\\n\\n\\n            if( x == w-1 ) nbitmask = FIX(nbitmask);\\n\\n            update(dp[!phase][nbitmask],(dp[phase][bitmask]+1));\\n\\n          }\\n\\n        }\\n\\n        continue;\\n\\n      Label2:;\\n\\n        // x x\\n\\n        if( !( GET(bitmask,x) && GET(bitmask,(x+1)) ) ) goto Label3;\\n\\n        if( GET(bitmask,x) != GET(bitmask,(x+1)) ) goto Label3;\\n\\n\\n\\n        color = GET(bitmask,x);\\n\\n        \\n\\n        // 0 0\\n\\n        if( !STAR ) {\\n\\n          nbitmask = bitmask;\\n\\n          SET(nbitmask,x,0);\\n\\n          SET(nbitmask,(x+1),0);\\n\\n          if( x == w-1 ) nbitmask = FIX(nbitmask);\\n\\n          update(dp[!phase][nbitmask],dp[phase][bitmask]+1);\\n\\n        }\\n\\n      Label3:;\\n\\n\\n\\n      }\\n\\n      if( field[y][x] == 2 || field[y][x] == 3 ) initter = false;\\n\\n      rep(i,_size) dp[phase][state[i]] = -1;\\n\\n      if( encounter >= 4 && dp[!phase][0] != -1 ) {\\n\\n        mini = min(mini,dp[!phase][0]);\\n\\n      }\\n\\n      phase = !phase;\\n\\n    }\\n\\n  }\\n\\n  if( dp[phase][0] != -1 ) mini = min(mini,dp[phase][0]);\\n\\n  if( mini != IINF ) printf(\\\"%d\\\\n\\\",mini-2);\\n\\n  else puts(\\\"0\\\");\\n\\n}\\n\\n\\n\\nint main(){\\n\\n  while( scanf(\\\"%d %d\\\",&h,&w), h|w ){\\n\\n    rep(i,h) rep(j,w) scanf(\\\"%d\\\",&field[i][j]);\\n\\n    compute();\\n\\n  }\\n\\n  return 0;\\n\\n}\", \"code_v0_loc\": \"214.0\", \"code_v1_loc\": \"178.0\"}\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!head -n 5 data/sample/codex_greedy_outputs_converted.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2ba2e8",
   "metadata": {},
   "source": [
    "### Step 26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbed90f5",
   "metadata": {},
   "source": [
    "Execution Start time: 17:46:53, End time: 17:46:54 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730a815a",
   "metadata": {},
   "source": [
    "**Thought**: The JSONL file contains columns such as `code_v0` and `code_v1`, but it does not have a column named `input`, which is expected by the evaluation script. We need to map the existing columns to the expected ones.\n",
    "\n",
    "Let's modify the `sample_eval_config.yaml` to match the column names in the JSONL file. We'll set `slow_code_col` to `code_v0` and `model_generated_potentially_faster_code_col` to `code_v1`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0f498c",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848935d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\n\nERRORS:\n- E999 IndentationError: unexpected indent\n\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /content/pie-perf/convert_csv_to_jsonl.py (17 lines total)]\n(5 more lines above)\n6:jsonl_file_path = 'data/sample/codex_greedy_outputs_converted.jsonl'\n7:\n8:# Open the CSV file and read its contents\n9:with open(csv_file_path, mode='r', newline='', encoding='utf-8') as csv_file:\n10:    csv_reader = csv.DictReader(csv_file)\n11:slow_code_col: \"code_v0\"\n12:model_generated_potentially_faster_code_col: \"code_v1\"\n13:    with open(jsonl_file_path, mode='w', encoding='utf-8') as jsonl_file:\n14:        for row in csv_reader:\n15:            # Convert each row to a JSON object and write it to the JSONL file\n16:            json.dump(row, jsonl_file)\n17:            jsonl_file.write('\\n')\n-------------------------------------------------\n\nThis is the original code before your edit\n-------------------------------------------------\n[File: /content/pie-perf/convert_csv_to_jsonl.py (17 lines total)]\n(5 more lines above)\n6:jsonl_file_path = 'data/sample/codex_greedy_outputs_converted.jsonl'\n7:\n8:# Open the CSV file and read its contents\n9:with open(csv_file_path, mode='r', newline='', encoding='utf-8') as csv_file:\n10:    csv_reader = csv.DictReader(csv_file)\n11:    \n12:    # Open the JSONL file for writing\n13:    with open(jsonl_file_path, mode='w', encoding='utf-8') as jsonl_file:\n14:        for row in csv_reader:\n15:            # Convert each row to a JSON object and write it to the JSONL file\n16:            json.dump(row, jsonl_file)\n17:            jsonl_file.write('\\n')\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 11:12\n",
    "slow_code_col: \"code_v0\"\n",
    "model_generated_potentially_faster_code_col: \"code_v1\"\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0e58fe",
   "metadata": {},
   "source": [
    "### Step 27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e81ed0",
   "metadata": {},
   "source": [
    "Execution Start time: 17:46:56, End time: 17:46:56 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768e20c8",
   "metadata": {},
   "source": [
    "**Thought**: It seems I mistakenly attempted to edit the wrong file. I intended to edit the `sample_eval_config.yaml` file, not the `convert_csv_to_jsonl.py` file. Let's correct this by opening the correct file and making the necessary changes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb6056f",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d4373a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/pie-perf/data/sample/sample_eval_config.yaml (16 lines total)]\n1:model_generated_outputs_path: \"data/sample/codex_greedy_outputs.jsonl\"\n2:inputs_outputs_basepath: \"data/codenet/public_test_cases/\"\n3:reference_file_path: \"data/sample/py_reference.jsonl\"\n4:output_report_file_path: \"data/sample//codex_greedy_outputs.jsonl.report\"\n5:num_problems_to_evaluate: 10\n6:num_trials: 2\n7:ignore_first_k: 1\n8:max_time_per_run: 10\n9:temp_dir: null\n10:model_generated_potentially_faster_code_col: \"generated_answers\"\n11:slow_code_col: \"input\"\n12:reference_code_col: \"target\"\n13:is_prompt_based: true\n14:cpu_number: 10\n15:return_if_acc_below: 1.0\n16:language: python\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open data/sample/sample_eval_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79659683",
   "metadata": {},
   "source": [
    "### Step 28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e39787",
   "metadata": {},
   "source": [
    "Execution Start time: 17:47:01, End time: 17:47:01 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7f402d",
   "metadata": {},
   "source": [
    "**Thought**: Now that the correct file is open, let's update the `sample_eval_config.yaml` file to match the column names in the JSONL file. We'll set `slow_code_col` to `code_v0` and `model_generated_potentially_faster_code_col` to `code_v1`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aeb44a",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68df2edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/pie-perf/data/sample/sample_eval_config.yaml (16 lines total)]\n1:model_generated_outputs_path: \"data/sample/codex_greedy_outputs.jsonl\"\n2:inputs_outputs_basepath: \"data/codenet/public_test_cases/\"\n3:reference_file_path: \"data/sample/py_reference.jsonl\"\n4:output_report_file_path: \"data/sample//codex_greedy_outputs.jsonl.report\"\n5:num_problems_to_evaluate: 10\n6:num_trials: 2\n7:ignore_first_k: 1\n8:max_time_per_run: 10\n9:temp_dir: null\n10:model_generated_potentially_faster_code_col: \"code_v1\"\n11:slow_code_col: \"code_v0\"\n12:reference_code_col: \"target\"\n13:is_prompt_based: true\n14:cpu_number: 10\n15:return_if_acc_below: 1.0\n16:language: python\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 10:11\n",
    "model_generated_potentially_faster_code_col: \"code_v1\"\n",
    "slow_code_col: \"code_v0\"\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3914a316",
   "metadata": {},
   "source": [
    "### Step 29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc91255",
   "metadata": {},
   "source": [
    "Execution Start time: 17:47:03, End time: 17:47:04 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4375dde",
   "metadata": {},
   "source": [
    "**Thought**: The `sample_eval_config.yaml` file has been successfully updated with the correct column names. Now, let's run the `run_eval.py` script again using the converted JSONL file for the model-generated outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ff1318",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e598a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3653, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 147, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 176, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'input'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/content/pie-perf/src/codenet_eval/run_eval.py\", line 484, in <module>\n    evaluate_generated_outputs(evaluation_config)\n  File \"/content/pie-perf/src/codenet_eval/run_eval.py\", line 43, in evaluate_generated_outputs\n    merged = read_inputs_and_prepare(cfg)\n  File \"/content/pie-perf/src/codenet_eval/run_eval.py\", line 114, in read_inputs_and_prepare\n    gen_df[\"slower_program\"] = gen_df[cfg.slow_code_col].apply(lambda x: x.strip())\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/frame.py\", line 3761, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3655, in get_loc\n    raise KeyError(key) from err\nKeyError: 'input'\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!PYTHONPATH=$(pwd) python src/codenet_eval/run_eval.py --model_generated_outputs_path data/sample/codex_greedy_outputs_converted.jsonl --output_report_file_path data/sample/codex_greedy_outputs_converted.jsonl.report --num_trials 2 --num_problems_to_evaluate 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522b05d7",
   "metadata": {},
   "source": [
    "### Step 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63edbab7",
   "metadata": {},
   "source": [
    "Execution Start time: 17:47:07, End time: 17:47:07 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1f09f3",
   "metadata": {},
   "source": [
    "**Thought**: The error persists, indicating that the script is still trying to access a column named `'input'`. This suggests that there might be another part of the script or configuration that still references this column.\n",
    "\n",
    "Let's search the `run_eval.py` script for any hardcoded references to `'input'` and update them to use the correct column name `'code_v0'`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fbc80b",
   "metadata": {},
   "source": [
    "**Action (search_file)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21afd7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Found 30 matches for \"input\" in /content/pie-perf/src/codenet_eval/run_eval.py:\nLine 4:1. Reads the inputs and model generated programs, writes them to a temporary directory.\nLine 9:export REF_FILE=\"data/codenet/splits/problem_id/2023-01-13_12-56pm/seq2seq_splits/test.jsonl\" && export OP=\"/usr1/amadaan/learning2perf/data/outputs/beam_outputs_codegen_2b_jan_13_split.jsonl\" && export CPU=30 && nohup python -u src/codenet_eval/run_eval.py  --model_generated_outputs_path ${OP} --reference_file_path ${REF_FILE} --output_report_file_path ${OP}.25_tries.report --slow_code_col input --model_generated_potentially_faster_code_col beam_generated_target_from_input --reference_code_col target --num_problems_to_evaluate -1 --cpu_number $CPU --max_time_per_run 10 --num_trials 25 --ignore_first_k 2\nLine 24:from src.codenet_eval.sandbox import run_code_on_inputs\nLine 43:    merged = read_inputs_and_prepare(cfg)\nLine 51:    logging.info(f\"inputs/outputs basepath: {cfg.inputs_outputs_basepath}\")\nLine 53:    # Step 1: Write the inputs and model generated programs to a temporary directory\nLine 64:        (\"input\", f\"_slow.{lang_file_ending}\"),\nLine 68:    # check if there are multiple generations per input\nLine 91:def read_inputs_and_prepare(cfg) -> pd.DataFrame:\nLine 97:        f\"Running each program {cfg.num_trials} times, skipping the first {cfg.ignore_first_k} runs, and getting the input-output pairs from {cfg.inputs_outputs_basepath}\"\nLine 111:            lambda x: get_input_from_prompt(x), axis=1\nLine 120:        ref_df[\"slower_program\"] = ref_df[\"input\"].apply(\nLine 123:            # entire input\nLine 126:        logging.info(f\"Unique inputs in reference: {len(ref_df['slower_program'].unique())}\")\nLine 146:        assert abs(len(merged) - len(gen_df)) < 10, f\"Merging should not lose too many rows! Check if the inputs are the same. Merge lost {len(gen_df) - len(merged)} rows. len(gen_df)={len(gen_df)}, len(merged)={len(merged)}\"\nLine 164:    # if the generated code is a list, then we have multiple generations per input. \nLine 208:                glob.glob(f\"{cfg.inputs_outputs_basepath}/{problem_id}/output*.txt\")\nLine 212:            ), f\"{cfg.inputs_outputs_basepath}/{problem_id} has no ground truth files!\"\nLine 214:                with open(f\"{cfg.inputs_outputs_basepath}/{problem_id}/output.{i}.txt\") as f:\nLine 281:        unit_test_data_basepath = f\"{cfg.inputs_outputs_basepath}/{problem_id}\"\nLine 284:            # run the generated program (maybe faster), input program (slower), and reference program (definitely faster)\nLine 294:                avg_time, std_time, avg_acc = run_code_on_inputs(  # type: ignore\nLine 385:        (run_metrics[f\"{gen_col}_acc\"] > 0.99) & (run_metrics[\"input_acc\"] > 0.99)\nLine 392:        f\"[Reported in CodeNet] input program (ms): {mean_std(run_metrics, 'cpu_time_v0')}\"\nLine 399:    logging.info(f\"[Our measurement] input program (ms): {mean_std(run_metrics, 'input_time')}\")\nLine 413:            f\"Found {len(run_metrics_improved)} problems where the {gen_col} program is faster than the input program\"\nLine 416:            f\"[Our measurement] input program (ms): {mean_std(run_metrics_improved, 'input_time')}\"\nLine 446:        run_metrics[\"input_time_mean\"] - run_metrics[\"reference_time_mean\"]\nLine 447:    ) / run_metrics[\"input_time_mean\"]\nLine 459:def get_input_from_prompt(\nEnd of matches for \"input\" in /content/pie-perf/src/codenet_eval/run_eval.py\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_file 'input' src/codenet_eval/run_eval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9249e44e",
   "metadata": {},
   "source": [
    "### Step 31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f711c3",
   "metadata": {},
   "source": [
    "Execution Start time: 17:47:10, End time: 17:47:10 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2258d988",
   "metadata": {},
   "source": [
    "**Thought**: The search results show multiple references to `'input'` in the `run_eval.py` script. Notably, line 64 mentions `(\"input\", f\"_slow.{lang_file_ending}\")`, which might be where the column name is being set.\n",
    "\n",
    "Let's open the `run_eval.py` script and modify the relevant lines to use the correct column name `'code_v0'`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1f50e1",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5d78aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/pie-perf/src/codenet_eval/run_eval.py (484 lines total)]\n1:\"\"\"\n2:Runs evaluation on the model generated outputs. The rough flow is as follows:\n3:\n4:1. Reads the inputs and model generated programs, writes them to a temporary directory.\n5:2. Runs each program several times and computes the average time taken and accuracy.\n6:3. Writes the results to a json file.\n7:\n8:Sample usage:\n9:export REF_FILE=\"data/codenet/splits/problem_id/2023-01-13_12-56pm/seq2seq_splits/test.jsonl\" && export OP=\"/usr1/amadaan/learning2perf/data/outputs/beam_outputs_codegen_2b_jan_13_split.jsonl\" && export CPU=30 && nohup python -u src/codenet_eval/run_eval.py  --model_generated_outputs_path ${OP} --reference_file_path ${REF_FILE} --output_report_file_path ${OP}.25_tries.report --slow_code_col input --model_generated_potentially_faster_code_col beam_generated_target_from_input --reference_code_col target --num_problems_to_evaluate -1 --cpu_number $CPU --max_time_per_run 10 --num_trials 25 --ignore_first_k 2\n10:\n11:\"\"\"\n12:import pathlib\n13:import tempfile\n14:from tqdm import tqdm\n15:import pandas as pd\n16:from typing import Dict, List, Tuple\n17:import os\n18:import logging\n19:import glob\n20:import numpy as np\n21:from collections import defaultdict\n22:\n23:from src.codenet_eval.evalconfig import EvaluationConfig\n24:from src.codenet_eval.sandbox import run_code_on_inputs\n25:\n26:import pdb\n27:\n28:logging.basicConfig(level=logging.CRITICAL)\n29:\n30:lang2file_ending = {\n31:    \"python\": \"py\", \n32:    \"cpp\": \"cpp\"\n33:}\n34:    \n35:\n36:\n37:    \n38:\n39:def evaluate_generated_outputs(cfg: EvaluationConfig) -> None:\n40:    \"\"\"Evaluates model generated programs for accuracy and runtime. See the documenation of the EvaluationConfig class for more details.\"\"\"\n41:\n42:    # Step 0\n43:    merged = read_inputs_and_prepare(cfg)\n44:    logging.info(f\"Number of programs to evaluate: {len(merged)}\")\n45:    logging.info(f\"Number of trials per program: {cfg.num_trials}\")\n46:    logging.info(f\"Number of trials to ignore: {cfg.ignore_first_k}\")\n47:    logging.info(f\"Maximum time per run: {cfg.max_time_per_run}\")\n48:    logging.info(f\"Input column: {cfg.slow_code_col}\")\n49:    logging.info(f\"Reference column: {cfg.reference_code_col}\")\n50:    logging.info(f\"Model generated column: {cfg.model_generated_potentially_faster_code_col}\")\n51:    logging.info(f\"inputs/outputs basepath: {cfg.inputs_outputs_basepath}\")\n52:\n53:    # Step 1: Write the inputs and model generated programs to a temporary directory\n54:    problem_id_to_ground_truths, output_code_location = write_programs_read_ground_truth(\n55:        cfg, merged\n56:    )\n57:\n58:    # Step 2: run the programs\n59:    \n60:    # (dataframe col, suffix for the file)\n61:    \n62:    lang_file_ending = lang2file_ending[cfg.language]\n63:    tag_to_path = [\n64:        (\"input\", f\"_slow.{lang_file_ending}\"),\n65:        (\"reference\", f\"_reference.{lang_file_ending}\"),\n66:    ]\n67:    \n68:    # check if there are multiple generations per input\n69:    is_multigen = isinstance(merged[cfg.model_generated_potentially_faster_code_col].iloc[0], list)\n70:    if is_multigen:\n71:        \n72:        num_generations = len(merged[cfg.model_generated_potentially_faster_code_col].iloc[0])\n73:        tag_to_path.extend([(f\"{cfg.model_generated_potentially_faster_code_col}_{i}\", f\"_maybe_faster_{i}.{lang_file_ending}\") for i in range(num_generations)])\n74:\n75:    else:\n76:        tag_to_path.append((cfg.model_generated_potentially_faster_code_col, f\"_maybe_faster_0.{lang_file_ending}\"))\n77:    \n78:    results = run_programs(cfg, merged, problem_id_to_ground_truths, output_code_location, tag_to_path)\n79:    \n80:    if is_multigen:\n81:        results = get_best_generation_per_submission(results, gen_col=cfg.model_generated_potentially_faster_code_col)\n82:    \n83:    # Step 3: summarize the results, write report\n84:    print_summary(cfg, merged, results, gen_col=cfg.model_generated_potentially_faster_code_col)\n85:\n86:    if isinstance(cfg.temp_dir, tempfile.TemporaryDirectory):\n87:        cfg.temp_dir.cleanup()\n88:        \n89:\n90:\n91:def read_inputs_and_prepare(cfg) -> pd.DataFrame:\n92:    \"\"\"Reads the model generated output, the reference, joins them, and returns a dataframe with the merged data.\"\"\"\n93:    logging.info(f\"Reading reference file from {cfg.reference_file_path}\")\n94:    logging.info(f\"Reading model generated outputs from {cfg.model_generated_outputs_path}\")\n95:\n96:    logging.info(\n97:        f\"Running each program {cfg.num_trials} times, skipping the first {cfg.ignore_first_k} runs, and getting the input-output pairs from {cfg.inputs_outputs_basepath}\"\n98:    )\n99:    \n100:    gen_df = pd.read_json(\n(384 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open src/codenet_eval/run_eval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0762d8fa",
   "metadata": {},
   "source": [
    "### Step 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5771e537",
   "metadata": {},
   "source": [
    "Execution Start time: 17:47:12, End time: 17:47:13 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279f1d40",
   "metadata": {},
   "source": [
    "**Thought**: The line 64 in the `run_eval.py` script is setting the tag for the input column as `\"input\"`. We need to change this to match the column name in our JSONL file, which is `\"code_v0\"`.\n",
    "\n",
    "Let's edit this line to use the correct column name.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f340b71",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123d70eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/pie-perf/src/codenet_eval/run_eval.py (484 lines total)]\n(13 more lines above)\n14:from tqdm import tqdm\n15:import pandas as pd\n16:from typing import Dict, List, Tuple\n17:import os\n18:import logging\n19:import glob\n20:import numpy as np\n21:from collections import defaultdict\n22:\n23:from src.codenet_eval.evalconfig import EvaluationConfig\n24:from src.codenet_eval.sandbox import run_code_on_inputs\n25:\n26:import pdb\n27:\n28:logging.basicConfig(level=logging.CRITICAL)\n29:\n30:lang2file_ending = {\n31:    \"python\": \"py\", \n32:    \"cpp\": \"cpp\"\n33:}\n34:    \n35:\n36:\n37:    \n38:\n39:def evaluate_generated_outputs(cfg: EvaluationConfig) -> None:\n40:    \"\"\"Evaluates model generated programs for accuracy and runtime. See the documenation of the EvaluationConfig class for more details.\"\"\"\n41:\n42:    # Step 0\n43:    merged = read_inputs_and_prepare(cfg)\n44:    logging.info(f\"Number of programs to evaluate: {len(merged)}\")\n45:    logging.info(f\"Number of trials per program: {cfg.num_trials}\")\n46:    logging.info(f\"Number of trials to ignore: {cfg.ignore_first_k}\")\n47:    logging.info(f\"Maximum time per run: {cfg.max_time_per_run}\")\n48:    logging.info(f\"Input column: {cfg.slow_code_col}\")\n49:    logging.info(f\"Reference column: {cfg.reference_code_col}\")\n50:    logging.info(f\"Model generated column: {cfg.model_generated_potentially_faster_code_col}\")\n51:    logging.info(f\"inputs/outputs basepath: {cfg.inputs_outputs_basepath}\")\n52:\n53:    # Step 1: Write the inputs and model generated programs to a temporary directory\n54:    problem_id_to_ground_truths, output_code_location = write_programs_read_ground_truth(\n55:        cfg, merged\n56:    )\n57:\n58:    # Step 2: run the programs\n59:    \n60:    # (dataframe col, suffix for the file)\n61:    \n62:    lang_file_ending = lang2file_ending[cfg.language]\n63:    tag_to_path = [\n64:        (\"code_v0\", f\"_slow.{lang_file_ending}\"),\n65:        (\"reference\", f\"_reference.{lang_file_ending}\"),\n66:    ]\n67:    \n68:    # check if there are multiple generations per input\n69:    is_multigen = isinstance(merged[cfg.model_generated_potentially_faster_code_col].iloc[0], list)\n70:    if is_multigen:\n71:        \n72:        num_generations = len(merged[cfg.model_generated_potentially_faster_code_col].iloc[0])\n73:        tag_to_path.extend([(f\"{cfg.model_generated_potentially_faster_code_col}_{i}\", f\"_maybe_faster_{i}.{lang_file_ending}\") for i in range(num_generations)])\n74:\n75:    else:\n76:        tag_to_path.append((cfg.model_generated_potentially_faster_code_col, f\"_maybe_faster_0.{lang_file_ending}\"))\n77:    \n78:    results = run_programs(cfg, merged, problem_id_to_ground_truths, output_code_location, tag_to_path)\n79:    \n80:    if is_multigen:\n81:        results = get_best_generation_per_submission(results, gen_col=cfg.model_generated_potentially_faster_code_col)\n82:    \n83:    # Step 3: summarize the results, write report\n84:    print_summary(cfg, merged, results, gen_col=cfg.model_generated_potentially_faster_code_col)\n85:\n86:    if isinstance(cfg.temp_dir, tempfile.TemporaryDirectory):\n87:        cfg.temp_dir.cleanup()\n88:        \n89:\n90:\n91:def read_inputs_and_prepare(cfg) -> pd.DataFrame:\n92:    \"\"\"Reads the model generated output, the reference, joins them, and returns a dataframe with the merged data.\"\"\"\n93:    logging.info(f\"Reading reference file from {cfg.reference_file_path}\")\n94:    logging.info(f\"Reading model generated outputs from {cfg.model_generated_outputs_path}\")\n95:\n96:    logging.info(\n97:        f\"Running each program {cfg.num_trials} times, skipping the first {cfg.ignore_first_k} runs, and getting the input-output pairs from {cfg.inputs_outputs_basepath}\"\n98:    )\n99:    \n100:    gen_df = pd.read_json(\n101:        cfg.model_generated_outputs_path, lines=True, orient=\"records\"\n102:    )\n103:    \n104:    # if file ends in .report, then it is a re-run. We can filter out the rows that have already been run and just return the new rows\n105:    if cfg.model_generated_outputs_path.endswith(\".report\"):\n106:        return _prepare_for_rerun(gen_df, cfg)\n107:    \n108:    logging.info(f\"Read {len(gen_df)} rows from {cfg.model_generated_outputs_path}\")\n109:    if cfg.is_prompt_based:\n110:        gen_df[\"slower_program\"] = gen_df.apply(\n111:            lambda x: get_input_from_prompt(x), axis=1\n112:        )\n113:    else:\n(371 more lines below)\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 64:64\n",
    "        (\"code_v0\", f\"_slow.{lang_file_ending}\"),\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c48f845",
   "metadata": {},
   "source": [
    "### Step 33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f112ea3",
   "metadata": {},
   "source": [
    "Execution Start time: 17:47:18, End time: 17:47:19 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598496b7",
   "metadata": {},
   "source": [
    "**Thought**: The `run_eval.py` script has been successfully updated to use the correct column name `code_v0`. Now, let's run the script again using the converted JSONL file for the model-generated outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c245bf3",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0b7a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3653, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 147, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 176, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'input'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/content/pie-perf/src/codenet_eval/run_eval.py\", line 484, in <module>\n    evaluate_generated_outputs(evaluation_config)\n  File \"/content/pie-perf/src/codenet_eval/run_eval.py\", line 43, in evaluate_generated_outputs\n    merged = read_inputs_and_prepare(cfg)\n  File \"/content/pie-perf/src/codenet_eval/run_eval.py\", line 114, in read_inputs_and_prepare\n    gen_df[\"slower_program\"] = gen_df[cfg.slow_code_col].apply(lambda x: x.strip())\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/frame.py\", line 3761, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3655, in get_loc\n    raise KeyError(key) from err\nKeyError: 'input'\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!PYTHONPATH=$(pwd) python src/codenet_eval/run_eval.py --model_generated_outputs_path data/sample/codex_greedy_outputs_converted.jsonl --output_report_file_path data/sample/codex_greedy_outputs_converted.jsonl.report --num_trials 2 --num_problems_to_evaluate 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f5eb55",
   "metadata": {},
   "source": [
    "### Step 34"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dc9c23",
   "metadata": {},
   "source": [
    "Execution Start time: 17:47:22, End time: 17:47:22 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0335e85",
   "metadata": {},
   "source": [
    "**Thought**: The error persists, indicating that there might be another reference to the `'input'` column elsewhere in the script or configuration. We need to ensure that all references to `'input'` are updated to `'code_v0'`.\n",
    "\n",
    "Let's search the entire repository for any remaining references to `'input'` to ensure we have updated all occurrences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d259aa",
   "metadata": {},
   "source": [
    "**Action (search_dir)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd2be71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "More than 741 files matched for \"input\" in /content/pie-perf. Please narrow your search.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_dir 'input'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6568325",
   "metadata": {},
   "source": [
    "### Step 35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9507da",
   "metadata": {},
   "source": [
    "Execution Start time: 17:47:24, End time: 17:47:24 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba58c4b",
   "metadata": {},
   "source": [
    "**Thought**: The search returned too many results, indicating that `'input'` is a common term in the repository. To narrow down the search, let's focus on the `src/codenet_eval` directory, as it contains the relevant evaluation scripts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42169ecd",
   "metadata": {},
   "source": [
    "**Action (search_dir)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad1d712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Found 81 matches for \"input\" in /content/pie-perf/src/codenet_eval:\n/content/pie-perf/src/codenet_eval/eval_submissions/all_submissions_time_eval.py (4 matches)\n/content/pie-perf/src/codenet_eval/eval_submissions/prep_for_eval.py (1 matches)\n/content/pie-perf/src/codenet_eval/evalconfig.py (7 matches)\n/content/pie-perf/src/codenet_eval/run_eval.py (29 matches)\n/content/pie-perf/src/codenet_eval/sandbox.py (40 matches)\nEnd of matches for \"input\" in /content/pie-perf/src/codenet_eval\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_dir 'input' src/codenet_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4084d7",
   "metadata": {},
   "source": [
    "### Step 36"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1de6b6",
   "metadata": {},
   "source": [
    "Execution Start time: 17:47:27, End time: 17:47:27 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be9d45b",
   "metadata": {},
   "source": [
    "**Thought**: The `run_eval.py` script has multiple references to `'input'`, and there are also references in `evalconfig.py` and `sandbox.py`. We need to ensure that all relevant references are updated to `'code_v0'`.\n",
    "\n",
    "Let's start by reviewing the `evalconfig.py` file to see if there are any references to `'input'` that need updating.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333a923e",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cb1e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/pie-perf/src/codenet_eval/evalconfig.py (178 lines total)]\n1:from typing import Any\n2:import yaml\n3:import argparse\n4:from dataclasses import dataclass\n5:\n6:\n7:@dataclass\n8:class EvaluationConfig:\n9:\n10:    # where is the input/output\n11:    model_generated_outputs_path: str  #  the file to be evaluated\n12:    inputs_outputs_basepath: str  # the basepath of the input and output files for evaluating each problem. unit tests are located in ${inputs_outputs_basepath}/${problem_id}/{input.txt, output.txt}\n13:    reference_file_path: str  # the reference file. Sometimes the reference file is not needed, when the outputs are located in the evaluation file\n14:    output_report_file_path: str  #  the output report file path.\n15:    \n16:    # language\n17:    language: str # programming language (e.g. python/cpp)\n18:\n19:    # parameters for running evaluation\n20:    num_problems_to_evaluate: int  # number of problems to evaluate. -1 means all problems.\n21:    num_trials: int  # number of times to execute each program for a test case\n22:    ignore_first_k: int  # ignore the first k runs\n23:    max_time_per_run: int  # maximum time to allow each run to take\n24:    temp_dir: Any  # temporary directory to use for writing the programs that are evaluated\n25:\n26:\n27:    # the columns for input, output, reference\n28:    model_generated_potentially_faster_code_col: str  # column in the output file that contains the model generated code\n29:    slow_code_col: str  # column in the output file that contains the slow code\n30:    reference_code_col: str  # column in the output file that contains the reference code\n31:    is_prompt_based: bool  # whether the outputs are generated from a few-shot model\n32:\n33:    cpu_number: int  # we use taskset to run the code on a specific cpu\n34:    \n35:    # optional parameters    \n36:    return_if_acc_below: float = None #  type: ignore | if the accuracy is below this value, then return immediately\n37:    cpp_results_path: str = None #  type: ignore | the path to the results of compiling and running the ref cpp code\n38:    cflags: str = None #  type: ignore | the cflags to use for compiling the code\n39:    \n40:\n41:    @classmethod\n42:    def from_args(cls, args):\n43:        return cls(\n44:            model_generated_outputs_path=args.model_generated_outputs_path,\n45:            inputs_outputs_basepath=args.inputs_outputs_basepath,\n46:            reference_file_path=args.reference_file_path,\n47:            num_problems_to_evaluate=args.num_problems_to_evaluate,\n48:            language=args.language, \n49:            num_trials=args.num_trials,\n50:            ignore_first_k=args.ignore_first_k,\n51:            max_time_per_run=args.max_time_per_run,\n52:            temp_dir=args.temp_dir,\n53:            model_generated_potentially_faster_code_col=args.model_generated_potentially_faster_code_col,\n54:            slow_code_col=args.slow_code_col,\n55:            reference_code_col=args.reference_code_col,\n56:            is_prompt_based=args.is_prompt_based,\n57:            output_report_file_path=args.output_report_file_path,\n58:            cpu_number=args.cpu_number,\n59:            cflags=args.cflags,\n60:            cpp_results_path=args.cpp_results_path,\n61:            return_if_acc_below=args.return_if_acc_below,\n62:        )\n63:\n64:    @classmethod\n65:    def from_yaml(cls, yaml_file: str) -> \"EvaluationConfig\":\n66:        with open(yaml_file) as f:\n67:            data = yaml.load(f, Loader=yaml.FullLoader)\n68:            return cls(**data)\n69:\n70:    @classmethod\n71:    def from_dict(cls, data: dict) -> \"EvaluationConfig\":\n72:        return cls(**data)\n73:    \n74:    def to_yaml(self) -> str:\n75:        return yaml.dump(self.__dict__)\n76:    \n77:    @staticmethod\n78:    def get_args() -> argparse.ArgumentParser:\n79:        \"\"\"Returns an ArgumentParser for the evaluation config.\"\"\"\n80:        args = argparse.ArgumentParser()\n81:\n82:        args.add_argument(\"--reference_file_path\", type=str)\n83:\n84:        args.add_argument(\n85:            \"--model_generated_outputs_path\",\n86:            type=str,\n87:            help=\"The output file path. It should be a jsonl file with outputs in `model_generated_potentially_faster_code_col`. See sample in `data/codenet/unit_test/output.jsonl`\",\n88:        )\n89:        \n90:        args.add_argument(\"--language\", type=str, help=\"programmming language (e.g. 'python' or 'cpp')\")\n91:\n92:        args.add_argument(\n93:            \"--model_generated_output_col\",\n94:            type=str,\n95:            default=\"generated_answer\",\n96:            help=\"Column name for the generated output\",\n97:        )\n98:\n99:        args.add_argument(\n100:            \"--model_generated_potentially_faster_code_col\",\n(78 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open src/codenet_eval/evalconfig.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784a1ddb",
   "metadata": {},
   "source": [
    "### Step 37"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0297d383",
   "metadata": {},
   "source": [
    "Execution Start time: 17:47:30, End time: 17:47:30 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df6eef3",
   "metadata": {},
   "source": [
    "**Thought**: The `evalconfig.py` file defines the configuration parameters, including `slow_code_col` and `model_generated_potentially_faster_code_col`, which we have already updated in the YAML file. There are no hardcoded references to `'input'` here.\n",
    "\n",
    "Let's proceed to check the `sandbox.py` file for any hardcoded references to `'input'` that might need updating.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19722ad5",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cc842c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/pie-perf/src/codenet_eval/sandbox.py (589 lines total)]\n1:# utilities and code that handles actual execution of programs\n2:import pathlib\n3:import shlex\n4:import subprocess\n5:from typing import Dict, List, Tuple, Union\n6:import time\n7:import numpy as np\n8:import resource\n9:import logging\n10:import psutil\n11:import os\n12:import sys\n13:import traceback\n14:import pdb\n15:import glob\n16:\n17:# disable logging from psutil\n18:logging.getLogger(\"psutil\").setLevel(logging.WARNING)\n19:\n20:# disable logging from resource\n21:logging.getLogger(\"resource\").setLevel(logging.WARNING)\n22:\n23:# disable logging from subprocess\n24:logging.getLogger(\"subprocess\").setLevel(logging.WARNING)\n25:\n26:logging.basicConfig(level=logging.CRITICAL)\n27:\n28:DEBUG=True\n29:\n30:\n31:def is_linux():\n32:    from sys import platform\n33:    if platform == \"linux\" or platform == \"linux2\":\n34:        return True\n35:    else: \n36:        return False\n37:\n38:\n39:def run_python_code_on_inputs(\n40:    code_path: str,\n41:    unit_test_data_basepath: str,\n42:    num_runs_per_test_case: int,\n43:    ignore_first_k: int,\n44:    max_seconds_per_run: int,\n45:    ground_truths: List[str] = None,  # type: ignore\n46:    num_test_cases: int = None,  # type: ignore\n47:    cpu_number: int = 1,  # which CPU to run the code on, counting begins from 1\n48:    return_per_trial_times: bool = False,\n49:    python_bin: str = \"python\",\n50:    return_dict: bool = False,\n51:    cflags: str = None, # type: ignore\n52:    return_if_acc_below: float = None,  # type: ignore | if the accuracy is below this value, then return\n53:) -> Union[Tuple[float, float, float], Tuple[float, float, float, List[List[float]]], Dict]:\n54:    \"\"\"\n55:    Run the given code on the inputs for the given problem_id, and returns (avg_time, std_time, avg_acc).\n56:    The inputs are sourced from the unit test data, where a number of files of the form: {input,output}.{0, 1, 2}.txt are present.\n57:\n58:\n59:    NOTE: It is optional to pass ground_truths. If they are not passed, then the accuracy will not be computed.\n60:\n61:\n62:    \"\"\"\n63:\n64:    if num_test_cases is None:\n65:        num_test_cases = len(ground_truths)\n66:\n67:    times_millisec, accs = [], []\n68:    per_trial_times = []\n69:    for test_case_idx in range(num_test_cases):\n70:        if is_linux(): \n71:            cmd = (\n72:                f\"taskset --cpu-list {cpu_number} {python_bin} {code_path}\"  # taskset 00 python code.py\n73:            )\n74:        else: \n75:            cmd = f\"{python_bin} {code_path}\"\n76:        subprocess_args = shlex.split(cmd)\n77:        input_file_path = f\"{unit_test_data_basepath}/input.{test_case_idx}.txt\"\n78:        # logging.info(f\"Running command: {cmd} < {input_file_path}\")\n79:        _per_trial_times = []\n80:        for trial_idx in range(num_runs_per_test_case):\n81:            try:\n82:                time_start = time.time()\n83:                output = run_cmd_for_time_eval(\n84:                    subprocess_args,\n85:                    input_file_path=input_file_path,\n86:                    timeout_seconds=max_seconds_per_run,\n87:                )\n88:                time_taken = time.time() - time_start\n89:                _per_trial_times.append(time_taken)\n90:                if output is None:\n91:                    return (np.nan, np.nan, 0)\n92:                    # timeout: since we have a generous timeout, this should not happen\n93:\n94:                if trial_idx >= ignore_first_k:\n95:                    times_millisec.append(time_taken * 1000)\n96:                    if ground_truths is not None:\n97:                        accuracy = get_accuracy(output, ground_truths[test_case_idx])\n98:                        if return_if_acc_below is not None and accuracy < return_if_acc_below:\n99:                            logging.info(f\"Accuracy {accuracy} below {return_if_acc_below}. Returning.\")\n100:                            return (time_taken, 0, accuracy)\n(489 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open src/codenet_eval/sandbox.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9b6068",
   "metadata": {},
   "source": [
    "### Step 38"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ce31cf",
   "metadata": {},
   "source": [
    "Execution Start time: 17:47:34, End time: 17:47:35 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e90364",
   "metadata": {},
   "source": [
    "**Thought**: The `sandbox.py` file contains a function `run_python_code_on_inputs` that references input files using the pattern `{input,output}.{0, 1, 2}.txt`. This suggests that the script expects input files in a specific format, which might not be directly related to the column name issue.\n",
    "\n",
    "Since the error persists, it might be beneficial to revisit the `run_eval.py` script and ensure that all references to the column name are correctly updated. Let's search for any remaining references to `'input'` in the `run_eval.py` script specifically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa550d8",
   "metadata": {},
   "source": [
    "**Action (search_file)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c62ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Found 29 matches for \"input\" in /content/pie-perf/src/codenet_eval/run_eval.py:\nLine 4:1. Reads the inputs and model generated programs, writes them to a temporary directory.\nLine 9:export REF_FILE=\"data/codenet/splits/problem_id/2023-01-13_12-56pm/seq2seq_splits/test.jsonl\" && export OP=\"/usr1/amadaan/learning2perf/data/outputs/beam_outputs_codegen_2b_jan_13_split.jsonl\" && export CPU=30 && nohup python -u src/codenet_eval/run_eval.py  --model_generated_outputs_path ${OP} --reference_file_path ${REF_FILE} --output_report_file_path ${OP}.25_tries.report --slow_code_col input --model_generated_potentially_faster_code_col beam_generated_target_from_input --reference_code_col target --num_problems_to_evaluate -1 --cpu_number $CPU --max_time_per_run 10 --num_trials 25 --ignore_first_k 2\nLine 24:from src.codenet_eval.sandbox import run_code_on_inputs\nLine 43:    merged = read_inputs_and_prepare(cfg)\nLine 51:    logging.info(f\"inputs/outputs basepath: {cfg.inputs_outputs_basepath}\")\nLine 53:    # Step 1: Write the inputs and model generated programs to a temporary directory\nLine 68:    # check if there are multiple generations per input\nLine 91:def read_inputs_and_prepare(cfg) -> pd.DataFrame:\nLine 97:        f\"Running each program {cfg.num_trials} times, skipping the first {cfg.ignore_first_k} runs, and getting the input-output pairs from {cfg.inputs_outputs_basepath}\"\nLine 111:            lambda x: get_input_from_prompt(x), axis=1\nLine 120:        ref_df[\"slower_program\"] = ref_df[\"input\"].apply(\nLine 123:            # entire input\nLine 126:        logging.info(f\"Unique inputs in reference: {len(ref_df['slower_program'].unique())}\")\nLine 146:        assert abs(len(merged) - len(gen_df)) < 10, f\"Merging should not lose too many rows! Check if the inputs are the same. Merge lost {len(gen_df) - len(merged)} rows. len(gen_df)={len(gen_df)}, len(merged)={len(merged)}\"\nLine 164:    # if the generated code is a list, then we have multiple generations per input. \nLine 208:                glob.glob(f\"{cfg.inputs_outputs_basepath}/{problem_id}/output*.txt\")\nLine 212:            ), f\"{cfg.inputs_outputs_basepath}/{problem_id} has no ground truth files!\"\nLine 214:                with open(f\"{cfg.inputs_outputs_basepath}/{problem_id}/output.{i}.txt\") as f:\nLine 281:        unit_test_data_basepath = f\"{cfg.inputs_outputs_basepath}/{problem_id}\"\nLine 284:            # run the generated program (maybe faster), input program (slower), and reference program (definitely faster)\nLine 294:                avg_time, std_time, avg_acc = run_code_on_inputs(  # type: ignore\nLine 385:        (run_metrics[f\"{gen_col}_acc\"] > 0.99) & (run_metrics[\"input_acc\"] > 0.99)\nLine 392:        f\"[Reported in CodeNet] input program (ms): {mean_std(run_metrics, 'cpu_time_v0')}\"\nLine 399:    logging.info(f\"[Our measurement] input program (ms): {mean_std(run_metrics, 'input_time')}\")\nLine 413:            f\"Found {len(run_metrics_improved)} problems where the {gen_col} program is faster than the input program\"\nLine 416:            f\"[Our measurement] input program (ms): {mean_std(run_metrics_improved, 'input_time')}\"\nLine 446:        run_metrics[\"input_time_mean\"] - run_metrics[\"reference_time_mean\"]\nLine 447:    ) / run_metrics[\"input_time_mean\"]\nLine 459:def get_input_from_prompt(\nEnd of matches for \"input\" in /content/pie-perf/src/codenet_eval/run_eval.py\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_file 'input' src/codenet_eval/run_eval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d45bff",
   "metadata": {},
   "source": [
    "### Step 39"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4476136",
   "metadata": {},
   "source": [
    "Execution Start time: 17:47:37, End time: 17:47:38 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079b1f2d",
   "metadata": {},
   "source": [
    "**Thought**: The search results show that line 120 in the `run_eval.py` script directly accesses the `input` column. We need to update this line to use the correct column name `code_v0`.\n",
    "\n",
    "Let's edit the `run_eval.py` script to update this reference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52426317",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76dd7c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\n\nERRORS:\n- E999 SyntaxError: '(' was never closed\n\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /content/pie-perf/src/codenet_eval/sandbox.py (589 lines total)]\n(114 more lines above)\n114:        return {\n115:            \"avg_time\": np.mean(times_millisec),\n116:            \"std_time\": np.std(times_millisec),\n117:            \"avg_acc\": np.mean(accs),\n118:        }\n119:    else:\n120:        ref_df[\"slower_program\"] = ref_df[\"code_v0\"].apply(\n121:\n122:\n123:# Maximal virtual memory for subprocesses (in bytes).\n124:MAX_VIRTUAL_MEMORY = 10 * 1024 * 1024 * 50  # 500 MB\n(465 more lines below)\n-------------------------------------------------\n\nThis is the original code before your edit\n-------------------------------------------------\n[File: /content/pie-perf/src/codenet_eval/sandbox.py (589 lines total)]\n(115 more lines above)\n115:            \"avg_time\": np.mean(times_millisec),\n116:            \"std_time\": np.std(times_millisec),\n117:            \"avg_acc\": np.mean(accs),\n118:        }\n119:    else:\n120:        return np.mean(times_millisec), np.std(times_millisec), np.mean(accs)  # type: ignore\n121:\n122:\n123:# Maximal virtual memory for subprocesses (in bytes).\n124:MAX_VIRTUAL_MEMORY = 10 * 1024 * 1024 * 50  # 500 MB\n125:\n(464 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 120:120\n",
    "        ref_df[\"slower_program\"] = ref_df[\"code_v0\"].apply(\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107dbfcd",
   "metadata": {},
   "source": [
    "### Step 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181c315d",
   "metadata": {},
   "source": [
    "Execution Start time: 17:47:40, End time: 17:47:40 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6ef8be",
   "metadata": {},
   "source": [
    "**Thought**: It seems there was a mistake in specifying the line number or the edit content. I intended to edit the `run_eval.py` file, not the `sandbox.py` file. Let's correct this by opening the correct file and making the necessary changes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ba4346",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f7b422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/pie-perf/src/codenet_eval/run_eval.py (484 lines total)]\n1:\"\"\"\n2:Runs evaluation on the model generated outputs. The rough flow is as follows:\n3:\n4:1. Reads the inputs and model generated programs, writes them to a temporary directory.\n5:2. Runs each program several times and computes the average time taken and accuracy.\n6:3. Writes the results to a json file.\n7:\n8:Sample usage:\n9:export REF_FILE=\"data/codenet/splits/problem_id/2023-01-13_12-56pm/seq2seq_splits/test.jsonl\" && export OP=\"/usr1/amadaan/learning2perf/data/outputs/beam_outputs_codegen_2b_jan_13_split.jsonl\" && export CPU=30 && nohup python -u src/codenet_eval/run_eval.py  --model_generated_outputs_path ${OP} --reference_file_path ${REF_FILE} --output_report_file_path ${OP}.25_tries.report --slow_code_col input --model_generated_potentially_faster_code_col beam_generated_target_from_input --reference_code_col target --num_problems_to_evaluate -1 --cpu_number $CPU --max_time_per_run 10 --num_trials 25 --ignore_first_k 2\n10:\n11:\"\"\"\n12:import pathlib\n13:import tempfile\n14:from tqdm import tqdm\n15:import pandas as pd\n16:from typing import Dict, List, Tuple\n17:import os\n18:import logging\n19:import glob\n20:import numpy as np\n21:from collections import defaultdict\n22:\n23:from src.codenet_eval.evalconfig import EvaluationConfig\n24:from src.codenet_eval.sandbox import run_code_on_inputs\n25:\n26:import pdb\n27:\n28:logging.basicConfig(level=logging.CRITICAL)\n29:\n30:lang2file_ending = {\n31:    \"python\": \"py\", \n32:    \"cpp\": \"cpp\"\n33:}\n34:    \n35:\n36:\n37:    \n38:\n39:def evaluate_generated_outputs(cfg: EvaluationConfig) -> None:\n40:    \"\"\"Evaluates model generated programs for accuracy and runtime. See the documenation of the EvaluationConfig class for more details.\"\"\"\n41:\n42:    # Step 0\n43:    merged = read_inputs_and_prepare(cfg)\n44:    logging.info(f\"Number of programs to evaluate: {len(merged)}\")\n45:    logging.info(f\"Number of trials per program: {cfg.num_trials}\")\n46:    logging.info(f\"Number of trials to ignore: {cfg.ignore_first_k}\")\n47:    logging.info(f\"Maximum time per run: {cfg.max_time_per_run}\")\n48:    logging.info(f\"Input column: {cfg.slow_code_col}\")\n49:    logging.info(f\"Reference column: {cfg.reference_code_col}\")\n50:    logging.info(f\"Model generated column: {cfg.model_generated_potentially_faster_code_col}\")\n51:    logging.info(f\"inputs/outputs basepath: {cfg.inputs_outputs_basepath}\")\n52:\n53:    # Step 1: Write the inputs and model generated programs to a temporary directory\n54:    problem_id_to_ground_truths, output_code_location = write_programs_read_ground_truth(\n55:        cfg, merged\n56:    )\n57:\n58:    # Step 2: run the programs\n59:    \n60:    # (dataframe col, suffix for the file)\n61:    \n62:    lang_file_ending = lang2file_ending[cfg.language]\n63:    tag_to_path = [\n64:        (\"code_v0\", f\"_slow.{lang_file_ending}\"),\n65:        (\"reference\", f\"_reference.{lang_file_ending}\"),\n66:    ]\n67:    \n68:    # check if there are multiple generations per input\n69:    is_multigen = isinstance(merged[cfg.model_generated_potentially_faster_code_col].iloc[0], list)\n70:    if is_multigen:\n71:        \n72:        num_generations = len(merged[cfg.model_generated_potentially_faster_code_col].iloc[0])\n73:        tag_to_path.extend([(f\"{cfg.model_generated_potentially_faster_code_col}_{i}\", f\"_maybe_faster_{i}.{lang_file_ending}\") for i in range(num_generations)])\n74:\n75:    else:\n76:        tag_to_path.append((cfg.model_generated_potentially_faster_code_col, f\"_maybe_faster_0.{lang_file_ending}\"))\n77:    \n78:    results = run_programs(cfg, merged, problem_id_to_ground_truths, output_code_location, tag_to_path)\n79:    \n80:    if is_multigen:\n81:        results = get_best_generation_per_submission(results, gen_col=cfg.model_generated_potentially_faster_code_col)\n82:    \n83:    # Step 3: summarize the results, write report\n84:    print_summary(cfg, merged, results, gen_col=cfg.model_generated_potentially_faster_code_col)\n85:\n86:    if isinstance(cfg.temp_dir, tempfile.TemporaryDirectory):\n87:        cfg.temp_dir.cleanup()\n88:        \n89:\n90:\n91:def read_inputs_and_prepare(cfg) -> pd.DataFrame:\n92:    \"\"\"Reads the model generated output, the reference, joins them, and returns a dataframe with the merged data.\"\"\"\n93:    logging.info(f\"Reading reference file from {cfg.reference_file_path}\")\n94:    logging.info(f\"Reading model generated outputs from {cfg.model_generated_outputs_path}\")\n95:\n96:    logging.info(\n97:        f\"Running each program {cfg.num_trials} times, skipping the first {cfg.ignore_first_k} runs, and getting the input-output pairs from {cfg.inputs_outputs_basepath}\"\n98:    )\n99:    \n100:    gen_df = pd.read_json(\n(384 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open src/codenet_eval/run_eval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bb11a2",
   "metadata": {},
   "source": [
    "### Step 41"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34722fa5",
   "metadata": {},
   "source": [
    "Execution Start time: 17:47:42, End time: 17:47:42 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7e7d92",
   "metadata": {},
   "source": [
    "**Thought**: Now that the `run_eval.py` file is open, let's locate line 120 and update the reference to the `input` column to use `code_v0`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34da2474",
   "metadata": {},
   "source": [
    "**Action (goto)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e28b008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/pie-perf/src/codenet_eval/run_eval.py (484 lines total)]\n(104 more lines above)\n105:    if cfg.model_generated_outputs_path.endswith(\".report\"):\n106:        return _prepare_for_rerun(gen_df, cfg)\n107:    \n108:    logging.info(f\"Read {len(gen_df)} rows from {cfg.model_generated_outputs_path}\")\n109:    if cfg.is_prompt_based:\n110:        gen_df[\"slower_program\"] = gen_df.apply(\n111:            lambda x: get_input_from_prompt(x), axis=1\n112:        )\n113:    else:\n114:        gen_df[\"slower_program\"] = gen_df[cfg.slow_code_col].apply(lambda x: x.strip())\n115:        \n116:\n117:\n118:    if cfg.reference_file_path is not None:\n119:        ref_df = pd.read_json(cfg.reference_file_path, lines=True, orient=\"records\")\n120:        ref_df[\"slower_program\"] = ref_df[\"input\"].apply(\n121:            lambda x: x.strip().replace(\"\\n\\n\\n\\n\\n\", \"\")\n122:            # TODO: remove this hack, which is needed because sometimes prompt-lib does not retain the \n123:            # entire input\n124:        )\n125:        \n126:        logging.info(f\"Unique inputs in reference: {len(ref_df['slower_program'].unique())}\")\n127:        gen_df[\"slower_program\"] = gen_df[\n128:            \"slower_program\"\n129:        ].apply(lambda x: x.strip().replace(\"\\n\\n\\n\\n\\n\", \"\"))\n130:        assert len(ref_df[\"submission_id_v0\"].unique()) == len(\n131:            ref_df\n132:        ), \"submission_id_v0 should be unique\"\n133:\n134:        merged = pd.merge(\n135:            gen_df,\n136:            ref_df,\n137:            left_on=\"slower_program\",\n138:            right_on=\"slower_program\",\n139:            suffixes=(\"\", \"_ref\"),\n140:            how=\"inner\",\n141:        )\n142:\n143:\n144:        merged = merged.drop_duplicates(subset=[\"slower_program\"])\n145:        \n146:        assert abs(len(merged) - len(gen_df)) < 10, f\"Merging should not lose too many rows! Check if the inputs are the same. Merge lost {len(gen_df) - len(merged)} rows. len(gen_df)={len(gen_df)}, len(merged)={len(merged)}\"\n147:    else:\n148:        assert (\n149:            cfg.reference_code_col in gen_df.columns\n150:        ), f\"Column {cfg.reference_code_col} not found in {cfg.model_generated_outputs_path}\"\n151:        merged = gen_df\n152:        \n153:        \n154:        merged = merged[merged[cfg.slow_code_col] != merged[cfg.reference_code_col]]\n155:\n156:    assert (\n157:        len(merged) > 0\n158:    ), f\"{cfg.slow_code_col} and {cfg.reference_code_col} are the same for all programs\"\n159:    \n160:    if cfg.num_problems_to_evaluate != -1:\n161:        merged = merged[: cfg.num_problems_to_evaluate]\n162:    \n163:    \n164:    # if the generated code is a list, then we have multiple generations per input. \n165:    # we add one column per generation\n166:    if isinstance(merged[cfg.model_generated_potentially_faster_code_col].iloc[0], list):\n167:        num_generations = len(merged[cfg.model_generated_potentially_faster_code_col].iloc[0])\n168:        for i in range(num_generations):\n169:            merged[f\"{cfg.model_generated_potentially_faster_code_col}_{i}\"] = merged[cfg.model_generated_potentially_faster_code_col].apply(lambda x: x[i])\n170:    return merged\n171:\n172:\n173:def _prepare_for_rerun(df: pd.DataFrame, cfg: EvaluationConfig) -> pd.DataFrame:\n174:    acc_columns = {\"generated_answers_acc\", \"generated_answer_acc\"}\n175:    acc_column = list(acc_columns.intersection(set(df.columns)))[0]\n176:    logging.info(\"ALERT! THIS IS A RERUN\")\n177:    logging.info(\"Preparing for rerun...\")\n178:    logging.info(f\"Found accuracy column: {acc_column}, {len(df)} rows\")\n179:    df = df[df[acc_column] > 0.99]\n180:    # remove all columns that have 'mean', 'std', 'acc' in them just to be safe\n181:    df = df[[c for c in df.columns if not any(x in c for x in [\"mean\", \"std\", \"acc\"])]]\n182:    logging.info(f\"Filtered to {len(df)} rows\")\n183:    if cfg.num_problems_to_evaluate != -1:\n184:        df = df[: cfg.num_problems_to_evaluate]\n185:    return df\n186:    \n187:\n188:def write_programs_read_ground_truth(\n189:    cfg: EvaluationConfig, merged: pd.DataFrame\n190:) -> Tuple[Dict[str, List[str]], str]:\n191:    # Writes all the programs to a temp directory, load ground truth\n192:    # we don't want to do I/O repeatedly as it adds to the variance\n193:    problem_id_to_ground_truths = defaultdict(list)\n194:    if cfg.temp_dir is None:\n195:        cfg.temp_dir = tempfile.TemporaryDirectory()\n196:        output_code_location = cfg.temp_dir.name\n197:    else:\n198:        output_code_location = cfg.temp_dir\n199:        pathlib.Path(output_code_location).mkdir(parents=True, exist_ok=True)\n200:\n201:    for _, row in tqdm(merged.iterrows(), total=len(merged), desc=\"writing programs\"):\n202:        problem_id = row[\"problem_id\"]\n203:\n204:        # read the ground truth\n(280 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goto 120"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8875df02",
   "metadata": {},
   "source": [
    "### Step 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30354c9c",
   "metadata": {},
   "source": [
    "Execution Start time: 17:47:45, End time: 17:47:45 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4f246c",
   "metadata": {},
   "source": [
    "**Thought**: Let's update line 120 to use the correct column name `code_v0` instead of `input`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130683e0",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddb4d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/pie-perf/src/codenet_eval/run_eval.py (484 lines total)]\n(69 more lines above)\n70:    if is_multigen:\n71:        \n72:        num_generations = len(merged[cfg.model_generated_potentially_faster_code_col].iloc[0])\n73:        tag_to_path.extend([(f\"{cfg.model_generated_potentially_faster_code_col}_{i}\", f\"_maybe_faster_{i}.{lang_file_ending}\") for i in range(num_generations)])\n74:\n75:    else:\n76:        tag_to_path.append((cfg.model_generated_potentially_faster_code_col, f\"_maybe_faster_0.{lang_file_ending}\"))\n77:    \n78:    results = run_programs(cfg, merged, problem_id_to_ground_truths, output_code_location, tag_to_path)\n79:    \n80:    if is_multigen:\n81:        results = get_best_generation_per_submission(results, gen_col=cfg.model_generated_potentially_faster_code_col)\n82:    \n83:    # Step 3: summarize the results, write report\n84:    print_summary(cfg, merged, results, gen_col=cfg.model_generated_potentially_faster_code_col)\n85:\n86:    if isinstance(cfg.temp_dir, tempfile.TemporaryDirectory):\n87:        cfg.temp_dir.cleanup()\n88:        \n89:\n90:\n91:def read_inputs_and_prepare(cfg) -> pd.DataFrame:\n92:    \"\"\"Reads the model generated output, the reference, joins them, and returns a dataframe with the merged data.\"\"\"\n93:    logging.info(f\"Reading reference file from {cfg.reference_file_path}\")\n94:    logging.info(f\"Reading model generated outputs from {cfg.model_generated_outputs_path}\")\n95:\n96:    logging.info(\n97:        f\"Running each program {cfg.num_trials} times, skipping the first {cfg.ignore_first_k} runs, and getting the input-output pairs from {cfg.inputs_outputs_basepath}\"\n98:    )\n99:    \n100:    gen_df = pd.read_json(\n101:        cfg.model_generated_outputs_path, lines=True, orient=\"records\"\n102:    )\n103:    \n104:    # if file ends in .report, then it is a re-run. We can filter out the rows that have already been run and just return the new rows\n105:    if cfg.model_generated_outputs_path.endswith(\".report\"):\n106:        return _prepare_for_rerun(gen_df, cfg)\n107:    \n108:    logging.info(f\"Read {len(gen_df)} rows from {cfg.model_generated_outputs_path}\")\n109:    if cfg.is_prompt_based:\n110:        gen_df[\"slower_program\"] = gen_df.apply(\n111:            lambda x: get_input_from_prompt(x), axis=1\n112:        )\n113:    else:\n114:        gen_df[\"slower_program\"] = gen_df[cfg.slow_code_col].apply(lambda x: x.strip())\n115:        \n116:\n117:\n118:    if cfg.reference_file_path is not None:\n119:        ref_df = pd.read_json(cfg.reference_file_path, lines=True, orient=\"records\")\n120:        ref_df[\"slower_program\"] = ref_df[\"code_v0\"].apply(\n121:            lambda x: x.strip().replace(\"\\n\\n\\n\\n\\n\", \"\")\n122:            # TODO: remove this hack, which is needed because sometimes prompt-lib does not retain the \n123:            # entire input\n124:        )\n125:        \n126:        logging.info(f\"Unique inputs in reference: {len(ref_df['slower_program'].unique())}\")\n127:        gen_df[\"slower_program\"] = gen_df[\n128:            \"slower_program\"\n129:        ].apply(lambda x: x.strip().replace(\"\\n\\n\\n\\n\\n\", \"\"))\n130:        assert len(ref_df[\"submission_id_v0\"].unique()) == len(\n131:            ref_df\n132:        ), \"submission_id_v0 should be unique\"\n133:\n134:        merged = pd.merge(\n135:            gen_df,\n136:            ref_df,\n137:            left_on=\"slower_program\",\n138:            right_on=\"slower_program\",\n139:            suffixes=(\"\", \"_ref\"),\n140:            how=\"inner\",\n141:        )\n142:\n143:\n144:        merged = merged.drop_duplicates(subset=[\"slower_program\"])\n145:        \n146:        assert abs(len(merged) - len(gen_df)) < 10, f\"Merging should not lose too many rows! Check if the inputs are the same. Merge lost {len(gen_df) - len(merged)} rows. len(gen_df)={len(gen_df)}, len(merged)={len(merged)}\"\n147:    else:\n148:        assert (\n149:            cfg.reference_code_col in gen_df.columns\n150:        ), f\"Column {cfg.reference_code_col} not found in {cfg.model_generated_outputs_path}\"\n151:        merged = gen_df\n152:        \n153:        \n154:        merged = merged[merged[cfg.slow_code_col] != merged[cfg.reference_code_col]]\n155:\n156:    assert (\n157:        len(merged) > 0\n158:    ), f\"{cfg.slow_code_col} and {cfg.reference_code_col} are the same for all programs\"\n159:    \n160:    if cfg.num_problems_to_evaluate != -1:\n161:        merged = merged[: cfg.num_problems_to_evaluate]\n162:    \n163:    \n164:    # if the generated code is a list, then we have multiple generations per input. \n165:    # we add one column per generation\n166:    if isinstance(merged[cfg.model_generated_potentially_faster_code_col].iloc[0], list):\n167:        num_generations = len(merged[cfg.model_generated_potentially_faster_code_col].iloc[0])\n168:        for i in range(num_generations):\n169:            merged[f\"{cfg.model_generated_potentially_faster_code_col}_{i}\"] = merged[cfg.model_generated_potentially_faster_code_col].apply(lambda x: x[i])\n(315 more lines below)\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 120:120\n",
    "        ref_df[\"slower_program\"] = ref_df[\"code_v0\"].apply(\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f70c05",
   "metadata": {},
   "source": [
    "### Step 43"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fbad20",
   "metadata": {},
   "source": [
    "Execution Start time: 17:47:48, End time: 17:47:49 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d115fd9",
   "metadata": {},
   "source": [
    "**Thought**: The `run_eval.py` script has been successfully updated to use the correct column name `code_v0`. Now, let's run the script again using the converted JSONL file for the model-generated outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd59a22",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c73d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3653, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 147, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 176, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'input'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/content/pie-perf/src/codenet_eval/run_eval.py\", line 484, in <module>\n    evaluate_generated_outputs(evaluation_config)\n  File \"/content/pie-perf/src/codenet_eval/run_eval.py\", line 43, in evaluate_generated_outputs\n    merged = read_inputs_and_prepare(cfg)\n  File \"/content/pie-perf/src/codenet_eval/run_eval.py\", line 114, in read_inputs_and_prepare\n    gen_df[\"slower_program\"] = gen_df[cfg.slow_code_col].apply(lambda x: x.strip())\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/frame.py\", line 3761, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3655, in get_loc\n    raise KeyError(key) from err\nKeyError: 'input'\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!PYTHONPATH=$(pwd) python src/codenet_eval/run_eval.py --model_generated_outputs_path data/sample/codex_greedy_outputs_converted.jsonl --output_report_file_path data/sample/codex_greedy_outputs_converted.jsonl.report --num_trials 2 --num_problems_to_evaluate 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862d5670",
   "metadata": {},
   "source": [
    "### Step 44"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa5a104",
   "metadata": {},
   "source": [
    "Execution Start time: 17:47:51, End time: 17:47:52 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7175ab",
   "metadata": {},
   "source": [
    "**Thought**: The error persists, indicating that there is still a reference to the `'input'` column in the script. It seems that the `gen_df` DataFrame is being accessed with `cfg.slow_code_col`, which should have been updated to `code_v0` in the configuration.\n",
    "\n",
    "Let's double-check the `sample_eval_config.yaml` file to ensure that the `slow_code_col` is correctly set to `code_v0`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795c427a",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795e1b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/pie-perf/data/sample/sample_eval_config.yaml (16 lines total)]\n1:model_generated_outputs_path: \"data/sample/codex_greedy_outputs.jsonl\"\n2:inputs_outputs_basepath: \"data/codenet/public_test_cases/\"\n3:reference_file_path: \"data/sample/py_reference.jsonl\"\n4:output_report_file_path: \"data/sample//codex_greedy_outputs.jsonl.report\"\n5:num_problems_to_evaluate: 10\n6:num_trials: 2\n7:ignore_first_k: 1\n8:max_time_per_run: 10\n9:temp_dir: null\n10:model_generated_potentially_faster_code_col: \"code_v1\"\n11:slow_code_col: \"code_v0\"\n12:reference_code_col: \"target\"\n13:is_prompt_based: true\n14:cpu_number: 10\n15:return_if_acc_below: 1.0\n16:language: python\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open data/sample/sample_eval_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68636f06",
   "metadata": {},
   "source": [
    "### Step 45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14630b59",
   "metadata": {},
   "source": [
    "Execution Start time: 17:47:54, End time: 17:47:54 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f27a8d",
   "metadata": {},
   "source": [
    "**Thought**: The `sample_eval_config.yaml` file correctly sets `slow_code_col` to `code_v0`. The issue might be elsewhere in the script or configuration. Let's ensure that the `gen_df` DataFrame is being populated correctly and that the column names are being accessed as expected.\n",
    "\n",
    "We should also verify if there are any other parts of the script where the column name might be hardcoded or incorrectly referenced. Let's search for `slow_code_col` in the `run_eval.py` script to ensure it's being used correctly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae7aac5",
   "metadata": {},
   "source": [
    "**Action (search_file)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eacc77b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Found 6 matches for \"slow_code_col\" in /content/pie-perf/src/codenet_eval/run_eval.py:\nLine 9:export REF_FILE=\"data/codenet/splits/problem_id/2023-01-13_12-56pm/seq2seq_splits/test.jsonl\" && export OP=\"/usr1/amadaan/learning2perf/data/outputs/beam_outputs_codegen_2b_jan_13_split.jsonl\" && export CPU=30 && nohup python -u src/codenet_eval/run_eval.py  --model_generated_outputs_path ${OP} --reference_file_path ${REF_FILE} --output_report_file_path ${OP}.25_tries.report --slow_code_col input --model_generated_potentially_faster_code_col beam_generated_target_from_input --reference_code_col target --num_problems_to_evaluate -1 --cpu_number $CPU --max_time_per_run 10 --num_trials 25 --ignore_first_k 2\nLine 48:    logging.info(f\"Input column: {cfg.slow_code_col}\")\nLine 114:        gen_df[\"slower_program\"] = gen_df[cfg.slow_code_col].apply(lambda x: x.strip())\nLine 154:        merged = merged[merged[cfg.slow_code_col] != merged[cfg.reference_code_col]]\nLine 158:    ), f\"{cfg.slow_code_col} and {cfg.reference_code_col} are the same for all programs\"\nLine 226:            # f.write(row[cfg.slow_code_col].strip())\nEnd of matches for \"slow_code_col\" in /content/pie-perf/src/codenet_eval/run_eval.py\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_file 'slow_code_col' src/codenet_eval/run_eval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01d284c",
   "metadata": {},
   "source": [
    "### Step 46"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f19c4b4",
   "metadata": {},
   "source": [
    "Execution Start time: 17:47:58, End time: 17:47:58 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86591660",
   "metadata": {},
   "source": [
    "**Thought**: The search results show that `slow_code_col` is being used in several places in the `run_eval.py` script. Notably, line 114 uses `gen_df[cfg.slow_code_col]`, which should correctly reference `code_v0` based on the configuration.\n",
    "\n",
    "Let's ensure that the `gen_df` DataFrame is being populated with the correct data and that the column names are being accessed as expected. We should also check if there are any other parts of the script where the column name might be hardcoded or incorrectly referenced.\n",
    "\n",
    "Let's open the relevant section of the `run_eval.py` script to verify the usage of `slow_code_col`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9658d49",
   "metadata": {},
   "source": [
    "**Action (goto)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d37e757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Error: <line> must be less than or equal to 16\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goto 114"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46545968",
   "metadata": {},
   "source": [
    "### Step 47"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477b0a55",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:00, End time: 17:48:00 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aa1189",
   "metadata": {},
   "source": [
    "**Thought**: It seems I attempted to navigate to a line number that doesn't exist in the currently open file. Let's open the `run_eval.py` file again and navigate to the relevant section to verify the usage of `slow_code_col`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264f0c26",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82309de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/pie-perf/src/codenet_eval/run_eval.py (484 lines total)]\n1:\"\"\"\n2:Runs evaluation on the model generated outputs. The rough flow is as follows:\n3:\n4:1. Reads the inputs and model generated programs, writes them to a temporary directory.\n5:2. Runs each program several times and computes the average time taken and accuracy.\n6:3. Writes the results to a json file.\n7:\n8:Sample usage:\n9:export REF_FILE=\"data/codenet/splits/problem_id/2023-01-13_12-56pm/seq2seq_splits/test.jsonl\" && export OP=\"/usr1/amadaan/learning2perf/data/outputs/beam_outputs_codegen_2b_jan_13_split.jsonl\" && export CPU=30 && nohup python -u src/codenet_eval/run_eval.py  --model_generated_outputs_path ${OP} --reference_file_path ${REF_FILE} --output_report_file_path ${OP}.25_tries.report --slow_code_col input --model_generated_potentially_faster_code_col beam_generated_target_from_input --reference_code_col target --num_problems_to_evaluate -1 --cpu_number $CPU --max_time_per_run 10 --num_trials 25 --ignore_first_k 2\n10:\n11:\"\"\"\n12:import pathlib\n13:import tempfile\n14:from tqdm import tqdm\n15:import pandas as pd\n16:from typing import Dict, List, Tuple\n17:import os\n18:import logging\n19:import glob\n20:import numpy as np\n21:from collections import defaultdict\n22:\n23:from src.codenet_eval.evalconfig import EvaluationConfig\n24:from src.codenet_eval.sandbox import run_code_on_inputs\n25:\n26:import pdb\n27:\n28:logging.basicConfig(level=logging.CRITICAL)\n29:\n30:lang2file_ending = {\n31:    \"python\": \"py\", \n32:    \"cpp\": \"cpp\"\n33:}\n34:    \n35:\n36:\n37:    \n38:\n39:def evaluate_generated_outputs(cfg: EvaluationConfig) -> None:\n40:    \"\"\"Evaluates model generated programs for accuracy and runtime. See the documenation of the EvaluationConfig class for more details.\"\"\"\n41:\n42:    # Step 0\n43:    merged = read_inputs_and_prepare(cfg)\n44:    logging.info(f\"Number of programs to evaluate: {len(merged)}\")\n45:    logging.info(f\"Number of trials per program: {cfg.num_trials}\")\n46:    logging.info(f\"Number of trials to ignore: {cfg.ignore_first_k}\")\n47:    logging.info(f\"Maximum time per run: {cfg.max_time_per_run}\")\n48:    logging.info(f\"Input column: {cfg.slow_code_col}\")\n49:    logging.info(f\"Reference column: {cfg.reference_code_col}\")\n50:    logging.info(f\"Model generated column: {cfg.model_generated_potentially_faster_code_col}\")\n51:    logging.info(f\"inputs/outputs basepath: {cfg.inputs_outputs_basepath}\")\n52:\n53:    # Step 1: Write the inputs and model generated programs to a temporary directory\n54:    problem_id_to_ground_truths, output_code_location = write_programs_read_ground_truth(\n55:        cfg, merged\n56:    )\n57:\n58:    # Step 2: run the programs\n59:    \n60:    # (dataframe col, suffix for the file)\n61:    \n62:    lang_file_ending = lang2file_ending[cfg.language]\n63:    tag_to_path = [\n64:        (\"code_v0\", f\"_slow.{lang_file_ending}\"),\n65:        (\"reference\", f\"_reference.{lang_file_ending}\"),\n66:    ]\n67:    \n68:    # check if there are multiple generations per input\n69:    is_multigen = isinstance(merged[cfg.model_generated_potentially_faster_code_col].iloc[0], list)\n70:    if is_multigen:\n71:        \n72:        num_generations = len(merged[cfg.model_generated_potentially_faster_code_col].iloc[0])\n73:        tag_to_path.extend([(f\"{cfg.model_generated_potentially_faster_code_col}_{i}\", f\"_maybe_faster_{i}.{lang_file_ending}\") for i in range(num_generations)])\n74:\n75:    else:\n76:        tag_to_path.append((cfg.model_generated_potentially_faster_code_col, f\"_maybe_faster_0.{lang_file_ending}\"))\n77:    \n78:    results = run_programs(cfg, merged, problem_id_to_ground_truths, output_code_location, tag_to_path)\n79:    \n80:    if is_multigen:\n81:        results = get_best_generation_per_submission(results, gen_col=cfg.model_generated_potentially_faster_code_col)\n82:    \n83:    # Step 3: summarize the results, write report\n84:    print_summary(cfg, merged, results, gen_col=cfg.model_generated_potentially_faster_code_col)\n85:\n86:    if isinstance(cfg.temp_dir, tempfile.TemporaryDirectory):\n87:        cfg.temp_dir.cleanup()\n88:        \n89:\n90:\n91:def read_inputs_and_prepare(cfg) -> pd.DataFrame:\n92:    \"\"\"Reads the model generated output, the reference, joins them, and returns a dataframe with the merged data.\"\"\"\n93:    logging.info(f\"Reading reference file from {cfg.reference_file_path}\")\n94:    logging.info(f\"Reading model generated outputs from {cfg.model_generated_outputs_path}\")\n95:\n96:    logging.info(\n97:        f\"Running each program {cfg.num_trials} times, skipping the first {cfg.ignore_first_k} runs, and getting the input-output pairs from {cfg.inputs_outputs_basepath}\"\n98:    )\n99:    \n100:    gen_df = pd.read_json(\n(384 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open src/codenet_eval/run_eval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d16884",
   "metadata": {},
   "source": [
    "### Step 48"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0fb388",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:02, End time: 17:48:03 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbce7c68",
   "metadata": {},
   "source": [
    "**Thought**: The `run_eval.py` script is open, and we can see the `read_inputs_and_prepare` function starting at line 91. Let's scroll down to inspect the relevant section where `slow_code_col` is used to ensure it's being accessed correctly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31b7217",
   "metadata": {},
   "source": [
    "**Action (scroll_down)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7470a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/pie-perf/src/codenet_eval/run_eval.py (484 lines total)]\n(98 more lines above)\n99:    \n100:    gen_df = pd.read_json(\n101:        cfg.model_generated_outputs_path, lines=True, orient=\"records\"\n102:    )\n103:    \n104:    # if file ends in .report, then it is a re-run. We can filter out the rows that have already been run and just return the new rows\n105:    if cfg.model_generated_outputs_path.endswith(\".report\"):\n106:        return _prepare_for_rerun(gen_df, cfg)\n107:    \n108:    logging.info(f\"Read {len(gen_df)} rows from {cfg.model_generated_outputs_path}\")\n109:    if cfg.is_prompt_based:\n110:        gen_df[\"slower_program\"] = gen_df.apply(\n111:            lambda x: get_input_from_prompt(x), axis=1\n112:        )\n113:    else:\n114:        gen_df[\"slower_program\"] = gen_df[cfg.slow_code_col].apply(lambda x: x.strip())\n115:        \n116:\n117:\n118:    if cfg.reference_file_path is not None:\n119:        ref_df = pd.read_json(cfg.reference_file_path, lines=True, orient=\"records\")\n120:        ref_df[\"slower_program\"] = ref_df[\"code_v0\"].apply(\n121:            lambda x: x.strip().replace(\"\\n\\n\\n\\n\\n\", \"\")\n122:            # TODO: remove this hack, which is needed because sometimes prompt-lib does not retain the \n123:            # entire input\n124:        )\n125:        \n126:        logging.info(f\"Unique inputs in reference: {len(ref_df['slower_program'].unique())}\")\n127:        gen_df[\"slower_program\"] = gen_df[\n128:            \"slower_program\"\n129:        ].apply(lambda x: x.strip().replace(\"\\n\\n\\n\\n\\n\", \"\"))\n130:        assert len(ref_df[\"submission_id_v0\"].unique()) == len(\n131:            ref_df\n132:        ), \"submission_id_v0 should be unique\"\n133:\n134:        merged = pd.merge(\n135:            gen_df,\n136:            ref_df,\n137:            left_on=\"slower_program\",\n138:            right_on=\"slower_program\",\n139:            suffixes=(\"\", \"_ref\"),\n140:            how=\"inner\",\n141:        )\n142:\n143:\n144:        merged = merged.drop_duplicates(subset=[\"slower_program\"])\n145:        \n146:        assert abs(len(merged) - len(gen_df)) < 10, f\"Merging should not lose too many rows! Check if the inputs are the same. Merge lost {len(gen_df) - len(merged)} rows. len(gen_df)={len(gen_df)}, len(merged)={len(merged)}\"\n147:    else:\n148:        assert (\n149:            cfg.reference_code_col in gen_df.columns\n150:        ), f\"Column {cfg.reference_code_col} not found in {cfg.model_generated_outputs_path}\"\n151:        merged = gen_df\n152:        \n153:        \n154:        merged = merged[merged[cfg.slow_code_col] != merged[cfg.reference_code_col]]\n155:\n156:    assert (\n157:        len(merged) > 0\n158:    ), f\"{cfg.slow_code_col} and {cfg.reference_code_col} are the same for all programs\"\n159:    \n160:    if cfg.num_problems_to_evaluate != -1:\n161:        merged = merged[: cfg.num_problems_to_evaluate]\n162:    \n163:    \n164:    # if the generated code is a list, then we have multiple generations per input. \n165:    # we add one column per generation\n166:    if isinstance(merged[cfg.model_generated_potentially_faster_code_col].iloc[0], list):\n167:        num_generations = len(merged[cfg.model_generated_potentially_faster_code_col].iloc[0])\n168:        for i in range(num_generations):\n169:            merged[f\"{cfg.model_generated_potentially_faster_code_col}_{i}\"] = merged[cfg.model_generated_potentially_faster_code_col].apply(lambda x: x[i])\n170:    return merged\n171:\n172:\n173:def _prepare_for_rerun(df: pd.DataFrame, cfg: EvaluationConfig) -> pd.DataFrame:\n174:    acc_columns = {\"generated_answers_acc\", \"generated_answer_acc\"}\n175:    acc_column = list(acc_columns.intersection(set(df.columns)))[0]\n176:    logging.info(\"ALERT! THIS IS A RERUN\")\n177:    logging.info(\"Preparing for rerun...\")\n178:    logging.info(f\"Found accuracy column: {acc_column}, {len(df)} rows\")\n179:    df = df[df[acc_column] > 0.99]\n180:    # remove all columns that have 'mean', 'std', 'acc' in them just to be safe\n181:    df = df[[c for c in df.columns if not any(x in c for x in [\"mean\", \"std\", \"acc\"])]]\n182:    logging.info(f\"Filtered to {len(df)} rows\")\n183:    if cfg.num_problems_to_evaluate != -1:\n184:        df = df[: cfg.num_problems_to_evaluate]\n185:    return df\n186:    \n187:\n188:def write_programs_read_ground_truth(\n189:    cfg: EvaluationConfig, merged: pd.DataFrame\n190:) -> Tuple[Dict[str, List[str]], str]:\n191:    # Writes all the programs to a temp directory, load ground truth\n192:    # we don't want to do I/O repeatedly as it adds to the variance\n193:    problem_id_to_ground_truths = defaultdict(list)\n194:    if cfg.temp_dir is None:\n195:        cfg.temp_dir = tempfile.TemporaryDirectory()\n196:        output_code_location = cfg.temp_dir.name\n197:    else:\n198:        output_code_location = cfg.temp_dir\n(286 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scroll_down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631340d9",
   "metadata": {},
   "source": [
    "### Step 49"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c73fa1",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:06, End time: 17:48:07 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2da67f6",
   "metadata": {},
   "source": [
    "**Thought**: The `gen_df[\"slower_program\"]` assignment on line 114 is correctly using `cfg.slow_code_col`, which should be `code_v0` based on the configuration. However, the error persists, suggesting there might be another issue with how the DataFrame is being populated or accessed.\n",
    "\n",
    "Let's add some debug statements to print the columns of `gen_df` after it is read to ensure that it contains the expected columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32263fc1",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6884be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/pie-perf/src/codenet_eval/run_eval.py (484 lines total)]\n(57 more lines above)\n58:    # Step 2: run the programs\n59:    \n60:    # (dataframe col, suffix for the file)\n61:    \n62:    lang_file_ending = lang2file_ending[cfg.language]\n63:    tag_to_path = [\n64:        (\"code_v0\", f\"_slow.{lang_file_ending}\"),\n65:        (\"reference\", f\"_reference.{lang_file_ending}\"),\n66:    ]\n67:    \n68:    # check if there are multiple generations per input\n69:    is_multigen = isinstance(merged[cfg.model_generated_potentially_faster_code_col].iloc[0], list)\n70:    if is_multigen:\n71:        \n72:        num_generations = len(merged[cfg.model_generated_potentially_faster_code_col].iloc[0])\n73:        tag_to_path.extend([(f\"{cfg.model_generated_potentially_faster_code_col}_{i}\", f\"_maybe_faster_{i}.{lang_file_ending}\") for i in range(num_generations)])\n74:\n75:    else:\n76:        tag_to_path.append((cfg.model_generated_potentially_faster_code_col, f\"_maybe_faster_0.{lang_file_ending}\"))\n77:    \n78:    results = run_programs(cfg, merged, problem_id_to_ground_truths, output_code_location, tag_to_path)\n79:    \n80:    if is_multigen:\n81:        results = get_best_generation_per_submission(results, gen_col=cfg.model_generated_potentially_faster_code_col)\n82:    \n83:    # Step 3: summarize the results, write report\n84:    print_summary(cfg, merged, results, gen_col=cfg.model_generated_potentially_faster_code_col)\n85:\n86:    if isinstance(cfg.temp_dir, tempfile.TemporaryDirectory):\n87:        cfg.temp_dir.cleanup()\n88:        \n89:\n90:\n91:def read_inputs_and_prepare(cfg) -> pd.DataFrame:\n92:    \"\"\"Reads the model generated output, the reference, joins them, and returns a dataframe with the merged data.\"\"\"\n93:    logging.info(f\"Reading reference file from {cfg.reference_file_path}\")\n94:    logging.info(f\"Reading model generated outputs from {cfg.model_generated_outputs_path}\")\n95:\n96:    logging.info(\n97:        f\"Running each program {cfg.num_trials} times, skipping the first {cfg.ignore_first_k} runs, and getting the input-output pairs from {cfg.inputs_outputs_basepath}\"\n98:    )\n99:    \n100:    gen_df = pd.read_json(\n101:        cfg.model_generated_outputs_path, lines=True, orient=\"records\"\n102:    )\n103:    \n104:    # if file ends in .report, then it is a re-run. We can filter out the rows that have already been run and just return the new rows\n105:    if cfg.model_generated_outputs_path.endswith(\".report\"):\n106:        return _prepare_for_rerun(gen_df, cfg)\n107:    \n108:    logging.info(f\"Columns in gen_df: {gen_df.columns}\")\n109:    if cfg.is_prompt_based:\n110:        gen_df[\"slower_program\"] = gen_df.apply(\n111:            lambda x: get_input_from_prompt(x), axis=1\n112:        )\n113:    else:\n114:        gen_df[\"slower_program\"] = gen_df[cfg.slow_code_col].apply(lambda x: x.strip())\n115:        \n116:\n117:\n118:    if cfg.reference_file_path is not None:\n119:        ref_df = pd.read_json(cfg.reference_file_path, lines=True, orient=\"records\")\n120:        ref_df[\"slower_program\"] = ref_df[\"code_v0\"].apply(\n121:            lambda x: x.strip().replace(\"\\n\\n\\n\\n\\n\", \"\")\n122:            # TODO: remove this hack, which is needed because sometimes prompt-lib does not retain the \n123:            # entire input\n124:        )\n125:        \n126:        logging.info(f\"Unique inputs in reference: {len(ref_df['slower_program'].unique())}\")\n127:        gen_df[\"slower_program\"] = gen_df[\n128:            \"slower_program\"\n129:        ].apply(lambda x: x.strip().replace(\"\\n\\n\\n\\n\\n\", \"\"))\n130:        assert len(ref_df[\"submission_id_v0\"].unique()) == len(\n131:            ref_df\n132:        ), \"submission_id_v0 should be unique\"\n133:\n134:        merged = pd.merge(\n135:            gen_df,\n136:            ref_df,\n137:            left_on=\"slower_program\",\n138:            right_on=\"slower_program\",\n139:            suffixes=(\"\", \"_ref\"),\n140:            how=\"inner\",\n141:        )\n142:\n143:\n144:        merged = merged.drop_duplicates(subset=[\"slower_program\"])\n145:        \n146:        assert abs(len(merged) - len(gen_df)) < 10, f\"Merging should not lose too many rows! Check if the inputs are the same. Merge lost {len(gen_df) - len(merged)} rows. len(gen_df)={len(gen_df)}, len(merged)={len(merged)}\"\n147:    else:\n148:        assert (\n149:            cfg.reference_code_col in gen_df.columns\n150:        ), f\"Column {cfg.reference_code_col} not found in {cfg.model_generated_outputs_path}\"\n151:        merged = gen_df\n152:        \n153:        \n154:        merged = merged[merged[cfg.slow_code_col] != merged[cfg.reference_code_col]]\n155:\n156:    assert (\n157:        len(merged) > 0\n(327 more lines below)\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 108:108\n",
    "    logging.info(f\"Columns in gen_df: {gen_df.columns}\")\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30686d21",
   "metadata": {},
   "source": [
    "### Step 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4179c7",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:09, End time: 17:48:10 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45db111b",
   "metadata": {},
   "source": [
    "**Thought**: The debug statement has been added to print the columns of `gen_df`. Let's run the script again to see the output and verify if the expected columns are present.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f68dc8e",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828201d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3653, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 147, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 176, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'input'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/content/pie-perf/src/codenet_eval/run_eval.py\", line 484, in <module>\n    evaluate_generated_outputs(evaluation_config)\n  File \"/content/pie-perf/src/codenet_eval/run_eval.py\", line 43, in evaluate_generated_outputs\n    merged = read_inputs_and_prepare(cfg)\n  File \"/content/pie-perf/src/codenet_eval/run_eval.py\", line 114, in read_inputs_and_prepare\n    gen_df[\"slower_program\"] = gen_df[cfg.slow_code_col].apply(lambda x: x.strip())\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/frame.py\", line 3761, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3655, in get_loc\n    raise KeyError(key) from err\nKeyError: 'input'\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!PYTHONPATH=$(pwd) python src/codenet_eval/run_eval.py --model_generated_outputs_path data/sample/codex_greedy_outputs_converted.jsonl --output_report_file_path data/sample/codex_greedy_outputs_converted.jsonl.report --num_trials 2 --num_problems_to_evaluate 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ade381",
   "metadata": {},
   "source": [
    "### Step 51"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0aa29d",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:13, End time: 17:48:14 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e5c88a",
   "metadata": {},
   "source": [
    "**Thought**: The error persists, indicating that the `gen_df` DataFrame does not contain the expected column `code_v0`. The debug statement should have printed the columns of `gen_df`, but it seems the error occurs before the logging statement is executed.\n",
    "\n",
    "Let's add a try-except block around the problematic line to catch the error and print the columns of `gen_df` for debugging purposes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261d861f",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764329e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\n\nERRORS:\n- E999 IndentationError: expected an indented block after 'else' statement on line 113\n\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /content/pie-perf/src/codenet_eval/run_eval.py (488 lines total)]\n(108 more lines above)\n108:    logging.info(f\"Columns in gen_df: {gen_df.columns}\")\n109:    if cfg.is_prompt_based:\n110:        gen_df[\"slower_program\"] = gen_df.apply(\n111:            lambda x: get_input_from_prompt(x), axis=1\n112:        )\n113:    else:\n114:    try:\n115:        gen_df[\"slower_program\"] = gen_df[cfg.slow_code_col].apply(lambda x: x.strip())\n116:    except KeyError as e:\n117:        logging.error(f\"KeyError: {e}. Available columns: {gen_df.columns}\")\n118:        raise\n119:        \n120:\n121:\n122:    if cfg.reference_file_path is not None:\n(366 more lines below)\n-------------------------------------------------\n\nThis is the original code before your edit\n-------------------------------------------------\n[File: /content/pie-perf/src/codenet_eval/run_eval.py (484 lines total)]\n(109 more lines above)\n109:    if cfg.is_prompt_based:\n110:        gen_df[\"slower_program\"] = gen_df.apply(\n111:            lambda x: get_input_from_prompt(x), axis=1\n112:        )\n113:    else:\n114:        gen_df[\"slower_program\"] = gen_df[cfg.slow_code_col].apply(lambda x: x.strip())\n115:        \n116:\n117:\n118:    if cfg.reference_file_path is not None:\n119:        ref_df = pd.read_json(cfg.reference_file_path, lines=True, orient=\"records\")\n(365 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 114:114\n",
    "    try:\n",
    "        gen_df[\"slower_program\"] = gen_df[cfg.slow_code_col].apply(lambda x: x.strip())\n",
    "    except KeyError as e:\n",
    "        logging.error(f\"KeyError: {e}. Available columns: {gen_df.columns}\")\n",
    "        raise\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bed47a1",
   "metadata": {},
   "source": [
    "### Step 52"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cc019f",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:18, End time: 17:48:19 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52194682",
   "metadata": {},
   "source": [
    "**Thought**: The error occurred because the `try` block was not properly indented under the `else` statement. Let's correct the indentation and add the try-except block again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4261685",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f40c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/pie-perf/src/codenet_eval/run_eval.py (487 lines total)]\n(62 more lines above)\n63:    tag_to_path = [\n64:        (\"code_v0\", f\"_slow.{lang_file_ending}\"),\n65:        (\"reference\", f\"_reference.{lang_file_ending}\"),\n66:    ]\n67:    \n68:    # check if there are multiple generations per input\n69:    is_multigen = isinstance(merged[cfg.model_generated_potentially_faster_code_col].iloc[0], list)\n70:    if is_multigen:\n71:        \n72:        num_generations = len(merged[cfg.model_generated_potentially_faster_code_col].iloc[0])\n73:        tag_to_path.extend([(f\"{cfg.model_generated_potentially_faster_code_col}_{i}\", f\"_maybe_faster_{i}.{lang_file_ending}\") for i in range(num_generations)])\n74:\n75:    else:\n76:        tag_to_path.append((cfg.model_generated_potentially_faster_code_col, f\"_maybe_faster_0.{lang_file_ending}\"))\n77:    \n78:    results = run_programs(cfg, merged, problem_id_to_ground_truths, output_code_location, tag_to_path)\n79:    \n80:    if is_multigen:\n81:        results = get_best_generation_per_submission(results, gen_col=cfg.model_generated_potentially_faster_code_col)\n82:    \n83:    # Step 3: summarize the results, write report\n84:    print_summary(cfg, merged, results, gen_col=cfg.model_generated_potentially_faster_code_col)\n85:\n86:    if isinstance(cfg.temp_dir, tempfile.TemporaryDirectory):\n87:        cfg.temp_dir.cleanup()\n88:        \n89:\n90:\n91:def read_inputs_and_prepare(cfg) -> pd.DataFrame:\n92:    \"\"\"Reads the model generated output, the reference, joins them, and returns a dataframe with the merged data.\"\"\"\n93:    logging.info(f\"Reading reference file from {cfg.reference_file_path}\")\n94:    logging.info(f\"Reading model generated outputs from {cfg.model_generated_outputs_path}\")\n95:\n96:    logging.info(\n97:        f\"Running each program {cfg.num_trials} times, skipping the first {cfg.ignore_first_k} runs, and getting the input-output pairs from {cfg.inputs_outputs_basepath}\"\n98:    )\n99:    \n100:    gen_df = pd.read_json(\n101:        cfg.model_generated_outputs_path, lines=True, orient=\"records\"\n102:    )\n103:    \n104:    # if file ends in .report, then it is a re-run. We can filter out the rows that have already been run and just return the new rows\n105:    if cfg.model_generated_outputs_path.endswith(\".report\"):\n106:        return _prepare_for_rerun(gen_df, cfg)\n107:    \n108:    logging.info(f\"Columns in gen_df: {gen_df.columns}\")\n109:    if cfg.is_prompt_based:\n110:        gen_df[\"slower_program\"] = gen_df.apply(\n111:            lambda x: get_input_from_prompt(x), axis=1\n112:        )\n113:    else:\n114:        try:\n115:            gen_df[\"slower_program\"] = gen_df[cfg.slow_code_col].apply(lambda x: x.strip())\n116:        except KeyError as e:\n117:            logging.error(f\"KeyError: {e}. Available columns: {gen_df.columns}\")\n118:            raise\n119:\n120:\n121:    if cfg.reference_file_path is not None:\n122:        ref_df = pd.read_json(cfg.reference_file_path, lines=True, orient=\"records\")\n123:        ref_df[\"slower_program\"] = ref_df[\"code_v0\"].apply(\n124:            lambda x: x.strip().replace(\"\\n\\n\\n\\n\\n\", \"\")\n125:            # TODO: remove this hack, which is needed because sometimes prompt-lib does not retain the \n126:            # entire input\n127:        )\n128:        \n129:        logging.info(f\"Unique inputs in reference: {len(ref_df['slower_program'].unique())}\")\n130:        gen_df[\"slower_program\"] = gen_df[\n131:            \"slower_program\"\n132:        ].apply(lambda x: x.strip().replace(\"\\n\\n\\n\\n\\n\", \"\"))\n133:        assert len(ref_df[\"submission_id_v0\"].unique()) == len(\n134:            ref_df\n135:        ), \"submission_id_v0 should be unique\"\n136:\n137:        merged = pd.merge(\n138:            gen_df,\n139:            ref_df,\n140:            left_on=\"slower_program\",\n141:            right_on=\"slower_program\",\n142:            suffixes=(\"\", \"_ref\"),\n143:            how=\"inner\",\n144:        )\n145:\n146:\n147:        merged = merged.drop_duplicates(subset=[\"slower_program\"])\n148:        \n149:        assert abs(len(merged) - len(gen_df)) < 10, f\"Merging should not lose too many rows! Check if the inputs are the same. Merge lost {len(gen_df) - len(merged)} rows. len(gen_df)={len(gen_df)}, len(merged)={len(merged)}\"\n150:    else:\n151:        assert (\n152:            cfg.reference_code_col in gen_df.columns\n153:        ), f\"Column {cfg.reference_code_col} not found in {cfg.model_generated_outputs_path}\"\n154:        merged = gen_df\n155:        \n156:        \n157:        merged = merged[merged[cfg.slow_code_col] != merged[cfg.reference_code_col]]\n158:\n159:    assert (\n160:        len(merged) > 0\n161:    ), f\"{cfg.slow_code_col} and {cfg.reference_code_col} are the same for all programs\"\n162:    \n(325 more lines below)\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 113:115\n",
    "    else:\n",
    "        try:\n",
    "            gen_df[\"slower_program\"] = gen_df[cfg.slow_code_col].apply(lambda x: x.strip())\n",
    "        except KeyError as e:\n",
    "            logging.error(f\"KeyError: {e}. Available columns: {gen_df.columns}\")\n",
    "            raise\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd1b2fa",
   "metadata": {},
   "source": [
    "### Step 53"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90da267d",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:21, End time: 17:48:22 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e6eb19",
   "metadata": {},
   "source": [
    "**Thought**: The try-except block has been successfully added. Let's run the script again to catch the error and print the columns of `gen_df` for debugging purposes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf32400",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b39318e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3653, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 147, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 176, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'input'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/content/pie-perf/src/codenet_eval/run_eval.py\", line 487, in <module>\n    evaluate_generated_outputs(evaluation_config)\n  File \"/content/pie-perf/src/codenet_eval/run_eval.py\", line 43, in evaluate_generated_outputs\n    merged = read_inputs_and_prepare(cfg)\n  File \"/content/pie-perf/src/codenet_eval/run_eval.py\", line 115, in read_inputs_and_prepare\n    gen_df[\"slower_program\"] = gen_df[cfg.slow_code_col].apply(lambda x: x.strip())\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/frame.py\", line 3761, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3655, in get_loc\n    raise KeyError(key) from err\nKeyError: 'input'\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!PYTHONPATH=$(pwd) python src/codenet_eval/run_eval.py --model_generated_outputs_path data/sample/codex_greedy_outputs_converted.jsonl --output_report_file_path data/sample/codex_greedy_outputs_converted.jsonl.report --num_trials 2 --num_problems_to_evaluate 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aa8f96",
   "metadata": {},
   "source": [
    "### Step 54"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a5c7ae",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:24, End time: 17:48:25 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5217a6",
   "metadata": {},
   "source": [
    "**Thought**: The error message indicates that the `KeyError` for `'input'` persists, and the debug statement should have printed the available columns in `gen_df`. However, it seems the error occurs before the logging statement is executed.\n",
    "\n",
    "To further investigate, let's manually print the columns of `gen_df` right after it is read, before any operations are performed on it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605ec161",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d911114c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/pie-perf/src/codenet_eval/run_eval.py (487 lines total)]\n(57 more lines above)\n58:    # Step 2: run the programs\n59:    \n60:    # (dataframe col, suffix for the file)\n61:    \n62:    lang_file_ending = lang2file_ending[cfg.language]\n63:    tag_to_path = [\n64:        (\"code_v0\", f\"_slow.{lang_file_ending}\"),\n65:        (\"reference\", f\"_reference.{lang_file_ending}\"),\n66:    ]\n67:    \n68:    # check if there are multiple generations per input\n69:    is_multigen = isinstance(merged[cfg.model_generated_potentially_faster_code_col].iloc[0], list)\n70:    if is_multigen:\n71:        \n72:        num_generations = len(merged[cfg.model_generated_potentially_faster_code_col].iloc[0])\n73:        tag_to_path.extend([(f\"{cfg.model_generated_potentially_faster_code_col}_{i}\", f\"_maybe_faster_{i}.{lang_file_ending}\") for i in range(num_generations)])\n74:\n75:    else:\n76:        tag_to_path.append((cfg.model_generated_potentially_faster_code_col, f\"_maybe_faster_0.{lang_file_ending}\"))\n77:    \n78:    results = run_programs(cfg, merged, problem_id_to_ground_truths, output_code_location, tag_to_path)\n79:    \n80:    if is_multigen:\n81:        results = get_best_generation_per_submission(results, gen_col=cfg.model_generated_potentially_faster_code_col)\n82:    \n83:    # Step 3: summarize the results, write report\n84:    print_summary(cfg, merged, results, gen_col=cfg.model_generated_potentially_faster_code_col)\n85:\n86:    if isinstance(cfg.temp_dir, tempfile.TemporaryDirectory):\n87:        cfg.temp_dir.cleanup()\n88:        \n89:\n90:\n91:def read_inputs_and_prepare(cfg) -> pd.DataFrame:\n92:    \"\"\"Reads the model generated output, the reference, joins them, and returns a dataframe with the merged data.\"\"\"\n93:    logging.info(f\"Reading reference file from {cfg.reference_file_path}\")\n94:    logging.info(f\"Reading model generated outputs from {cfg.model_generated_outputs_path}\")\n95:\n96:    logging.info(\n97:        f\"Running each program {cfg.num_trials} times, skipping the first {cfg.ignore_first_k} runs, and getting the input-output pairs from {cfg.inputs_outputs_basepath}\"\n98:    )\n99:    \n100:    gen_df = pd.read_json(\n101:        cfg.model_generated_outputs_path, lines=True, orient=\"records\"\n102:    )\n103:    \n104:    # if file ends in .report, then it is a re-run. We can filter out the rows that have already been run and just return the new rows\n105:    if cfg.model_generated_outputs_path.endswith(\".report\"):\n106:        return _prepare_for_rerun(gen_df, cfg)\n107:    \n108:    print(f\"Columns in gen_df: {gen_df.columns}\")\n109:    if cfg.is_prompt_based:\n110:        gen_df[\"slower_program\"] = gen_df.apply(\n111:            lambda x: get_input_from_prompt(x), axis=1\n112:        )\n113:    else:\n114:        try:\n115:            gen_df[\"slower_program\"] = gen_df[cfg.slow_code_col].apply(lambda x: x.strip())\n116:        except KeyError as e:\n117:            logging.error(f\"KeyError: {e}. Available columns: {gen_df.columns}\")\n118:            raise\n119:\n120:\n121:    if cfg.reference_file_path is not None:\n122:        ref_df = pd.read_json(cfg.reference_file_path, lines=True, orient=\"records\")\n123:        ref_df[\"slower_program\"] = ref_df[\"code_v0\"].apply(\n124:            lambda x: x.strip().replace(\"\\n\\n\\n\\n\\n\", \"\")\n125:            # TODO: remove this hack, which is needed because sometimes prompt-lib does not retain the \n126:            # entire input\n127:        )\n128:        \n129:        logging.info(f\"Unique inputs in reference: {len(ref_df['slower_program'].unique())}\")\n130:        gen_df[\"slower_program\"] = gen_df[\n131:            \"slower_program\"\n132:        ].apply(lambda x: x.strip().replace(\"\\n\\n\\n\\n\\n\", \"\"))\n133:        assert len(ref_df[\"submission_id_v0\"].unique()) == len(\n134:            ref_df\n135:        ), \"submission_id_v0 should be unique\"\n136:\n137:        merged = pd.merge(\n138:            gen_df,\n139:            ref_df,\n140:            left_on=\"slower_program\",\n141:            right_on=\"slower_program\",\n142:            suffixes=(\"\", \"_ref\"),\n143:            how=\"inner\",\n144:        )\n145:\n146:\n147:        merged = merged.drop_duplicates(subset=[\"slower_program\"])\n148:        \n149:        assert abs(len(merged) - len(gen_df)) < 10, f\"Merging should not lose too many rows! Check if the inputs are the same. Merge lost {len(gen_df) - len(merged)} rows. len(gen_df)={len(gen_df)}, len(merged)={len(merged)}\"\n150:    else:\n151:        assert (\n152:            cfg.reference_code_col in gen_df.columns\n153:        ), f\"Column {cfg.reference_code_col} not found in {cfg.model_generated_outputs_path}\"\n154:        merged = gen_df\n155:        \n156:        \n157:        merged = merged[merged[cfg.slow_code_col] != merged[cfg.reference_code_col]]\n(330 more lines below)\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit 108:108\n",
    "    print(f\"Columns in gen_df: {gen_df.columns}\")\n",
    "end_of_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067a6f6a",
   "metadata": {},
   "source": [
    "### Step 55"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4028d94f",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:28, End time: 17:48:29 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3173d168",
   "metadata": {},
   "source": [
    "**Thought**: The print statement has been added to display the columns of `gen_df`. Let's run the script again to see the output and verify if the expected columns are present.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f35a74",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f82ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Columns in gen_df: Index(['user_id', 'problem_id', 'language', 'submission_id_v0',\n       'submission_id_v1', 'cpu_time_v0', 'cpu_time_v1', 'memory_v0',\n       'memory_v1', 'status_v0', 'status_v1', 'improvement_frac', 'code_v0',\n       'code_v1', 'code_v0_loc', 'code_v1_loc'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3653, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 147, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 176, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'input'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/content/pie-perf/src/codenet_eval/run_eval.py\", line 487, in <module>\n    evaluate_generated_outputs(evaluation_config)\n  File \"/content/pie-perf/src/codenet_eval/run_eval.py\", line 43, in evaluate_generated_outputs\n    merged = read_inputs_and_prepare(cfg)\n  File \"/content/pie-perf/src/codenet_eval/run_eval.py\", line 115, in read_inputs_and_prepare\n    gen_df[\"slower_program\"] = gen_df[cfg.slow_code_col].apply(lambda x: x.strip())\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/frame.py\", line 3761, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3655, in get_loc\n    raise KeyError(key) from err\nKeyError: 'input'\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!PYTHONPATH=$(pwd) python src/codenet_eval/run_eval.py --model_generated_outputs_path data/sample/codex_greedy_outputs_converted.jsonl --output_report_file_path data/sample/codex_greedy_outputs_converted.jsonl.report --num_trials 2 --num_problems_to_evaluate 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c9a0a8",
   "metadata": {},
   "source": [
    "### Step 56"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca5dc81",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:32, End time: 17:48:32 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a3ead5",
   "metadata": {},
   "source": [
    "**Thought**: The output shows that the `gen_df` DataFrame contains the expected columns, including `code_v0` and `code_v1`. However, the error persists, indicating that the script is still trying to access a column named `'input'`.\n",
    "\n",
    "It seems there might be a misconfiguration or a hardcoded reference elsewhere in the script or configuration. Let's search the entire repository for any remaining references to `'input'` to ensure we have updated all occurrences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260479d6",
   "metadata": {},
   "source": [
    "**Action (search_dir)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed447adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Found 98 matches for \"input\" in /content/pie-perf/src:\n/content/pie-perf/src/codenet_eval/eval_submissions/all_submissions_time_eval.py (4 matches)\n/content/pie-perf/src/codenet_eval/eval_submissions/prep_for_eval.py (1 matches)\n/content/pie-perf/src/codenet_eval/evalconfig.py (7 matches)\n/content/pie-perf/src/codenet_eval/run_eval.py (28 matches)\n/content/pie-perf/src/codenet_eval/sandbox.py (40 matches)\n/content/pie-perf/src/make_splits.py (3 matches)\n/content/pie-perf/src/utils/format_utils.py (6 matches)\n/content/pie-perf/src/utils/print_diffs.py (6 matches)\n/content/pie-perf/src/utils/remove_seps.py (3 matches)\nEnd of matches for \"input\" in /content/pie-perf/src\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_dir 'input' src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a54260d",
   "metadata": {},
   "source": [
    "### Step 57"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5c82ed",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:35, End time: 17:48:35 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cc1adf",
   "metadata": {},
   "source": [
    "**Thought**: The search results indicate multiple references to `'input'` across various files in the `src` directory. The `sandbox.py` file has a significant number of matches, which might be relevant to the error we're encountering.\n",
    "\n",
    "Let's open the `sandbox.py` file and review the references to `'input'` to determine if any need to be updated to `code_v0`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ab7f72",
   "metadata": {},
   "source": [
    "**Action (open)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a906091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File: /content/pie-perf/src/codenet_eval/sandbox.py (589 lines total)]\n1:# utilities and code that handles actual execution of programs\n2:import pathlib\n3:import shlex\n4:import subprocess\n5:from typing import Dict, List, Tuple, Union\n6:import time\n7:import numpy as np\n8:import resource\n9:import logging\n10:import psutil\n11:import os\n12:import sys\n13:import traceback\n14:import pdb\n15:import glob\n16:\n17:# disable logging from psutil\n18:logging.getLogger(\"psutil\").setLevel(logging.WARNING)\n19:\n20:# disable logging from resource\n21:logging.getLogger(\"resource\").setLevel(logging.WARNING)\n22:\n23:# disable logging from subprocess\n24:logging.getLogger(\"subprocess\").setLevel(logging.WARNING)\n25:\n26:logging.basicConfig(level=logging.CRITICAL)\n27:\n28:DEBUG=True\n29:\n30:\n31:def is_linux():\n32:    from sys import platform\n33:    if platform == \"linux\" or platform == \"linux2\":\n34:        return True\n35:    else: \n36:        return False\n37:\n38:\n39:def run_python_code_on_inputs(\n40:    code_path: str,\n41:    unit_test_data_basepath: str,\n42:    num_runs_per_test_case: int,\n43:    ignore_first_k: int,\n44:    max_seconds_per_run: int,\n45:    ground_truths: List[str] = None,  # type: ignore\n46:    num_test_cases: int = None,  # type: ignore\n47:    cpu_number: int = 1,  # which CPU to run the code on, counting begins from 1\n48:    return_per_trial_times: bool = False,\n49:    python_bin: str = \"python\",\n50:    return_dict: bool = False,\n51:    cflags: str = None, # type: ignore\n52:    return_if_acc_below: float = None,  # type: ignore | if the accuracy is below this value, then return\n53:) -> Union[Tuple[float, float, float], Tuple[float, float, float, List[List[float]]], Dict]:\n54:    \"\"\"\n55:    Run the given code on the inputs for the given problem_id, and returns (avg_time, std_time, avg_acc).\n56:    The inputs are sourced from the unit test data, where a number of files of the form: {input,output}.{0, 1, 2}.txt are present.\n57:\n58:\n59:    NOTE: It is optional to pass ground_truths. If they are not passed, then the accuracy will not be computed.\n60:\n61:\n62:    \"\"\"\n63:\n64:    if num_test_cases is None:\n65:        num_test_cases = len(ground_truths)\n66:\n67:    times_millisec, accs = [], []\n68:    per_trial_times = []\n69:    for test_case_idx in range(num_test_cases):\n70:        if is_linux(): \n71:            cmd = (\n72:                f\"taskset --cpu-list {cpu_number} {python_bin} {code_path}\"  # taskset 00 python code.py\n73:            )\n74:        else: \n75:            cmd = f\"{python_bin} {code_path}\"\n76:        subprocess_args = shlex.split(cmd)\n77:        input_file_path = f\"{unit_test_data_basepath}/input.{test_case_idx}.txt\"\n78:        # logging.info(f\"Running command: {cmd} < {input_file_path}\")\n79:        _per_trial_times = []\n80:        for trial_idx in range(num_runs_per_test_case):\n81:            try:\n82:                time_start = time.time()\n83:                output = run_cmd_for_time_eval(\n84:                    subprocess_args,\n85:                    input_file_path=input_file_path,\n86:                    timeout_seconds=max_seconds_per_run,\n87:                )\n88:                time_taken = time.time() - time_start\n89:                _per_trial_times.append(time_taken)\n90:                if output is None:\n91:                    return (np.nan, np.nan, 0)\n92:                    # timeout: since we have a generous timeout, this should not happen\n93:\n94:                if trial_idx >= ignore_first_k:\n95:                    times_millisec.append(time_taken * 1000)\n96:                    if ground_truths is not None:\n97:                        accuracy = get_accuracy(output, ground_truths[test_case_idx])\n98:                        if return_if_acc_below is not None and accuracy < return_if_acc_below:\n99:                            logging.info(f\"Accuracy {accuracy} below {return_if_acc_below}. Returning.\")\n100:                            return (time_taken, 0, accuracy)\n(489 more lines below)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open src/codenet_eval/sandbox.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084c3acf",
   "metadata": {},
   "source": [
    "### Step 58"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0f4813",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:38, End time: 17:48:38 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e358767",
   "metadata": {},
   "source": [
    "**Thought**: The `sandbox.py` file contains a function `run_python_code_on_inputs` that references input files using the pattern `{input,output}.{0, 1, 2}.txt`. This suggests that the script expects input files in a specific format, which might not be directly related to the column name issue.\n",
    "\n",
    "Since the error persists, it might be beneficial to revisit the `run_eval.py` script and ensure that all references to the column name are correctly updated. Let's search for any remaining references to `'input'` in the `run_eval.py` script specifically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a570d819",
   "metadata": {},
   "source": [
    "**Action (search_file)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e238ca9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Found 28 matches for \"input\" in /content/pie-perf/src/codenet_eval/run_eval.py:\nLine 4:1. Reads the inputs and model generated programs, writes them to a temporary directory.\nLine 9:export REF_FILE=\"data/codenet/splits/problem_id/2023-01-13_12-56pm/seq2seq_splits/test.jsonl\" && export OP=\"/usr1/amadaan/learning2perf/data/outputs/beam_outputs_codegen_2b_jan_13_split.jsonl\" && export CPU=30 && nohup python -u src/codenet_eval/run_eval.py  --model_generated_outputs_path ${OP} --reference_file_path ${REF_FILE} --output_report_file_path ${OP}.25_tries.report --slow_code_col input --model_generated_potentially_faster_code_col beam_generated_target_from_input --reference_code_col target --num_problems_to_evaluate -1 --cpu_number $CPU --max_time_per_run 10 --num_trials 25 --ignore_first_k 2\nLine 24:from src.codenet_eval.sandbox import run_code_on_inputs\nLine 43:    merged = read_inputs_and_prepare(cfg)\nLine 51:    logging.info(f\"inputs/outputs basepath: {cfg.inputs_outputs_basepath}\")\nLine 53:    # Step 1: Write the inputs and model generated programs to a temporary directory\nLine 68:    # check if there are multiple generations per input\nLine 91:def read_inputs_and_prepare(cfg) -> pd.DataFrame:\nLine 97:        f\"Running each program {cfg.num_trials} times, skipping the first {cfg.ignore_first_k} runs, and getting the input-output pairs from {cfg.inputs_outputs_basepath}\"\nLine 111:            lambda x: get_input_from_prompt(x), axis=1\nLine 126:            # entire input\nLine 129:        logging.info(f\"Unique inputs in reference: {len(ref_df['slower_program'].unique())}\")\nLine 149:        assert abs(len(merged) - len(gen_df)) < 10, f\"Merging should not lose too many rows! Check if the inputs are the same. Merge lost {len(gen_df) - len(merged)} rows. len(gen_df)={len(gen_df)}, len(merged)={len(merged)}\"\nLine 167:    # if the generated code is a list, then we have multiple generations per input. \nLine 211:                glob.glob(f\"{cfg.inputs_outputs_basepath}/{problem_id}/output*.txt\")\nLine 215:            ), f\"{cfg.inputs_outputs_basepath}/{problem_id} has no ground truth files!\"\nLine 217:                with open(f\"{cfg.inputs_outputs_basepath}/{problem_id}/output.{i}.txt\") as f:\nLine 284:        unit_test_data_basepath = f\"{cfg.inputs_outputs_basepath}/{problem_id}\"\nLine 287:            # run the generated program (maybe faster), input program (slower), and reference program (definitely faster)\nLine 297:                avg_time, std_time, avg_acc = run_code_on_inputs(  # type: ignore\nLine 388:        (run_metrics[f\"{gen_col}_acc\"] > 0.99) & (run_metrics[\"input_acc\"] > 0.99)\nLine 395:        f\"[Reported in CodeNet] input program (ms): {mean_std(run_metrics, 'cpu_time_v0')}\"\nLine 402:    logging.info(f\"[Our measurement] input program (ms): {mean_std(run_metrics, 'input_time')}\")\nLine 416:            f\"Found {len(run_metrics_improved)} problems where the {gen_col} program is faster than the input program\"\nLine 419:            f\"[Our measurement] input program (ms): {mean_std(run_metrics_improved, 'input_time')}\"\nLine 449:        run_metrics[\"input_time_mean\"] - run_metrics[\"reference_time_mean\"]\nLine 450:    ) / run_metrics[\"input_time_mean\"]\nLine 462:def get_input_from_prompt(\nEnd of matches for \"input\" in /content/pie-perf/src/codenet_eval/run_eval.py\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_file 'input' src/codenet_eval/run_eval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b97961a",
   "metadata": {},
   "source": [
    "### Step 59"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c814a3c7",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:52, End time: 17:48:52 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0a4ecc",
   "metadata": {},
   "source": [
    "**Thought**: Exit due to context window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f986b98",
   "metadata": {},
   "source": [
    "**Action (None)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1821e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Exited"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exit_context"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
