{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e57a9ca",
   "metadata": {},
   "source": [
    "### Query\n",
    "Train a Block-Skim model on the SQuAD-1.1 dataset using the `run_squad.sh` script and evaluate its performance. Report the exact match and f1 score as a json structured as follows: {\"exact_match\": 0.0, \"f1\": 0.0} (replace 0.0 with the actual values).\n",
    "\n",
    "Additional instructions:\n",
    "1. Only load the first 10 examples of each (training/validation) set in the dataset.\n",
    "2. Run for only 1 epoch.\n",
    "3. Use the following hyper-parameters: per_gpu_train_batch_size=16, per_gpu_eval_batch_size=16, learning_rate=3e-5, max_seq_length=512, skim_factor=0.1, balance_factor=20.\n",
    "\n",
    "Git repository: https://github.com/chandlerguan/blockskim\n",
    "\n",
    "I have already executed some of the required steps. Now, your goal is to complete the remaining steps and submit the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f6a8b0",
   "metadata": {},
   "source": [
    "### Prompt\n",
    "<details>\n",
    "[{'role': 'system', 'content': 'Interact in a jupyter notebook to solve the request.\\nOutput a thought and an action.\\nThought can reason about the current situation, and action allows you to interact with the Jupyter notebook. There are three types of actions:\\n(1) execute, which executes a notebook cell with the command under the \"content\" field. This could be a bash command (beginning with !), a python command, or magic command (beginning with %).\\n(2) edit, which allows you to replace existing lines in a file with new lines. To replace lines, you will need to provide the lines as they appear before the replacement, and the lines to replace them with.\\nIn the \"content\" field, format should be ```{filename}\\n<<<BEFORE_EDIT>>>\\nlines before edit\\nlines before edit\\n<<<AFTER_EDIT>>>\\nlines after edit\\nlines after edit```\\n  - Include the entire lines WITH indentation. After editing, make sure indentation is correct.\\n(3) submit, which submits the final answer (or null if there\\'s no answer to submit), formatted as JSON (make sure to use double quotes for keys and values).\\nAlways output exactly one \"thought\" and one \"action\", and encapsulate the content with <content>...</content> tags.\\nI will provide the cell output (observation) for you.\\n\\nTips:\\n- When a github repository is given, you should start by cloning it. You can read the readme file and browse the repo for installation and usage instructions.\\n- Avoid using conda, instead use pip for package management.\\n- Each execution has a 5-minute timeout. After this period, you can decide whether to interrupt the kernel or continue waiting.\\n  You should interrupt if: (1) you are prompted with an interactive message (e.g., \"Do you want to continue? [Y/n]\"). In that case, you can bypass it by executing echo \"y\" | command, or (2) the execution is taking longer than expected because the wrong task is being executed, such as a different model or incorrect number of epochs.\\n  You should continue to wait if the current execution is correct and proceeding as expected.\\n\\nHere are some examples.\\nGit repository: EleutherAI/lm-evaluation-harness\\nQuery: Evaluate EleutherAI/gpt-j-6B on hellaswag. What\\'s the score?\\nThought: First, I need to clone the GitHub repository \\'EleutherAI/lm-evaluation-harness\\'.\\nAction: execute\\n<content>\\n!git clone https://github.com/EleutherAI/lm-evaluation-harness.git\\n</content>\\nObservation: ```\\nCloning into \\'lm-evaluation-harness\\'...\\n```\\nThought: The repository has been cloned. Next, I should check the README file for installation and usage instructions to proceed with evaluating EleutherAI/gpt-j-6B on hellaswag.\\nAction: execute\\n<content>\\n!cat lm-evaluation-harness/README.md\\n</content>\\nObservation: ```# Language Model Evaluation Harness ... truncated ...\\n```\\nThought: The README provides detailed instructions for installing the `lm-eval` package and evaluating models like GPT-J-6B on tasks like hellaswag. I need to install the package and its dependencies first.\\nAction: execute\\n<content>\\n!pip install -e lm-evaluation-harness/.\\n</content>\\nObservation: ```\\nObtaining file:///home/nora/lm-evaluation-harness\\n  Installing build dependencies ... truncated ...\\n```\\nThought: The `lm-eval` package and its dependencies have been installed successfully. Now, I can proceed with evaluating the EleutherAI/gpt-j-6B model on the hellaswag task as per the instructions in the README.\\nAction: execute\\n<content>\\n!lm_eval --model hf --model_args pretrained=EleutherAI/gpt-j-6B --tasks hellaswag --device cuda:0 --batch_size 8\\n</content>\\nObservation: ```\\n{\\n  \"hellaswag\": {\\n    \"acc\": 0.24603658536585363,\\n    \"acc_norm\": 0.34436059706446426,\\n  },  ... truncated ...\\n```\\nThought: The `lm-eval` was executed successfully, providing the metrics I needed. The `acc` metric is 0.2460. The `acc_norm` metric is 0.3444\\nAction: submit\\n<content>\\n{\"acc\": 0.2460, \"acc_norm\": 0.3444}\\n</content>\\n\\nQuery: use pandas to read the file \\'data.csv\\' and display the first 2 rows.\\nThought: First, I need to read the file \\'data.csv\\' using the pandas library.\\nAction: execute\\n<content>\\nimport pandas as pd\\npd.read_csv(\\'data.csv\\').head()\\n</content>\\nObservation: ```\\n    name\\n  0  Alice\\n   1    Bob\\n  2  Charlie\\n```\\nThought: The file \\'data.csv\\' has been read successfully and the first 2 rows have been displayed.\\nAction: submit\\n<content>\\n[\"Alice\", \"Bob\"]\\n</content>\\n\\nQuery: Change the model name in the \\'config.json\\' file from \\'roberta-base\\' to \\'roberta-large\\'.\\nThought: First, I need to read the \\'config.json\\' file to find the location where I can replace the model name.\\nAction: execute\\n<content>\\n!cat config.json\\n</content>\\nObservation: ```\\n{\\n    \"model\": \"roberta-base\",\\n    \"batch_size\": 8,\\n    \"epochs\": 10\\n}\\n```\\nThought: The file has been read successfully. Now, I can replace the model name in the \\'config.json\\' file.\\nAction: edit\\n<content>\\nconfig.json\\n<<<BEFORE_EDIT>>>\\n{\\n    \"model\": \"roberta-base\",\\n    \"batch_size\": 8,\\n<<<AFTER_EDIT>>>\\n{\\n    \"model\": \"roberta-large\",\\n    \"batch_size\": 8,\\n</content>\\nObservation: \"editing file config.json\"\\nThought: The model name has been replaced in the \\'config.json\\' file.\\nAction: submit\\n<content>\\nnull\\n</content>\\n'}, {'role': 'user', 'content': 'Query: Train a Block-Skim model on the SQuAD-1.1 dataset using the `run_squad.sh` script and evaluate its performance. Report the exact match and f1 score as a json structured as follows: {\"exact_match\": 0.0, \"f1\": 0.0} (replace 0.0 with the actual values).\\n\\nAdditional instructions:\\n1. Only load the first 10 examples of each (training/validation) set in the dataset.\\n2. Run for only 1 epoch.\\n3. Use the following hyper-parameters: per_gpu_train_batch_size=16, per_gpu_eval_batch_size=16, learning_rate=3e-5, max_seq_length=512, skim_factor=0.1, balance_factor=20.\\n\\nGit repository: https://github.com/chandlerguan/blockskim\\n\\nI have already executed some of the required steps. Now, your goal is to complete the remaining steps and submit the answer.'}]\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9631513d",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ad4712",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:22, End time: 17:48:22 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d2d4cc",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccbff6a",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038e5b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cloning into 'blockskim'...\nremote: Enumerating objects: 791, done.\u001b[K\nremote: Counting objects: 100% (791/791)\u001b[K\nremote: Counting objects: 100% (791/791), done.\u001b[K\nremote: Compressing objects: 100% (423/423)\u001b[K\nremote: Compressing objects: 100% (423/423), done.\u001b[K\nremote: Total 791 (delta 420), reused 732 (delta 361), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (791/791)\nReceiving objects: 100% (791/791), 412.31 KiB | 25.77 MiB/s, done.\nResolving deltas: 100% (420/420)\nResolving deltas: 100% (420/420), done.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!git clone https://github.com/chandlerguan/blockskim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5aab18",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ecf7da",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:22, End time: 17:48:22 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137bf9ce",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d359a22f",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825a0094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/blockskim\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd blockskim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330d3081",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d87ab5",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:22, End time: 17:48:22 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf7e616",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0b0ee6",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83467dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%mkdir dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7783b2",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd1b668",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:22, End time: 17:48:23 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc97422",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cf92ed",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a51024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/blockskim/dataset\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2634683b",
   "metadata": {},
   "source": [
    "### Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6e118e",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:23, End time: 17:48:24 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdcb9a9",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5370f8fb",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89355e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Downloading...\nFrom: https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\nTo: /content/blockskim/dataset/train-v1.1.json\n\n  0%|                                               | 0.00/8.12M [00:00<?, ?B/s]\n\n\n\nDownloading...\nFrom: https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\nTo: /content/blockskim/dataset/dev-v1.1.json\n\n  0%|                                               | 0.00/1.05M [00:00<?, ?B/s]\n\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!gdown https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\n",
    "!gdown https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be95d41",
   "metadata": {},
   "source": [
    "### Step 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e49d97",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:24, End time: 17:48:26 (duration: 1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20f1294",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebf1441",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc542358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('train-v1.1.json', 'r') as f:\n",
    "    data_train = json.load(f)\n",
    "\n",
    "with open('dev-v1.1.json', 'r') as f:\n",
    "    data_test = json.load(f)\n",
    "\n",
    "keyes_train = list(data_train.keys())\n",
    "for key in keyes_train:\n",
    "  if key not in keyes_train[:10]:\n",
    "    del data_train[key]\n",
    "\n",
    "keyes_test = list(data_test.keys())\n",
    "for key in keyes_test:\n",
    "  if key not in keyes_test[:10]:\n",
    "    del data_test[key]\n",
    "\n",
    "# write in file\n",
    "with open('train-v1.1.json', 'w') as f:\n",
    "    json.dump(data_train, f)\n",
    "\n",
    "with open('dev-v1.1.json', 'w') as f:\n",
    "    json.dump(data_test, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c342c5",
   "metadata": {},
   "source": [
    "### Step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ad74f2",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:26, End time: 17:48:26 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe7599a",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e2a992",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d325f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/content/blockskim\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /content/blockskim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7f007a",
   "metadata": {},
   "source": [
    "### Step 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210a46f6",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:26, End time: 17:48:27 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565f0b92",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d87ed2",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6c5bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n\n            if args.version_2_with_negative:\n                logger.warning(\"tensorflow_datasets does not handle version 2 of SQuAD.\")\n\n            tfds_examples = tfds.load(\"squad\")\n            examples = SquadV1Processor().get_examples_from_dataset(tfds_examples, evaluate=evaluate)\n        else:\n            processor = SquadV2Processor() if args.version_2_with_negative else SquadV1Processor()\n            if evaluate:\n                examples = processor.get_dev_examples(args.data_dir, filename=args.predict_file)\n                examples = examples[:args.num_eval_examples]\n            else:\n                print(f\"loading train examples from {args.data_dir} grom {args.train_file} , {args.predict_file}\")\n                examples = processor.get_train_examples(args.data_dir, filename=args.train_file)\n                examples = examples[:args.num_train_examples]\n\n        features, dataset = squad_convert_examples_to_features(\n            examples=examples,\n            tokenizer=tokenizer,\n            max_seq_length=args.max_seq_length,\n            doc_stride=args.doc_stride,\n            max_query_length=args.max_query_length,\n            is_training=not evaluate,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/blockskim/src/run_squad.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "            processor = SquadV2Processor() if args.version_2_with_negative else SquadV1Processor()\n",
    "            if evaluate:\n",
    "                examples = processor.get_dev_examples(args.data_dir, filename=args.predict_file)\n",
    "            else:\n",
    "                examples = processor.get_train_examples(args.data_dir, filename=args.train_file)\n",
    "\n",
    "        features, dataset = squad_convert_examples_to_features(\n",
    "<<<AFTER_EDIT>>>\n",
    "            processor = SquadV2Processor() if args.version_2_with_negative else SquadV1Processor()\n",
    "            if evaluate:\n",
    "                examples = processor.get_dev_examples(args.data_dir, filename=args.predict_file)\n",
    "                examples = examples[:args.num_eval_examples]\n",
    "            else:\n",
    "                print(f\"loading train examples from {args.data_dir} grom {args.train_file} , {args.predict_file}\")\n",
    "                examples = processor.get_train_examples(args.data_dir, filename=args.train_file)\n",
    "                examples = examples[:args.num_train_examples]\n",
    "\n",
    "        features, dataset = squad_convert_examples_to_features(\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2532cdf8",
   "metadata": {},
   "source": [
    "### Step 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992d4e3f",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:27, End time: 17:48:27 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b96646",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f408abbc",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfa8399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n    parser.add_argument(\"--threads\", type=int, default=1, help=\"multiple threads for converting example to features\")\n\n    parser.add_argument(\"--block_skim\", action=\"store_true\", help=\"add block skim module\")\n    parser.add_argument(\"--actual_skim\", action=\"store_true\", help=\"perform actual skimming on input seq dim.\")\n    parser.add_argument(\"--block_size\", type=int, default=32, help=\"block size for block skim module\")\n    parser.add_argument(\"--skim_factor\", default=0.0001, type=float, help=\"factor for skim predictor\")\n    parser.add_argument(\"--balance_factor\", default=1, type=float, help=\"factor for skim predictor\")\n    parser.add_argument(\"--cache_name\", type=str, help=\"cached feature dir\")\n    parser.add_argument(\"--augment_layers\", type=int, nargs=\"+\", help=\"layers to augment blockskim module\")\n    parser.add_argument(\"--skim_threshold\", type=float, default=0.001, help=\"threshold for skim predictor\")\n    parser.add_argument(\"--num_eval_examples\", type=int, default=-1, help=\"number of examples to evaluate\")\n    parser.add_argument(\"--num_train_examples\", type=int, default=-1, help=\"number of examples to train\")\n\n    args = parser.parse_args()\n\n    if args.doc_stride >= args.max_seq_length - args.max_query_length:\n        logger.warning(\n            \"WARNING - You've set a doc stride which may be superior to the document length in some \"\n            \"examples. This could result in errors when building features from the examples. Please reduce the doc \"\n            \"stride or increase the maximum length to ensure the features are correctly built.\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/blockskim/src/run_squad.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "    parser.add_argument(\"--cache_name\", type=str, help=\"cached feature dir\")\n",
    "    parser.add_argument(\"--augment_layers\", type=int, nargs=\"+\", help=\"layers to augment blockskim module\")\n",
    "    parser.add_argument(\"--skim_threshold\", type=float, default=0.001, help=\"threshold for skim predictor\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "<<<AFTER_EDIT>>>\n",
    "    parser.add_argument(\"--cache_name\", type=str, help=\"cached feature dir\")\n",
    "    parser.add_argument(\"--augment_layers\", type=int, nargs=\"+\", help=\"layers to augment blockskim module\")\n",
    "    parser.add_argument(\"--skim_threshold\", type=float, default=0.001, help=\"threshold for skim predictor\")\n",
    "    parser.add_argument(\"--num_eval_examples\", type=int, default=-1, help=\"number of examples to evaluate\")\n",
    "    parser.add_argument(\"--num_train_examples\", type=int, default=-1, help=\"number of examples to train\")\n",
    "\n",
    "    args = parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25674590",
   "metadata": {},
   "source": [
    "### Step 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16148ad",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:27, End time: 17:48:28 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897d64d2",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b6d0bd",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df22c2ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        Any layer with in index other than [0,1,2,3] will result in an error. See base class PreTrainedModel for more\n        information about head pruning\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            group_idx = int(layer / self.config.inner_group_num)\n            inner_group_idx = int(layer - group_idx * self.config.inner_group_num)\n            self.encoder.albert_layer_groups[group_idx].albert_layers[inner_group_idx].attention.prune_heads(heads)\n\n    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=BaseModelOutputWithPooling,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/blockskim/src/modeling_albert_skim.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=BaseModelOutputWithPooling,\n",
    "<<<AFTER_EDIT>>>\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=BaseModelOutputWithPooling,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8e17c3",
   "metadata": {},
   "source": [
    "### Step 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e12832b",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:28, End time: 17:48:28 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b98e7e7",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e6330b",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a1dbf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        return self.predictions.decoder\n\n    def set_output_embeddings(self, new_embeddings):\n        self.predictions.decoder = new_embeddings\n\n    def get_input_embeddings(self):\n        return self.albert.embeddings.word_embeddings\n\n    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=MaskedLMOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/blockskim/src/modeling_albert_skim.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=MaskedLMOutput,\n",
    "<<<AFTER_EDIT>>>\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=MaskedLMOutput,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce70d979",
   "metadata": {},
   "source": [
    "### Step 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814c4826",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:28, End time: 17:48:29 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b42a197",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d173fa7e",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1956c2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        self.config = config\n\n        self.albert = AlbertModel(config)\n        self.dropout = nn.Dropout(config.classifier_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=SequenceClassifierOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/blockskim/src/modeling_albert_skim.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=SequenceClassifierOutput,\n",
    "<<<AFTER_EDIT>>>\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=SequenceClassifierOutput,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf43edd",
   "metadata": {},
   "source": [
    "### Step 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00067090",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:29, End time: 17:48:29 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58edc656",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab2bd9b",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c209335d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        self.num_labels = config.num_labels\n\n        self.albert = AlbertModel(config, add_pooling_layer=False)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=TokenClassifierOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/blockskim/src/modeling_albert_skim.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=TokenClassifierOutput,\n",
    "<<<AFTER_EDIT>>>\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=TokenClassifierOutput,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030f573b",
   "metadata": {},
   "source": [
    "### Step 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbb0b75",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:29, End time: 17:48:30 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3e273d",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d54958",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d83f222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.albert = AlbertModel(config, add_pooling_layer=False)\n        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=QuestionAnsweringModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/blockskim/src/modeling_albert_skim.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=QuestionAnsweringModelOutput,\n",
    "<<<AFTER_EDIT>>>\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=QuestionAnsweringModelOutput,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb24a22",
   "metadata": {},
   "source": [
    "### Step 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad2406d",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:30, End time: 17:48:30 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457641c1",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0c4225",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257bc5d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        super().__init__(config)\n\n        self.albert = AlbertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, 1)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=MultipleChoiceModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/blockskim/src/modeling_albert_skim.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=MultipleChoiceModelOutput,\n",
    "<<<AFTER_EDIT>>>\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=MultipleChoiceModelOutput,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8579231f",
   "metadata": {},
   "source": [
    "### Step 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69265068",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:30, End time: 17:48:31 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b2662d",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272c3002",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c8cb51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        \"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n            self.encoder.layer[layer].attention.self.skim_predictor.prune_heads(heads)\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/blockskim/src/modeling_bert_skim.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n",
    "<<<AFTER_EDIT>>>\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe1ad96",
   "metadata": {},
   "source": [
    "### Step 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5689f7",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:31, End time: 17:48:32 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c098af1",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f17e92",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ec1035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        self.init_weights()\n\n    def get_output_embeddings(self):\n        return self.cls.predictions.decoder\n\n    def set_output_embeddings(self, new_embeddings):\n        self.cls.predictions.decoder = new_embeddings\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=MaskedLMOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/blockskim/src/modeling_bert_skim.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=MaskedLMOutput,\n",
    "<<<AFTER_EDIT>>>\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=MaskedLMOutput,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696cf121",
   "metadata": {},
   "source": [
    "### Step 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3812675e",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:32, End time: 17:48:32 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4771de3",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38794b1",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a2d337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        self.config = config\n\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=SequenceClassifierOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/blockskim/src/modeling_bert_skim.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=SequenceClassifierOutput,\n",
    "<<<AFTER_EDIT>>>\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=SequenceClassifierOutput,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db88ba88",
   "metadata": {},
   "source": [
    "### Step 19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58e32bc",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:32, End time: 17:48:33 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2de411",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162afaf2",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d247828f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        super().__init__(config)\n\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, 1)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=MultipleChoiceModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/blockskim/src/modeling_bert_skim.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=MultipleChoiceModelOutput,\n",
    "<<<AFTER_EDIT>>>\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=MultipleChoiceModelOutput,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0ed21a",
   "metadata": {},
   "source": [
    "### Step 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eecdc89",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:33, End time: 17:48:33 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac8359f",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0029b441",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c9da69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config, add_pooling_layer=False)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=TokenClassifierOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/blockskim/src/modeling_bert_skim.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=TokenClassifierOutput,\n",
    "<<<AFTER_EDIT>>>\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=TokenClassifierOutput,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40b0fa8",
   "metadata": {},
   "source": [
    "### Step 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d67fe05",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:33, End time: 17:48:34 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6ea0c8",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38877efb",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1346ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config, add_pooling_layer=False)\n        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=QuestionAnsweringModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/blockskim/src/modeling_bert_skim.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=QuestionAnsweringModelOutput,\n",
    "<<<AFTER_EDIT>>>\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=QuestionAnsweringModelOutput,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed5f1fc",
   "metadata": {},
   "source": [
    "### Step 22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8987c2",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:34, End time: 17:48:34 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401e3476",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a3f498",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2c038c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n    def _prune_heads(self, heads_to_prune):\n        \"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.transformer.layer[layer].attention.prune_heads(heads)\n\n    @add_start_docstrings_to_model_forward(DISTILBERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=BaseModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/blockskim/src/modeling_distilbert_skim.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(DISTILBERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=BaseModelOutput,\n",
    "<<<AFTER_EDIT>>>\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(DISTILBERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=BaseModelOutput,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0be5a8",
   "metadata": {},
   "source": [
    "### Step 23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b73e07d",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:34, End time: 17:48:35 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f91508",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1247b590",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba079aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        self.mlm_loss_fct = nn.CrossEntropyLoss()\n\n    def get_output_embeddings(self):\n        return self.vocab_projector\n\n    def set_output_embeddings(self, new_embeddings):\n        self.vocab_projector = new_embeddings\n\n    @add_start_docstrings_to_model_forward(DISTILBERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=MaskedLMOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/blockskim/src/modeling_distilbert_skim.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(DISTILBERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=MaskedLMOutput,\n",
    "<<<AFTER_EDIT>>>\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(DISTILBERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=MaskedLMOutput,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49a8659",
   "metadata": {},
   "source": [
    "### Step 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4644d28b",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:35, End time: 17:48:35 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696a63bc",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8fc370",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31334341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n\n        self.distilbert = DistilBertModel(config)\n        self.pre_classifier = nn.Linear(config.dim, config.dim)\n        self.classifier = nn.Linear(config.dim, config.num_labels)\n        self.dropout = nn.Dropout(config.seq_classif_dropout)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(DISTILBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=SequenceClassifierOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/blockskim/src/modeling_distilbert_skim.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(DISTILBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=SequenceClassifierOutput,\n",
    "<<<AFTER_EDIT>>>\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(DISTILBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=SequenceClassifierOutput,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09347d5",
   "metadata": {},
   "source": [
    "### Step 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef55fc7a",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:35, End time: 17:48:36 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bb68b4",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0f2c01",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c20b6c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n\n        self.distilbert = DistilBertModel(config)\n        self.qa_outputs = nn.Linear(config.dim, config.num_labels)\n        assert config.num_labels == 2\n        self.dropout = nn.Dropout(config.qa_dropout)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(DISTILBERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=QuestionAnsweringModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/blockskim/src/modeling_distilbert_skim.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(DISTILBERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=QuestionAnsweringModelOutput,\n",
    "<<<AFTER_EDIT>>>\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(DISTILBERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=QuestionAnsweringModelOutput,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8cc7ce",
   "metadata": {},
   "source": [
    "### Step 26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e614d1",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:36, End time: 17:48:36 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aacc453",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae89fe0",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9668af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n        self.num_labels = config.num_labels\n\n        self.distilbert = DistilBertModel(config)\n        self.dropout = nn.Dropout(config.dropout)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(DISTILBERT_INPUTS_DOCSTRING)\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=TokenClassifierOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/blockskim/src/modeling_distilbert_skim.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(DISTILBERT_INPUTS_DOCSTRING)\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=TokenClassifierOutput,\n",
    "<<<AFTER_EDIT>>>\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(DISTILBERT_INPUTS_DOCSTRING)\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=TokenClassifierOutput,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef8499c",
   "metadata": {},
   "source": [
    "### Step 27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b522deb",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:36, End time: 17:48:37 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97b3d12",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba40b08d",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db35453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\nexcept ImportError:\n    from tensorboardX import SummaryWriter\n\nfrom squad.squad import squad_convert_examples_to_features\n\nlogger = logging.getLogger(__name__)\n\nfrom modeling_bert_skim import BertForQuestionAnswering as BertForQuestionAnsweringWithSkim\nfrom modeling_albert_skim import AlbertForQuestionAnswering as AlbertForQuestionAnsweringWithSkim\nfrom modeling_blockskim import compute_skim_mask\nfrom squad.squad import SquadV1Processor, SquadV2Processor\n\nMODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\nMODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n\n\ndef set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/blockskim/src/run_squad.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "from modeling_bert_skim import BertForQuestionAnswering as BertForQuestionAnsweringWithSkim\n",
    "from modeling_albert_skim import AlbertForQuestionAnswering as AlbertForQuestionAnsweringWithSkim\n",
    "from modeling_blockskim import compute_skim_mask\n",
    "from squad.transformer_squad_processor import SquadV1Processor, SquadV2Processor\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\n",
    "<<<AFTER_EDIT>>>\n",
    "from modeling_bert_skim import BertForQuestionAnswering as BertForQuestionAnsweringWithSkim\n",
    "from modeling_albert_skim import AlbertForQuestionAnswering as AlbertForQuestionAnsweringWithSkim\n",
    "from modeling_blockskim import compute_skim_mask\n",
    "from squad.squad import SquadV1Processor, SquadV2Processor\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bc64bb",
   "metadata": {},
   "source": [
    "### Step 28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e9a031",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:37, End time: 17:48:37 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fad4f6a",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0299d248",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696bb2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n    features = []\n\n    threads = min(threads, cpu_count())\n    with Pool(threads, initializer=squad_convert_example_to_features_init, initargs=(tokenizer,)) as p:\n        annotate_ = partial(\n            squad_convert_example_to_features,\n            max_seq_length=max_seq_length,\n            doc_stride=doc_stride,\n            max_query_length=max_query_length,\n            padding_strategy=padding_strategy,\n            is_training=is_training,\n        )\n        features = list(\n            tqdm(\n                p.imap(annotate_, examples, chunksize=32),\n                total=len(examples),\n                desc=\"convert squad examples to features\",\n                disable=not tqdm_enabled,\n            )"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/blockskim/src/squad/squad.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "            doc_stride=doc_stride,\n",
    "            max_query_length=max_query_length,\n",
    "            padding_strategy=padding_strategy,\n",
    "            is_training=True,\n",
    "        )\n",
    "        features = list(\n",
    "<<<AFTER_EDIT>>>\n",
    "            doc_stride=doc_stride,\n",
    "            max_query_length=max_query_length,\n",
    "            padding_strategy=padding_strategy,\n",
    "            is_training=is_training,\n",
    "        )\n",
    "        features = list(\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee27866",
   "metadata": {},
   "source": [
    "### Step 29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde9097d",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:37, End time: 17:48:37 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f10bbe3",
   "metadata": {},
   "source": [
    "**Thought**: [pre-executed by user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f0ee95",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018b81e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n                else:\n                    doc_offset = len(truncated_query) + sequence_added_tokens\n\n                start_position = tok_start_position - doc_start + doc_offset\n                end_position = tok_end_position - doc_start + doc_offset\n\n        # chandler\n        # answer_mask: mask for answer position\n        answer_mask = np.zeros_like(span[\"token_type_ids\"])\n\n        # for evid_start_pos, evid_end_pos in evid_positions:\n        #     answer_mask[evid_start_pos:evid_end_pos+1] = 2\n\n        answer_mask[start_position:end_position+1] = 1\n\n        features.append(\n            SquadFeatures(\n                span[\"input_ids\"],\n                span[\"attention_mask\"],\n                span[\"token_type_ids\"],"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/content/blockskim/src/squad/squad.py\n",
    "<<<BEFORE_EDIT>>>\n",
    "        # answer_mask: mask for answer position\n",
    "        answer_mask = np.zeros_like(span[\"token_type_ids\"])\n",
    "\n",
    "        for evid_start_pos, evid_end_pos in evid_positions:\n",
    "            answer_mask[evid_start_pos:evid_end_pos+1] = 2\n",
    "\n",
    "        answer_mask[start_position:end_position+1] = 1\n",
    "<<<AFTER_EDIT>>>\n",
    "        # answer_mask: mask for answer position\n",
    "        answer_mask = np.zeros_like(span[\"token_type_ids\"])\n",
    "\n",
    "        # for evid_start_pos, evid_end_pos in evid_positions:\n",
    "        #     answer_mask[evid_start_pos:evid_end_pos+1] = 2\n",
    "\n",
    "        answer_mask[start_position:end_position+1] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5040fd",
   "metadata": {},
   "source": [
    "### Step 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eacce34",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:40, End time: 17:48:40 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19eb977",
   "metadata": {},
   "source": [
    "**Thought**: Now that the necessary edits have been made to the code, I can proceed to run the `run_squad.sh` script with the specified hyperparameters and settings. This will train the Block-Skim model on the SQuAD-1.1 dataset and evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f5e53e",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8e6365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bash: run_squad.sh: No such file or directory\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!bash run_squad.sh --num_train_examples 10 --num_eval_examples 10 --num_train_epochs 1 --per_gpu_train_batch_size 16 --per_gpu_eval_batch_size 16 --learning_rate 3e-5 --max_seq_length 512 --skim_factor 0.1 --balance_factor 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8704299",
   "metadata": {},
   "source": [
    "### Step 31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93175c1",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:41, End time: 17:48:42 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208f9d9a",
   "metadata": {},
   "source": [
    "**Thought**: It seems that the `run_squad.sh` script is not found in the current directory. I need to locate the script within the repository and then execute it with the specified parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f270191",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85191399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!find . -name \"run_squad.sh\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84751354",
   "metadata": {},
   "source": [
    "### Step 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87868529",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:44, End time: 17:48:44 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf31ea5",
   "metadata": {},
   "source": [
    "**Thought**: It appears that the `run_squad.sh` script is not present in the repository. I need to check the repository structure to see if there is any other script or instructions for running the training and evaluation process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b054e1ba",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b73149b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ".:\nREADME.md  dataset  fig  requirements.txt  scripts  src\n\n./dataset:\ndev-v1.1.json  train-v1.1.json\n\n./fig:\nresults.png  schematic.png  speedup_hotpot.png\tspeedup_squad.png\n\n./scripts:\neval_squad.sh  finetune_mrqa.sh  finetune_squad.sh\n\n./src:\n__init__.py\t\t     modeling_outputs_skim.py  run_squad_prune.py\ndistillation\t\t     mrqa\t\t       squad\nmodeling_albert_skim.py      run_hotpotqa_evid.py      utils\nmodeling_bert_skim.py\t     run_mrqa.py\t       utils_skim.py\nmodeling_blockskim.py\t     run_squad.py\nmodeling_distilbert_skim.py  run_squad_distill.py\n\n./src/distillation:\nREADME.md\t\t  requirements.txt\t       training_configs\ndistiller.py\t\t  run_squad_w_distillation.py  utils.py\ngrouped_batch_sampler.py  scripts\nlm_seqs_dataset.py\t  train.py\n\n./src/distillation/scripts:\nbinarized_data.py  extract.py  extract_distilbert.py  token_counts.py\n\n./src/distillation/training_configs:\ndistilbert-base-cased.json\t\t distilgpt2.json\ndistilbert-base-multilingual-cased.json  distilroberta-base.json\ndistilbert-base-uncased.json\n\n./src/mrqa:\nmrqa_official_eval.py  run_mrqa_util.py  util_squad.py\tutils_squad_evaluate.py\n\n./src/squad:\nmrqa2squad.py  squad.py  transformer_squad_processor.py  utils_qa.py\n\n./src/utils:\nREADME.md  calculate_prune_dict.py  process_hotpotqa.py\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ls -R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1257518e",
   "metadata": {},
   "source": [
    "### Step 33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaef4e46",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:48, End time: 17:48:48 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8118e5e7",
   "metadata": {},
   "source": [
    "**Thought**: The `run_squad.sh` script is not present, but there is a `scripts` directory containing `eval_squad.sh` and `finetune_squad.sh`. The `finetune_squad.sh` script might be used for training the model on the SQuAD dataset. I should inspect this script to see if it can be used for training with the specified parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03de3ebc",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abb4aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DATA_DIR=datasets/squad\n\nBALANCE_FACTOR=20\nSKIM_FACTOR=0.1\n\npython src/run_squad.py \\\n  --model_type bert \\\n  --skim_factor ${SKIM_FACTOR} \\\n  --balance_factor ${BALANCE_FACTOR} \\\n  --pruning_k ${PRUNING_K} \\\n  --model_name_or_path bert-base-uncased \\\n  --seed 42 \\\n  --do_lower_case \\\n  --do_train \\\n  --do_eval \\\n  --train_file gold_train.json \\\n  --predict_file gold_validation.json \\\n  --data_dir ${DATA_DIR} \\\n  --per_gpu_train_batch_size 16 \\\n  --per_gpu_eval_batch_size 16 \\\n  --learning_rate 3e-5 \\\n  --num_train_epochs 2.0 \\\n  --max_seq_length 512 \\\n  --doc_stride 128 \\\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!cat scripts/finetune_squad.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6111cf3e",
   "metadata": {},
   "source": [
    "### Step 34"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ac6033",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:51, End time: 17:48:51 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d406b05",
   "metadata": {},
   "source": [
    "**Thought**: The `finetune_squad.sh` script is used for training the model on the SQuAD dataset. I can modify this script to include the parameters specified in the query, such as limiting the number of examples to 10 for both training and evaluation, and setting the number of epochs to 1. I will edit the script accordingly and then execute it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b7b2f1",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b80516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n  --do_eval \\\n  --train_file gold_train.json \\\n  --predict_file gold_validation.json \\\n  --data_dir ${DATA_DIR} \\\n  --per_gpu_train_batch_size 16 \\\n  --per_gpu_eval_batch_size 16 \\\n  --learning_rate 3e-5 \\\n  --num_train_epochs 1.0 \\\n  --num_train_examples 10 \\\n  --num_eval_examples 10 \\\n  --max_seq_length 512 \\\n  --doc_stride 128 \\\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scripts/finetune_squad.sh\n",
    "<<<BEFORE_EDIT>>>\n",
    "  --num_train_epochs 2.0 \\\n",
    "<<<AFTER_EDIT>>>\n",
    "  --num_train_epochs 1.0 \\\n",
    "  --num_train_examples 10 \\\n",
    "  --num_eval_examples 10 \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adf15a9",
   "metadata": {},
   "source": [
    "### Step 35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f52b758",
   "metadata": {},
   "source": [
    "Execution Start time: 17:48:53, End time: 17:49:03 (duration: 10 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6235e61",
   "metadata": {},
   "source": [
    "**Thought**: The `finetune_squad.sh` script has been modified to include the specified parameters. Now, I can execute this script to train the Block-Skim model on the SQuAD-1.1 dataset and evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ea6f3e",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df452279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "usage: run_squad.py [-h] --model_type MODEL_TYPE --model_name_or_path\n                    MODEL_NAME_OR_PATH --output_dir OUTPUT_DIR\n                    [--data_dir DATA_DIR] [--train_file TRAIN_FILE]\n                    [--predict_file PREDICT_FILE] [--config_name CONFIG_NAME]\n                    [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]\n                    [--version_2_with_negative]\n                    [--null_score_diff_threshold NULL_SCORE_DIFF_THRESHOLD]\n                    [--max_seq_length MAX_SEQ_LENGTH]\n                    [--doc_stride DOC_STRIDE]\n                    [--max_query_length MAX_QUERY_LENGTH] [--do_train]\n                    [--do_eval] [--evaluate_during_training] [--do_lower_case]\n                    [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n                    [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n                    [--learning_rate LEARNING_RATE]\n                    [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n                    [--weight_decay WEIGHT_DECAY]\n                    [--adam_epsilon ADAM_EPSILON]\n                    [--max_grad_norm MAX_GRAD_NORM]\n                    [--num_train_epochs NUM_TRAIN_EPOCHS]\n                    [--max_steps MAX_STEPS] [--warmup_steps WARMUP_STEPS]\n                    [--n_best_size N_BEST_SIZE]\n                    [--max_answer_length MAX_ANSWER_LENGTH]\n                    [--verbose_logging] [--lang_id LANG_ID]\n                    [--logging_steps LOGGING_STEPS] [--save_steps SAVE_STEPS]\n                    [--eval_all_checkpoints] [--no_cuda]\n                    [--overwrite_output_dir] [--overwrite_cache] [--seed SEED]\n                    [--local_rank LOCAL_RANK] [--fp16]\n                    [--fp16_opt_level FP16_OPT_LEVEL] [--server_ip SERVER_IP]\n                    [--server_port SERVER_PORT] [--threads THREADS]\n                    [--block_skim] [--actual_skim] [--block_size BLOCK_SIZE]\n                    [--skim_factor SKIM_FACTOR]\n                    [--balance_factor BALANCE_FACTOR]\n                    [--cache_name CACHE_NAME]\n                    [--augment_layers AUGMENT_LAYERS [AUGMENT_LAYERS ...]]\n                    [--skim_threshold SKIM_THRESHOLD]\n                    [--num_eval_examples NUM_EVAL_EXAMPLES]\n                    [--num_train_examples NUM_TRAIN_EXAMPLES]\nrun_squad.py: error: the following arguments are required: --output_dir\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!bash scripts/finetune_squad.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b26285",
   "metadata": {},
   "source": [
    "### Step 36"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fff27dc",
   "metadata": {},
   "source": [
    "Execution Start time: 17:49:05, End time: 17:49:06 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5560434d",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the `--output_dir` argument is required but not provided in the `finetune_squad.sh` script. I need to specify an output directory where the model checkpoints and evaluation results will be saved. I will edit the script to include the `--output_dir` argument and then execute it again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642b4120",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595b5295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\n  --predict_file gold_validation.json \\\n  --data_dir ${DATA_DIR} \\\n  --per_gpu_train_batch_size 16 \\\n  --per_gpu_eval_batch_size 16 \\\n  --learning_rate 3e-5 \\\n  --num_train_epochs 1.0 \\\n  --num_train_examples 10 \\\n  --num_eval_examples 10 \\\n  --output_dir ./output \\\n  --max_seq_length 512 \\\n  --doc_stride 128 \\\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scripts/finetune_squad.sh\n",
    "<<<BEFORE_EDIT>>>\n",
    "  --num_eval_examples 10 \\\n",
    "<<<AFTER_EDIT>>>\n",
    "  --num_eval_examples 10 \\\n",
    "  --output_dir ./output \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3f84f4",
   "metadata": {},
   "source": [
    "### Step 37"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9485d79",
   "metadata": {},
   "source": [
    "Execution Start time: 17:49:08, End time: 17:49:11 (duration: 3 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e295b4e5",
   "metadata": {},
   "source": [
    "**Thought**: The `finetune_squad.sh` script has been updated to include the `--output_dir` argument. Now, I can execute the script again to train and evaluate the Block-Skim model on the SQuAD-1.1 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9b4371",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e498c793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "usage: run_squad.py [-h] --model_type MODEL_TYPE --model_name_or_path\n                    MODEL_NAME_OR_PATH --output_dir OUTPUT_DIR\n                    [--data_dir DATA_DIR] [--train_file TRAIN_FILE]\n                    [--predict_file PREDICT_FILE] [--config_name CONFIG_NAME]\n                    [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]\n                    [--version_2_with_negative]\n                    [--null_score_diff_threshold NULL_SCORE_DIFF_THRESHOLD]\n                    [--max_seq_length MAX_SEQ_LENGTH]\n                    [--doc_stride DOC_STRIDE]\n                    [--max_query_length MAX_QUERY_LENGTH] [--do_train]\n                    [--do_eval] [--evaluate_during_training] [--do_lower_case]\n                    [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n                    [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n                    [--learning_rate LEARNING_RATE]\n                    [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n                    [--weight_decay WEIGHT_DECAY]\n                    [--adam_epsilon ADAM_EPSILON]\n                    [--max_grad_norm MAX_GRAD_NORM]\n                    [--num_train_epochs NUM_TRAIN_EPOCHS]\n                    [--max_steps MAX_STEPS] [--warmup_steps WARMUP_STEPS]\n                    [--n_best_size N_BEST_SIZE]\n                    [--max_answer_length MAX_ANSWER_LENGTH]\n                    [--verbose_logging] [--lang_id LANG_ID]\n                    [--logging_steps LOGGING_STEPS] [--save_steps SAVE_STEPS]\n                    [--eval_all_checkpoints] [--no_cuda]\n                    [--overwrite_output_dir] [--overwrite_cache] [--seed SEED]\n                    [--local_rank LOCAL_RANK] [--fp16]\n                    [--fp16_opt_level FP16_OPT_LEVEL] [--server_ip SERVER_IP]\n                    [--server_port SERVER_PORT] [--threads THREADS]\n                    [--block_skim] [--actual_skim] [--block_size BLOCK_SIZE]\n                    [--skim_factor SKIM_FACTOR]\n                    [--balance_factor BALANCE_FACTOR]\n                    [--cache_name CACHE_NAME]\n                    [--augment_layers AUGMENT_LAYERS [AUGMENT_LAYERS ...]]\n                    [--skim_threshold SKIM_THRESHOLD]\n                    [--num_eval_examples NUM_EVAL_EXAMPLES]\n                    [--num_train_examples NUM_TRAIN_EXAMPLES]\nrun_squad.py: error: unrecognized arguments: --pruning_k\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!bash scripts/finetune_squad.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba448e1",
   "metadata": {},
   "source": [
    "### Step 38"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92101da2",
   "metadata": {},
   "source": [
    "Execution Start time: 17:49:13, End time: 17:49:13 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db7260a",
   "metadata": {},
   "source": [
    "**Thought**: \n",
    "Thought: The error indicates that the `--pruning_k` argument is not recognized by the `run_squad.py` script. I need to remove this argument from the `finetune_squad.sh` script and try running it again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e398c0",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ef05a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\nBALANCE_FACTOR=20\nSKIM_FACTOR=0.1\n\npython src/run_squad.py \\\n  --model_type bert \\\n  --skim_factor ${SKIM_FACTOR} \\\n  --balance_factor ${BALANCE_FACTOR} \\\n#  --pruning_k ${PRUNING_K} \\\n  --model_name_or_path bert-base-uncased \\\n  --seed 42 \\\n  --do_lower_case \\\n  --do_train \\\n  --do_eval \\\n  --train_file gold_train.json \\"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scripts/finetune_squad.sh\n",
    "<<<BEFORE_EDIT>>>\n",
    "  --pruning_k ${PRUNING_K} \\\n",
    "<<<AFTER_EDIT>>>\n",
    "#  --pruning_k ${PRUNING_K} \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9775b5ad",
   "metadata": {},
   "source": [
    "### Step 39"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87d227b",
   "metadata": {},
   "source": [
    "Execution Start time: 17:49:16, End time: 17:49:19 (duration: 3 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd6c9b0",
   "metadata": {},
   "source": [
    "**Thought**: The `--pruning_k` argument has been commented out in the `finetune_squad.sh` script. Now, I can execute the script again to train and evaluate the Block-Skim model on the SQuAD-1.1 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5a0a4c",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaef5a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "usage: run_squad.py [-h] --model_type MODEL_TYPE --model_name_or_path\n                    MODEL_NAME_OR_PATH --output_dir OUTPUT_DIR\n                    [--data_dir DATA_DIR] [--train_file TRAIN_FILE]\n                    [--predict_file PREDICT_FILE] [--config_name CONFIG_NAME]\n                    [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]\n                    [--version_2_with_negative]\n                    [--null_score_diff_threshold NULL_SCORE_DIFF_THRESHOLD]\n                    [--max_seq_length MAX_SEQ_LENGTH]\n                    [--doc_stride DOC_STRIDE]\n                    [--max_query_length MAX_QUERY_LENGTH] [--do_train]\n                    [--do_eval] [--evaluate_during_training] [--do_lower_case]\n                    [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n                    [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n                    [--learning_rate LEARNING_RATE]\n                    [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n                    [--weight_decay WEIGHT_DECAY]\n                    [--adam_epsilon ADAM_EPSILON]\n                    [--max_grad_norm MAX_GRAD_NORM]\n                    [--num_train_epochs NUM_TRAIN_EPOCHS]\n                    [--max_steps MAX_STEPS] [--warmup_steps WARMUP_STEPS]\n                    [--n_best_size N_BEST_SIZE]\n                    [--max_answer_length MAX_ANSWER_LENGTH]\n                    [--verbose_logging] [--lang_id LANG_ID]\n                    [--logging_steps LOGGING_STEPS] [--save_steps SAVE_STEPS]\n                    [--eval_all_checkpoints] [--no_cuda]\n                    [--overwrite_output_dir] [--overwrite_cache] [--seed SEED]\n                    [--local_rank LOCAL_RANK] [--fp16]\n                    [--fp16_opt_level FP16_OPT_LEVEL] [--server_ip SERVER_IP]\n                    [--server_port SERVER_PORT] [--threads THREADS]\n                    [--block_skim] [--actual_skim] [--block_size BLOCK_SIZE]\n                    [--skim_factor SKIM_FACTOR]\n                    [--balance_factor BALANCE_FACTOR]\n                    [--cache_name CACHE_NAME]\n                    [--augment_layers AUGMENT_LAYERS [AUGMENT_LAYERS ...]]\n                    [--skim_threshold SKIM_THRESHOLD]\n                    [--num_eval_examples NUM_EVAL_EXAMPLES]\n                    [--num_train_examples NUM_TRAIN_EXAMPLES]\nrun_squad.py: error: the following arguments are required: --model_name_or_path, --output_dir\nscripts/finetune_squad.sh: line 11: --model_name_or_path: command not found\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!bash scripts/finetune_squad.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fc68df",
   "metadata": {},
   "source": [
    "### Step 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ed063a",
   "metadata": {},
   "source": [
    "Execution Start time: 17:49:26, End time: 17:49:27 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13cb6a6",
   "metadata": {},
   "source": [
    "**Thought**: The error indicates that the `--model_name_or_path` and `--output_dir` arguments are required but not recognized. It seems there might be an issue with how the script is structured or executed. I need to ensure that these arguments are correctly passed to the `run_squad.py` script within the `finetune_squad.sh` script. I will review and correct the script structure to ensure proper execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c339bbcb",
   "metadata": {},
   "source": [
    "**Action (edit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cd80e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edit was successful. Here's the relevant content *after edit* for your inspection (you should check indentation is correct):\nDATA_DIR=datasets/squad\n\nBALANCE_FACTOR=20\nSKIM_FACTOR=0.1\n\npython src/run_squad.py \\\n  --model_type bert \\\n  --skim_factor ${SKIM_FACTOR} \\\n  --balance_factor ${BALANCE_FACTOR} \\\n  --model_name_or_path bert-base-uncased \\\n  --seed 42 \\\n  --do_lower_case \\\n  --do_train \\\n  --do_eval \\\n  --train_file train-v1.1.json \\\n  --predict_file dev-v1.1.json \\\n  --data_dir ./dataset \\\n  --per_gpu_train_batch_size 16 \\\n  --per_gpu_eval_batch_size 16 \\\n  --learning_rate 3e-5 \\\n  --num_train_epochs 1.0 \\\n  --num_train_examples 10 \\\n  --num_eval_examples 10 \\\n  --output_dir ./output \\\n  --max_seq_length 512 \\\n  --doc_stride 128 \\\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scripts/finetune_squad.sh\n",
    "<<<BEFORE_EDIT>>>\n",
    "python src/run_squad.py \\\n",
    "  --model_type bert \\\n",
    "  --skim_factor ${SKIM_FACTOR} \\\n",
    "  --balance_factor ${BALANCE_FACTOR} \\\n",
    "#  --pruning_k ${PRUNING_K} \\\n",
    "  --model_name_or_path bert-base-uncased \\\n",
    "  --seed 42 \\\n",
    "  --do_lower_case \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --train_file gold_train.json \\\n",
    "  --predict_file gold_validation.json \\\n",
    "  --data_dir ${DATA_DIR} \\\n",
    "  --per_gpu_train_batch_size 16 \\\n",
    "  --per_gpu_eval_batch_size 16 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 1.0 \\\n",
    "  --num_train_examples 10 \\\n",
    "  --num_eval_examples 10 \\\n",
    "  --output_dir ./output \\\n",
    "  --max_seq_length 512 \\\n",
    "  --doc_stride 128 \\\n",
    "<<<AFTER_EDIT>>>\n",
    "python src/run_squad.py \\\n",
    "  --model_type bert \\\n",
    "  --skim_factor ${SKIM_FACTOR} \\\n",
    "  --balance_factor ${BALANCE_FACTOR} \\\n",
    "  --model_name_or_path bert-base-uncased \\\n",
    "  --seed 42 \\\n",
    "  --do_lower_case \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --train_file train-v1.1.json \\\n",
    "  --predict_file dev-v1.1.json \\\n",
    "  --data_dir ./dataset \\\n",
    "  --per_gpu_train_batch_size 16 \\\n",
    "  --per_gpu_eval_batch_size 16 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 1.0 \\\n",
    "  --num_train_examples 10 \\\n",
    "  --num_eval_examples 10 \\\n",
    "  --output_dir ./output \\\n",
    "  --max_seq_length 512 \\\n",
    "  --doc_stride 128 \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547917db",
   "metadata": {},
   "source": [
    "### Step 41"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727f2191",
   "metadata": {},
   "source": [
    "Execution Start time: 17:49:28, End time: 17:50:39 (duration: 71 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f7e899",
   "metadata": {},
   "source": [
    "**Thought**: The `finetune_squad.sh` script has been corrected to include the correct paths for the training and evaluation files, as well as the data directory. Now, I can execute the script again to train and evaluate the Block-Skim model on the SQuAD-1.1 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa72f9f5",
   "metadata": {},
   "source": [
    "**Action (execute)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5229f325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "08/24/2024 17:49:30 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\nconfig.json:   0%|                                    | 0.00/570 [00:00<?, ?B/s]\n\n[INFO|configuration_utils.py:726] 2024-08-24 17:49:30,931 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n[INFO|configuration_utils.py:789] 2024-08-24 17:49:30,932 >> Model config BertConfig {\n  \"_name_or_path\": \"bert-base-uncased\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.40.2\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\n\ntokenizer_config.json:   0%|                         | 0.00/48.0 [00:00<?, ?B/s]\ntokenizer_config.json: 100%|██████████████████| 48.0/48.0 [00:00<00:00, 582kB/s]\n[INFO|configuration_utils.py:726] 2024-08-24 17:49:30,975 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n[INFO|configuration_utils.py:789] 2024-08-24 17:49:30,975 >> Model config BertConfig {\n  \"_name_or_path\": \"bert-base-uncased\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.40.2\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\n\nvocab.txt:   0%|                                     | 0.00/232k [00:00<?, ?B/s]\n\n\ntokenizer.json:   0%|                                | 0.00/466k [00:00<?, ?B/s]\n\n[INFO|tokenization_utils_base.py:2087] 2024-08-24 17:49:31,158 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n[INFO|tokenization_utils_base.py:2087] 2024-08-24 17:49:31,158 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:2087] 2024-08-24 17:49:31,158 >> loading file special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:2087] 2024-08-24 17:49:31,158 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2087] 2024-08-24 17:49:31,158 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n[INFO|configuration_utils.py:726] 2024-08-24 17:49:31,159 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n[INFO|configuration_utils.py:789] 2024-08-24 17:49:31,159 >> Model config BertConfig {\n  \"_name_or_path\": \"bert-base-uncased\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.40.2\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\n\nmodel.safetensors:   0%|                             | 0.00/440M [00:00<?, ?B/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n[INFO|modeling_utils.py:3429] 2024-08-24 17:49:32,726 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n[WARNING|modeling_utils.py:4172] 2024-08-24 17:49:34,130 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n08/24/2024 17:49:34 - INFO - __main__ - Training/evaluation parameters Namespace(model_type='bert', model_name_or_path='bert-base-uncased', output_dir='./output', data_dir='./dataset', train_file='train-v1.1.json', predict_file='dev-v1.1.json', config_name='', tokenizer_name='', cache_dir='', version_2_with_negative=False, null_score_diff_threshold=0.0, max_seq_length=512, doc_stride=128, max_query_length=64, do_train=True, do_eval=True, evaluate_during_training=False, do_lower_case=True, per_gpu_train_batch_size=16, per_gpu_eval_batch_size=16, learning_rate=3e-05, gradient_accumulation_steps=1, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, n_best_size=20, max_answer_length=30, verbose_logging=False, lang_id=0, logging_steps=500, save_steps=500, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=42, local_rank=-1, fp16=False, fp16_opt_level='O1', server_ip='', server_port='', threads=1, block_skim=False, actual_skim=False, block_size=32, skim_factor=0.1, balance_factor=20.0, cache_name=None, augment_layers=None, skim_threshold=0.001, num_eval_examples=10, num_train_examples=10, n_gpu=0, device=device(type='cpu'))\n08/24/2024 17:49:34 - INFO - __main__ - Creating features from dataset file at ./dataset\nloading train examples from ./dataset grom train-v1.1.json , dev-v1.1.json\n\n  0%|                                                   | 0/442 [00:00<?, ?it/s]\n  0%|▏                                          | 2/442 [00:00<01:08,  6.46it/s]\n  1%|▍                                          | 5/442 [00:00<00:34, 12.66it/s]\n  2%|▋                                          | 7/442 [00:00<00:42, 10.25it/s]\n  2%|▉                                          | 9/442 [00:00<00:36, 11.93it/s]\n  2%|█                                         | 11/442 [00:00<00:36, 11.67it/s]\n  3%|█▏                                        | 13/442 [00:01<00:41, 10.35it/s]\n  3%|█▍                                        | 15/442 [00:01<00:35, 12.18it/s]\n  4%|█▌                                        | 17/442 [00:01<00:34, 12.39it/s]\n  4%|█▊                                        | 19/442 [00:01<00:39, 10.68it/s]\n  5%|█▉                                        | 21/442 [00:01<00:41, 10.08it/s]\n  5%|██▏                                       | 23/442 [00:02<00:43,  9.65it/s]\n  6%|██▍                                       | 25/442 [00:02<00:38, 10.95it/s]\n  7%|██▉                                       | 31/442 [00:02<00:20, 19.86it/s]\n  8%|███▌                                      | 37/442 [00:02<00:14, 27.00it/s]\n 10%|████                                      | 43/442 [00:02<00:12, 31.13it/s]\n 11%|████▍                                     | 47/442 [00:02<00:12, 32.02it/s]\n 12%|████▊                                     | 51/442 [00:02<00:12, 30.76it/s]\n 12%|█████▏                                    | 55/442 [00:03<00:11, 32.63it/s]\n 13%|█████▌                                    | 59/442 [00:03<00:12, 29.89it/s]\n 14%|██████                                    | 64/442 [00:03<00:13, 28.73it/s]\n 15%|██████▍                                   | 68/442 [00:03<00:12, 31.15it/s]\n 16%|██████▊                                   | 72/442 [00:03<00:11, 31.73it/s]\n 17%|███████▏                                  | 76/442 [00:03<00:11, 30.95it/s]\n 18%|███████▋                                  | 81/442 [00:03<00:10, 33.95it/s]\n 19%|████████                                  | 85/442 [00:04<00:12, 29.08it/s]\n 20%|████████▍                                 | 89/442 [00:04<00:14, 23.77it/s]\n 21%|████████▋                                 | 92/442 [00:04<00:15, 22.90it/s]\n 21%|█████████                                 | 95/442 [00:04<00:14, 23.67it/s]\n 22%|█████████▎                                | 98/442 [00:04<00:21, 15.84it/s]\n 23%|█████████▎                               | 101/442 [00:05<00:18, 18.09it/s]\n 24%|█████████▋                               | 104/442 [00:05<00:20, 16.58it/s]\n 24%|█████████▉                               | 107/442 [00:05<00:18, 18.01it/s]\n 25%|██████████▏                              | 110/442 [00:05<00:16, 20.24it/s]\n 26%|██████████▍                              | 113/442 [00:05<00:15, 21.56it/s]\n 26%|██████████▊                              | 116/442 [00:05<00:15, 20.82it/s]\n 27%|███████████▏                             | 120/442 [00:05<00:13, 23.32it/s]\n 28%|███████████▌                             | 125/442 [00:06<00:11, 28.17it/s]\n 29%|███████████▉                             | 129/442 [00:06<00:10, 30.73it/s]\n 30%|████████████▎                            | 133/442 [00:06<00:10, 30.18it/s]\n 31%|████████████▋                            | 137/442 [00:06<00:11, 27.07it/s]\n 32%|█████████████                            | 141/442 [00:06<00:10, 29.71it/s]\n 33%|█████████████▍                           | 145/442 [00:06<00:10, 29.13it/s]\n 34%|█████████████▊                           | 149/442 [00:06<00:09, 30.22it/s]\n 35%|██████████████▏                          | 153/442 [00:07<00:12, 24.04it/s]\n 35%|██████████████▍                          | 156/442 [00:07<00:13, 21.06it/s]\n 36%|██████████████▋                          | 159/442 [00:07<00:16, 17.21it/s]\n 36%|██████████████▉                          | 161/442 [00:07<00:16, 17.45it/s]\n 37%|███████████████▏                         | 164/442 [00:07<00:14, 18.58it/s]\n 38%|███████████████▍                         | 167/442 [00:07<00:13, 20.06it/s]\n 38%|███████████████▊                         | 170/442 [00:08<00:14, 18.27it/s]\n 39%|███████████████▉                         | 172/442 [00:08<00:15, 16.89it/s]\n 39%|████████████████▏                        | 174/442 [00:08<00:15, 16.92it/s]\n 40%|████████████████▎                        | 176/442 [00:08<00:15, 17.22it/s]\n 40%|████████████████▌                        | 178/442 [00:08<00:15, 17.30it/s]\n 41%|████████████████▋                        | 180/442 [00:08<00:17, 15.16it/s]\n 41%|████████████████▉                        | 183/442 [00:09<00:20, 12.91it/s]\n 42%|█████████████████▏                       | 185/442 [00:09<00:19, 13.08it/s]\n 42%|█████████████████▎                       | 187/442 [00:09<00:24, 10.58it/s]\n 43%|█████████████████▌                       | 190/442 [00:09<00:18, 13.50it/s]\n 44%|█████████████████▉                       | 193/442 [00:09<00:16, 15.11it/s]\n 44%|██████████████████                       | 195/442 [00:09<00:16, 15.32it/s]\n 45%|██████████████████▎                      | 198/442 [00:10<00:14, 17.05it/s]\n 45%|██████████████████▌                      | 200/442 [00:10<00:14, 16.85it/s]\n 46%|██████████████████▊                      | 203/442 [00:10<00:14, 16.55it/s]\n 47%|███████████████████▏                     | 207/442 [00:10<00:12, 19.04it/s]\n 47%|███████████████████▍                     | 209/442 [00:10<00:13, 17.82it/s]\n 48%|███████████████████▌                     | 211/442 [00:10<00:15, 14.68it/s]\n 48%|███████████████████▊                     | 214/442 [00:10<00:13, 16.76it/s]\n 49%|████████████████████▏                    | 217/442 [00:11<00:12, 17.80it/s]\n 50%|████████████████████▍                    | 220/442 [00:11<00:11, 19.34it/s]\n 50%|████████████████████▋                    | 223/442 [00:11<00:12, 18.22it/s]\n 51%|████████████████████▊                    | 225/442 [00:11<00:14, 15.25it/s]\n 51%|█████████████████████                    | 227/442 [00:11<00:13, 15.41it/s]\n 52%|█████████████████████▎                   | 230/442 [00:11<00:11, 18.51it/s]\n 53%|█████████████████████▌                   | 233/442 [00:12<00:11, 18.31it/s]\n 53%|█████████████████████▊                   | 235/442 [00:12<00:11, 18.13it/s]\n 54%|██████████████████████                   | 238/442 [00:12<00:10, 20.01it/s]\n 55%|██████████████████████▍                  | 242/442 [00:12<00:08, 23.16it/s]\n 55%|██████████████████████▋                  | 245/442 [00:12<00:17, 11.19it/s]\n 56%|███████████████████████                  | 248/442 [00:13<00:15, 12.50it/s]\n 57%|███████████████████████▍                 | 252/442 [00:13<00:12, 14.92it/s]\n 57%|███████████████████████▌                 | 254/442 [00:13<00:12, 15.44it/s]\n 58%|███████████████████████▊                 | 257/442 [00:13<00:11, 16.69it/s]\n 59%|████████████████████████                 | 260/442 [00:13<00:10, 17.58it/s]\n 60%|████████████████████████▍                | 263/442 [00:13<00:10, 16.85it/s]\n 60%|████████████████████████▋                | 266/442 [00:14<00:09, 19.39it/s]\n 61%|████████████████████████▉                | 269/442 [00:14<00:08, 20.82it/s]\n 62%|█████████████████████████▏               | 272/442 [00:14<00:08, 20.67it/s]\n 62%|█████████████████████████▌               | 275/442 [00:14<00:08, 19.67it/s]\n 63%|█████████████████████████▊               | 278/442 [00:14<00:08, 18.37it/s]\n 64%|██████████████████████████               | 281/442 [00:14<00:08, 19.70it/s]\n 64%|██████████████████████████▎              | 284/442 [00:14<00:08, 18.38it/s]\n 65%|██████████████████████████▋              | 288/442 [00:15<00:07, 20.62it/s]\n 66%|██████████████████████████▉              | 291/442 [00:15<00:07, 21.53it/s]\n 67%|███████████████████████████▎             | 294/442 [00:15<00:06, 21.97it/s]\n 67%|███████████████████████████▌             | 297/442 [00:15<00:06, 22.10it/s]\n 68%|███████████████████████████▉             | 301/442 [00:15<00:05, 25.43it/s]\n 69%|████████████████████████████▏            | 304/442 [00:15<00:05, 23.73it/s]\n 69%|████████████████████████████▍            | 307/442 [00:16<00:07, 18.18it/s]\n 70%|████████████████████████████▊            | 310/442 [00:16<00:06, 20.45it/s]\n 71%|█████████████████████████████            | 313/442 [00:16<00:06, 20.43it/s]\n 72%|█████████████████████████████▍           | 317/442 [00:16<00:05, 22.51it/s]\n 72%|█████████████████████████████▋           | 320/442 [00:16<00:05, 24.06it/s]\n 73%|█████████████████████████████▉           | 323/442 [00:16<00:05, 22.91it/s]\n 74%|██████████████████████████████▏          | 326/442 [00:16<00:06, 19.29it/s]\n 74%|██████████████████████████████▌          | 329/442 [00:17<00:07, 16.14it/s]\n 75%|██████████████████████████████▉          | 333/442 [00:17<00:05, 18.63it/s]\n 76%|███████████████████████████████▎         | 337/442 [00:17<00:04, 22.14it/s]\n 77%|███████████████████████████████▋         | 341/442 [00:17<00:03, 25.46it/s]\n 78%|███████████████████████████████▉         | 344/442 [00:17<00:04, 23.93it/s]\n 79%|████████████████████████████████▏        | 347/442 [00:17<00:03, 24.68it/s]\n 79%|████████████████████████████████▍        | 350/442 [00:17<00:03, 23.41it/s]\n 80%|████████████████████████████████▋        | 353/442 [00:18<00:03, 22.74it/s]\n 81%|█████████████████████████████████        | 356/442 [00:18<00:03, 23.96it/s]\n 81%|█████████████████████████████████▎       | 359/442 [00:18<00:03, 23.08it/s]\n 82%|█████████████████████████████████▌       | 362/442 [00:18<00:04, 17.22it/s]\n 82%|█████████████████████████████████▊       | 364/442 [00:18<00:04, 16.58it/s]\n 83%|█████████████████████████████████▉       | 366/442 [00:18<00:04, 16.92it/s]\n 83%|██████████████████████████████████▏      | 369/442 [00:19<00:04, 18.03it/s]\n 84%|██████████████████████████████████▍      | 371/442 [00:19<00:04, 15.47it/s]\n 85%|██████████████████████████████████▋      | 374/442 [00:19<00:04, 16.93it/s]\n 85%|██████████████████████████████████▉      | 376/442 [00:19<00:03, 16.75it/s]\n 86%|███████████████████████████████████▏     | 380/442 [00:19<00:02, 21.96it/s]\n 87%|███████████████████████████████████▌     | 383/442 [00:19<00:03, 18.36it/s]\n 87%|███████████████████████████████████▊     | 386/442 [00:19<00:02, 18.90it/s]\n 88%|████████████████████████████████████     | 389/442 [00:20<00:02, 20.88it/s]\n 89%|████████████████████████████████████▎    | 392/442 [00:20<00:02, 19.06it/s]\n 89%|████████████████████████████████████▋    | 395/442 [00:20<00:02, 17.31it/s]\n 90%|████████████████████████████████████▊    | 397/442 [00:20<00:03, 14.26it/s]\n 90%|█████████████████████████████████████    | 399/442 [00:20<00:03, 13.74it/s]\n 91%|█████████████████████████████████████▏   | 401/442 [00:20<00:02, 14.81it/s]\n 91%|█████████████████████████████████████▍   | 404/442 [00:21<00:02, 17.45it/s]\n 92%|█████████████████████████████████████▊   | 407/442 [00:21<00:01, 19.82it/s]\n 93%|██████████████████████████████████████   | 410/442 [00:21<00:01, 21.90it/s]\n 93%|██████████████████████████████████████▎  | 413/442 [00:21<00:01, 21.90it/s]\n 94%|██████████████████████████████████████▌  | 416/442 [00:21<00:01, 23.30it/s]\n 95%|██████████████████████████████████████▊  | 419/442 [00:21<00:01, 20.37it/s]\n 95%|███████████████████████████████████████▏ | 422/442 [00:21<00:01, 17.32it/s]\n 96%|███████████████████████████████████████▎ | 424/442 [00:22<00:01, 17.25it/s]\n 96%|███████████████████████████████████████▌ | 426/442 [00:22<00:01, 15.62it/s]\n 97%|███████████████████████████████████████▋ | 428/442 [00:22<00:00, 15.03it/s]\n 98%|████████████████████████████████████████ | 432/442 [00:22<00:00, 19.73it/s]\n 98%|████████████████████████████████████████▎| 435/442 [00:23<00:00,  7.84it/s]\n 99%|████████████████████████████████████████▋| 438/442 [00:23<00:00, 10.00it/s]\n100%|████████████████████████████████████████▉| 441/442 [00:23<00:00, 11.66it/s]\n100%|█████████████████████████████████████████| 442/442 [00:23<00:00, 18.63it/s]\n\nconvert squad examples to features:   0%|                | 0/10 [00:00<?, ?it/s]\nconvert squad examples to features:  10%|▊       | 1/10 [00:00<00:00,  9.07it/s]\nconvert squad examples to features: 100%|███████| 10/10 [00:00<00:00, 90.49it/s]\n\nadd example index and unique id:   0%|                   | 0/10 [00:00<?, ?it/s]\nadd example index and unique id: 100%|██████| 10/10 [00:00<00:00, 303935.07it/s]\n/content/blockskim/src/squad/squad.py:439: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n  all_answer_mask = torch.tensor([f.answer_mask for f in features], dtype=torch.float)\n08/24/2024 17:49:58 - INFO - __main__ - Saving features into cached file ./dataset/cached_train_bert-base-uncased_512\n/usr/local/lib/python3.10/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n08/24/2024 17:49:59 - INFO - __main__ - ***** Running training *****\n08/24/2024 17:49:59 - INFO - __main__ -   Num examples = 10\n08/24/2024 17:49:59 - INFO - __main__ -   Num Epochs = 1\n08/24/2024 17:49:59 - INFO - __main__ -   Instantaneous batch size per GPU = 16\n08/24/2024 17:49:59 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16\n08/24/2024 17:49:59 - INFO - __main__ -   Gradient Accumulation steps = 1\n08/24/2024 17:49:59 - INFO - __main__ -   Total optimization steps = 1\n\nEpoch:   0%|                                              | 0/1 [00:00<?, ?it/s]\n\nIteration:   0%|                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n\nIteration: 100%|██████████████████████████████████| 1/1 [00:27<00:00, 27.47s/it]\u001b[A\nIteration: 100%|██████████████████████████████████| 1/1 [00:27<00:00, 27.47s/it]\n\nEpoch: 100%|██████████████████████████████████████| 1/1 [00:27<00:00, 27.47s/it]\nEpoch: 100%|██████████████████████████████████████| 1/1 [00:27<00:00, 27.47s/it]\n08/24/2024 17:50:27 - INFO - __main__ -  global_step = 2, average loss = 3.0821452140808105\n08/24/2024 17:50:27 - INFO - __main__ - Saving model checkpoint to ./output\n[INFO|configuration_utils.py:471] 2024-08-24 17:50:27,118 >> Configuration saved in ./output/config.json\n[INFO|modeling_utils.py:2590] 2024-08-24 17:50:27,377 >> Model weights saved in ./output/model.safetensors\n[INFO|tokenization_utils_base.py:2488] 2024-08-24 17:50:27,383 >> tokenizer config file saved in ./output/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2497] 2024-08-24 17:50:27,383 >> Special tokens file saved in ./output/special_tokens_map.json\n[INFO|configuration_utils.py:724] 2024-08-24 17:50:27,397 >> loading configuration file ./output/config.json\n[INFO|configuration_utils.py:789] 2024-08-24 17:50:27,398 >> Model config BertConfig {\n  \"_name_or_path\": \"./output\",\n  \"architectures\": [\n    \"BertForQuestionAnswering\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.40.2\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\n[INFO|modeling_utils.py:3426] 2024-08-24 17:50:27,398 >> loading weights file ./output/model.safetensors\n[INFO|modeling_utils.py:4170] 2024-08-24 17:50:27,707 >> All model checkpoint weights were used when initializing BertForQuestionAnswering.\n\n[INFO|modeling_utils.py:4178] 2024-08-24 17:50:27,707 >> All the weights of BertForQuestionAnswering were initialized from the model checkpoint at ./output.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n[INFO|tokenization_utils_base.py:2085] 2024-08-24 17:50:27,744 >> loading file vocab.txt\n[INFO|tokenization_utils_base.py:2085] 2024-08-24 17:50:27,744 >> loading file added_tokens.json\n[INFO|tokenization_utils_base.py:2085] 2024-08-24 17:50:27,744 >> loading file special_tokens_map.json\n[INFO|tokenization_utils_base.py:2085] 2024-08-24 17:50:27,744 >> loading file tokenizer_config.json\n[INFO|tokenization_utils_base.py:2085] 2024-08-24 17:50:27,744 >> loading file tokenizer.json\n08/24/2024 17:50:27 - INFO - __main__ - Loading checkpoints saved during training for evaluation\n08/24/2024 17:50:27 - INFO - __main__ - Evaluate the following checkpoints: ['./output']\n[INFO|configuration_utils.py:724] 2024-08-24 17:50:27,776 >> loading configuration file ./output/config.json\n[INFO|configuration_utils.py:789] 2024-08-24 17:50:27,776 >> Model config BertConfig {\n  \"_name_or_path\": \"./output\",\n  \"architectures\": [\n    \"BertForQuestionAnswering\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.40.2\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\n[INFO|modeling_utils.py:3426] 2024-08-24 17:50:27,777 >> loading weights file ./output/model.safetensors\n[INFO|modeling_utils.py:4170] 2024-08-24 17:50:28,087 >> All model checkpoint weights were used when initializing BertForQuestionAnswering.\n\n[INFO|modeling_utils.py:4178] 2024-08-24 17:50:28,087 >> All the weights of BertForQuestionAnswering were initialized from the model checkpoint at ./output.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n08/24/2024 17:50:28 - INFO - __main__ - Creating features from dataset file at ./dataset\n\n  0%|                                                    | 0/48 [00:00<?, ?it/s]\n  2%|▉                                           | 1/48 [00:00<00:07,  5.90it/s]\n  8%|███▋                                        | 4/48 [00:00<00:03, 12.35it/s]\n 15%|██████▍                                     | 7/48 [00:00<00:03, 13.57it/s]\n 23%|█████████▊                                 | 11/48 [00:00<00:01, 18.68it/s]\n 27%|███████████▋                               | 13/48 [00:00<00:01, 18.66it/s]\n 33%|██████████████▎                            | 16/48 [00:01<00:01, 16.21it/s]\n 40%|█████████████████                          | 19/48 [00:01<00:01, 17.94it/s]\n 48%|████████████████████▌                      | 23/48 [00:01<00:01, 20.55it/s]\n 54%|███████████████████████▎                   | 26/48 [00:01<00:01, 16.52it/s]\n 58%|█████████████████████████                  | 28/48 [00:01<00:01, 16.87it/s]\n 65%|███████████████████████████▊               | 31/48 [00:01<00:00, 19.40it/s]\n 71%|██████████████████████████████▍            | 34/48 [00:01<00:00, 20.05it/s]\n 77%|█████████████████████████████████▏         | 37/48 [00:02<00:00, 18.02it/s]\n 83%|███████████████████████████████████▊       | 40/48 [00:02<00:00, 18.18it/s]\n 88%|█████████████████████████████████████▋     | 42/48 [00:02<00:00, 18.54it/s]\n 94%|████████████████████████████████████████▎  | 45/48 [00:02<00:00, 19.12it/s]\n100%|███████████████████████████████████████████| 48/48 [00:02<00:00, 19.43it/s]\n100%|███████████████████████████████████████████| 48/48 [00:02<00:00, 17.85it/s]\n\nconvert squad examples to features:   0%|                | 0/10 [00:00<?, ?it/s]\nconvert squad examples to features: 100%|██████| 10/10 [00:00<00:00, 116.84it/s]\n\nadd example index and unique id:   0%|                   | 0/10 [00:00<?, ?it/s]\nadd example index and unique id: 100%|██████| 10/10 [00:00<00:00, 231729.50it/s]\n08/24/2024 17:50:31 - INFO - __main__ - Saving features into cached file ./dataset/cached_dev_bert-base-uncased_512\n08/24/2024 17:50:31 - INFO - __main__ - ***** Running evaluation  *****\n08/24/2024 17:50:31 - INFO - __main__ -   Num examples = 10\n08/24/2024 17:50:31 - INFO - __main__ -   Batch size = 16\n\nEvaluating:   0%|                                         | 0/1 [00:00<?, ?it/s]\nEvaluating: 100%|█████████████████████████████████| 1/1 [00:07<00:00,  7.60s/it]\nEvaluating: 100%|█████████████████████████████████| 1/1 [00:07<00:00,  7.60s/it]\n08/24/2024 17:50:38 - INFO - __main__ -   Evaluation done in total 7.600910 secs (0.760091 sec per example)\n[INFO|squad_metrics.py:401] 2024-08-24 17:50:38,648 >> Writing predictions to: ./output/predictions_.json\n[INFO|squad_metrics.py:403] 2024-08-24 17:50:38,648 >> Writing nbest to: ./output/nbest_predictions_.json\n08/24/2024 17:50:38 - INFO - __main__ - Results: {'exact': 0.0, 'f1': 1.7391304347826089, 'total': 10, 'HasAns_exact': 0.0, 'HasAns_f1': 1.7391304347826089, 'HasAns_total': 10, 'best_exact': 0.0, 'best_exact_thresh': 0.0, 'best_f1': 1.7391304347826089, 'best_f1_thresh': 0.0}\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!bash scripts/finetune_squad.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b59c4b",
   "metadata": {},
   "source": [
    "### Step 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617ac46d",
   "metadata": {},
   "source": [
    "Execution Start time: 17:50:41, End time: 17:50:41 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086934cd",
   "metadata": {},
   "source": [
    "**Thought**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9c643a",
   "metadata": {},
   "source": [
    "**Action ()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f16e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thought or action not provided in the correct format. Please retry while making sure the response is correctly formatted and does not exceed 1500 tokens."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65b72586",
   "metadata": {},
   "source": [
    "### Step 43"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482b8669",
   "metadata": {},
   "source": [
    "Execution Start time: 17:50:42, End time: 17:50:42 (duration: 0 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5c4927",
   "metadata": {},
   "source": [
    "**Thought**: The training and evaluation process has completed successfully, and the results have been logged. The exact match score is 0.0, and the F1 score is 1.7391. I will now submit these results in the required JSON format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c0bc15",
   "metadata": {},
   "source": [
    "**Action (submit)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771d1e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'exact_match': 0.0, 'f1': 1.7391}"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
